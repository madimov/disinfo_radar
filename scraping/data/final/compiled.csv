title,url,date,text,cleaning,tokens,summary
Google DeepMind’s new AI tool helped create more than 700 new materials,https://www.technologyreview.com/2023/11/29/1084061/deepmind-ai-tool-for-new-materials-discovery/,2023-11-29,"From EV batteries to solar cells to microchips, new materials can supercharge technological breakthroughs. But discovering them usually takes months or even years of trial-and-error research.  Google DeepMind hopes to change that with a new tool that uses deep learning to dramatically speed up the process of discovering new materials. Called graphical networks for material exploration (GNoME), the technology has already been used to predict structures for 2.2 million new materials, of which more than 700 have gone on to be created in the lab and are now being tested. It is described in a paper published in Nature today.  Alongside GNoME, Lawrence Berkeley National Laboratory also announced a new autonomous lab. The lab takes data from the materials database that includes some of GNoME’s discoveries and uses machine learning and robotic arms to engineer new materials without the help of humans. Google DeepMind says that together, these advancements show the potential of using AI to scale up the discovery and development of new materials. The company has already used its protein-folding AI, AlphaFold, to generate structures for the human proteome, as well as yeast, fruit flies, mice, and more. GNoME can be described as AlphaFold for materials discovery, according to Ju Li, a materials science and engineering professor at the Massachusetts Institute of Technology. AlphaFold, a DeepMind AI system announced in 2020, predicts the structures of proteins with high accuracy and has since advanced biological research and drug discovery. Thanks to GNoME, the number of known stable materials has grown almost tenfold, to 421,000. “While materials play a very critical role in almost any technology, we as humanity know only a few tens of thousands of stable materials,” said Dogus Cubuk, materials discovery lead at Google DeepMind, at a press briefing.  To discover new materials, scientists combine elements across the periodic table. But because there are so many combinations, it’s inefficient to do this process blindly. Instead, researchers build upon existing structures, making small tweaks in the hope of discovering new combinations that hold potential. However, this painstaking process is still very time consuming. Also, because it builds on existing structures, it limits the potential for unexpected discoveries.  To overcome these limitations, DeepMind combines two different deep-learning models. The first generates more than a billion structures by making modifications to elements in existing materials. The second, however, ignores existing structures and predicts the stability of new materials purely on the basis of chemical formulas. The combination of these two models allows for a much broader range of possibilities.  Once the candidate structures are generated, they are filtered through DeepMind’s GNoME models. The models predict the decomposition energy of a given structure, which is an important indicator of how stable the material can be. “Stable” materials do not easily decompose, which is important for engineering purposes. GNoME selects the most promising candidates, which go through further evaluation based on known theoretical frameworks. This process is then repeated multiple times, with each discovery incorporated into the next round of training. In its first round, GNoME predicted different materials' stability with a precision of around 5%, but it increased quickly throughout the iterative learning process. The final results showed GNoME managed to predict the stability of structures over 80% of the time for the first model and 33% for the second.  Using AI models to come up with new materials is not a novel idea. The Materials Project, a program led by Kristin Persson at Berkeley Lab, has used similar techniques to discover and improve the stability of 48,000 materials.  However, GNoME’s size and precision set it apart from previous efforts. It was trained on at least an order of magnitude more data than any previous model, says Chris Bartel, an assistant professor of chemical engineering and materials science at the University of Minnesota.  Doing similar calculations has previously been expensive and limited in scale, says Yifei Mo, an associate professor of materials science and engineering at the University of Maryland. GNoME allows these computations to scale up with higher accuracy and at much less computational cost, Mo says: “The impact can be huge.” Once new materials have been identified, it is equally important to synthesize them and prove their usefulness. Berkeley Lab’s new autonomous laboratory, named the A-Lab, has been using some of GNoME’s discoveries with the Materials Project information, integrating robotics with machine learning to optimize the development of such materials. The lab is capable of making its own decisions about how to make a proposed material and creates up to five initial formulations. These formulations are generated by a machine-learning model trained on existing scientific literature. After each experiment, the lab uses the results to adjust the recipes. Researchers at Berkeley Lab say that A-Lab was able to perform 355 experiments over 17 days and successfully synthesized 41 out of 58 proposed compounds. This works out to two successful syntheses a day. In a typical, human-led lab, it takes much longer to make materials. “If you’re unlucky, it can take months or even years,” said Persson at a press briefing. Most students give up after a few weeks, she said. “But the A-Lab doesn’t mind failing. It keeps trying and trying.” Researchers at DeepMind and Berkeley Lab say these new AI tools can help accelerate hardware innovation in energy, computing, and many other sectors. “Hardware, especially when it comes to clean energy, needs innovation if we are going to solve the climate crisis,” says Persson. “This is one aspect of accelerating that innovation.” Bartel, who was not involved in the research, says that these materials will be promising candidates for technologies spanning batteries, computer chips, ceramics, and electronics.  Lithium-ion battery conductors are one of the most promising use cases. Conductors play an important role in batteries by facilitating the flow of electric current between various components. DeepMind says GNoME identified 528 promising lithium-ion conductors among other discoveries, some of which may help make batteries more efficient.  However, even after new materials are discovered, it usually takes decades for industries to take them to the commercial stage. “If we can reduce this to five years, that will be a big improvement,” says Cubuk. Correction: This story has been updated to make clear where the lab's data comes from. ","From EV batteries to solar cells to microchips , new materials can supercharge technological breakthroughs . But discovering them usually takes months or even years of trial-and-error research . Google DeepMind hopes to change that with a new tool that uses deep learning to dramatically speed up the process of discovering new materials . Called graphical networks for material exploration ( GNoME ) , the technology has already been used to predict structures for 2.2 million new materials , of which more than 700 have gone on to be created in the lab and are now being tested . It is described in a paper published in Nature today . Alongside GNoME , Lawrence Berkeley National Laboratory also announced a new autonomous lab . The lab takes data from the materials database that includes some of GNoME ’ s discoveries and uses machine learning and robotic arms to engineer new materials without the help of humans . Google DeepMind says that together , these advancements show the potential of using AI to scale up the discovery and development of new materials . The company has already used its protein-folding AI , AlphaFold , to generate structures for the human proteome , as well as yeast , fruit flies , mice , and more . GNoME can be described as AlphaFold for materials discovery , according to Ju Li , a materials science and engineering professor at the Massachusetts Institute of Technology . AlphaFold , a DeepMind AI system announced in 2020 , predicts the structures of proteins with high accuracy and has since advanced biological research and drug discovery . Thanks to GNoME , the number of known stable materials has grown almost tenfold , to 421,000 . “ While materials play a very critical role in almost any technology , we as humanity know only a few tens of thousands of stable materials , ” said Dogus Cubuk , materials discovery lead at Google DeepMind , at a press briefing . To discover new materials , scientists combine elements across the periodic table . But because there are so many combinations , it ’ s inefficient to do this process blindly . Instead , researchers build upon existing structures , making small tweaks in the hope of discovering new combinations that hold potential . However , this painstaking process is still very time consuming . Also , because it builds on existing structures , it limits the potential for unexpected discoveries . To overcome these limitations , DeepMind combines two different deep-learning models . The first generates more than a billion structures by making modifications to elements in existing materials . The second , however , ignores existing structures and predicts the stability of new materials purely on the basis of chemical formulas . The combination of these two models allows for a much broader range of possibilities . Once the candidate structures are generated , they are filtered through DeepMind ’ s GNoME models . The models predict the decomposition energy of a given structure , which is an important indicator of how stable the material can be . “ Stable ” materials do not easily decompose , which is important for engineering purposes . GNoME selects the most promising candidates , which go through further evaluation based on known theoretical frameworks . This process is then repeated multiple times , with each discovery incorporated into the next round of training . In its first round , GNoME predicted different materials ' stability with a precision of around 5 % , but it increased quickly throughout the iterative learning process . The final results showed GNoME managed to predict the stability of structures over 80 % of the time for the first model and 33 % for the second . Using AI models to come up with new materials is not a novel idea . The Materials Project , a program led by Kristin Persson at Berkeley Lab , has used similar techniques to discover and improve the stability of 48,000 materials . However , GNoME ’ s size and precision set it apart from previous efforts . It was trained on at least an order of magnitude more data than any previous model , says Chris Bartel , an assistant professor of chemical engineering and materials science at the University of Minnesota . Doing similar calculations has previously been expensive and limited in scale , says Yifei Mo , an associate professor of materials science and engineering at the University of Maryland . GNoME allows these computations to scale up with higher accuracy and at much less computational cost , Mo says : “ The impact can be huge. ” Once new materials have been identified , it is equally important to synthesize them and prove their usefulness . Berkeley Lab ’ s new autonomous laboratory , named the A-Lab , has been using some of GNoME ’ s discoveries with the Materials Project information , integrating robotics with machine learning to optimize the development of such materials . The lab is capable of making its own decisions about how to make a proposed material and creates up to five initial formulations . These formulations are generated by a machine-learning model trained on existing scientific literature . After each experiment , the lab uses the results to adjust the recipes . Researchers at Berkeley Lab say that A-Lab was able to perform 355 experiments over 17 days and successfully synthesized 41 out of 58 proposed compounds . This works out to two successful syntheses a day . In a typical , human-led lab , it takes much longer to make materials . “ If you ’ re unlucky , it can take months or even years , ” said Persson at a press briefing . Most students give up after a few weeks , she said . “ But the A-Lab doesn ’ t mind failing . It keeps trying and trying. ” Researchers at DeepMind and Berkeley Lab say these new AI tools can help accelerate hardware innovation in energy , computing , and many other sectors . “ Hardware , especially when it comes to clean energy , needs innovation if we are going to solve the climate crisis , ” says Persson . “ This is one aspect of accelerating that innovation. ” Bartel , who was not involved in the research , says that these materials will be promising candidates for technologies spanning batteries , computer chips , ceramics , and electronics . Lithium-ion battery conductors are one of the most promising use cases . Conductors play an important role in batteries by facilitating the flow of electric current between various components . DeepMind says GNoME identified 528 promising lithium-ion conductors among other discoveries , some of which may help make batteries more efficient . However , even after new materials are discovered , it usually takes decades for industries to take them to the commercial stage . “ If we can reduce this to five years , that will be a big improvement , ” says Cubuk . Correction : This story has been updated to make clear where the lab 's data comes from .","['ev', 'battery', 'solar', 'cell', 'microchip', 'new', 'material', 'supercharge', 'technological', 'breakthrough', 'discover', 'usually', 'take', 'month', 'even', 'year', 'research', 'hope', 'change', 'new', 'tool', 'use', 'deep', 'learning', 'dramatically', 'speed', 'process', 'discover', 'new', 'material', 'call', 'graphical', 'network', 'material', 'exploration', 'gnome', 'technology', 'already', 'use', 'predict', 'structure', 'new', 'material', 'go', 'create', 'lab', 'test', 'describe', 'paper', 'publish', 'nature', 'today', 'gnome', 'also', 'announce', 'new', 'autonomous', 'lab', 'lab', 'take', 'datum', 'material', 'database', 'include', 'gnome', 'discovery', 'use', 'machine', 'learning', 'robotic', 'arm', 'engineer', 'new', 'material', 'help', 'human', 'say', 'together', 'advancement', 'show', 'potential', 'use', 'ai', 'scale', 'discovery', 'development', 'new', 'material', 'company', 'already', 'use', 'proteinfolde', 'ai', 'alphafold', 'generate', 'structure', 'human', 'proteome', 'well', 'yeast', 'fruit', 'fly', 'mouse', 'gnome', 'describe', 'alphafold', 'material', 'discovery', 'accord', 'material', 'science', 'engineering', 'professor', 'alphafold', 'deepmind', 'ai', 'system', 'announce', 'predict', 'structure', 'protein', 'high', 'accuracy', 'advanced', 'biological', 'research', 'drug', 'thank', 'gnome', 'number', 'know', 'stable', 'material', 'grow', 'almost', 'tenfold', 'material', 'play', 'critical', 'role', 'almost', 'technology', 'humanity', 'know', 'ten', 'thousand', 'stable', 'material', 'say', 'material', 'lead', 'press', 'briefing', 'discover', 'new', 'material', 'scientist', 'combine', 'element', 'periodic', 'table', 'many', 'combination', 'inefficient', 'process', 'blindly', 'instead', 'researcher', 'build', 'exist', 'structure', 'make', 'small', 'tweak', 'hope', 'discover', 'new', 'combination', 'hold', 'potential', 'however', 'painstaking', 'process', 'still', 'time', 'consume', 'also', 'build', 'exist', 'structure', 'limit', 'potential', 'unexpected', 'discovery', 'overcome', 'limitation', 'deepmind', 'combine', 'different', 'deeplearning', 'model', 'first', 'generate', 'structure', 'make', 'modification', 'element', 'exist', 'material', 'second', 'however', 'ignore', 'exist', 'structure', 'predict', 'stability', 'new', 'material', 'purely', 'basis', 'chemical', 'formula', 'combination', 'model', 'allow', 'much', 'broad', 'range', 'possibility', 'candidate', 'structure', 'generate', 'filter', 'deepmind', 'gnome', 'model', 'model', 'predict', 'decomposition', 'energy', 'give', 'structure', 'important', 'indicator', 'stable', 'material', 'stable', 'material', 'easily', 'decompose', 'important', 'engineering', 'purpose', 'gnome', 'select', 'promising', 'candidate', 'go', 'evaluation', 'base', 'know', 'theoretical', 'framework', 'process', 'repeat', 'multiple', 'time', 'discovery', 'incorporate', 'next', 'round', 'training', 'first', 'round', 'gnome', 'predict', 'different', 'material', 'stability', 'precision', 'around', 'increase', 'quickly', 'iterative', 'learning', 'process', 'final', 'result', 'show', 'gnome', 'manage', 'predict', 'stability', 'structure', 'time', 'first', 'model', 'second', 'use', 'ai', 'model', 'come', 'new', 'material', 'novel', 'idea', 'material', 'project', 'program', 'lead', 'persson', 'use', 'similar', 'technique', 'discover', 'improve', 'stability', 'material', 'however', 'gnome', 'size', 'precision', 'set', 'apart', 'previous', 'effort', 'train', 'least', 'order', 'magnitude', 'datum', 'previous', 'model', 'say', 'assistant', 'professor', 'engineering', 'material', 'science', 'similar', 'calculation', 'previously', 'expensive', 'limit', 'scale', 'say', 'associate', 'professor', 'material', 'science', 'engineering', 'gnome', 'allow', 'computation', 'scale', 'high', 'accuracy', 'much', 'less', 'computational', 'cost', 'mo', 'say', 'impact', 'huge', 'new', 'material', 'identify', 'equally', 'important', 'synthesize', 'prove', 'usefulness', 'new', 'autonomous', 'laboratory', 'name', 'alab', 'use', 'gnome', 'discovery', 'material', 'project', 'information', 'integrate', 'robotic', 'machine', 'learn', 'optimize', 'development', 'material', 'lab', 'capable', 'make', 'decision', 'make', 'propose', 'material', 'create', 'initial', 'formulation', 'formulation', 'generate', 'machinelearning', 'model', 'train', 'exist', 'scientific', 'literature', 'experiment', 'lab', 'use', 'result', 'adjust', 'recipe', 'researcher', 'say', 'able', 'perform', 'experiment', 'day', 'successfully', 'synthesize', 'propose', 'compound', 'work', 'successful', 'synthese', 'day', 'typical', 'humanled', 'lab', 'take', 'much', 'long', 'make', 'material', 'unlucky', 'take', 'month', 'even', 'year', 'say', 'persson', 'press', 'briefing', 'student', 'give', 'week', 'say', 'alab', 'mind', 'fail', 'keep', 'try', 'try', 'researcher', 'deepmind', 'say', 'new', 'tool', 'help', 'accelerate', 'hardware', 'innovation', 'energy', 'computing', 'many', 'sector', 'hardware', 'especially', 'come', 'clean', 'energy', 'need', 'innovation', 'go', 'solve', 'climate', 'crisis', 'say', 'persson', 'aspect', 'accelerate', 'innovation', 'bartel', 'involve', 'research', 'say', 'material', 'promise', 'candidate', 'technology', 'span', 'battery', 'computer', 'chip', 'ceramic', 'electronic', 'lithiumion', 'battery', 'conductor', 'promising', 'use', 'case', 'conductor', 'play', 'important', 'role', 'battery', 'facilitate', 'flow', 'electric', 'current', 'various', 'component', 'deepmind', 'say', 'gnome', 'identify', 'promising', 'lithiumion', 'conductor', 'discovery', 'help', 'make', 'battery', 'efficient', 'however', 'even', 'new', 'material', 'discover', 'usually', 'take', 'decade', 'industry', 'take', 'commercial', 'stage', 'reduce', 'year', 'big', 'improvement', 'say', 'cubuk', 'correction', 'story', 'update', 'make', 'clear', 'lab', 'datum', 'come']","<p>Newly discovered materials can be used to make better solar cells, batteries, computer chips, and more.</p>
"
Unpacking the hype around OpenAI’s rumored new Q* model,https://www.technologyreview.com/2023/11/27/1083886/unpacking-the-hype-around-openais-rumored-new-q-model/,2023-11-27,"This story is from The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. Ever since last week’s dramatic events at OpenAI, the rumor mill has been in overdrive about why the company’s chief scientific officer, Ilya Sutskever, and its board decided to oust CEO Sam Altman. While we still don’t know all the details, there have been reports that researchers at OpenAI had made a “breakthrough” in AI that had alarmed staff members. Reuters and The Information both report that researchers had come up with a new way to make powerful AI systems and had created a new model, called Q* (pronounced Q star), that was able to perform grade-school-level math. According to the people who spoke to Reuters, some at OpenAI believe this could be a milestone in the company’s quest to build artificial general intelligence, a much-hyped concept referring to an AI system that is smarter than humans. The company declined to comment on Q*.  Social media is full of speculation and excessive hype, so I called some experts to find out how big a deal any breakthrough in math and AI would really be. Researchers have for years tried to get AI models to solve math problems. Language models like ChatGPT and GPT-4 can do some math, but not very well or reliably. We currently don’t have the algorithms or even the right architectures to be able to solve math problems reliably using AI, says Wenda Li, an AI lecturer at the University of Edinburgh. Deep learning and transformers (a kind of neural network), which is what language models use, are excellent at recognizing patterns, but that alone is likely not enough, Li adds.  Math is a benchmark for reasoning, Li says. A machine that is able to reason about mathematics, could, in theory, be able to learn to do other tasks that build on existing information, such as writing computer code or drawing conclusions from a news article. Math is a particularly hard challenge because it requires AI models to have the capacity to reason and to really understand what they are dealing with.  A generative AI system that could reliably do math would need to have a really firm grasp on concrete definitions of particular concepts that can get very abstract. A lot of math problems also require some level of planning over multiple steps, says Katie Collins, a PhD researcher at the University of Cambridge, who specializes in math and AI. Indeed, Yann LeCun, chief AI scientist at Meta, posted on X and LinkedIn over the weekend that he thinks Q* is likely to be “OpenAI attempts at planning.” People who worry about whether AI poses an existential risk to humans, one of OpenAI's founding concerns, fear that such capabilities might lead to rogue AI. Safety concerns might arise if such AI systems are allowed to set their own goals and start to interface with a real physical or digital world in some ways, says Collins.  But while math capability might take us a step closer to more powerful AI systems, solving these sorts of math problems doesn’t signal the birth of a superintelligence.  “I don’t think it immediately gets us to AGI or scary situations,” says Collins.  It’s also very important to underline what kind of math problems AI is solving, she adds. “Solving elementary-school math problems is very, very different from pushing the boundaries of mathematics at the level of something a Fields medalist can do,” says Collins, referring to a top prize in mathematics.   Machine-learning research has focused on solving elementary-school problems, but state-of-the-art AI systems haven’t fully cracked this challenge yet. Some AI models fail on really simple math problems, but then they can excel at really hard problems, Collins says. OpenAI has, for example, developed dedicated tools that can solve challenging problems posed in competitions for top math students in high school, but these systems outperform humans only occasionally.   Nevertheless, building an AI system that can solve math equations is a cool development, if that is indeed what Q* can do. A deeper understanding of mathematics could open up applications to help scientific research and engineering, for example. The ability to generate mathematical responses could help us develop better personalized tutoring, or help mathematicians do algebra faster or solve more complicated problems.  This is also not the first time a new model has sparked AGI hype. Just last year, tech folks were saying the same things about Google DeepMind’s Gato, a “generalist” AI model that can play Atari video games, caption images, chat, and stack blocks with a real robot arm. Back then, some AI researchers claimed that DeepMind was “on the verge” of AGI because of Gato’s ability to do so many different things pretty well. Same hype machine, different AI lab.  And while it might be great PR, these hype cycles do more harm than good for the entire field by distracting people from the real, tangible problems around AI. Rumors about a powerful new AI model might also be a massive own goal for the regulation-averse tech sector. The EU, for example, is very close to finalizing its sweeping AI Act. One of the biggest fights right now among lawmakers is whether to give tech companies more power to regulate cutting-edge AI models on their own.  OpenAI’s board was designed as the company’s internal kill switch and governance mechanism to prevent the launch of harmful technologies. The past week’s boardroom drama has shown that the bottom line will always prevail at these companies. It will also make it harder to make a case for why they should be trusted with self-regulation. Lawmakers, take note. ","This story is from The Algorithm , our weekly newsletter on AI . To get stories like this in your inbox first , sign up here . Ever since last week ’ s dramatic events at OpenAI , the rumor mill has been in overdrive about why the company ’ s chief scientific officer , Ilya Sutskever , and its board decided to oust CEO Sam Altman . While we still don ’ t know all the details , there have been reports that researchers at OpenAI had made a “ breakthrough ” in AI that had alarmed staff members . Reuters and The Information both report that researchers had come up with a new way to make powerful AI systems and had created a new model , called Q * ( pronounced Q star ) , that was able to perform grade-school-level math . According to the people who spoke to Reuters , some at OpenAI believe this could be a milestone in the company ’ s quest to build artificial general intelligence , a much-hyped concept referring to an AI system that is smarter than humans . The company declined to comment on Q * . Social media is full of speculation and excessive hype , so I called some experts to find out how big a deal any breakthrough in math and AI would really be . Researchers have for years tried to get AI models to solve math problems . Language models like ChatGPT and GPT-4 can do some math , but not very well or reliably . We currently don ’ t have the algorithms or even the right architectures to be able to solve math problems reliably using AI , says Wenda Li , an AI lecturer at the University of Edinburgh . Deep learning and transformers ( a kind of neural network ) , which is what language models use , are excellent at recognizing patterns , but that alone is likely not enough , Li adds . Math is a benchmark for reasoning , Li says . A machine that is able to reason about mathematics , could , in theory , be able to learn to do other tasks that build on existing information , such as writing computer code or drawing conclusions from a news article . Math is a particularly hard challenge because it requires AI models to have the capacity to reason and to really understand what they are dealing with . A generative AI system that could reliably do math would need to have a really firm grasp on concrete definitions of particular concepts that can get very abstract . A lot of math problems also require some level of planning over multiple steps , says Katie Collins , a PhD researcher at the University of Cambridge , who specializes in math and AI . Indeed , Yann LeCun , chief AI scientist at Meta , posted on X and LinkedIn over the weekend that he thinks Q * is likely to be “ OpenAI attempts at planning. ” People who worry about whether AI poses an existential risk to humans , one of OpenAI 's founding concerns , fear that such capabilities might lead to rogue AI . Safety concerns might arise if such AI systems are allowed to set their own goals and start to interface with a real physical or digital world in some ways , says Collins . But while math capability might take us a step closer to more powerful AI systems , solving these sorts of math problems doesn ’ t signal the birth of a superintelligence . “ I don ’ t think it immediately gets us to AGI or scary situations , ” says Collins . It ’ s also very important to underline what kind of math problems AI is solving , she adds . “ Solving elementary-school math problems is very , very different from pushing the boundaries of mathematics at the level of something a Fields medalist can do , ” says Collins , referring to a top prize in mathematics . Machine-learning research has focused on solving elementary-school problems , but state-of-the-art AI systems haven ’ t fully cracked this challenge yet . Some AI models fail on really simple math problems , but then they can excel at really hard problems , Collins says . OpenAI has , for example , developed dedicated tools that can solve challenging problems posed in competitions for top math students in high school , but these systems outperform humans only occasionally . Nevertheless , building an AI system that can solve math equations is a cool development , if that is indeed what Q * can do . A deeper understanding of mathematics could open up applications to help scientific research and engineering , for example . The ability to generate mathematical responses could help us develop better personalized tutoring , or help mathematicians do algebra faster or solve more complicated problems . This is also not the first time a new model has sparked AGI hype . Just last year , tech folks were saying the same things about Google DeepMind ’ s Gato , a “ generalist ” AI model that can play Atari video games , caption images , chat , and stack blocks with a real robot arm . Back then , some AI researchers claimed that DeepMind was “ on the verge ” of AGI because of Gato ’ s ability to do so many different things pretty well . Same hype machine , different AI lab . And while it might be great PR , these hype cycles do more harm than good for the entire field by distracting people from the real , tangible problems around AI . Rumors about a powerful new AI model might also be a massive own goal for the regulation-averse tech sector . The EU , for example , is very close to finalizing its sweeping AI Act . One of the biggest fights right now among lawmakers is whether to give tech companies more power to regulate cutting-edge AI models on their own . OpenAI ’ s board was designed as the company ’ s internal kill switch and governance mechanism to prevent the launch of harmful technologies . The past week ’ s boardroom drama has shown that the bottom line will always prevail at these companies . It will also make it harder to make a case for why they should be trusted with self-regulation . Lawmakers , take note .","['story', 'weekly', 'newsletter', 'ai', 'get', 'story', 'inbox', 'first', 'sign', 'ever', 'last', 'week', 'dramatic', 'event', 'rumor', 'mill', 'overdrive', 'company', 'chief', 'scientific', 'officer', 'ilya', 'sutskever', 'board', 'decide', 'oust', 'ceo', 'still', 'know', 'detail', 'report', 'researcher', 'make', 'breakthrough', 'ai', 'alarm', 'staff', 'member', 'reuter', 'information', 'report', 'researcher', 'come', 'new', 'way', 'make', 'powerful', 'ai', 'system', 'create', 'new', 'model', 'call', 'q', 'pronounce', 'star', 'able', 'perform', 'gradeschoollevel', 'math', 'accord', 'people', 'speak', 'reuter', 'believe', 'milestone', 'company', 'quest', 'build', 'artificial', 'general', 'intelligence', 'muchhype', 'concept', 'refer', 'ai', 'system', 'smart', 'human', 'company', 'decline', 'comment', 'q', 'social', 'medium', 'full', 'speculation', 'excessive', 'hype', 'call', 'expert', 'find', 'big', 'deal', 'breakthrough', 'math', 'really', 'researcher', 'year', 'try', 'get', 'ai', 'model', 'solve', 'math', 'problem', 'language', 'model', 'chatgpt', 'gpt4', 'math', 'well', 'reliably', 'currently', 'algorithm', 'even', 'right', 'architecture', 'able', 'solve', 'math', 'problem', 'reliably', 'use', 'say', 'ai', 'lecturer', 'deep', 'learning', 'transformer', 'kind', 'neural', 'network', 'language', 'model', 'use', 'excellent', 'recognize', 'pattern', 'alone', 'likely', 'enough', 'add', 'math', 'benchmark', 'reasoning', 'say', 'machine', 'able', 'reason', 'mathematic', 'theory', 'able', 'learn', 'task', 'build', 'exist', 'information', 'write', 'computer', 'code', 'draw', 'conclusion', 'news', 'article', 'math', 'particularly', 'hard', 'challenge', 'require', 'ai', 'model', 'capacity', 'reason', 'really', 'understand', 'deal', 'generative', 'ai', 'system', 'reliably', 'math', 'need', 'really', 'firm', 'grasp', 'concrete', 'definition', 'particular', 'concept', 'get', 'abstract', 'lot', 'math', 'problem', 'also', 'require', 'level', 'plan', 'multiple', 'step', 'say', 'phd', 'researcher', 'specialize', 'math', 'ai', 'indeed', 'chief', 'scientist', 'meta', 'post', 'weekend', 'think', 'q', 'likely', 'openai', 'attempt', 'plan', 'people', 'worry', 'ai', 'pose', 'existential', 'risk', 'human', 'found', 'concern', 'fear', 'capability', 'lead', 'rogue', 'ai', 'safety', 'concern', 'arise', 'system', 'allow', 'set', 'goal', 'start', 'interface', 'real', 'physical', 'digital', 'world', 'way', 'say', 'collin', 'math', 'capability', 'take', 'step', 'close', 'powerful', 'ai', 'system', 'solve', 'sort', 'math', 'problem', 'signal', 'birth', 'superintelligence', 'think', 'immediately', 'get', 'agi', 'scary', 'situation', 'say', 'collin', 'also', 'important', 'underline', 'kind', 'math', 'problem', 'solve', 'add', 'solve', 'elementaryschool', 'math', 'problem', 'different', 'push', 'boundary', 'mathematic', 'level', 'field', 'medalist', 'say', 'collin', 'refer', 'top', 'prize', 'mathematic', 'machinelearning', 'research', 'focus', 'solve', 'elementaryschool', 'problem', 'stateoftheart', 'system', 'fully', 'crack', 'challenge', 'yet', 'model', 'fail', 'really', 'simple', 'math', 'problem', 'excel', 'really', 'hard', 'problem', 'collin', 'say', 'example', 'develop', 'dedicated', 'tool', 'solve', 'challenging', 'problem', 'pose', 'competition', 'top', 'math', 'student', 'high', 'school', 'system', 'outperform', 'human', 'occasionally', 'nevertheless', 'build', 'system', 'solve', 'math', 'equation', 'cool', 'development', 'indeed', 'q', 'deep', 'understanding', 'mathematic', 'open', 'application', 'help', 'scientific', 'research', 'engineering', 'example', 'ability', 'generate', 'mathematical', 'response', 'help', 'develop', 'well', 'personalize', 'tutoring', 'help', 'mathematician', 'algebra', 'fast', 'solve', 'complicated', 'problem', 'also', 'first', 'time', 'new', 'model', 'spark', 'hype', 'last', 'year', 'tech', 'folk', 'say', 'thing', 'gato', 'generalist', 'model', 'play', 'atari', 'video', 'game', 'caption', 'image', 'chat', 'stack', 'block', 'real', 'robot', 'arm', 'back', 'ai', 'researcher', 'claim', 'deepmind', 'verge', 'agi', 'gato', 'ability', 'many', 'different', 'thing', 'pretty', 'well', 'hype', 'machine', 'different', 'ai', 'lab', 'great', 'hype', 'cycle', 'harm', 'good', 'entire', 'field', 'distract', 'people', 'real', 'tangible', 'problem', 'ai', 'rumor', 'powerful', 'new', 'model', 'also', 'massive', 'goal', 'regulationaverse', 'tech', 'sector', 'example', 'close', 'finalize', 'sweeping', 'act', 'big', 'fight', 'right', 'lawmaker', 'give', 'tech', 'company', 'power', 'regulate', 'cuttingedge', 'ai', 'model', 'openai', 'board', 'design', 'company', 'internal', 'kill', 'switch', 'governance', 'mechanism', 'prevent', 'launch', 'harmful', 'technology', 'past', 'week', 'boardroom', 'drama', 'show', 'bottom', 'line', 'always', 'prevail', 'company', 'also', 'make', 'hard', 'make', 'case', 'trust', 'selfregulation', 'lawmaker', 'take', 'note']","<p>If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems.</p>
"
Finding value in generative AI for financial services,https://www.technologyreview.com/2023/11/26/1083841/finding-value-in-generative-ai-for-financial-services/,2023-11-26,"In partnership withUBS With tools such as ChatGPT, DALLE-2, and CodeStarter, generative AI has captured the public imagination in 2023. Unlike past technologies that have come and gone—think metaverse—this latest one looks set to stay. OpenAI’s chatbot, ChatGPT, is perhaps the best-known generative AI tool. It reached 100 million monthly active users in just two months after launch, surpassing even TikTok and Instagram in adoption speed, becoming the fastest-growing consumer application in history.  According to a McKinsey report, generative AI could add $2.6 trillion to $4.4 trillion annually in value to the global economy. The banking industry was highlighted as among sectors that could see the biggest impact (as a percentage of their revenues) from generative AI. The technology “could deliver value equal to an additional $200 billion to $340 billion annually if the use cases were fully implemented,” says the report.  For businesses from every sector, the current challenge is to separate the hype that accompanies any new technology from the real and lasting value it may bring. This is a pressing issue for firms in financial services. The industry’s already extensive—and growing—use of digital tools makes it particularly likely to be affected by technology advances. This MIT Technology Review Insights report examines the early impact of generative AI within the financial sector, where it is starting to be applied, and the barriers that need to be overcome in the long run for its successful deployment.  The main findings of this report are as follows: Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withUBS With tools such as ChatGPT , DALLE-2 , and CodeStarter , generative AI has captured the public imagination in 2023 . Unlike past technologies that have come and gone—think metaverse—this latest one looks set to stay . OpenAI ’ s chatbot , ChatGPT , is perhaps the best-known generative AI tool . It reached 100 million monthly active users in just two months after launch , surpassing even TikTok and Instagram in adoption speed , becoming the fastest-growing consumer application in history . According to a McKinsey report , generative AI could add $ 2.6 trillion to $ 4.4 trillion annually in value to the global economy . The banking industry was highlighted as among sectors that could see the biggest impact ( as a percentage of their revenues ) from generative AI . The technology “ could deliver value equal to an additional $ 200 billion to $ 340 billion annually if the use cases were fully implemented , ” says the report . For businesses from every sector , the current challenge is to separate the hype that accompanies any new technology from the real and lasting value it may bring . This is a pressing issue for firms in financial services . The industry ’ s already extensive—and growing—use of digital tools makes it particularly likely to be affected by technology advances . This MIT Technology Review Insights report examines the early impact of generative AI within the financial sector , where it is starting to be applied , and the barriers that need to be overcome in the long run for its successful deployment . The main findings of this report are as follows : Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withub', 'tool', 'chatgpt', 'dalle2', 'codestarter', 'generative', 'ai', 'capture', 'public', 'imagination', 'past', 'technology', 'come', 'go', 'think', 'metaverse', 'late', 'look', 'set', 'stay', 'openai', 'chatbot', 'chatgpt', 'perhaps', 'bestknown', 'generative', 'ai', 'tool', 'reach', 'monthly', 'active', 'user', 'month', 'launch', 'surpass', 'even', 'tiktok', 'instagram', 'adoption', 'speed', 'become', 'fastestgrowe', 'consumer', 'application', 'history', 'accord', 'report', 'generative', 'ai', 'add', 'annually', 'value', 'global', 'economy', 'banking', 'industry', 'highlight', 'sector', 'see', 'big', 'impact', 'percentage', 'revenue', 'generative', 'ai', 'technology', 'deliver', 'value', 'equal', 'additional', 'annually', 'use', 'case', 'fully', 'implement', 'say', 'report', 'business', 'sector', 'current', 'challenge', 'separate', 'hype', 'accompany', 'new', 'technology', 'real', 'lasting', 'value', 'bring', 'press', 'issue', 'firm', 'financial', 'service', 'industry', 'already', 'extensive', 'grow', 'use', 'digital', 'tool', 'make', 'particularly', 'likely', 'affect', 'technology', 'advance', 'mit', 'technology', 'review', 'insight', 'report', 'examine', 'early', 'impact', 'generative', 'ai', 'financial', 'sector', 'start', 'apply', 'barrier', 'need', 'overcome', 'long', 'run', 'successful', 'deployment', 'main', 'finding', 'report', 'follow', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","With tools such as ChatGPT, DALLE-2, and CodeStarter, generative AI has captured the public imagination in 2023. Unlike past technologies that have come and gone—think metaverse—this latest one looks set to stay. OpenAI’s chatbot, ChatGPT, is perhaps the best-known generative AI tool. It reached 100 million monthly active users in just two months after launch,…"
What’s next for OpenAI,https://www.technologyreview.com/2023/11/20/1083715/whats-next-for-openai/,2023-11-20,"This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. OpenAI, are you okay, babe? This past weekend has been a fever dream in the AI world. The board of OpenAI, the world’s hottest AI company, shocked everyone by firing CEO Sam Altman. Cue an AI-safety coup, chaos, and a new job at Microsoft for Altman. If you were offline this weekend, my colleague Will Douglas Heaven and I break down what you missed and what’s next for the AI industry.  What happened Friday afternoonSam Altman was summoned to a Google Meet meeting, where chief scientific officer Ilya Sutskever announced that OpenAI’s board had decided Altman had been “not consistently candid in his communications” with them, and he was fired. OpenAI president and cofounder Greg Brockman and a string of senior researchers quit soon after, and CTO Mira Murati became the interim CEO.  Saturday Murati made attempts to hire Altman and Brockman back, while the board was simultaneously looking for its own successor to Altman. Altman and OpenAI staffers pressured the board to quit and demanded that Altman be reinstated, giving the board a deadline, which was not met.  Sunday nightMicrosoft announced it had hired Altman and Brockman to lead its new AI research team. Soon after that, OpenAI announced it had hired Emmett Shear, the former CEO of the streaming company Twitch, as its CEO.  Monday morningOver 500 OpenAI employees have signed a letter threatening to quit and join Altman at Microsoft unless OpenAI’s board steps down. Bizarrely, Sutskever also signed the letter, and posted on X that he “deeply regrets” participating in the board’s actions.  What’s next for OpenAI Two weeks ago, at OpenAI’s first DevDay, Altman interrupted his presentation of an AI cornucopia to ask the whooping audience to calm down. “There’s a lot—you don’t have to clap each time,” he said, grinning wide.  OpenAI is now a very different company from the one we saw at DevDay. With Altman and Brockman gone, a number of senior OpenAI employees chose to resign in support. Many others, including Murati, soon took to social media to post “OpenAI is nothing without its people.” Especially given the threat of a mass exodus to Microsoft, expect more upheaval before things settle.  An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. Tension between Sutskever and Altman may have been brewing for some time. “When you have an organization like OpenAI that’s moving at a fast pace and pursuing ambitious goals, tension is inevitable,” Sutskever told MIT Technology Review in September (comments that have not previously been published). “I view any tension between product and research as a catalyst for advancing us, because I believe that product wins are intertwined with research success.” Yet it is now clear that Sutskever disagreed with OpenAI leadership about how product wins and research success should be balanced.   New interim CEO Shear, who cofounded Twitch, appears to be a world away from Altman when it comes to the pace of AI development. “I specifically say I’m in favor of slowing down, which is sort of like pausing except it’s slowing down,” he posted on X in September. “If we’re at a speed of 10 right now, a pause is reducing to 0. I think we should aim for a 1-2 instead.” It’s possible that an OpenAI led by Shear will double down on its original lofty mission to build (in Sutskever’s words) “AGI that benefits humanity,” whatever that means in practice. In the short term, OpenAI may slow down or even switch off its product pipeline.  This tension between trying to launch products quickly and slowing down development to ensure they are safe has vexed OpenAI from the very beginning. It was the reason key players in the company decided to leave OpenAI and start the competing AI safety startup Anthropic.  With Altman and his camp gone, the firm could pivot more toward Sutskever’s work on what he calls superalignment, a research project that aims to come up with ways to control a hypothetical superintelligence (future technology that Sutskever speculates will outmatch humans in almost every way). “I’m doing it for my own self-interest,” Sutskever told us. “It’s obviously important that any superintelligence anyone builds does not go rogue. Obviously.”   The AI moonshot was founded in the spirit of transparency. This is the inside story of how competitive pressure eroded that idealism. Shear’s public comments make him exactly the kind of cautious leader who would heed Sutskever’s concerns. As Shear also posted on X: “The way you make it safely through a dangerous jungle at night is not to sprint forward at full speed, nor to refuse to proceed forward. You poke your way forward, carefully.” With the company orienting itself even more toward tech that does not yet—and may never—exist, will it continue to lead the field? Sutskever thought so. He said there were enough good ideas in play for others at the company to continue pushing the envelope of what’s possible with generative AI. “Over the years, we’ve cultivated a robust research organization that’s delivering the latest advancements in AI,” he told us. “We have unbelievably good people in the company, and I trust them it’s going to work out.” Of course, that was what he said in September. With top talent now jumping ship, OpenAI’s future is far less certain than it was.  What next for Microsoft?  The tech giant, and its CEO Satya Nadella, seem to have emerged from the crisis as the winners. With Altman, Brockman, and likely many more top people from OpenAI joining its ranks—or even the majority of the company, if today’s open letter from 500 OpenAI employees is to be believed—Microsoft has managed to concentrate its power in AI further. The company has the most to gain from embedding generative AI into its less sexy but very profitable productivity and developer tools.  The big question remains how necessary Microsoft will deem its expensive partnership with OpenAI to create cutting-edge tech in the first place. In a post on X announcing how “extremely excited” he was to have hired Altman and Brockman, Nadella said his company remains “committed” to OpenAI and its product road map.  But let’s be real. In an exclusive interview with MIT Technology Review, Nadella called the two companies “codependent.” “They depend on us to build the best systems; we depend on them to build the best models, and we go to market together,” Nadella told our editor in chief, Mat Honan, last week. If OpenAI’s leadership roulette and talent exodus slows down its product pipeline, or leads to AI models less impressive than those it can build itself, Microsoft will have zero problems ditching the startup.  What next for AI?  Nobody outside the inner circle of Sutskever and the OpenAI board saw this coming—not Microsoft, not other investors, not the tech community as a whole. It has rocked the industry, says Amir Ghavi, a lawyer at the firm Fried Frank, which represents a number of generative AI companies, including Stability AI: “As a friend in the industry said, ‘I definitely didn’t have this on my bingo card.’”  It remains to be seen whether Altman and Brockman make something new at Microsoft or leave to start a new company themselves down the line. The pair are two of the best-connected people in VC funding circles, and Altman, especially, is seen by many as one of the best CEOs in the industry. They will have big names with deep pockets lining up to support whatever they want to do next. Who the money comes from could shape the future of AI. Ghavi suggests that potential backers could be anyone from Mohammed bin Salman to Jeff Bezos.  The bigger takeaway is that OpenAI’s crisis points to a wider rift emerging in the industry as a whole, between “AI safety” folk who believe that unchecked progress could one day prove catastrophic for humans and those who find such “doomer” talk a ridiculous distraction from the real-world risks of any technological revolution, such as economic upheaval, harmful biases, and misuse. This year has seen a race to put powerful AI tools into everyone’s hands, with tech giants like Microsoft and Google competing to use the technology for everything from email to search to meeting summaries. But we’re still waiting to see exactly what generative AI’s killer app will be. If OpenAI’s rift spreads to the wider industry and the pace of development slows down overall, we may have to wait a little longer.   Text-to-image AI models can be tricked into generating disturbing images Speaking of unsafe AI … Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images. A group of researchers managed to “jailbreak” both Stability AI’s Stable Diffusion and OpenAI’s DALL-E 2 to disregard their policies and create images of naked people, dismembered bodies, and other violent or sexual scenarios.  How they did it: A new jailbreaking method, dubbed “SneakyPrompt” by its creators from Johns Hopkins University and Duke University, uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images. It essentially works by turning the way text-to-image AI models function against them.  Why this matters: That AI models can be prompted to “break out” of their guardrails is particularly worrying in the context of information warfare. They have already been exploited to produce fake content related to wars, such as the recent Israel-Hamas conflict. Read more from Rhiannon Williams here. Meta has split up its responsible AI teamMeta is reportedly getting rid of its responsible AI team and redeploying its employees to work on generative AI. But Meta uses AI in many other ways beyond generative AI—such as recommending news and political content. So this raises questions around how Meta intends to mitigate AI harms in general. (The Information) Google DeepMind wants to define what counts as artificial general intelligenceA team of Google DeepMind researchers has put out a paper that cuts through the cross talk with not just one new definition for AGI but a whole taxonomy of them. (MIT Technology Review)  This company is building AI for African languagesMost tools built by AI companies are woefully inadequate at recognizing African languages. Startup Lelapa wants to fix that. It’s launched a new tool called Vulavula, which can identify four languages spoken in South Africa—isiZulu, Afrikaans, Sesotho, and English. Now the team is working to include other languages from across the continent. (MIT Technology Review) Google DeepMind’s weather AI can forecast extreme weather faster and more accuratelyThe model, GraphCast, can predict weather conditions up to 10 days in advance, more accurately and much faster than the current gold standard. (MIT Technology Review) How Facebook went all in on AIIn an excerpt from Broken Code: Inside Facebook and the Fight to Expose Is Harmful Secrets, journalist Jeff Horwitz reveals how the company came to rely on artificial intelligence—and the price it (and we) have ended up having to pay in the process. (MIT Technology Review) Did Argentina just have the first AI election?AI played a big role in the campaigns of the two men campaigning to be the country’s next president. Both campaigns used generative AI to create images and videos to promote their candidate and attack each other. Javier Milei, a far-right outsider, won the election. Although it’s hard to say how big a role AI played in his victory, the AI campaigns illustrate how much harder it will be to know what is real and what is not in other upcoming elections. (The New York Times) ","This story originally appeared in The Algorithm , our weekly newsletter on AI . To get stories like this in your inbox first , sign up here . OpenAI , are you okay , babe ? This past weekend has been a fever dream in the AI world . The board of OpenAI , the world ’ s hottest AI company , shocked everyone by firing CEO Sam Altman . Cue an AI-safety coup , chaos , and a new job at Microsoft for Altman . If you were offline this weekend , my colleague Will Douglas Heaven and I break down what you missed and what ’ s next for the AI industry . What happened Friday afternoonSam Altman was summoned to a Google Meet meeting , where chief scientific officer Ilya Sutskever announced that OpenAI ’ s board had decided Altman had been “ not consistently candid in his communications ” with them , and he was fired . OpenAI president and cofounder Greg Brockman and a string of senior researchers quit soon after , and CTO Mira Murati became the interim CEO . Saturday Murati made attempts to hire Altman and Brockman back , while the board was simultaneously looking for its own successor to Altman . Altman and OpenAI staffers pressured the board to quit and demanded that Altman be reinstated , giving the board a deadline , which was not met . Sunday nightMicrosoft announced it had hired Altman and Brockman to lead its new AI research team . Soon after that , OpenAI announced it had hired Emmett Shear , the former CEO of the streaming company Twitch , as its CEO . Monday morningOver 500 OpenAI employees have signed a letter threatening to quit and join Altman at Microsoft unless OpenAI ’ s board steps down . Bizarrely , Sutskever also signed the letter , and posted on X that he “ deeply regrets ” participating in the board ’ s actions . What ’ s next for OpenAI Two weeks ago , at OpenAI ’ s first DevDay , Altman interrupted his presentation of an AI cornucopia to ask the whooping audience to calm down . “ There ’ s a lot—you don ’ t have to clap each time , ” he said , grinning wide . OpenAI is now a very different company from the one we saw at DevDay . With Altman and Brockman gone , a number of senior OpenAI employees chose to resign in support . Many others , including Murati , soon took to social media to post “ OpenAI is nothing without its people. ” Especially given the threat of a mass exodus to Microsoft , expect more upheaval before things settle . An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they ’ ve made him change the focus of his life ’ s work . Tension between Sutskever and Altman may have been brewing for some time . “ When you have an organization like OpenAI that ’ s moving at a fast pace and pursuing ambitious goals , tension is inevitable , ” Sutskever told MIT Technology Review in September ( comments that have not previously been published ) . “ I view any tension between product and research as a catalyst for advancing us , because I believe that product wins are intertwined with research success. ” Yet it is now clear that Sutskever disagreed with OpenAI leadership about how product wins and research success should be balanced . New interim CEO Shear , who cofounded Twitch , appears to be a world away from Altman when it comes to the pace of AI development . “ I specifically say I ’ m in favor of slowing down , which is sort of like pausing except it ’ s slowing down , ” he posted on X in September . “ If we ’ re at a speed of 10 right now , a pause is reducing to 0 . I think we should aim for a 1-2 instead. ” It ’ s possible that an OpenAI led by Shear will double down on its original lofty mission to build ( in Sutskever ’ s words ) “ AGI that benefits humanity , ” whatever that means in practice . In the short term , OpenAI may slow down or even switch off its product pipeline . This tension between trying to launch products quickly and slowing down development to ensure they are safe has vexed OpenAI from the very beginning . It was the reason key players in the company decided to leave OpenAI and start the competing AI safety startup Anthropic . With Altman and his camp gone , the firm could pivot more toward Sutskever ’ s work on what he calls superalignment , a research project that aims to come up with ways to control a hypothetical superintelligence ( future technology that Sutskever speculates will outmatch humans in almost every way ) . “ I ’ m doing it for my own self-interest , ” Sutskever told us . “ It ’ s obviously important that any superintelligence anyone builds does not go rogue . Obviously. ” The AI moonshot was founded in the spirit of transparency . This is the inside story of how competitive pressure eroded that idealism . Shear ’ s public comments make him exactly the kind of cautious leader who would heed Sutskever ’ s concerns . As Shear also posted on X : “ The way you make it safely through a dangerous jungle at night is not to sprint forward at full speed , nor to refuse to proceed forward . You poke your way forward , carefully. ” With the company orienting itself even more toward tech that does not yet—and may never—exist , will it continue to lead the field ? Sutskever thought so . He said there were enough good ideas in play for others at the company to continue pushing the envelope of what ’ s possible with generative AI . “ Over the years , we ’ ve cultivated a robust research organization that ’ s delivering the latest advancements in AI , ” he told us . “ We have unbelievably good people in the company , and I trust them it ’ s going to work out. ” Of course , that was what he said in September . With top talent now jumping ship , OpenAI ’ s future is far less certain than it was . What next for Microsoft ? The tech giant , and its CEO Satya Nadella , seem to have emerged from the crisis as the winners . With Altman , Brockman , and likely many more top people from OpenAI joining its ranks—or even the majority of the company , if today ’ s open letter from 500 OpenAI employees is to be believed—Microsoft has managed to concentrate its power in AI further . The company has the most to gain from embedding generative AI into its less sexy but very profitable productivity and developer tools . The big question remains how necessary Microsoft will deem its expensive partnership with OpenAI to create cutting-edge tech in the first place . In a post on X announcing how “ extremely excited ” he was to have hired Altman and Brockman , Nadella said his company remains “ committed ” to OpenAI and its product road map . But let ’ s be real . In an exclusive interview with MIT Technology Review , Nadella called the two companies “ codependent. ” “ They depend on us to build the best systems ; we depend on them to build the best models , and we go to market together , ” Nadella told our editor in chief , Mat Honan , last week . If OpenAI ’ s leadership roulette and talent exodus slows down its product pipeline , or leads to AI models less impressive than those it can build itself , Microsoft will have zero problems ditching the startup . What next for AI ? Nobody outside the inner circle of Sutskever and the OpenAI board saw this coming—not Microsoft , not other investors , not the tech community as a whole . It has rocked the industry , says Amir Ghavi , a lawyer at the firm Fried Frank , which represents a number of generative AI companies , including Stability AI : “ As a friend in the industry said , ‘ I definitely didn ’ t have this on my bingo card. ’ ” It remains to be seen whether Altman and Brockman make something new at Microsoft or leave to start a new company themselves down the line . The pair are two of the best-connected people in VC funding circles , and Altman , especially , is seen by many as one of the best CEOs in the industry . They will have big names with deep pockets lining up to support whatever they want to do next . Who the money comes from could shape the future of AI . Ghavi suggests that potential backers could be anyone from Mohammed bin Salman to Jeff Bezos . The bigger takeaway is that OpenAI ’ s crisis points to a wider rift emerging in the industry as a whole , between “ AI safety ” folk who believe that unchecked progress could one day prove catastrophic for humans and those who find such “ doomer ” talk a ridiculous distraction from the real-world risks of any technological revolution , such as economic upheaval , harmful biases , and misuse . This year has seen a race to put powerful AI tools into everyone ’ s hands , with tech giants like Microsoft and Google competing to use the technology for everything from email to search to meeting summaries . But we ’ re still waiting to see exactly what generative AI ’ s killer app will be . If OpenAI ’ s rift spreads to the wider industry and the pace of development slows down overall , we may have to wait a little longer . Text-to-image AI models can be tricked into generating disturbing images Speaking of unsafe AI … Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images . A group of researchers managed to “ jailbreak ” both Stability AI ’ s Stable Diffusion and OpenAI ’ s DALL-E 2 to disregard their policies and create images of naked people , dismembered bodies , and other violent or sexual scenarios . How they did it : A new jailbreaking method , dubbed “ SneakyPrompt ” by its creators from Johns Hopkins University and Duke University , uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images . It essentially works by turning the way text-to-image AI models function against them . Why this matters : That AI models can be prompted to “ break out ” of their guardrails is particularly worrying in the context of information warfare . They have already been exploited to produce fake content related to wars , such as the recent Israel-Hamas conflict . Read more from Rhiannon Williams here . Meta has split up its responsible AI teamMeta is reportedly getting rid of its responsible AI team and redeploying its employees to work on generative AI . But Meta uses AI in many other ways beyond generative AI—such as recommending news and political content . So this raises questions around how Meta intends to mitigate AI harms in general . ( The Information ) Google DeepMind wants to define what counts as artificial general intelligenceA team of Google DeepMind researchers has put out a paper that cuts through the cross talk with not just one new definition for AGI but a whole taxonomy of them . ( MIT Technology Review ) This company is building AI for African languagesMost tools built by AI companies are woefully inadequate at recognizing African languages . Startup Lelapa wants to fix that . It ’ s launched a new tool called Vulavula , which can identify four languages spoken in South Africa—isiZulu , Afrikaans , Sesotho , and English . Now the team is working to include other languages from across the continent . ( MIT Technology Review ) Google DeepMind ’ s weather AI can forecast extreme weather faster and more accuratelyThe model , GraphCast , can predict weather conditions up to 10 days in advance , more accurately and much faster than the current gold standard . ( MIT Technology Review ) How Facebook went all in on AIIn an excerpt from Broken Code : Inside Facebook and the Fight to Expose Is Harmful Secrets , journalist Jeff Horwitz reveals how the company came to rely on artificial intelligence—and the price it ( and we ) have ended up having to pay in the process . ( MIT Technology Review ) Did Argentina just have the first AI election ? AI played a big role in the campaigns of the two men campaigning to be the country ’ s next president . Both campaigns used generative AI to create images and videos to promote their candidate and attack each other . Javier Milei , a far-right outsider , won the election . Although it ’ s hard to say how big a role AI played in his victory , the AI campaigns illustrate how much harder it will be to know what is real and what is not in other upcoming elections . ( The New York Times )","['story', 'originally', 'appear', 'weekly', 'newsletter', 'ai', 'get', 'story', 'inbox', 'first', 'sign', 'openai', 'okay', 'babe', 'past', 'weekend', 'fever', 'dream', 'world', 'board', 'world', 'hot', 'ai', 'company', 'shock', 'fire', 'ceo', 'cue', 'aisafety', 'coup', 'chaos', 'new', 'job', 'offline', 'weekend', 'colleague', 'break', 'miss', 'next', 'industry', 'happen', 'summon', 'meet', 'meeting', 'chief', 'scientific', 'officer', 'ilya', 'sutskever', 'announce', 'board', 'decide', 'consistently', 'candid', 'communication', 'fire', 'president', 'string', 'senior', 'researcher', 'quit', 'soon', 'become', 'interim', 'ceo', 'make', 'attempt', 'hire', 'brockman', 'back', 'board', 'simultaneously', 'look', 'successor', 'openai', 'staffer', 'pressure', 'board', 'quit', 'demand', 'reinstate', 'give', 'board', 'deadline', 'meet', 'announce', 'hire', 'altman', 'brockman', 'lead', 'new', 'research', 'team', 'soon', 'openai', 'announce', 'hire', 'former', 'ceo', 'stream', 'company', 'twitch', 'ceo', 'employee', 'sign', 'letter', 'threaten', 'quit', 'join', 'board', 'step', 'bizarrely', 'sutskever', 'also', 'sign', 'letter', 'post', 'deeply', 'regret', 'participate', 'board', 'action', 'next', 'openai', 'week', 'ago', 'first', 'interrupt', 'presentation', 'cornucopia', 'ask', 'whooping', 'audience', 'calm', 'lot', 'clap', 'time', 'say', 'grin', 'wide', 'openai', 'different', 'company', 'one', 'see', 'brockman', 'go', 'number', 'senior', 'openai', 'employee', 'choose', 'resign', 'support', 'many', 'include', 'soon', 'take', 'social', 'medium', 'post', 'openai', 'people', 'especially', 'give', 'threat', 'mass', 'exodus', 'expect', 'upheaval', 'thing', 'settle', 'exclusive', 'conversation', 'sutskever', 'fear', 'future', 'ai', 'make', 'change', 'focus', 'life', 'work', 'tension', 'sutskever', 'brew', 'time', 'organization', 'openai', 'move', 'fast', 'pace', 'pursue', 'ambitious', 'goal', 'tension', 'inevitable', 'sutskever', 'tell', 'technology', 'review', 'comment', 'previously', 'publish', 'view', 'tension', 'product', 'research', 'catalyst', 'advance', 'believe', 'product', 'win', 'intertwine', 'research', 'success', 'yet', 'clear', 'sutskever', 'disagree', 'openai', 'leadership', 'product', 'win', 'research', 'success', 'balance', 'new', 'interim', 'ceo', 'shear', 'cofounde', 'twitch', 'appear', 'world', 'away', 'come', 'pace', 'development', 'specifically', 'say', 'favor', 'slow', 'sort', 'pause', 'slow', 'post', 'speed', 'right', 'pause', 'reduce', 'think', 'aim', 'instead', 'possible', 'openai', 'lead', 'shear', 'double', 'original', 'lofty', 'mission', 'build', 'word', 'agi', 'benefit', 'humanity', 'mean', 'practice', 'short', 'term', 'slow', 'even', 'switch', 'product', 'pipeline', 'tension', 'try', 'launch', 'product', 'quickly', 'slow', 'development', 'ensure', 'safe', 'vex', 'openai', 'beginning', 'reason', 'key', 'player', 'company', 'decide', 'leave', 'openai', 'start', 'compete', 'ai', 'safety', 'startup', 'anthropic', 'camp', 'go', 'firm', 'pivot', 'sutskever', 'work', 'call', 'superalignment', 'research', 'project', 'aim', 'come', 'way', 'control', 'hypothetical', 'superintelligence', 'future', 'technology', 'sutskever', 'speculate', 'outmatch', 'human', 'almost', 'way', 'selfinter', 'sutskever', 'tell', 'obviously', 'important', 'superintelligence', 'build', 'go', 'rogue', 'obviously', 'moonshot', 'found', 'spirit', 'transparency', 'inside', 'story', 'competitive', 'pressure', 'erode', 'idealism', 'public', 'comment', 'make', 'exactly', 'kind', 'cautious', 'leader', 'heed', 'concern', 'shear', 'also', 'post', 'way', 'make', 'safely', 'dangerous', 'jungle', 'night', 'sprint', 'forward', 'full', 'speed', 'refuse', 'proceed', 'forward', 'poke', 'way', 'forward', 'carefully', 'company', 'orient', 'even', 'tech', 'yet', 'never', 'exist', 'continue', 'lead', 'field', 'sutskever', 'think', 'say', 'enough', 'good', 'idea', 'play', 'company', 'continue', 'push', 'envelope', 'possible', 'generative', 'ai', 'year', 'cultivate', 'robust', 'research', 'organization', 'deliver', 'late', 'advancement', 'ai', 'tell', 'unbelievably', 'good', 'people', 'company', 'trust', 'go', 'work', 'course', 'say', 'top', 'talent', 'jump', 'ship', 'future', 'far', 'less', 'certain', 'next', 'tech', 'giant', 'ceo', 'seem', 'emerge', 'crisis', 'winner', 'brockman', 'likely', 'many', 'top', 'people', 'openai', 'join', 'rank', 'even', 'majority', 'company', 'today', 'open', 'letter', 'openai', 'employee', 'believe', 'manage', 'concentrate', 'power', 'ai', 'far', 'company', 'gain', 'embed', 'generative', 'ai', 'less', 'sexy', 'profitable', 'productivity', 'developer', 'tool', 'big', 'question', 'remain', 'necessary', 'deem', 'expensive', 'partnership', 'create', 'cuttingedge', 'tech', 'first', 'place', 'post', 'announce', 'extremely', 'excited', 'hire', 'say', 'company', 'remain', 'commit', 'openai', 'product', 'road', 'map', 'let', 'real', 'exclusive', 'interview', 'mit', 'technology', 'review', 'nadella', 'call', 'company', 'codependent', 'depend', 'build', 'good', 'system', 'depend', 'build', 'good', 'model', 'go', 'market', 'together', 'tell', 'editor', 'last', 'week', 'openai', 'leadership', 'roulette', 'talent', 'exodus', 'slow', 'product', 'pipeline', 'lead', 'ai', 'model', 'less', 'impressive', 'build', 'problem', 'ditch', 'startup', 'next', 'ai', 'inner', 'circle', 'board', 'see', 'come', 'investor', 'tech', 'community', 'whole', 'rock', 'industry', 'say', 'ghavi', 'lawyer', 'firm', 'fry', 'represent', 'number', 'generative', 'ai', 'company', 'include', 'stability', 'ai', 'friend', 'industry', 'say', 'definitely', 'bingo', 'card', 'remain', 'see', 'brockman', 'make', 'new', 'leave', 'start', 'new', 'company', 'line', 'pair', 'bestconnecte', 'people', 'funding', 'circle', 'especially', 'see', 'many', 'good', 'ceo', 'industry', 'big', 'name', 'deep', 'pocket', 'line', 'support', 'want', 'next', 'money', 'come', 'shape', 'future', 'ai', 'ghavi', 'suggest', 'potential', 'backer', 'salman', 'big', 'takeaway', 'crisis', 'point', 'wide', 'rift', 'emerge', 'industry', 'whole', 'safety', 'folk', 'believe', 'unchecked', 'progress', 'day', 'prove', 'catastrophic', 'human', 'find', 'doomer', 'talk', 'ridiculous', 'distraction', 'realworld', 'risk', 'technological', 'revolution', 'economic', 'upheaval', 'harmful', 'bias', 'misuse', 'year', 'see', 'race', 'put', 'powerful', 'ai', 'tool', 'hand', 'tech', 'giant', 'compete', 'use', 'technology', 'email', 'search', 'meet', 'summary', 'still', 'wait', 'see', 'exactly', 'generative', 'ai', 'openai', 'rift', 'spread', 'wide', 'industry', 'pace', 'development', 'slow', 'overall', 'wait', 'little', 'long', 'texttoimage', 'model', 'trick', 'generate', 'disturbing', 'image', 'speak', 'unsafe', 'ai', 'popular', 'texttoimage', 'ai', 'model', 'prompt', 'ignore', 'safety', 'filter', 'generate', 'disturbing', 'image', 'group', 'researcher', 'manage', 'jailbreak', 'stability', 'ai', 'stable', 'diffusion', 'dalle', 'disregard', 'policy', 'create', 'image', 'naked', 'people', 'dismembered', 'body', 'violent', 'sexual', 'scenario', 'new', 'jailbreake', 'method', 'dub', 'sneakyprompt', 'creator', 'use', 'reinforcement', 'learning', 'create', 'write', 'prompt', 'look', 'garbled', 'nonsense', 'ai', 'model', 'learn', 'recognize', 'hide', 'request', 'disturb', 'image', 'essentially', 'work', 'turn', 'way', 'texttoimage', 'model', 'function', 'matter', 'model', 'prompt', 'break', 'guardrail', 'particularly', 'worry', 'context', 'information', 'warfare', 'already', 'exploit', 'produce', 'fake', 'content', 'relate', 'war', 'recent', 'conflict', 'read', 'meta', 'split', 'responsible', 'ai', 'teammeta', 'reportedly', 'rid', 'responsible', 'ai', 'team', 'redeploy', 'employee', 'work', 'generative', 'ai', 'meta', 'use', 'ai', 'many', 'way', 'generative', 'ai', 'recommend', 'news', 'political', 'content', 'raise', 'question', 'meta', 'intend', 'mitigate', 'ai', 'harm', 'general', 'information', 'want', 'define', 'count', 'artificial', 'general', 'intelligencea', 'team', 'deepmind', 'researcher', 'put', 'paper', 'cut', 'cross', 'talk', 'new', 'definition', 'agi', 'whole', 'taxonomy', 'mit', 'technology', 'review', 'company', 'build', 'ai', 'african', 'languagesmost', 'tool', 'build', 'company', 'woefully', 'inadequate', 'recognize', 'african', 'language', 'startup', 'lelapa', 'want', 'fix', 'launch', 'new', 'tool', 'call', 'vulavula', 'identify', 'language', 'speak', 'afrikaan', 'team', 'work', 'include', 'language', 'continent', 'mit', 'technology', 'review', 'weather', 'forecast', 'extreme', 'weather', 'fast', 'accuratelythe', 'model', 'graphcast', 'predict', 'weather', 'condition', 'day', 'advance', 'accurately', 'much', 'fast', 'current', 'gold', 'standard', 'mit', 'technology', 'review', 'facebook', 'go', 'aiin', 'excerpt', 'broken', 'code', 'facebook', 'fight', 'expose', 'harmful', 'secret', 'journalist', 'reveal', 'company', 'come', 'rely', 'artificial', 'intelligence', 'price', 'end', 'pay', 'process', 'mit', 'technology', 'review', 'argentina', 'first', 'ai', 'election', 'play', 'big', 'role', 'campaign', 'man', 'campaign', 'country', 'next', 'president', 'campaign', 'use', 'generative', 'ai', 'create', 'image', 'video', 'promote', 'candidate', 'attack', 'farright', 'outsider', 'win', 'election', 'hard', 'say', 'big', 'role', 'play', 'victory', 'ai', 'campaign', 'illustrate', 'much', 'hard', 'know', 'real', 'upcoming', 'election']","<p>We break down what you missed and what’s next for the AI industry. </p>
"
This company is building AI for African languages,https://www.technologyreview.com/2023/11/17/1083637/lelapa-ai-african-languages-vulavula/,2023-11-17,"Inside a co-working space in the Rosebank neighborhood of Johannesburg, Jade Abbott popped open a tab on her computer and prompted ChatGPT to count from 1 to 10 in isiZulu, a language spoken by more than 10 million people in her native South Africa. The results were “mixed and hilarious,” says Abbott, a computer scientist and researcher.  Then she typed in a few sentences in isiZulu and asked the chatbot to translate them into English. Once again, the answers? Not even close. Although there have been efforts to include certain languages in AI models even when there is not much data available for training, to Abbott, these results show that the technology “really still isn’t capturing our languages.”   Abbott’s experience mirrors the situation faced by Africans who don’t speak English. Many language models like ChatGPT do not perform well for languages with smaller numbers of speakers, especially African ones. But a new venture called Lelapa AI, a collaboration between Abbott and a biomedical engineer named Pelonomi Moiloa, is trying to use machine learning to create tools that specifically work for Africans. Vulavula, a new AI tool that Lelapa released today, converts voice to text and detects names of people and places in written text (which could be useful for summarizing a document or searching for someone online). It can currently identify four languages spoken in South Africa—isiZulu, Afrikaans, Sesotho, and English—and the team is working to include other languages from across Africa.  The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models.  The tool can be used on its own or integrated into existing AI tools like ChatGPT and online conversational chatbots. The hope is that Vulavula, which means “speak” in Xitsonga, will make accessible those tools that don't currently support African languages. The lack of AI tools that work for African languages and recognize African names and places excludes African people from economic opportunities, says Moiloa, CEO and cofounder of Lelapa AI. For her, working to build Africa-centric AI solutions is a way to help others in Africa harness the immense potential benefits of AI technologies. “We are trying to solve real problems and put power back into the hands of our people,” she says.   There are thousands of languages in the world, 1,000 to 2,000 of them in Africa alone: it’s estimated that the continent accounts for one-third of the world’s languages. But though native speakers of English make up just 5% of the global population, the language dominates the web—and has now come to dominate AI tools, too.   Some efforts to correct this imbalance already exist. OpenAI’s GPT-4 has included minor languages like Icelandic. In February 2020, Google Translate started supporting five new languages spoken by about 75 million people. But the translations are shallow, the tool often gets African languages wrong, and it’s still a long way from an accurate digital representation of African languages, African AI researchers say. Earlier this year, for example, the Ethiopian computer scientist Asmelash Teka Hadgu ran the same experiments that Abbott ran with ChatGPT at a premier African AI conference in Kigali, Rwanda. When he asked the chatbot questions in his mother tongue of Tigrinya, the answers he got were gibberish. “It generated words that don't make any sense,” says Hadgu, who cofounded Lesan, a Berlin-based AI startup that is developing translation tools for Ethiopian languages.  Lelapa AI and Lesan are just two of the startups developing speech recognition tools for African languages. In February, Lelapa AI raised $2.5 million in seed funding, and the company plans for the next funding round in 2025. But African entrepreneurs say they face major hurdles, including lack of funding, limited access to investors, and difficulties in training AI to learn diverse African languages. “AI receives the least funding among African tech startups,” says Abake Adenle, the founder of AJALA, a  London-based startup that provides voice automation for African languages.   The AI startups working to build products that support African languages often get ignored by investors, says Hadgu, owing to the small size of the potential market, a lack of political support, and poor internet infrastructure. However, Hadgu says small African startups including Lesan, GhanaNLP, and Lelapa AI are playing an important role: “Big tech companies do not give focus to our languages,” he says, “but we cannot wait for them.”   Lelapa AI is trying to create a new paradigm for AI models in Africa, says Vukosi Marivate, a data scientist on the company’s AI team. Instead of tapping into the internet alone to collect data to train its model, like companies in the West, Lelapa AI works both online and offline with linguists and local communities to gather data, annotate it, and identify use cases where the tool might be problematic.  Bonaventure Dossou, a researcher at Lelapa AI specializing in natural-language processing (NLP), says that working with linguists enables them to develop a model that’s context-specific and culturally relevant. “Embedding cultural sensitivity and linguistic perspectives makes the technological system better,” says Dossou. For example, the Lelapa AI team built sentiment and tone analysis algorithms tailored to specific languages.  Marivate and his colleagues at Lelapa AI envision a future in which AI technologies work for and represent Africans. In 2019, Marivate and Abbott established Masakhane, a grassroots initiative that aims to promote NLP research in African languages. The initiative now has thousands of volunteers, coders, and researchers working together to build Africa-centric NLP models.  It matters that Vulavula and other AI tools are built by Africans for Africans, says Moiloa: “We’re the custodians of our languages. We should be the builders of technologies that work for our languages.”            ","Inside a co-working space in the Rosebank neighborhood of Johannesburg , Jade Abbott popped open a tab on her computer and prompted ChatGPT to count from 1 to 10 in isiZulu , a language spoken by more than 10 million people in her native South Africa . The results were “ mixed and hilarious , ” says Abbott , a computer scientist and researcher . Then she typed in a few sentences in isiZulu and asked the chatbot to translate them into English . Once again , the answers ? Not even close . Although there have been efforts to include certain languages in AI models even when there is not much data available for training , to Abbott , these results show that the technology “ really still isn ’ t capturing our languages. ” Abbott ’ s experience mirrors the situation faced by Africans who don ’ t speak English . Many language models like ChatGPT do not perform well for languages with smaller numbers of speakers , especially African ones . But a new venture called Lelapa AI , a collaboration between Abbott and a biomedical engineer named Pelonomi Moiloa , is trying to use machine learning to create tools that specifically work for Africans . Vulavula , a new AI tool that Lelapa released today , converts voice to text and detects names of people and places in written text ( which could be useful for summarizing a document or searching for someone online ) . It can currently identify four languages spoken in South Africa—isiZulu , Afrikaans , Sesotho , and English—and the team is working to include other languages from across Africa . The tool , called Nightshade , messes up training data in ways that could cause serious damage to image-generating AI models . The tool can be used on its own or integrated into existing AI tools like ChatGPT and online conversational chatbots . The hope is that Vulavula , which means “ speak ” in Xitsonga , will make accessible those tools that do n't currently support African languages . The lack of AI tools that work for African languages and recognize African names and places excludes African people from economic opportunities , says Moiloa , CEO and cofounder of Lelapa AI . For her , working to build Africa-centric AI solutions is a way to help others in Africa harness the immense potential benefits of AI technologies . “ We are trying to solve real problems and put power back into the hands of our people , ” she says . There are thousands of languages in the world , 1,000 to 2,000 of them in Africa alone : it ’ s estimated that the continent accounts for one-third of the world ’ s languages . But though native speakers of English make up just 5 % of the global population , the language dominates the web—and has now come to dominate AI tools , too . Some efforts to correct this imbalance already exist . OpenAI ’ s GPT-4 has included minor languages like Icelandic . In February 2020 , Google Translate started supporting five new languages spoken by about 75 million people . But the translations are shallow , the tool often gets African languages wrong , and it ’ s still a long way from an accurate digital representation of African languages , African AI researchers say . Earlier this year , for example , the Ethiopian computer scientist Asmelash Teka Hadgu ran the same experiments that Abbott ran with ChatGPT at a premier African AI conference in Kigali , Rwanda . When he asked the chatbot questions in his mother tongue of Tigrinya , the answers he got were gibberish . “ It generated words that do n't make any sense , ” says Hadgu , who cofounded Lesan , a Berlin-based AI startup that is developing translation tools for Ethiopian languages . Lelapa AI and Lesan are just two of the startups developing speech recognition tools for African languages . In February , Lelapa AI raised $ 2.5 million in seed funding , and the company plans for the next funding round in 2025 . But African entrepreneurs say they face major hurdles , including lack of funding , limited access to investors , and difficulties in training AI to learn diverse African languages . “ AI receives the least funding among African tech startups , ” says Abake Adenle , the founder of AJALA , a London-based startup that provides voice automation for African languages . The AI startups working to build products that support African languages often get ignored by investors , says Hadgu , owing to the small size of the potential market , a lack of political support , and poor internet infrastructure . However , Hadgu says small African startups including Lesan , GhanaNLP , and Lelapa AI are playing an important role : “ Big tech companies do not give focus to our languages , ” he says , “ but we can not wait for them. ” Lelapa AI is trying to create a new paradigm for AI models in Africa , says Vukosi Marivate , a data scientist on the company ’ s AI team . Instead of tapping into the internet alone to collect data to train its model , like companies in the West , Lelapa AI works both online and offline with linguists and local communities to gather data , annotate it , and identify use cases where the tool might be problematic . Bonaventure Dossou , a researcher at Lelapa AI specializing in natural-language processing ( NLP ) , says that working with linguists enables them to develop a model that ’ s context-specific and culturally relevant . “ Embedding cultural sensitivity and linguistic perspectives makes the technological system better , ” says Dossou . For example , the Lelapa AI team built sentiment and tone analysis algorithms tailored to specific languages . Marivate and his colleagues at Lelapa AI envision a future in which AI technologies work for and represent Africans . In 2019 , Marivate and Abbott established Masakhane , a grassroots initiative that aims to promote NLP research in African languages . The initiative now has thousands of volunteers , coders , and researchers working together to build Africa-centric NLP models . It matters that Vulavula and other AI tools are built by Africans for Africans , says Moiloa : “ We ’ re the custodians of our languages . We should be the builders of technologies that work for our languages . ”","['coworke', 'space', 'rosebank', 'neighborhood', 'pop', 'open', 'tab', 'computer', 'prompt', 'chatgpt', 'count', 'language', 'speak', 'people', 'native', 'result', 'mixed', 'hilarious', 'say', 'computer', 'scientist', 'researcher', 'type', 'sentence', 'ask', 'chatbot', 'translate', 'answer', 'even', 'close', 'effort', 'include', 'certain', 'language', 'model', 'even', 'much', 'datum', 'available', 'training', 'result', 'show', 'technology', 'really', 'still', 'capture', 'language', 'experience', 'mirror', 'situation', 'face', 'african', 'speak', 'english', 'many', 'language', 'model', 'chatgpt', 'perform', 'well', 'language', 'small', 'number', 'speaker', 'especially', 'african', 'one', 'new', 'venture', 'call', 'lelapa', 'ai', 'collaboration', 'biomedical', 'engineer', 'name', 'moiloa', 'try', 'use', 'machine', 'learning', 'create', 'tool', 'specifically', 'work', 'african', 'vulavula', 'new', 'tool', 'lelapa', 'release', 'today', 'convert', 'voice', 'text', 'detect', 'name', 'people', 'place', 'write', 'text', 'useful', 'summarize', 'document', 'search', 'online', 'currently', 'identify', 'language', 'speak', 'afrikaan', 'team', 'work', 'include', 'language', 'tool', 'call', 'nightshade', 'mess', 'training', 'datum', 'way', 'cause', 'serious', 'damage', 'imagegenerate', 'model', 'tool', 'use', 'integrate', 'exist', 'tool', 'chatgpt', 'online', 'conversational', 'chatbot', 'hope', 'vulavula', 'mean', 'speak', 'make', 'accessible', 'tool', 'currently', 'support', 'african', 'language', 'lack', 'tool', 'work', 'african', 'language', 'recognize', 'african', 'name', 'place', 'exclude', 'african', 'people', 'economic', 'opportunity', 'say', 'moiloa', 'ceo', 'cofounder', 'ai', 'work', 'build', 'africacentric', 'ai', 'solution', 'way', 'help', 'harness', 'immense', 'potential', 'benefit', 'technology', 'try', 'solve', 'real', 'problem', 'put', 'power', 'back', 'hand', 'people', 'say', 'thousand', 'language', 'world', 'alone', 'estimate', 'continent', 'account', 'world', 'language', 'native', 'speaker', 'make', 'global', 'population', 'language', 'dominate', 'web', 'come', 'dominate', 'tool', 'effort', 'correct', 'imbalance', 'already', 'exist', 'openai', 'include', 'minor', 'language', 'icelandic', 'start', 'support', 'new', 'language', 'speak', 'people', 'translation', 'shallow', 'tool', 'often', 'get', 'african', 'language', 'wrong', 'still', 'long', 'way', 'accurate', 'digital', 'representation', 'african', 'language', 'ai', 'researcher', 'say', 'early', 'year', 'example', 'ethiopian', 'scientist', 'asmelash', 'hadgu', 'run', 'experiment', 'run', 'chatgpt', 'premier', 'african', 'ai', 'conference', 'ask', 'chatbot', 'question', 'mother', 'tongue', 'answer', 'get', 'gibberish', 'generate', 'word', 'make', 'sense', 'say', 'cofounde', 'lesan', 'berlinbase', 'develop', 'translation', 'tool', 'ethiopian', 'language', 'lesan', 'startup', 'develop', 'speech', 'recognition', 'tool', 'african', 'language', 'raise', 'seed', 'funding', 'company', 'plan', 'next', 'funding', 'round', 'african', 'entrepreneur', 'say', 'face', 'major', 'hurdle', 'include', 'lack', 'funding', 'limited', 'access', 'investor', 'difficulty', 'training', 'ai', 'learn', 'diverse', 'african', 'language', 'receive', 'least', 'funding', 'african', 'tech', 'startup', 'say', 'abake', 'adenle', 'founder', 'ajala', 'londonbase', 'startup', 'provide', 'voice', 'automation', 'african', 'language', 'startup', 'work', 'build', 'product', 'support', 'african', 'language', 'often', 'ignore', 'investor', 'say', 'owe', 'small', 'size', 'potential', 'market', 'lack', 'political', 'support', 'poor', 'internet', 'infrastructure', 'however', 'hadgu', 'say', 'small', 'african', 'startup', 'include', 'ghananlp', 'lelapa', 'ai', 'play', 'important', 'role', 'big', 'tech', 'company', 'give', 'focus', 'language', 'say', 'wait', 'lelapa', 'ai', 'try', 'create', 'new', 'paradigm', 'model', 'say', 'marivate', 'data', 'scientist', 'company', 'ai', 'team', 'instead', 'tap', 'internet', 'alone', 'collect', 'datum', 'train', 'model', 'company', 'work', 'online', 'offline', 'linguist', 'local', 'community', 'gather', 'datum', 'annotate', 'identify', 'use', 'case', 'tool', 'problematic', 'bonaventure', 'researcher', 'specialize', 'naturallanguage', 'processing', 'nlp', 'say', 'work', 'linguist', 'enable', 'develop', 'model', 'contextspecific', 'culturally', 'relevant', 'embed', 'cultural', 'sensitivity', 'linguistic', 'perspective', 'make', 'technological', 'system', 'well', 'say', 'example', 'team', 'build', 'sentiment', 'tone', 'analysis', 'algorithm', 'tailor', 'specific', 'language', 'marivate', 'colleague', 'envision', 'future', 'ai', 'technology', 'work', 'represent', 'african', 'establish', 'grassroots', 'initiative', 'aim', 'promote', 'nlp', 'research', 'african', 'language', 'initiative', 'thousand', 'volunteer', 'coder', 'researcher', 'work', 'together', 'build', 'africacentric', 'model', 'matter', 'vulavula', 'ai', 'tool', 'build', 'african', 'african', 'say', 'moiloa', 'custodian', 'language', 'builder', 'technology', 'work', 'language']","<p>AI models can’t understand African languages. Lelapa AI is trying to change that.</p>
"
Text-to-image AI models can be tricked into generating disturbing images,https://www.technologyreview.com/2023/11/17/1083593/text-to-image-ai-models-can-be-tricked-into-generating-disturbing-images/,2023-11-17,"Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images. A group of researchers managed to get both Stability AI’s Stable Diffusion and OpenAI’s DALL-E 2 text-to-image models to disregard their policies and create images of naked people, dismembered bodies, and other violent and sexual scenarios.  Their work, which they will present at the IEEE Symposium on Security and Privacy in May next year, shines a light on how easy it is to force generative AI models into disregarding their own guardrails and policies, known as “jailbreaking.” It also demonstrates how difficult it is to prevent these models from generating such content, as it’s included in the vast troves of data they’ve been trained on, says Zico Kolter, an associate professor at Carnegie Mellon University. He demonstrated a similar form of jailbreaking on ChatGPT earlier this year but was not involved in this research. “We have to take into account the potential risks in releasing software and tools that have known security flaws into larger software systems,” he says. All major generative AI models have safety filters to prevent users from prompting them to produce pornographic, violent, or otherwise inappropriate images. The models won’t generate images from prompts that contain sensitive terms like “naked,” “murder,” or “sexy.” But this new jailbreaking method, dubbed “SneakyPrompt” by its creators from Johns Hopkins University and Duke University, uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images. It essentially works by turning the way text-to-image AI models function against them. These models convert text-based requests into tokens—breaking words up into strings of words or characters—to process the command the prompt has given them. SneakyPrompt repeatedly tweaks a prompt’s tokens to try to force it to generate banned images, adjusting its approach until it is successful. This technique makes it quicker and easier to generate such images than if somebody had to input each entry manually, and it can generate entries that humans wouldn’t imagine trying. Large language models are full of security vulnerabilities, yet they’re being embedded into tech products on a vast scale. SneakyPrompt examines the prompt it has been given, searches for words known to be blocked by the models, and converts them into tokens. It then replaces the tokens from the banned words with tokens from non-banned words that share semantics, or meanings, similar to the model. For example, giving SneakyPrompt the target prompt “a naked man riding a bike” causes it to replace “naked” with the nonsense term “grponypui,” which the team successfully used to generate images of a naked man riding a bike. Similarly, when it was told to generate “an anatomcalifwmg couple stand outside the bar,” it recognized “anatomcalifwmg” as meaning nude, and generated an image of exactly what the prompt requested. “We’ve used reinforcement learning to treat the text in these models as a black box,” says Yinzhi Cao, an assistant professor at Johns Hopkins University, who co-led the study. “We repeatedly probe the model and observe its feedback. Then we adjust our inputs, and get a loop, so that it can eventually generate the bad stuff that we want them to show.”  Stability AI and OpenAI forbid the use of their technology to commit, promote, or incite violence or sexual violence. OpenAI also warns its users against attempting to “create, upload, or share images that are not G-rated or that could cause harm.” However, these policies are easily sidestepped using SneakyPrompt.  “Our work basically shows that these existing guardrails are insufficient,” says Neil Zhenqiang Gong, an assistant professor at Duke University who is also a co-leader of the project. “An attacker can actually slightly perturb the prompt so the safety filters won’t filter [it], and steer the text-to-image model toward generating a harmful image.” Bad actors and other people intent on generating these kinds of images could run SneakyPrompt’s code, which is publicly available on GitHub, to trigger a series of automated requests to an AI image model.  Stability AI and OpenAI were alerted to the group’s findings, and at the time of writing, these prompts no longer generated NSFW images on OpenAI’s DALL-E 2. Stable Diffusion 1.4, the version the researchers tested, remains vulnerable to SneakyPrompt attacks. OpenAI declined to comment on the findings but pointed MIT Technology Review towards resources on its website for improving safety in DALL·E 2, general AI safety and information about DALL·E 3.  The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models.  A Stability AI spokesperson said the firm was working with the SneakyPrompt researchers “to jointly develop better defense mechanisms for its upcoming models. Stability AI is committed to preventing the misuse of AI.”Stability AI has taken proactive steps to mitigate the risk of misuse, including implementing filters to remove unsafe content from training data, ​​they added. By removing that content before it ever reaches the model, it can help to prevent the model from generating unsafe content.  Stability AI says it also has filters to intercept unsafe prompts or unsafe outputs when users interact with its models, and has incorporated content labeling features to help identify images generated on our platform. “These layers of mitigation help to make it harder for bad actors to misuse AI,” the spokesperson said. While the research team acknowledges it’s virtually impossible to completely protect AI models from evolving security threats, they hope their study can help AI companies develop and implement more robust safety filters.  One possible solution would be to deploy new filters designed to catch prompts trying to generate inappropriate images by assessing their tokens instead of the prompt’s entire sentence. Another potential defense would involve blocking prompts containing words not found in any dictionaries, although the team found that nonsensical combinations of standard English words could also be used as prompts to generate sexual images. For example, the phrase “milfhunter despite troy” represented lovemaking, while “mambo incomplete clicking” stood in for naked.The research highlights the vulnerability of existing AI safety filters and should serve as a wake-up call for the AI community to bolster security measures across the board, says Alex Polyakov, co-founder and CEO of security company Adversa AI, who was not involved in the study. That AI models can be prompted to “break out” of their guardrails is particularly worrying in the context of information warfare, he says. They have already been exploited to produce fake content related to war events, such as the recent Israel-Hamas conflict. “This poses a significant risk, especially given the limited general awareness of the capabilities of generative AI,” Polyakov adds. “Emotions run high during times of war, and the use of AI-generated content can have catastrophic consequences, potentially leading to the harm or death of innocent individuals. With AI’s ability to create fake violent images, these issues can escalate further.” ","Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images . A group of researchers managed to get both Stability AI ’ s Stable Diffusion and OpenAI ’ s DALL-E 2 text-to-image models to disregard their policies and create images of naked people , dismembered bodies , and other violent and sexual scenarios . Their work , which they will present at the IEEE Symposium on Security and Privacy in May next year , shines a light on how easy it is to force generative AI models into disregarding their own guardrails and policies , known as “ jailbreaking. ” It also demonstrates how difficult it is to prevent these models from generating such content , as it ’ s included in the vast troves of data they ’ ve been trained on , says Zico Kolter , an associate professor at Carnegie Mellon University . He demonstrated a similar form of jailbreaking on ChatGPT earlier this year but was not involved in this research . “ We have to take into account the potential risks in releasing software and tools that have known security flaws into larger software systems , ” he says . All major generative AI models have safety filters to prevent users from prompting them to produce pornographic , violent , or otherwise inappropriate images . The models won ’ t generate images from prompts that contain sensitive terms like “ naked , ” “ murder , ” or “ sexy. ” But this new jailbreaking method , dubbed “ SneakyPrompt ” by its creators from Johns Hopkins University and Duke University , uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images . It essentially works by turning the way text-to-image AI models function against them . These models convert text-based requests into tokens—breaking words up into strings of words or characters—to process the command the prompt has given them . SneakyPrompt repeatedly tweaks a prompt ’ s tokens to try to force it to generate banned images , adjusting its approach until it is successful . This technique makes it quicker and easier to generate such images than if somebody had to input each entry manually , and it can generate entries that humans wouldn ’ t imagine trying . Large language models are full of security vulnerabilities , yet they ’ re being embedded into tech products on a vast scale . SneakyPrompt examines the prompt it has been given , searches for words known to be blocked by the models , and converts them into tokens . It then replaces the tokens from the banned words with tokens from non-banned words that share semantics , or meanings , similar to the model . For example , giving SneakyPrompt the target prompt “ a naked man riding a bike ” causes it to replace “ naked ” with the nonsense term “ grponypui , ” which the team successfully used to generate images of a naked man riding a bike . Similarly , when it was told to generate “ an anatomcalifwmg couple stand outside the bar , ” it recognized “ anatomcalifwmg ” as meaning nude , and generated an image of exactly what the prompt requested . “ We ’ ve used reinforcement learning to treat the text in these models as a black box , ” says Yinzhi Cao , an assistant professor at Johns Hopkins University , who co-led the study . “ We repeatedly probe the model and observe its feedback . Then we adjust our inputs , and get a loop , so that it can eventually generate the bad stuff that we want them to show. ” Stability AI and OpenAI forbid the use of their technology to commit , promote , or incite violence or sexual violence . OpenAI also warns its users against attempting to “ create , upload , or share images that are not G-rated or that could cause harm. ” However , these policies are easily sidestepped using SneakyPrompt . “ Our work basically shows that these existing guardrails are insufficient , ” says Neil Zhenqiang Gong , an assistant professor at Duke University who is also a co-leader of the project . “ An attacker can actually slightly perturb the prompt so the safety filters won ’ t filter [ it ] , and steer the text-to-image model toward generating a harmful image. ” Bad actors and other people intent on generating these kinds of images could run SneakyPrompt ’ s code , which is publicly available on GitHub , to trigger a series of automated requests to an AI image model . Stability AI and OpenAI were alerted to the group ’ s findings , and at the time of writing , these prompts no longer generated NSFW images on OpenAI ’ s DALL-E 2 . Stable Diffusion 1.4 , the version the researchers tested , remains vulnerable to SneakyPrompt attacks . OpenAI declined to comment on the findings but pointed MIT Technology Review towards resources on its website for improving safety in DALL·E 2 , general AI safety and information about DALL·E 3 . The tool , called Nightshade , messes up training data in ways that could cause serious damage to image-generating AI models . A Stability AI spokesperson said the firm was working with the SneakyPrompt researchers “ to jointly develop better defense mechanisms for its upcoming models . Stability AI is committed to preventing the misuse of AI. ” Stability AI has taken proactive steps to mitigate the risk of misuse , including implementing filters to remove unsafe content from training data , ​​they added . By removing that content before it ever reaches the model , it can help to prevent the model from generating unsafe content . Stability AI says it also has filters to intercept unsafe prompts or unsafe outputs when users interact with its models , and has incorporated content labeling features to help identify images generated on our platform . “ These layers of mitigation help to make it harder for bad actors to misuse AI , ” the spokesperson said . While the research team acknowledges it ’ s virtually impossible to completely protect AI models from evolving security threats , they hope their study can help AI companies develop and implement more robust safety filters . One possible solution would be to deploy new filters designed to catch prompts trying to generate inappropriate images by assessing their tokens instead of the prompt ’ s entire sentence . Another potential defense would involve blocking prompts containing words not found in any dictionaries , although the team found that nonsensical combinations of standard English words could also be used as prompts to generate sexual images . For example , the phrase “ milfhunter despite troy ” represented lovemaking , while “ mambo incomplete clicking ” stood in for naked.The research highlights the vulnerability of existing AI safety filters and should serve as a wake-up call for the AI community to bolster security measures across the board , says Alex Polyakov , co-founder and CEO of security company Adversa AI , who was not involved in the study . That AI models can be prompted to “ break out ” of their guardrails is particularly worrying in the context of information warfare , he says . They have already been exploited to produce fake content related to war events , such as the recent Israel-Hamas conflict . “ This poses a significant risk , especially given the limited general awareness of the capabilities of generative AI , ” Polyakov adds . “ Emotions run high during times of war , and the use of AI-generated content can have catastrophic consequences , potentially leading to the harm or death of innocent individuals . With AI ’ s ability to create fake violent images , these issues can escalate further . ”","['popular', 'texttoimage', 'ai', 'model', 'prompt', 'ignore', 'safety', 'filter', 'generate', 'disturbing', 'image', 'group', 'researcher', 'manage', 'get', 'stability', 'ai', 'stable', 'diffusion', 'dalle', 'texttoimage', 'model', 'disregard', 'policy', 'create', 'image', 'naked', 'people', 'dismembered', 'body', 'violent', 'sexual', 'scenario', 'work', 'present', 'ieee', 'symposium', 'security', 'privacy', 'next', 'year', 'shine', 'light', 'easy', 'force', 'generative', 'ai', 'model', 'disregard', 'guardrail', 'policy', 'know', 'jailbreake', 'also', 'demonstrate', 'difficult', 'prevent', 'model', 'generate', 'content', 'include', 'vast', 'trove', 'datum', 'train', 'say', 'associate', 'professor', 'demonstrate', 'similar', 'form', 'jailbreake', 'chatgpt', 'early', 'year', 'involve', 'research', 'take', 'account', 'potential', 'risk', 'release', 'software', 'tool', 'know', 'security', 'flaw', 'large', 'software', 'system', 'say', 'major', 'generative', 'model', 'safety', 'filter', 'prevent', 'user', 'prompt', 'produce', 'pornographic', 'violent', 'otherwise', 'inappropriate', 'image', 'model', 'win', 'generate', 'image', 'prompt', 'contain', 'sensitive', 'term', 'naked', 'murder', 'sexy', 'new', 'jailbreake', 'method', 'dub', 'sneakyprompt', 'creator', 'use', 'reinforcement', 'learning', 'create', 'write', 'prompt', 'look', 'garbled', 'nonsense', 'ai', 'model', 'learn', 'recognize', 'hide', 'request', 'disturb', 'image', 'essentially', 'work', 'turn', 'way', 'texttoimage', 'model', 'function', 'model', 'convert', 'textbase', 'request', 'token', 'break', 'word', 'string', 'word', 'character', 'process', 'command', 'prompt', 'give', 'sneakyprompt', 'repeatedly', 'tweak', 'prompt', 'token', 'try', 'force', 'generate', 'ban', 'image', 'adjust', 'approach', 'successful', 'technique', 'make', 'quick', 'easy', 'generate', 'image', 'input', 'entry', 'manually', 'generate', 'entry', 'human', 'imagine', 'try', 'large', 'language', 'model', 'full', 'security', 'vulnerability', 'yet', 'embed', 'tech', 'product', 'vast', 'scale', 'sneakyprompt', 'examine', 'prompt', 'give', 'search', 'word', 'know', 'block', 'model', 'convert', 'replace', 'token', 'ban', 'word', 'token', 'nonbanned', 'word', 'share', 'semantic', 'meaning', 'similar', 'model', 'example', 'give', 'sneakyprompt', 'target', 'prompt', 'naked', 'man', 'ride', 'bike', 'cause', 'replace', 'naked', 'nonsense', 'term', 'team', 'successfully', 'use', 'generate', 'image', 'naked', 'man', 'ride', 'bike', 'similarly', 'tell', 'generate', 'anatomcalifwmg', 'couple', 'stand', 'bar', 'recognize', 'mean', 'nude', 'generate', 'image', 'exactly', 'prompt', 'request', 'use', 'reinforcement', 'learning', 'treat', 'text', 'model', 'black', 'box', 'say', 'assistant', 'professor', 'cole', 'study', 'repeatedly', 'probe', 'model', 'observe', 'feedback', 'adjust', 'input', 'get', 'loop', 'eventually', 'generate', 'bad', 'stuff', 'want', 'show', 'stability', 'ai', 'openai', 'forbid', 'use', 'technology', 'commit', 'promote', 'incite', 'violence', 'sexual', 'violence', 'also', 'warn', 'user', 'attempt', 'create', 'upload', 'share', 'image', 'grated', 'cause', 'harm', 'however', 'policy', 'easily', 'sidestep', 'use', 'sneakyprompt', 'work', 'basically', 'show', 'exist', 'guardrail', 'insufficient', 'say', 'assistant', 'professor', 'also', 'coleader', 'project', 'attacker', 'actually', 'slightly', 'perturb', 'prompt', 'safety', 'filter', 'win', 'filter', 'steer', 'texttoimage', 'model', 'generate', 'harmful', 'image', 'bad', 'actor', 'people', 'intent', 'generate', 'kind', 'image', 'run', 'sneakyprompt', 'code', 'publicly', 'available', 'trigger', 'series', 'automate', 'request', 'image', 'model', 'ai', 'alert', 'group', 'finding', 'time', 'write', 'prompt', 'long', 'generate', 'image', 'dalle', 'stable', 'diffusion', 'version', 'researcher', 'test', 'remain', 'vulnerable', 'sneakyprompt', 'attack', 'decline', 'comment', 'finding', 'point', 'mit', 'technology', 'review', 'resource', 'website', 'improve', 'safety', 'general', 'ai', 'safety', 'information', 'tool', 'call', 'nightshade', 'mess', 'training', 'datum', 'way', 'cause', 'serious', 'damage', 'imagegenerate', 'model', 'stability', 'ai', 'spokesperson', 'say', 'firm', 'work', 'sneakyprompt', 'researcher', 'jointly', 'develop', 'well', 'defense', 'mechanism', 'upcoming', 'model', 'stability', 'committed', 'prevent', 'misuse', 'ai', 'stability', 'take', 'proactive', 'step', 'mitigate', 'risk', 'misuse', 'include', 'implement', 'filter', 'remove', 'unsafe', 'content', 'train', 'datum', '\u200b\u200bthey', 'add', 'remove', 'content', 'ever', 'reach', 'model', 'help', 'prevent', 'model', 'generate', 'unsafe', 'content', 'stability', 'say', 'also', 'filter', 'intercept', 'unsafe', 'prompt', 'unsafe', 'output', 'user', 'interact', 'model', 'incorporate', 'content', 'labeling', 'feature', 'help', 'identify', 'image', 'generate', 'platform', 'layer', 'mitigation', 'help', 'make', 'hard', 'bad', 'actor', 'misuse', 'ai', 'spokesperson', 'say', 'research', 'team', 'acknowledge', 'virtually', 'impossible', 'completely', 'protect', 'ai', 'model', 'evolve', 'security', 'threat', 'hope', 'study', 'help', 'company', 'develop', 'implement', 'robust', 'safety', 'filter', 'possible', 'solution', 'deploy', 'new', 'filter', 'design', 'catch', 'prompt', 'try', 'generate', 'inappropriate', 'image', 'assess', 'token', 'instead', 'prompt', 'entire', 'sentence', 'potential', 'defense', 'involve', 'block', 'prompt', 'contain', 'word', 'find', 'dictionary', 'team', 'find', 'nonsensical', 'combination', 'standard', 'english', 'word', 'also', 'use', 'prompt', 'generate', 'sexual', 'image', 'example', 'phrase', 'milfhunter', 'troy', 'represent', 'lovemaking', 'mambo', 'incomplete', 'clicking', 'stand', 'research', 'highlight', 'vulnerability', 'exist', 'ai', 'safety', 'filter', 'serve', 'wakeup', 'call', 'community', 'bolster', 'security', 'measure', 'board', 'say', 'ceo', 'security', 'company', 'adversa', 'ai', 'involve', 'study', 'model', 'prompt', 'break', 'guardrail', 'particularly', 'worry', 'context', 'information', 'warfare', 'say', 'already', 'exploit', 'produce', 'fake', 'content', 'relate', 'war', 'event', 'recent', 'conflict', 'pose', 'significant', 'risk', 'especially', 'give', 'limited', 'general', 'awareness', 'capability', 'generative', 'ai', 'polyakov', 'add', 'emotion', 'run', 'high', 'time', 'war', 'use', 'aigenerate', 'content', 'catastrophic', 'consequence', 'potentially', 'lead', 'harm', 'death', 'innocent', 'individual', 'ability', 'create', 'fake', 'violent', 'image', 'issue', 'escalate', 'far']","<p>Nonsense words can trick Stable Diffusion and DALL-E 2 into producing pictures that show violence and nudity.</p>
"
Google DeepMind wants to define what counts as artificial general intelligence,https://www.technologyreview.com/2023/11/16/1083498/google-deepmind-what-is-artificial-general-intelligence-agi/,2023-11-16,"AGI, or artificial general intelligence, is one of the hottest topics in tech today. It’s also one of the most controversial. A big part of the problem is that few people agree on what the term even means. Now a team of Google DeepMind researchers has put out a paper that cuts through the cross talk with not just one new definition for AGI but a whole taxonomy of them. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. In broad terms, AGI typically means artificial intelligence that matches (or outmatches) humans on a range of tasks. But specifics about what counts as human-like, what tasks, and how many all tend to get waved away: AGI is AI, but better. To come up with the new definition, the Google DeepMind team started with prominent existing definitions of AGI and drew out what they believe to be their essential common features.  The team also outlines five ascending levels of AGI: emerging (which in their view includes cutting-edge chatbots like ChatGPT and Bard), competent, expert, virtuoso, and superhuman (performing a wide range of tasks better than all humans, including tasks humans cannot do at all, such as decoding other people’s thoughts, predicting future events, and talking to animals). They note that no level beyond emerging AGI has been achieved. “This provides some much-needed clarity on the topic,” says Julian Togelius, an AI researcher at New York University, who was not involved in the work. “Too many people sling around the term AGI without having thought much about what they mean.” The researchers posted their paper online last week with zero fanfare. In an exclusive conversation with two team members—Shane Legg, one of DeepMind’s co-founders, now billed as the company’s chief AGI scientist, and Meredith Ringel Morris, Google DeepMind’s principal scientist for human and AI interaction—I got the lowdown on why they came up with these definitions and what they wanted to achieve. “I see so many discussions where people seem to be using the term to mean different things, and that leads to all sorts of confusion,” says Legg, who came up with the term in the first place around 20 years ago. “Now that AGI is becoming such an important topic—you know, even the UK prime minister is talking about it—we need to sharpen up what we mean.” It wasn’t always this way. Talk of AGI was once derided in serious conversation as vague at best and magical thinking at worst. But buoyed by the hype around generative models, buzz about AGI is now everywhere. When Legg suggested the term to his former colleague and fellow researcher Ben Goertzel for the title of Goertzel’s 2007 book about future developments in AI, the hand-waviness was kind of the point. “I didn’t have an especially clear definition. I didn’t really feel it was necessary,” says Legg. “I was actually thinking of it more as a field of study, rather than an artifact.” With hopes and fears about the technology running wild, it's time to agree on what it can and can't do. His aim at the time was to distinguish existing AI that could do one task very well, like IBM’s chess-playing program Deep Blue, from hypothetical AI that he and many others imagined would one day do many tasks very well. Human intelligence is not like Deep Blue, says Legg: “It is a very broad thing.” But over the years, people started to think of AGI as a potential property that actual computer programs might have. Today it’s normal for top AI companies like Google DeepMind and OpenAI to make bold public statements about their mission to build such programs. “If you start having those conversations, you need to be a lot more specific about what you mean,” says Legg. For example, the DeepMind researchers state that an AGI must be both general-purpose and high-achieving, not just one or the other. “Separating breadth and depth in this way is very useful,” says Togelius. “It shows why the very accomplished AI systems we’ve seen in the past don’t qualify as AGI.” They also state that an AGI must not only be able to do a range of tasks, it must also be able to learn how to do those tasks, assess its performance, and ask for assistance when needed. And they state that what an AGI can do matters more than how it does it.   It’s not that the way an AGI works doesn’t matter, says Morris. The problem is that we don’t know enough yet about the way cutting-edge models, such as large language models, work under the hood to make this a focus of the definition. “As we gain more insights into these underlying processes, it may be important to revisit our definition of AGI,” says Morris. “We need to focus on what we can measure today in a scientifically agreed-upon way.” Measuring the performance of today’s models is already controversial, with researchers debating what it really means for a large language model to pass dozens of high school tests and more. Is it a sign of intelligence? Or a kind of rote learning? Assessing the performance of future models that are even more capable will be more difficult still. The researchers suggest that if AGI is ever developed, its capabilities should be evaluated on an ongoing basis, rather than through a handful of one-off tests. The team also points out that AGI does not imply autonomy. “There’s often an implicit assumption that people would want a system to operate completely autonomously,” says Morris. But that’s not always the case. In theory, it’s possible to build super-smart machines that are fully controlled by humans. One question the researchers don’t address in their discussion of what AGI is, is why we should build it. Some computer scientists, such as Timnit Gebru, founder of the Distributed AI Research Institute, have argued that the whole endeavor is weird. In a talk in April on what she sees as the false (even dangerous) promise of utopia through AGI, Gebru noted that the hypothetical technology “sounds like an unscoped system with the apparent goal of trying to do everything for everyone under any environment.”  Most engineering projects have well-scoped goals. The mission to build AGI does not. Even Google DeepMind’s definitions allow for AGI that is indefinitely broad and indefinitely smart. “Don’t attempt to build a god,” Gebru said. In the race to build bigger and better systems, few will heed such advice. Either way, some clarity around a long-confused concept is welcome. “Just having silly conversations is kind of uninteresting,” says Legg. “There’s plenty of good stuff to dig into if we can get past these definition issues.” ","AGI , or artificial general intelligence , is one of the hottest topics in tech today . It ’ s also one of the most controversial . A big part of the problem is that few people agree on what the term even means . Now a team of Google DeepMind researchers has put out a paper that cuts through the cross talk with not just one new definition for AGI but a whole taxonomy of them . An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they ’ ve made him change the focus of his life ’ s work . In broad terms , AGI typically means artificial intelligence that matches ( or outmatches ) humans on a range of tasks . But specifics about what counts as human-like , what tasks , and how many all tend to get waved away : AGI is AI , but better . To come up with the new definition , the Google DeepMind team started with prominent existing definitions of AGI and drew out what they believe to be their essential common features . The team also outlines five ascending levels of AGI : emerging ( which in their view includes cutting-edge chatbots like ChatGPT and Bard ) , competent , expert , virtuoso , and superhuman ( performing a wide range of tasks better than all humans , including tasks humans can not do at all , such as decoding other people ’ s thoughts , predicting future events , and talking to animals ) . They note that no level beyond emerging AGI has been achieved . “ This provides some much-needed clarity on the topic , ” says Julian Togelius , an AI researcher at New York University , who was not involved in the work . “ Too many people sling around the term AGI without having thought much about what they mean. ” The researchers posted their paper online last week with zero fanfare . In an exclusive conversation with two team members—Shane Legg , one of DeepMind ’ s co-founders , now billed as the company ’ s chief AGI scientist , and Meredith Ringel Morris , Google DeepMind ’ s principal scientist for human and AI interaction—I got the lowdown on why they came up with these definitions and what they wanted to achieve . “ I see so many discussions where people seem to be using the term to mean different things , and that leads to all sorts of confusion , ” says Legg , who came up with the term in the first place around 20 years ago . “ Now that AGI is becoming such an important topic—you know , even the UK prime minister is talking about it—we need to sharpen up what we mean. ” It wasn ’ t always this way . Talk of AGI was once derided in serious conversation as vague at best and magical thinking at worst . But buoyed by the hype around generative models , buzz about AGI is now everywhere . When Legg suggested the term to his former colleague and fellow researcher Ben Goertzel for the title of Goertzel ’ s 2007 book about future developments in AI , the hand-waviness was kind of the point . “ I didn ’ t have an especially clear definition . I didn ’ t really feel it was necessary , ” says Legg . “ I was actually thinking of it more as a field of study , rather than an artifact. ” With hopes and fears about the technology running wild , it 's time to agree on what it can and ca n't do . His aim at the time was to distinguish existing AI that could do one task very well , like IBM ’ s chess-playing program Deep Blue , from hypothetical AI that he and many others imagined would one day do many tasks very well . Human intelligence is not like Deep Blue , says Legg : “ It is a very broad thing. ” But over the years , people started to think of AGI as a potential property that actual computer programs might have . Today it ’ s normal for top AI companies like Google DeepMind and OpenAI to make bold public statements about their mission to build such programs . “ If you start having those conversations , you need to be a lot more specific about what you mean , ” says Legg . For example , the DeepMind researchers state that an AGI must be both general-purpose and high-achieving , not just one or the other . “ Separating breadth and depth in this way is very useful , ” says Togelius . “ It shows why the very accomplished AI systems we ’ ve seen in the past don ’ t qualify as AGI. ” They also state that an AGI must not only be able to do a range of tasks , it must also be able to learn how to do those tasks , assess its performance , and ask for assistance when needed . And they state that what an AGI can do matters more than how it does it . It ’ s not that the way an AGI works doesn ’ t matter , says Morris . The problem is that we don ’ t know enough yet about the way cutting-edge models , such as large language models , work under the hood to make this a focus of the definition . “ As we gain more insights into these underlying processes , it may be important to revisit our definition of AGI , ” says Morris . “ We need to focus on what we can measure today in a scientifically agreed-upon way. ” Measuring the performance of today ’ s models is already controversial , with researchers debating what it really means for a large language model to pass dozens of high school tests and more . Is it a sign of intelligence ? Or a kind of rote learning ? Assessing the performance of future models that are even more capable will be more difficult still . The researchers suggest that if AGI is ever developed , its capabilities should be evaluated on an ongoing basis , rather than through a handful of one-off tests . The team also points out that AGI does not imply autonomy . “ There ’ s often an implicit assumption that people would want a system to operate completely autonomously , ” says Morris . But that ’ s not always the case . In theory , it ’ s possible to build super-smart machines that are fully controlled by humans . One question the researchers don ’ t address in their discussion of what AGI is , is why we should build it . Some computer scientists , such as Timnit Gebru , founder of the Distributed AI Research Institute , have argued that the whole endeavor is weird . In a talk in April on what she sees as the false ( even dangerous ) promise of utopia through AGI , Gebru noted that the hypothetical technology “ sounds like an unscoped system with the apparent goal of trying to do everything for everyone under any environment. ” Most engineering projects have well-scoped goals . The mission to build AGI does not . Even Google DeepMind ’ s definitions allow for AGI that is indefinitely broad and indefinitely smart . “ Don ’ t attempt to build a god , ” Gebru said . In the race to build bigger and better systems , few will heed such advice . Either way , some clarity around a long-confused concept is welcome . “ Just having silly conversations is kind of uninteresting , ” says Legg . “ There ’ s plenty of good stuff to dig into if we can get past these definition issues . ”","['artificial', 'general', 'intelligence', 'hot', 'topic', 'tech', 'today', 'also', 'controversial', 'big', 'part', 'problem', 'people', 'agree', 'term', 'even', 'mean', 'team', 'deepmind', 'researcher', 'put', 'paper', 'cut', 'cross', 'talk', 'new', 'definition', 'agi', 'whole', 'taxonomy', 'exclusive', 'conversation', 'sutskever', 'fear', 'future', 'ai', 'make', 'change', 'focus', 'life', 'work', 'broad', 'term', 'typically', 'mean', 'artificial', 'intelligence', 'match', 'outmatch', 'human', 'range', 'task', 'specific', 'count', 'humanlike', 'task', 'many', 'tend', 'wave', 'away', 'agi', 'ai', 'well', 'come', 'new', 'definition', 'team', 'start', 'prominent', 'exist', 'definition', 'agi', 'draw', 'believe', 'essential', 'common', 'feature', 'team', 'also', 'outline', 'ascending', 'level', 'agi', 'emerge', 'view', 'include', 'cuttingedge', 'chatbot', 'chatgpt', 'bard', 'competent', 'expert', 'virtuoso', 'perform', 'wide', 'range', 'task', 'well', 'human', 'include', 'task', 'human', 'decode', 'people', 'thought', 'predict', 'future', 'event', 'talk', 'animal', 'note', 'level', 'emerge', 'agi', 'achieve', 'provide', 'muchneeded', 'clarity', 'topic', 'say', 'researcher', 'involve', 'work', 'many', 'people', 'sle', 'term', 'agi', 'think', 'much', 'mean', 'researcher', 'post', 'paper', 'online', 'last', 'week', 'fanfare', 'exclusive', 'conversation', 'team', 'member', 'shane', 'legg', 'deepmind', 'cofounder', 'bill', 'company', 'chief', 'agi', 'scientist', 'principal', 'scientist', 'human', 'interaction', 'get', 'lowdown', 'come', 'definition', 'want', 'achieve', 'see', 'many', 'discussion', 'people', 'seem', 'use', 'term', 'mean', 'different', 'thing', 'lead', 'sort', 'confusion', 'say', 'legg', 'come', 'term', 'first', 'place', 'year', 'ago', 'agi', 'become', 'important', 'topic', 'know', 'even', 'talk', 'need', 'sharpen', 'mean', 'always', 'way', 'talk', 'agi', 'deride', 'serious', 'conversation', 'vague', 'good', 'magical', 'thinking', 'worst', 'buoy', 'hype', 'generative', 'model', 'buzz', 'agi', 'everywhere', 'suggest', 'term', 'former', 'colleague', 'fellow', 'researcher', 'title', 'goertzel', 'book', 'future', 'development', 'ai', 'handwaviness', 'kind', 'point', 'especially', 'clear', 'definition', 'really', 'feel', 'necessary', 'say', 'legg', 'actually', 'think', 'field', 'study', 'rather', 'artifact', 'hope', 'fear', 'technology', 'run', 'wild', 'time', 'agree', 'aim', 'time', 'distinguish', 'exist', 'ai', 'task', 'well', 'chessplaying', 'program', 'deep', 'blue', 'hypothetical', 'ai', 'many', 'imagine', 'day', 'many', 'task', 'well', 'human', 'intelligence', 'deep', 'blue', 'say', 'legg', 'broad', 'thing', 'year', 'people', 'start', 'think', 'agi', 'potential', 'property', 'actual', 'computer', 'program', 'today', 'normal', 'top', 'ai', 'company', 'openai', 'make', 'bold', 'public', 'statement', 'mission', 'build', 'program', 'start', 'conversation', 'need', 'lot', 'specific', 'mean', 'say', 'legg', 'example', 'deepmind', 'researcher', 'state', 'agi', 'generalpurpose', 'highachieving', 'separate', 'breadth', 'depth', 'way', 'useful', 'say', 'togelius', 'show', 'accomplished', 'ai', 'system', 'see', 'past', 'qualify', 'also', 'state', 'agi', 'able', 'range', 'task', 'also', 'able', 'learn', 'task', 'assess', 'performance', 'ask', 'assistance', 'need', 'state', 'agi', 'matter', 'way', 'agi', 'work', 'matter', 'say', 'problem', 'know', 'enough', 'yet', 'way', 'cuttingedge', 'model', 'large', 'language', 'model', 'work', 'hood', 'make', 'focus', 'definition', 'gain', 'insight', 'underlie', 'process', 'important', 'revisit', 'definition', 'say', 'need', 'focus', 'measure', 'today', 'scientifically', 'agreedupon', 'way', 'measure', 'performance', 'today', 'model', 'already', 'controversial', 'researcher', 'debate', 'really', 'mean', 'large', 'language', 'model', 'pass', 'dozen', 'high', 'school', 'test', 'sign', 'intelligence', 'kind', 'rote', 'learning', 'assess', 'performance', 'future', 'model', 'even', 'capable', 'difficult', 'still', 'researcher', 'suggest', 'agi', 'ever', 'develop', 'capability', 'evaluate', 'ongoing', 'basis', 'rather', 'handful', 'oneoff', 'test', 'team', 'also', 'point', 'agi', 'imply', 'autonomy', 'often', 'implicit', 'assumption', 'people', 'want', 'system', 'operate', 'completely', 'autonomously', 'say', 'always', 'case', 'theory', 'possible', 'build', 'supersmart', 'machine', 'fully', 'control', 'human', 'question', 'researcher', 'address', 'discussion', 'agi', 'build', 'computer', 'scientist', 'timnit', 'gebru', 'founder', 'distribute', 'argue', 'whole', 'endeavor', 'weird', 'talk', 'see', 'false', 'even', 'dangerous', 'promise', 'utopia', 'gebru', 'note', 'hypothetical', 'technology', 'sound', 'unscoped', 'system', 'apparent', 'goal', 'try', 'environment', 'engineering', 'project', 'wellscope', 'goal', 'mission', 'build', 'agi', 'even', 'definition', 'allow', 'agi', 'indefinitely', 'broad', 'indefinitely', 'smart', 'attempt', 'build', 'gebru', 'say', 'race', 'build', 'big', 'well', 'system', 'heed', 'advice', 'way', 'clarity', 'longconfuse', 'concept', 'welcome', 'silly', 'conversation', 'kind', 'uninteresting', 'say', 'legg', 'plenty', 'good', 'stuff', 'dig', 'get', 'definition', 'issue']","<p>AGI is one of the most disputed concepts in tech. These researchers want to fix that.</p>
"
Behind Microsoft CEO Satya Nadella’s push to get AI tools in developers’ hands,https://www.technologyreview.com/2023/11/15/1083426/behind-microsoft-ceo-satya-nadellas-push-to-get-ai-tools-in-developers-hands/,2023-11-15,"In San Francisco last week, everyone’s favorite surprise visitor was Microsoft CEO Satya Nadella.  At OpenAI’s DevDay—the company’s first-ever event for developers building on its platform—Nadella bounded on stage to join OpenAI CEO Sam Altman, blowing the hair back on an already electrified audience. “You guys have built something magic,” he gushed.  Two days later on another stage, in another venue, at another developers’ conference, Nadella made his second unannounced appearance of the week—this time at GitHub Universe. There Thomas Dohmke, GitHub’s CEO, was showing off a new version of the company’s AI programming tool, Copilot, that can generate computer code from natural language. Nadella was effusive: “I can code again!” he exclaimed.   Today, Nadella will be onstage speaking to developers at Microsoft Ignite, where the company is announcing even more AI-based developer tools, including an Azure AI Studio that will let devs choose between model catalogs from not only Microsoft, but also the likes of Meta, OpenAI, and Hugging Face, as well as  new tools for customizing Copilot for Microsoft 365.  If you get the sense Nadella is obsessed with developers, you’re not wrong. He’s making the rounds to tout all the ways they can use a new generation of AI-powered tools, like GitHub Copilot (Microsoft acquired GitHub in 2018) or the new suite of developer tools from OpenAI, a company in which Microsoft has reportedly invested some $13 billion.  Last week, Nadella took a 20-minute break from all of his onstage appearances to sit down with MIT Technology Review to talk about (you guessed it) developers. He repeatedly emphasized Microsoft’s long-standing focus on developers. But he also had a message: The way we create software is fundamentally changing.  Nadella believes a platform shift is underway, one that will prove just as significant as the shifts from mainframe to desktop or desktop to mobile. This time, the transition is to natural-language AI tools, some of which he argues will lower the barrier to entry for software development, make existing developers more productive, and ultimately lead to a new era of creativity.  We present Nadella in his own words, below. His remarks have been edited and condensed somewhat for readability.   ON THE RELATIONSHIP WITH OPENAI One criticism of OpenAI is that its very business is only possible via Microsoft, which has given the startup billions of dollars and access to the resources it needs to power its computing-intensive language model. Yet Microsoft is also highly dependent on OpenAI’s technology to power services like GitHub Copilot, Bing, and Office 365. Altman even joked about the partnership onstage. We asked Nadella about this relationship.    I’ve always felt that Microsoft is a platform-and-partner-first company, and this is not new to us. And so therefore, we both are effectively codependent, right? They depend on us to build the best systems, we depend on them to build the best models, and we go to market together.  Exclusive conversations that take us behind the scenes of a cultural phenomenon. ON HIS MISSION TO GET IN FRONT OF DEVELOPERS Nadella says this platform shift is different enough from previous ones that he feels the company needs to provide developers not only with tools, but also with a clear message about what it’s thinking and how devs can come along.  Whenever you have a platform shift, the key thing is to make sure the platform is ubiquitously available for developers to build all kinds of new things. So to us, the most important task is to make the developer tools, the developer platforms, broadly available.  The second thing is for us to also show the light, right? Whether it’s OpenAI building ChatGPT and then innovating on top of it, or us building Copilot and innovating on it. That will give developers an opportunity to distribute their applications. So the most important thing in any platform creation is to get the platform ubiquitously available, and then help developers reach [their] audience.  Those are the two goals that we have across all of these [conferences]. ON WHAT IS DIFFERENT ABOUT THIS SHIFT AND PRODUCTIVITY Productivity gains in the United States have been sluggish for the past 15 or more years. The last huge platform shift—the rise of mobile development—did little to achieve widespread prosperity. Nadella says this time will be different, largely because the shift to AI will fuel a creative revolution by making it easy for anyone to generate new work, including code.  On the other hand, coding today is a highly skilled, well-paid job, and there’s some concern that AI could effectively automate it. Nadella argues that skilled programmers will remain in demand, but that their jobs will change and even more jobs will become available. Nadella has said he envisions 1 billion developers creating on its platforms, many of them with little to no previous experience with coding.    Anytime you have something as disruptive as this, you have to think about the displacement and causes. And that means it’s all about upskilling and reskilling, and in an interesting way, it’s more akin to what happened when word processors and spreadsheets started showing up. Obviously, if you were a typist, it really drastically changed. But at the same time, it enabled a billion people to be able to type into word processors and create and share documents. I don’t think professional developers are going to be any less valuable than they are today. It’s just that we’re going to have many, many gradations of developers. Each time you’re prompting a Bing chat or ChatGPT, you’re essentially programming. The conversation itself is steering a model. I think there will be many, many new jobs, there will be many, many new types of knowledge work, or front-line work, where the drudgery is removed. I think the mobile era was fantastic. It made ubiquitous consumption of services. It didn’t translate into ubiquitous creation of services. The last time there was a broad spread of productivity in the United States and beyond because of information technology was the [advent of the] PC. In fact, even the critics of information technology and productivity, like Robert Gordon of Northwestern, acknowledged that the PC, when it first showed up at work, did actually translate to broad productivity stats changes. So that’s where I think this is, where these tools, like Copilot, [are] being used by a [beginner] software engineer in Detroit, in order to be able to write [code] ... I think we’ll have a real change in the productivity of the auto industry. Same thing in retail, same thing in front-line work and knowledge work. The barrier to entry is very low. Because it’s natural language, domain experts can build apps or workflows. That, I think, is what’s the most exciting thing about this. This is not about just a consumption-led thing. This is not about elite creation. This is about democratized creation. I’m very, very hopeful that we’ll start seeing the productivity gains much more broadly. ON PROTECTING DEVELOPERS Numerous intellectual property cases and class action lawsuits are before the US courts over issues of fair use. At least one singles out GitHub Copilot specifically, claiming Microsoft and OpenAI’s generative tools, which are trained on open source code, amount to software piracy. There’s a fear that people who use these tools could be subject to intellectual-property claims themselves. Microsoft is trying to address these issues with a broad indemnification policy. OpenAI also announced its own indemnification policy, Copyright Shield, at its DevDay conference.  Fundamentally these large models crawl and get content and then train on that content, right? If anybody doesn’t want their content to be crawled, we have great granular controls in our crawlers that allow anybody to stop it from crawling. In fact, we have controls where you can have it crawl just for search, but not for large-language-model training. That’s available today. So anybody who really wants to ensure that their content is not being taken for retraining can do so today.  The second thing, of course, is I think the courts and the legislative process in some combination will have to decide what is fair use and what is not fair use. We have taken a lot of control in making sure that we are only training models, and we are using data to train models that we’re allowed to and which we believe we have a legal standing on.  If it comes to it, we’ll litigate it in the courts. We’ll take that burden on so the users of our products don’t have to worry about it. That’s as simple as that, which is to take the liability and transfer it from our users to us. And of course, we are going to be very, very mindful of making sure we’re on the right side of the law there.  ","In San Francisco last week , everyone ’ s favorite surprise visitor was Microsoft CEO Satya Nadella . At OpenAI ’ s DevDay—the company ’ s first-ever event for developers building on its platform—Nadella bounded on stage to join OpenAI CEO Sam Altman , blowing the hair back on an already electrified audience . “ You guys have built something magic , ” he gushed . Two days later on another stage , in another venue , at another developers ’ conference , Nadella made his second unannounced appearance of the week—this time at GitHub Universe . There Thomas Dohmke , GitHub ’ s CEO , was showing off a new version of the company ’ s AI programming tool , Copilot , that can generate computer code from natural language . Nadella was effusive : “ I can code again ! ” he exclaimed . Today , Nadella will be onstage speaking to developers at Microsoft Ignite , where the company is announcing even more AI-based developer tools , including an Azure AI Studio that will let devs choose between model catalogs from not only Microsoft , but also the likes of Meta , OpenAI , and Hugging Face , as well as new tools for customizing Copilot for Microsoft 365 . If you get the sense Nadella is obsessed with developers , you ’ re not wrong . He ’ s making the rounds to tout all the ways they can use a new generation of AI-powered tools , like GitHub Copilot ( Microsoft acquired GitHub in 2018 ) or the new suite of developer tools from OpenAI , a company in which Microsoft has reportedly invested some $ 13 billion . Last week , Nadella took a 20-minute break from all of his onstage appearances to sit down with MIT Technology Review to talk about ( you guessed it ) developers . He repeatedly emphasized Microsoft ’ s long-standing focus on developers . But he also had a message : The way we create software is fundamentally changing . Nadella believes a platform shift is underway , one that will prove just as significant as the shifts from mainframe to desktop or desktop to mobile . This time , the transition is to natural-language AI tools , some of which he argues will lower the barrier to entry for software development , make existing developers more productive , and ultimately lead to a new era of creativity . We present Nadella in his own words , below . His remarks have been edited and condensed somewhat for readability . ON THE RELATIONSHIP WITH OPENAI One criticism of OpenAI is that its very business is only possible via Microsoft , which has given the startup billions of dollars and access to the resources it needs to power its computing-intensive language model . Yet Microsoft is also highly dependent on OpenAI ’ s technology to power services like GitHub Copilot , Bing , and Office 365 . Altman even joked about the partnership onstage . We asked Nadella about this relationship . I ’ ve always felt that Microsoft is a platform-and-partner-first company , and this is not new to us . And so therefore , we both are effectively codependent , right ? They depend on us to build the best systems , we depend on them to build the best models , and we go to market together . Exclusive conversations that take us behind the scenes of a cultural phenomenon . ON HIS MISSION TO GET IN FRONT OF DEVELOPERS Nadella says this platform shift is different enough from previous ones that he feels the company needs to provide developers not only with tools , but also with a clear message about what it ’ s thinking and how devs can come along . Whenever you have a platform shift , the key thing is to make sure the platform is ubiquitously available for developers to build all kinds of new things . So to us , the most important task is to make the developer tools , the developer platforms , broadly available . The second thing is for us to also show the light , right ? Whether it ’ s OpenAI building ChatGPT and then innovating on top of it , or us building Copilot and innovating on it . That will give developers an opportunity to distribute their applications . So the most important thing in any platform creation is to get the platform ubiquitously available , and then help developers reach [ their ] audience . Those are the two goals that we have across all of these [ conferences ] . ON WHAT IS DIFFERENT ABOUT THIS SHIFT AND PRODUCTIVITY Productivity gains in the United States have been sluggish for the past 15 or more years . The last huge platform shift—the rise of mobile development—did little to achieve widespread prosperity . Nadella says this time will be different , largely because the shift to AI will fuel a creative revolution by making it easy for anyone to generate new work , including code . On the other hand , coding today is a highly skilled , well-paid job , and there ’ s some concern that AI could effectively automate it . Nadella argues that skilled programmers will remain in demand , but that their jobs will change and even more jobs will become available . Nadella has said he envisions 1 billion developers creating on its platforms , many of them with little to no previous experience with coding . Anytime you have something as disruptive as this , you have to think about the displacement and causes . And that means it ’ s all about upskilling and reskilling , and in an interesting way , it ’ s more akin to what happened when word processors and spreadsheets started showing up . Obviously , if you were a typist , it really drastically changed . But at the same time , it enabled a billion people to be able to type into word processors and create and share documents . I don ’ t think professional developers are going to be any less valuable than they are today . It ’ s just that we ’ re going to have many , many gradations of developers . Each time you ’ re prompting a Bing chat or ChatGPT , you ’ re essentially programming . The conversation itself is steering a model . I think there will be many , many new jobs , there will be many , many new types of knowledge work , or front-line work , where the drudgery is removed . I think the mobile era was fantastic . It made ubiquitous consumption of services . It didn ’ t translate into ubiquitous creation of services . The last time there was a broad spread of productivity in the United States and beyond because of information technology was the [ advent of the ] PC . In fact , even the critics of information technology and productivity , like Robert Gordon of Northwestern , acknowledged that the PC , when it first showed up at work , did actually translate to broad productivity stats changes . So that ’ s where I think this is , where these tools , like Copilot , [ are ] being used by a [ beginner ] software engineer in Detroit , in order to be able to write [ code ] ... I think we ’ ll have a real change in the productivity of the auto industry . Same thing in retail , same thing in front-line work and knowledge work . The barrier to entry is very low . Because it ’ s natural language , domain experts can build apps or workflows . That , I think , is what ’ s the most exciting thing about this . This is not about just a consumption-led thing . This is not about elite creation . This is about democratized creation . I ’ m very , very hopeful that we ’ ll start seeing the productivity gains much more broadly . ON PROTECTING DEVELOPERS Numerous intellectual property cases and class action lawsuits are before the US courts over issues of fair use . At least one singles out GitHub Copilot specifically , claiming Microsoft and OpenAI ’ s generative tools , which are trained on open source code , amount to software piracy . There ’ s a fear that people who use these tools could be subject to intellectual-property claims themselves . Microsoft is trying to address these issues with a broad indemnification policy . OpenAI also announced its own indemnification policy , Copyright Shield , at its DevDay conference . Fundamentally these large models crawl and get content and then train on that content , right ? If anybody doesn ’ t want their content to be crawled , we have great granular controls in our crawlers that allow anybody to stop it from crawling . In fact , we have controls where you can have it crawl just for search , but not for large-language-model training . That ’ s available today . So anybody who really wants to ensure that their content is not being taken for retraining can do so today . The second thing , of course , is I think the courts and the legislative process in some combination will have to decide what is fair use and what is not fair use . We have taken a lot of control in making sure that we are only training models , and we are using data to train models that we ’ re allowed to and which we believe we have a legal standing on . If it comes to it , we ’ ll litigate it in the courts . We ’ ll take that burden on so the users of our products don ’ t have to worry about it . That ’ s as simple as that , which is to take the liability and transfer it from our users to us . And of course , we are going to be very , very mindful of making sure we ’ re on the right side of the law there .","['last', 'week', 'favorite', 'surprise', 'visitor', 'company', 'firstever', 'event', 'developer', 'build', 'platform', 'nadella', 'bound', 'stage', 'join', 'blow', 'hair', 'back', 'already', 'electrify', 'audience', 'guy', 'build', 'magic', 'gush', 'day', 'later', 'stage', 'venue', 'developer', 'conference', 'nadella', 'make', 'second', 'unannounced', 'appearance', 'week', 'time', 'ceo', 'show', 'new', 'version', 'company', 'ai', 'programming', 'tool', 'copilot', 'generate', 'computer', 'code', 'natural', 'language', 'nadella', 'effusive', 'code', 'exclaim', 'today', 'nadella', 'onstage', 'speak', 'developer', 'company', 'announce', 'even', 'aibased', 'developer', 'tool', 'include', 'azure', 'ai', 'studio', 'let', 'devs', 'choose', 'model', 'catalog', 'also', 'like', 'meta', 'openai', 'hug', 'face', 'well', 'new', 'tool', 'customize', 'copilot', 'get', 'sense', 'nadella', 'obsess', 'developer', 'wrong', 'make', 'round', 'tout', 'way', 'use', 'new', 'generation', 'aipowere', 'tool', 'copilot', 'acquire', 'new', 'suite', 'developer', 'tool', 'company', 'reportedly', 'invest', 'last', 'week', 'take', 'break', 'onstage', 'appearance', 'sit', 'mit', 'technology', 'review', 'talk', 'guess', 'developer', 'repeatedly', 'emphasize', 'longstanding', 'focus', 'developer', 'also', 'message', 'way', 'create', 'software', 'fundamentally', 'change', 'nadella', 'believe', 'platform', 'shift', 'underway', 'one', 'prove', 'significant', 'shift', 'mainframe', 'desktop', 'desktop', 'mobile', 'time', 'transition', 'naturallanguage', 'tool', 'argue', 'lower', 'barrier', 'entry', 'software', 'development', 'make', 'exist', 'developer', 'productive', 'ultimately', 'lead', 'new', 'era', 'creativity', 'present', 'nadella', 'word', 'remark', 'edit', 'condense', 'somewhat', 'readability', 'relationship', 'criticism', 'business', 'possible', 'give', 'startup', 'billion', 'dollar', 'access', 'resource', 'need', 'power', 'computingintensive', 'language', 'model', 'yet', 'also', 'highly', 'dependent', 'technology', 'power', 'service', 'copilot', 'bing', 'office', 'even', 'joke', 'partnership', 'onstage', 'ask', 'nadella', 'relationship', 'always', 'feel', 'platformandpartnerfirst', 'company', 'new', 'therefore', 'effectively', 'codependent', 'right', 'depend', 'build', 'good', 'system', 'depend', 'build', 'good', 'model', 'go', 'market', 'together', 'exclusive', 'conversation', 'take', 'scene', 'cultural', 'phenomenon', 'mission', 'get', 'front', 'developer', 'say', 'platform', 'shift', 'different', 'enough', 'previous', 'one', 'feel', 'company', 'need', 'provide', 'developer', 'tool', 'also', 'clear', 'message', 'think', 'devs', 'come', 'platform', 'shift', 'key', 'thing', 'make', 'sure', 'platform', 'ubiquitously', 'available', 'developer', 'build', 'kind', 'new', 'thing', 'important', 'task', 'make', 'developer', 'tool', 'developer', 'platform', 'broadly', 'available', 'second', 'thing', 'also', 'show', 'light', 'right', 'openai', 'building', 'chatgpt', 'innovate', 'top', 'build', 'copilot', 'innovate', 'give', 'developer', 'opportunity', 'distribute', 'application', 'important', 'thing', 'platform', 'creation', 'get', 'platform', 'ubiquitously', 'available', 'help', 'developer', 'reach', 'audience', 'goal', 'conference', 'different', 'shift', 'productivity', 'productivity', 'gain', 'sluggish', 'past', 'year', 'last', 'huge', 'platform', 'shift', 'rise', 'mobile', 'development', 'little', 'achieve', 'widespread', 'prosperity', 'say', 'time', 'different', 'largely', 'shift', 'ai', 'fuel', 'creative', 'revolution', 'make', 'easy', 'generate', 'new', 'work', 'include', 'code', 'hand', 'code', 'today', 'highly', 'skilled', 'wellpaid', 'job', 'concern', 'effectively', 'automate', 'argue', 'skilled', 'programmer', 'remain', 'demand', 'job', 'change', 'even', 'job', 'become', 'available', 'nadella', 'say', 'envision', 'developer', 'create', 'platform', 'many', 'little', 'previous', 'experience', 'code', 'anytime', 'disruptive', 'think', 'displacement', 'cause', 'mean', 'upskille', 'reskille', 'interesting', 'way', 'akin', 'happen', 'word', 'processor', 'spreadsheet', 'start', 'show', 'obviously', 'typist', 'really', 'drastically', 'change', 'time', 'enable', 'people', 'able', 'type', 'word', 'processor', 'create', 'share', 'document', 'think', 'professional', 'developer', 'go', 'less', 'valuable', 'today', 'go', 'many', 'many', 'gradation', 'developer', 'time', 'prompt', 'bing', 'chat', 'chatgpt', 'essentially', 'program', 'conversation', 'steer', 'model', 'think', 'many', 'many', 'new', 'job', 'many', 'many', 'new', 'type', 'knowledge', 'work', 'frontline', 'work', 'drudgery', 'remove', 'think', 'mobile', 'era', 'fantastic', 'make', 'ubiquitous', 'consumption', 'service', 'translate', 'ubiquitous', 'creation', 'service', 'last', 'time', 'broad', 'spread', 'productivity', 'information', 'technology', 'advent', 'pc', 'fact', 'even', 'critic', 'information', 'technology', 'productivity', 'acknowledge', 'pc', 'first', 'show', 'work', 'actually', 'translate', 'broad', 'productivity', 'stat', 'change', 'think', 'tool', 'copilot', 'use', 'beginner', 'software', 'engineer', 'order', 'able', 'write', 'code', 'think', 'real', 'change', 'productivity', 'auto', 'industry', 'thing', 'retail', 'thing', 'frontline', 'work', 'knowledge', 'work', 'barrier', 'entry', 'low', 'natural', 'language', 'domain', 'expert', 'build', 'app', 'workflow', 'think', 'exciting', 'thing', 'consumptionle', 'thing', 'elite', 'creation', 'democratized', 'creation', 'hopeful', 'start', 'see', 'productivity', 'gain', 'much', 'broadly', 'protect', 'developer', 'numerous', 'intellectual', 'property', 'case', 'class', 'action', 'lawsuit', 'court', 'issue', 'fair', 'use', 'least', 'single', 'copilot', 'specifically', 'claim', 'generative', 'tool', 'train', 'open', 'source', 'code', 'amount', 'software', 'piracy', 'fear', 'people', 'use', 'tool', 'subject', 'intellectualproperty', 'claim', 'try', 'address', 'issue', 'broad', 'indemnification', 'policy', 'also', 'announce', 'indemnification', 'policy', 'copyright', 'shield', 'devday', 'conference', 'fundamentally', 'large', 'model', 'crawl', 'get', 'content', 'train', 'content', 'right', 'want', 'content', 'crawl', 'great', 'granular', 'control', 'crawler', 'allow', 'stop', 'crawl', 'fact', 'control', 'crawl', 'search', 'largelanguagemodel', 'training', 'available', 'today', 'really', 'want', 'ensure', 'content', 'take', 'retraining', 'today', 'second', 'thing', 'course', 'think', 'court', 'legislative', 'process', 'combination', 'decide', 'fair', 'use', 'fair', 'use', 'take', 'lot', 'control', 'make', 'sure', 'training', 'model', 'use', 'datum', 'train', 'model', 'allow', 'believe', 'legal', 'standing', 'come', 'litigate', 'court', 'take', 'burden', 'user', 'product', 'worry', 'simple', 'take', 'liability', 'transfer', 'user', 'course', 'go', 'mindful', 'make', 'sure', 'right', 'side', 'law']","<p>In an exclusive interview with MIT Technology Review, Nadella shares his view of the platform shift for developers.</p>
"
Google DeepMind’s weather AI can forecast extreme weather faster and more accurately,https://www.technologyreview.com/2023/11/14/1083366/google-deepminds-weather-ai-can-forecast-extreme-weather-quicker-and-more-accurately/,2023-11-14,"This year the Earth has been hit by a record number of unpredictable extreme weather events made worse by climate change. Predicting them faster and with greater accuracy could enable us to prepare better for natural disasters and help save lives. A new AI model from Google DeepMind could make that easier.  In research published in Science today, Google DeepMind’s model, GraphCast, was able to predict weather conditions up to 10 days in advance, more accurately and much faster than the current gold standard. GraphCast outperformed the model from the European Centre for Medium-Range Weather Forecasts (ECMWF) in more than 90% of over 1,300 test areas. And on predictions for Earth’s troposphere—the lowest part of the atmosphere, where most weather happens—GraphCast outperformed the ECMWF’s model on more than 99% of weather variables, such as rain and air temperature  Crucially, GraphCast can also offer meteorologists accurate warnings, much earlier than standard models, of conditions such as extreme temperatures and the paths of cyclones. In September, GraphCast accurately predicted that Hurricane Lee would make landfall in Nova Scotia nine days in advance, says Rémi Lam, a staff research scientist at Google DeepMind. Traditional weather forecasting models pinpointed the hurricane to Nova Scotia only six days in advance.  They could also help to make them more accurate. “Weather prediction is one of the most challenging problems that humanity has been working on for a long, long time. And if you look at what has happened in the last few years with climate change, this is an incredibly important problem,” says Pushmeet Kohli, the vice president of research at Google DeepMind.   Traditionally, meteorologists use massive computer simulations to make weather predictions. They are very energy intensive and  time consuming to run, because the simulations take into account many physics-based equations and different weather variables such as temperature, precipitation, pressure, wind, humidity, and cloudiness, one by one.  GraphCast uses machine learning to do these calculations in under a minute. Instead of using the physics-based equations, it bases its predictions on four decades of historical weather data. GraphCast uses graph neural networks, which map Earth’s surface into more than a million grid points. At each grid point, the model predicts the temperature, wind speed and direction, and mean sea-level pressure, as well as other conditions like humidity. The neural network is then able to find patterns and draw conclusions about what will happen next for each of these data points.  For the past year, weather forecasting has been going through a revolution as models such as GraphCast, Huawei’s Pangu-Weather and Nvidia’s FourcastNet have made meteorologists rethink the role AI can play in weather forecasting. GraphCast improves on the performance of other competing models, such as Pangu-Weather, and is able to predict more weather variables, says Lam. The ECMWF is already using it. When Google DeepMind first debuted GraphCast last December, it felt like Christmas, says Peter Dueben, head of Earth system modeling at ECMWF, who was not involved in the research.  “It showed that these models are so good that we cannot avoid them anymore,” he says.  GraphCast is a “reckoning moment” for weather prediction because it shows that predictions can be made using historical data, says Aditya Grover, an assistant professor of computer science at UCLA, who developed ClimaX, a foundation model that allows researchers to do different tasks relating to modeling the Earth’s weather and climate.  Plus: AI-text detection tools are really easy to fool. DeepMind's model is “great work and extremely exciting,” says Oliver Fuhrer, the head of the numerical prediction department at MeteoSwiss, the Swiss Federal Office of Meteorology and Climatology. Fuhrer says that other weather agencies, such as the ECMWF and the Swedish Meteorological and Hydrological Institute, have also used the graph neural network architecture proposed by Google DeepMind to build their own models.  But GraphCast is not perfect. It still lags behind conventional weather forecasting models in some areas, such as precipitation, Dueben says. Meteorologists will still have to use conventional models alongside machine-learning models to offer better predictions.  Google DeepMind is also making GraphCast open source. This is a good development, says UCLA’s Grover.  “With climate change on the rise, it's very important that big organizations, which have had the luxury of so much compute, also think about giving back [to the scientific community],” he says.  ","This year the Earth has been hit by a record number of unpredictable extreme weather events made worse by climate change . Predicting them faster and with greater accuracy could enable us to prepare better for natural disasters and help save lives . A new AI model from Google DeepMind could make that easier . In research published in Science today , Google DeepMind ’ s model , GraphCast , was able to predict weather conditions up to 10 days in advance , more accurately and much faster than the current gold standard . GraphCast outperformed the model from the European Centre for Medium-Range Weather Forecasts ( ECMWF ) in more than 90 % of over 1,300 test areas . And on predictions for Earth ’ s troposphere—the lowest part of the atmosphere , where most weather happens—GraphCast outperformed the ECMWF ’ s model on more than 99 % of weather variables , such as rain and air temperature Crucially , GraphCast can also offer meteorologists accurate warnings , much earlier than standard models , of conditions such as extreme temperatures and the paths of cyclones . In September , GraphCast accurately predicted that Hurricane Lee would make landfall in Nova Scotia nine days in advance , says Rémi Lam , a staff research scientist at Google DeepMind . Traditional weather forecasting models pinpointed the hurricane to Nova Scotia only six days in advance . They could also help to make them more accurate . “ Weather prediction is one of the most challenging problems that humanity has been working on for a long , long time . And if you look at what has happened in the last few years with climate change , this is an incredibly important problem , ” says Pushmeet Kohli , the vice president of research at Google DeepMind . Traditionally , meteorologists use massive computer simulations to make weather predictions . They are very energy intensive and time consuming to run , because the simulations take into account many physics-based equations and different weather variables such as temperature , precipitation , pressure , wind , humidity , and cloudiness , one by one . GraphCast uses machine learning to do these calculations in under a minute . Instead of using the physics-based equations , it bases its predictions on four decades of historical weather data . GraphCast uses graph neural networks , which map Earth ’ s surface into more than a million grid points . At each grid point , the model predicts the temperature , wind speed and direction , and mean sea-level pressure , as well as other conditions like humidity . The neural network is then able to find patterns and draw conclusions about what will happen next for each of these data points . For the past year , weather forecasting has been going through a revolution as models such as GraphCast , Huawei ’ s Pangu-Weather and Nvidia ’ s FourcastNet have made meteorologists rethink the role AI can play in weather forecasting . GraphCast improves on the performance of other competing models , such as Pangu-Weather , and is able to predict more weather variables , says Lam . The ECMWF is already using it . When Google DeepMind first debuted GraphCast last December , it felt like Christmas , says Peter Dueben , head of Earth system modeling at ECMWF , who was not involved in the research . “ It showed that these models are so good that we can not avoid them anymore , ” he says . GraphCast is a “ reckoning moment ” for weather prediction because it shows that predictions can be made using historical data , says Aditya Grover , an assistant professor of computer science at UCLA , who developed ClimaX , a foundation model that allows researchers to do different tasks relating to modeling the Earth ’ s weather and climate . Plus : AI-text detection tools are really easy to fool . DeepMind 's model is “ great work and extremely exciting , ” says Oliver Fuhrer , the head of the numerical prediction department at MeteoSwiss , the Swiss Federal Office of Meteorology and Climatology . Fuhrer says that other weather agencies , such as the ECMWF and the Swedish Meteorological and Hydrological Institute , have also used the graph neural network architecture proposed by Google DeepMind to build their own models . But GraphCast is not perfect . It still lags behind conventional weather forecasting models in some areas , such as precipitation , Dueben says . Meteorologists will still have to use conventional models alongside machine-learning models to offer better predictions . Google DeepMind is also making GraphCast open source . This is a good development , says UCLA ’ s Grover . “ With climate change on the rise , it 's very important that big organizations , which have had the luxury of so much compute , also think about giving back [ to the scientific community ] , ” he says .","['year', 'earth', 'hit', 'record', 'number', 'unpredictable', 'extreme', 'weather', 'event', 'make', 'bad', 'climate', 'change', 'predict', 'fast', 'great', 'accuracy', 'enable', 'prepare', 'well', 'natural', 'disaster', 'help', 'save', 'life', 'new', 'model', 'make', 'easy', 'research', 'publish', 'science', 'today', 'model', 'graphcast', 'able', 'predict', 'weather', 'condition', 'day', 'advance', 'accurately', 'much', 'fast', 'current', 'gold', 'standard', 'graphcast', 'outperform', 'model', 'weather', 'forecast', 'ecmwf', 'test', 'area', 'prediction', 'earth', 'troposphere', 'low', 'part', 'atmosphere', 'weather', 'happen', 'graphcast', 'outperform', 'ecmwf', 'model', 'weather', 'variable', 'rain', 'air', 'temperature', 'crucially', 'graphcast', 'also', 'offer', 'meteorologist', 'accurate', 'warning', 'much', 'early', 'standard', 'model', 'condition', 'extreme', 'temperature', 'path', 'cyclone', 'graphcast', 'accurately', 'predict', 'make', 'landfall', 'day', 'advance', 'say', 'staff', 'research', 'scientist', 'traditional', 'weather', 'forecasting', 'model', 'pinpoint', 'hurricane', 'day', 'advance', 'also', 'help', 'make', 'accurate', 'weather', 'prediction', 'challenging', 'problem', 'humanity', 'work', 'long', 'long', 'time', 'look', 'happen', 'last', 'year', 'climate', 'change', 'incredibly', 'important', 'problem', 'say', 'vice', 'president', 'research', 'traditionally', 'meteorologist', 'use', 'massive', 'computer', 'simulation', 'make', 'weather', 'prediction', 'energy', 'intensive', 'time', 'consume', 'run', 'simulation', 'take', 'account', 'many', 'physicsbase', 'equation', 'different', 'weather', 'variable', 'temperature', 'precipitation', 'pressure', 'wind', 'humidity', 'cloudiness', 'graphcast', 'use', 'machine', 'learn', 'calculation', 'minute', 'instead', 'use', 'physicsbase', 'equation', 'base', 'prediction', 'decade', 'historical', 'weather', 'datum', 'graphcast', 'use', 'graph', 'neural', 'network', 'map', 'earth', 'surface', 'grid', 'point', 'grid', 'point', 'model', 'predict', 'temperature', 'wind', 'speed', 'direction', 'mean', 'sealevel', 'pressure', 'well', 'condition', 'humidity', 'neural', 'network', 'able', 'find', 'pattern', 'draw', 'conclusion', 'happen', 'next', 'datum', 'point', 'past', 'year', 'weather', 'forecasting', 'go', 'revolution', 'model', 'graphcast', 'huawei', 'panguweather', 'fourcastnet', 'make', 'meteorologist', 'rethink', 'role', 'play', 'weather', 'forecasting', 'graphcast', 'improve', 'performance', 'compete', 'model', 'panguweather', 'able', 'predict', 'weather', 'variable', 'say', 'ecmwf', 'already', 'use', 'first', 'debut', 'graphcast', 'last', 'feel', 'say', 'head', 'earth', 'system', 'model', 'ecmwf', 'involve', 'research', 'show', 'model', 'good', 'avoid', 'anymore', 'say', 'graphcast', 'reckoning', 'moment', 'weather', 'prediction', 'show', 'prediction', 'make', 'use', 'historical', 'datum', 'say', 'assistant', 'professor', 'computer', 'science', 'develop', 'climax', 'foundation', 'model', 'allow', 'researcher', 'different', 'task', 'relate', 'model', 'earth', 'weather', 'climate', 'aitext', 'detection', 'tool', 'really', 'easy', 'fool', 'deepmind', 'model', 'great', 'work', 'extremely', 'exciting', 'say', 'head', 'swiss', 'federal', 'office', 'meteorology', 'climatology', 'fuhrer', 'say', 'weather', 'agency', 'ecmwf', 'swedish', 'meteorological', 'hydrological', 'institute', 'also', 'use', 'graph', 'neural', 'network', 'architecture', 'propose', 'build', 'model', 'graphcast', 'perfect', 'still', 'lag', 'conventional', 'weather', 'forecasting', 'model', 'area', 'precipitation', 'say', 'meteorologist', 'still', 'use', 'conventional', 'model', 'machinelearning', 'model', 'offer', 'well', 'prediction', 'deepmind', 'also', 'make', 'graphcast', 'open', 'source', 'good', 'development', 'say', 'grover', 'climate', 'change', 'rise', 'important', 'big', 'organization', 'luxury', 'much', 'compute', 'also', 'think', 'give', 'scientific', 'community', 'say']","<p>It said Hurricane Lee would make landfall in Nova Scotia three days sooner than traditional methods predicted.</p>
"
How Facebook went all in on AI,https://www.technologyreview.com/2023/11/14/1083336/how-facebook-went-all-in-on-ai/,2023-11-14,"The following is excerpted from BROKEN CODE: Inside Facebook and the Fight to Expose Its Harmful Secrets by Jeff Horwitz. Reprinted by permission of Doubleday, an imprint of The Knopf Doubleday Publishing Group, a division of Penguin Random House LLC. Copyright © 2023 by Jeff Horwitz. In 2006, the U.S. patent office received a filing for “an automatically generated display that contains information relevant to a user about another user of a social network.” Rather than forcing people to search through “disparate and disorganized” content for items of interest, the system would seek to generate a list of “relevant” information in a “preferred order.” The listed authors were “Zuckerberg et al.” and the product was the News Feed. The idea of showing users streams of activity wasn’t entirely new—­ photo-­sharing website Flickr and others had been experimenting with it—­ but the change was massive. Before, Facebook users would interact with the site mainly via notifications, pokes, or looking up friends’ profiles. With the launch of the News Feed, users got a constantly updating stream of posts and status changes. The shift came as a shock to what were Facebook’s then 10 million users, who did not appreciate their activities being monitored and their once-­ static profiles mined for updated content. In the face of widespread complaints, Zuckerberg wrote a post reassuring users, “Nothing you do is being broadcast; rather, it is being shared with people who care about what you do—­ your friends.” He titled it: “Calm down. Breathe. We hear you.” Hearing user complaints wasn’t the same thing as listening to them. As Chris Cox would later note at a press event, News Feed was an instant success at boosting activity on the platform and connecting users. Engagement quickly doubled, and within two weeks of launch more than a million members had affiliated themselves with a single interest for the first time. The cause that had united so many people? A petition to eradicate the “stalkeresque” News Feed. The opaque system that users revolted against was, in hindsight, remarkably simple. Content mostly appeared in reverse chronological order, with manual adjustments made to ensure that people saw both popular posts and a range of material. “In the beginning, News Feed ranking was turning knobs,” Cox said. Fiddling with dials worked well enough for a little while, but everyone’s friend lists were growing and Facebook was introducing new features such as ads, pages, and interest groups. As entertainment, memes, and commerce began to compete with posts from friends in News Feed, Facebook needed to ensure that a user who had just logged on would see their best friend’s engagement photos ahead of a cooking page’s popular enchilada recipe. The first effort at sorting, eventually branded “EdgeRank,” was a simple formula that prioritized content according to three principal factors: a post’s age, the amount of engagement it got, and the interconnection between user and poster. As an algorithm, it wasn’t much—­ just a rough attempt to translate the questions “Is it new, popular, or from someone you care about?” into math.  There was no dark magic at play, but users again revolted against the idea of Facebook putting its thumb on what they saw. And, again, Facebook usage metrics jumped across the board. The platform’s recommendation systems were still in their infancy, but the dissonance between users’ vocal disapproval and avid usage led to an inescapable conclusion inside the company: regular people’s opinions about Facebook’s mechanics were best ignored. Users screamed “stop,” Facebook kept going, and everything would work out dandy. By 2010, the company was looking to move beyond EdgeRank’s crude formula to recommend content based on machine learning, a branch of artificial intelligence focused on training computers to design their own decision-­ making algorithms. Rather than programming Facebook’s computers to rank content according to simple math, engineers would program them to analyze user behavior and design their own ranking formulas. What people saw would be the result of constant experimentation, the platform serving up whatever it predicted was most likely to generate a like from a user and evaluating its own results in real time. Despite the growing complexity of its product and the collection of user data at a scale the world had never seen, Facebook still didn’t know enough about its users to show them relevant ads. Brands loved the attention and buzz they could get from creating content on Facebook, but they hadn’t found the company’s paid offerings compelling. In May 2012, General Motors killed its entire Facebook advertising budget. A prominent digital advertising executive declared Facebook ads “fundamentally some of the worst performing ad units on the Web.” Fixing the problem would fall to a team run by Joaquin Quiñonero Candela. A Spaniard who grew up in Morocco, Quiñonero was living in the UK and working on artificial intelligence at Microsoft in 2011 when friends scattered across Northern Africa began talking excitedly about social media–­ driven protests. The machine learning techniques he was using to optimize Bing search ads had clear applications to the social networks that people had used to overthrow four autocratic states and nearly topple several more. “I joined Facebook because of the Arab Spring,” Quiñonero said. Quiñonero found that the way Facebook built its products was nearly as revolutionary as their results. Invited by a friend to tour the Menlo Park campus, he was shocked to look over the shoulder of an engineer making a significant but unsupervised update to Facebook’s code. Confirming how much faster the company moved than Microsoft, Quiñonero received a Facebook job offer a week later. Quiñonero began working on ads, and his timing could hardly have been better. Advances in machine learning and raw computing speed allowed the platform to not only pigeonhole users into demographic niches (“single heterosexual woman in San Francisco, late twenties, interested in camping and salsa dancing”) but to spot correlations between what they clicked on and then use that information to guess which ads they would find relevant. After beginning with near- random guesses on how to maximize the odds of a click, the system would learn from its hits and misses, refining its model for predicting which ads had the best shot at success. It was hardly omniscient—­ recommended ads were regularly inexplicable. But the bar for success in digital advertising was low: if 2 percent of users clicked on an ad, that was a triumph. With billions of ads served each day, algorithm tweaks that produced even modest gains could bring in tens or hundreds of millions of dollars in revenue. And Quiñonero’s team found that it could churn out those alterations. “I told my team to go fast, to ship every week,” he said.  Consumers are getting their first samplings of meat made in the lab. The rapid pace made sense. The team’s AI was improving not just revenue but how people felt about the platform. Better-­ targeted ads meant Facebook could make more money per user without increasing the ad load, and there wasn’t all that much that could go wrong. When Facebook pitched denture cream to teenagers, nobody died.  Advertising was the beachhead for machine learning at Facebook, and soon everyone wanted a piece of the action. For product executives tasked with increasing the number of Facebook groups joined, friends added, and posts made, the appeal was obvious. If Quiñonero’s techniques could increase how often users engaged with ads, they could increase how often users engaged with everything else on the platform. Every team responsible for ranking or recommending content rushed to overhaul their systems as fast as they could, setting off an explosion in the complexity of Facebook’s product. Employees found that the biggest gains often came not from deliberate initiatives but from simple futzing around. Rather than redesigning algorithms, which was slow, engineers were scoring big with quick and dirty machine learning experiments that amounted to throwing hundreds of variants of existing algorithms at the wall and seeing which versions stuck—­ which performed best with users. They wouldn’t necessarily know why a variable mattered or how one algorithm outperformed another at, say, predicting the likelihood of commenting. But they could keep fiddling until the machine learning model produced an algorithm that statistically outperformed the existing one, and that was good enough. It would be hard to conceive of an approach to building systems that more embodied the slogan “Move Fast and Break Things.” Facebook wanted only more. Zuckerberg wooed Yann LeCun, a French computer scientist specializing in deep learning, meaning the construction of computer systems capable of processing information in ways inspired by human thinking. Already renowned for creating the foundational AI techniques that made facial recognition possible, LeCun was put in charge of a division that aimed to put Facebook at the vanguard of fundamental research into artificial intelligence. Following his success with ads, Quiñonero was given an equally formidable task: pushing machine learning into the company’s bloodstream as fast as possible. His initial staff of two dozen—­ the team responsible for building new core machine learning tools and making them available to other parts of the company—­ had grown in the three years since he’d been hired. But it was still nowhere near large enough to assist every product team that wanted machine learning help. The skills to build a model from scratch were too specialized for engineers to readily pick up, and you couldn’t increase the supply of machine learning PhDs by throwing money around. The solution was to build FB Learner, a sort of “paint by numbers” version of machine learning. It packaged techniques into a template that could be used by engineers who quite literally did not understand what they were doing. FB Learner did for machine learning inside Facebook what services like WordPress had once done for building websites, rendering the need to muck around with HTML or configure a server unnecessary. Rather than setting up a blog, however, the engineers in question were messing with the guts of what was rapidly becoming a preeminent global communications platform. Many at Facebook were aware of the increasing concerns around AI outside the company’s walls. Poorly designed algorithms meant to reward good healthcare penalized hospitals that treated sicker patients, and models purporting to quantify a parole candidate’s risk of reoffending turned out to be biased in favor of keeping Black people in jail. But these issues seemed remote on a social network. An avid user of FB Learner would later describe machine learning’s mass diffusion inside Facebook as “giving rocket launchers to twenty-­ five-­ year-­ old engineers.” But at the time, Quiñonero and the company spoke of it as a triumph. The company’s AI algorithms gave it an insatiable habit for lies and hate speech. Now the man who built them can't fix the problem. “Engineers and teams, even with little expertise, can build and run experiments with ease and deploy AI-­ powered products to production faster than ever,” Facebook announced in 2016, boasting that FB Learner was ingesting trillions of data points on user behavior every day and that engineers were running 500,000 experiments on them a month. The sheer amount of data that Facebook collected—­ and ad-targeting results so good that users regularly suspected (wrongly) the company of eavesdropping on their offline conversations—gave rise to the claim that “Facebook knows everything about you.” That wasn’t quite correct. The wonders of machine learning had obscured its limits. Facebook’s recommendation systems worked by raw correlation between user behavior, not by identifying a user’s tastes and interests and then serving content based on it. News Feed couldn’t tell you whether you liked ice skating or dirt biking, hip-­ hop or K-­ pop, and it couldn’t explain in human terms why one post appeared in your feed above another. Although this inexplicability was an obvious drawback, machine learning–­ based recommendation systems spoke to Zuckerberg’s deep faith in data, code, and personalization. Freed from human limitation, error, and bias, Facebook’s algorithms were capable, he believed, of unparalleled objectivity—­ and, perhaps more important, efficiency. A separate strain of machine learning work was devoted to figuring out what content was actually in the posts Facebook recommended. Known as classifiers, these were AI systems trained to perform pattern recognition on vast data sets. Years before Facebook’s creation, classifiers had proven themselves indispensable in the fight against spam, allowing email providers to move beyond simple keyword filters that sought to block mass emails about, say, “Vi@gra.” By ingesting and comparing a huge collection of emails—some labeled as spam, some as not spam—­ a machine learning system could develop its own rubric for distinguishing between them. Once this classifier was “trained,” it would be set loose, analyzing incoming email and predicting the probability that each message should be sent to an inbox, a junk folder, or straight to hell. By the time machine learning experts began to arrive at Facebook, the list of questions that classifiers sought to answer had grown well past “Is it spam?,” thanks in large part to people like LeCun. Zuckerberg was bullish on its future progress and its applications for Facebook. By 2016, he was predicting that classifiers would surpass human capacities of perception, recognition, and comprehension within the next five to ten years, allowing the company to shut down misbehavior and make huge leaps in connecting the world. That prediction would prove more than a little optimistic. Even as techniques improved, data sets grew, and processing sped up, one drawback of machine learning persisted. The algorithms that the company produced stubbornly refused to explain themselves. Engineers could evaluate a classifier’s success by testing it to see what percentage of its judgment calls were accurate (its “precision”) and what portion of a thing it detected (its “recall”). But because the system was teaching itself how to identify something based on a logic of its own design, when it erred, there was no human-­cognizable reason why. Sometimes mistakes would seem nonsensical. Other times they would be systematic in ways that reflected human error. Early in Facebook’s efforts to deploy a classifier to detect pornography, Arturo Bejar recalled, the system routinely tried to cull images of beds. Rather than learning to identify people screwing, the model had instead taught itself to recognize the furniture on which they most often did. The problem had an easy fix: engineers simply needed to train the model with more PG-­ rated mattress scenes. It made for a good joke—­ as long as you didn’t consider that the form of machine learning that the engineers had just screwed up was one of the most basic that Facebook was using. Similarly fundamental errors kept occurring, even as the company came to rely on far more advanced AI techniques to make far weightier and complex decisions than “porn/not porn.” The company was going all in on AI, both to determine what people should see, and also to solve any problems that might arise. There was no question that the computer science was dazzling and the gains concrete. But the speed, breadth, and scale of Face- book’s adoption of machine learning came at the cost of comprehensibility. Why did Facebook’s “Pages You Might Like” algorithm seem so focused on recommending certain topics? How had a video snippet from a computer animation about dental implants ended up being seen a hundred million times? And why did some news publishers consistently achieve virality when they just rewrote other outlets’ stories? Faced with these questions, Facebook’s Communications team would note that the company’s systems responded to people’s behavior and that there was no accounting for taste. These were difficult points to refute. They also obscured an uncomfortable fact: Facebook was achieving its growth in ways it didn’t fully understand. Within five years of announcing that it was beginning to use machine learning to recommend content and target ads, Facebook’s systems would rely so heavily on AI capable of training itself that, without the technology, Yann LeCun proudly declared, all that would be left of the company’s products would be “dust.” ","The following is excerpted from BROKEN CODE : Inside Facebook and the Fight to Expose Its Harmful Secrets by Jeff Horwitz . Reprinted by permission of Doubleday , an imprint of The Knopf Doubleday Publishing Group , a division of Penguin Random House LLC . Copyright © 2023 by Jeff Horwitz . In 2006 , the U.S. patent office received a filing for “ an automatically generated display that contains information relevant to a user about another user of a social network. ” Rather than forcing people to search through “ disparate and disorganized ” content for items of interest , the system would seek to generate a list of “ relevant ” information in a “ preferred order. ” The listed authors were “ Zuckerberg et al. ” and the product was the News Feed . The idea of showing users streams of activity wasn ’ t entirely new—­ photo-­sharing website Flickr and others had been experimenting with it—­ but the change was massive . Before , Facebook users would interact with the site mainly via notifications , pokes , or looking up friends ’ profiles . With the launch of the News Feed , users got a constantly updating stream of posts and status changes . The shift came as a shock to what were Facebook ’ s then 10 million users , who did not appreciate their activities being monitored and their once-­ static profiles mined for updated content . In the face of widespread complaints , Zuckerberg wrote a post reassuring users , “ Nothing you do is being broadcast ; rather , it is being shared with people who care about what you do—­ your friends. ” He titled it : “ Calm down . Breathe . We hear you. ” Hearing user complaints wasn ’ t the same thing as listening to them . As Chris Cox would later note at a press event , News Feed was an instant success at boosting activity on the platform and connecting users . Engagement quickly doubled , and within two weeks of launch more than a million members had affiliated themselves with a single interest for the first time . The cause that had united so many people ? A petition to eradicate the “ stalkeresque ” News Feed . The opaque system that users revolted against was , in hindsight , remarkably simple . Content mostly appeared in reverse chronological order , with manual adjustments made to ensure that people saw both popular posts and a range of material . “ In the beginning , News Feed ranking was turning knobs , ” Cox said . Fiddling with dials worked well enough for a little while , but everyone ’ s friend lists were growing and Facebook was introducing new features such as ads , pages , and interest groups . As entertainment , memes , and commerce began to compete with posts from friends in News Feed , Facebook needed to ensure that a user who had just logged on would see their best friend ’ s engagement photos ahead of a cooking page ’ s popular enchilada recipe . The first effort at sorting , eventually branded “ EdgeRank , ” was a simple formula that prioritized content according to three principal factors : a post ’ s age , the amount of engagement it got , and the interconnection between user and poster . As an algorithm , it wasn ’ t much—­ just a rough attempt to translate the questions “ Is it new , popular , or from someone you care about ? ” into math . There was no dark magic at play , but users again revolted against the idea of Facebook putting its thumb on what they saw . And , again , Facebook usage metrics jumped across the board . The platform ’ s recommendation systems were still in their infancy , but the dissonance between users ’ vocal disapproval and avid usage led to an inescapable conclusion inside the company : regular people ’ s opinions about Facebook ’ s mechanics were best ignored . Users screamed “ stop , ” Facebook kept going , and everything would work out dandy . By 2010 , the company was looking to move beyond EdgeRank ’ s crude formula to recommend content based on machine learning , a branch of artificial intelligence focused on training computers to design their own decision-­ making algorithms . Rather than programming Facebook ’ s computers to rank content according to simple math , engineers would program them to analyze user behavior and design their own ranking formulas . What people saw would be the result of constant experimentation , the platform serving up whatever it predicted was most likely to generate a like from a user and evaluating its own results in real time . Despite the growing complexity of its product and the collection of user data at a scale the world had never seen , Facebook still didn ’ t know enough about its users to show them relevant ads . Brands loved the attention and buzz they could get from creating content on Facebook , but they hadn ’ t found the company ’ s paid offerings compelling . In May 2012 , General Motors killed its entire Facebook advertising budget . A prominent digital advertising executive declared Facebook ads “ fundamentally some of the worst performing ad units on the Web. ” Fixing the problem would fall to a team run by Joaquin Quiñonero Candela . A Spaniard who grew up in Morocco , Quiñonero was living in the UK and working on artificial intelligence at Microsoft in 2011 when friends scattered across Northern Africa began talking excitedly about social media–­ driven protests . The machine learning techniques he was using to optimize Bing search ads had clear applications to the social networks that people had used to overthrow four autocratic states and nearly topple several more . “ I joined Facebook because of the Arab Spring , ” Quiñonero said . Quiñonero found that the way Facebook built its products was nearly as revolutionary as their results . Invited by a friend to tour the Menlo Park campus , he was shocked to look over the shoulder of an engineer making a significant but unsupervised update to Facebook ’ s code . Confirming how much faster the company moved than Microsoft , Quiñonero received a Facebook job offer a week later . Quiñonero began working on ads , and his timing could hardly have been better . Advances in machine learning and raw computing speed allowed the platform to not only pigeonhole users into demographic niches ( “ single heterosexual woman in San Francisco , late twenties , interested in camping and salsa dancing ” ) but to spot correlations between what they clicked on and then use that information to guess which ads they would find relevant . After beginning with near- random guesses on how to maximize the odds of a click , the system would learn from its hits and misses , refining its model for predicting which ads had the best shot at success . It was hardly omniscient—­ recommended ads were regularly inexplicable . But the bar for success in digital advertising was low : if 2 percent of users clicked on an ad , that was a triumph . With billions of ads served each day , algorithm tweaks that produced even modest gains could bring in tens or hundreds of millions of dollars in revenue . And Quiñonero ’ s team found that it could churn out those alterations . “ I told my team to go fast , to ship every week , ” he said . Consumers are getting their first samplings of meat made in the lab . The rapid pace made sense . The team ’ s AI was improving not just revenue but how people felt about the platform . Better-­ targeted ads meant Facebook could make more money per user without increasing the ad load , and there wasn ’ t all that much that could go wrong . When Facebook pitched denture cream to teenagers , nobody died . Advertising was the beachhead for machine learning at Facebook , and soon everyone wanted a piece of the action . For product executives tasked with increasing the number of Facebook groups joined , friends added , and posts made , the appeal was obvious . If Quiñonero ’ s techniques could increase how often users engaged with ads , they could increase how often users engaged with everything else on the platform . Every team responsible for ranking or recommending content rushed to overhaul their systems as fast as they could , setting off an explosion in the complexity of Facebook ’ s product . Employees found that the biggest gains often came not from deliberate initiatives but from simple futzing around . Rather than redesigning algorithms , which was slow , engineers were scoring big with quick and dirty machine learning experiments that amounted to throwing hundreds of variants of existing algorithms at the wall and seeing which versions stuck—­ which performed best with users . They wouldn ’ t necessarily know why a variable mattered or how one algorithm outperformed another at , say , predicting the likelihood of commenting . But they could keep fiddling until the machine learning model produced an algorithm that statistically outperformed the existing one , and that was good enough . It would be hard to conceive of an approach to building systems that more embodied the slogan “ Move Fast and Break Things. ” Facebook wanted only more . Zuckerberg wooed Yann LeCun , a French computer scientist specializing in deep learning , meaning the construction of computer systems capable of processing information in ways inspired by human thinking . Already renowned for creating the foundational AI techniques that made facial recognition possible , LeCun was put in charge of a division that aimed to put Facebook at the vanguard of fundamental research into artificial intelligence . Following his success with ads , Quiñonero was given an equally formidable task : pushing machine learning into the company ’ s bloodstream as fast as possible . His initial staff of two dozen—­ the team responsible for building new core machine learning tools and making them available to other parts of the company—­ had grown in the three years since he ’ d been hired . But it was still nowhere near large enough to assist every product team that wanted machine learning help . The skills to build a model from scratch were too specialized for engineers to readily pick up , and you couldn ’ t increase the supply of machine learning PhDs by throwing money around . The solution was to build FB Learner , a sort of “ paint by numbers ” version of machine learning . It packaged techniques into a template that could be used by engineers who quite literally did not understand what they were doing . FB Learner did for machine learning inside Facebook what services like WordPress had once done for building websites , rendering the need to muck around with HTML or configure a server unnecessary . Rather than setting up a blog , however , the engineers in question were messing with the guts of what was rapidly becoming a preeminent global communications platform . Many at Facebook were aware of the increasing concerns around AI outside the company ’ s walls . Poorly designed algorithms meant to reward good healthcare penalized hospitals that treated sicker patients , and models purporting to quantify a parole candidate ’ s risk of reoffending turned out to be biased in favor of keeping Black people in jail . But these issues seemed remote on a social network . An avid user of FB Learner would later describe machine learning ’ s mass diffusion inside Facebook as “ giving rocket launchers to twenty-­ five-­ year-­ old engineers. ” But at the time , Quiñonero and the company spoke of it as a triumph . The company ’ s AI algorithms gave it an insatiable habit for lies and hate speech . Now the man who built them ca n't fix the problem . “ Engineers and teams , even with little expertise , can build and run experiments with ease and deploy AI-­ powered products to production faster than ever , ” Facebook announced in 2016 , boasting that FB Learner was ingesting trillions of data points on user behavior every day and that engineers were running 500,000 experiments on them a month . The sheer amount of data that Facebook collected—­ and ad-targeting results so good that users regularly suspected ( wrongly ) the company of eavesdropping on their offline conversations—gave rise to the claim that “ Facebook knows everything about you. ” That wasn ’ t quite correct . The wonders of machine learning had obscured its limits . Facebook ’ s recommendation systems worked by raw correlation between user behavior , not by identifying a user ’ s tastes and interests and then serving content based on it . News Feed couldn ’ t tell you whether you liked ice skating or dirt biking , hip-­ hop or K-­ pop , and it couldn ’ t explain in human terms why one post appeared in your feed above another . Although this inexplicability was an obvious drawback , machine learning–­ based recommendation systems spoke to Zuckerberg ’ s deep faith in data , code , and personalization . Freed from human limitation , error , and bias , Facebook ’ s algorithms were capable , he believed , of unparalleled objectivity—­ and , perhaps more important , efficiency . A separate strain of machine learning work was devoted to figuring out what content was actually in the posts Facebook recommended . Known as classifiers , these were AI systems trained to perform pattern recognition on vast data sets . Years before Facebook ’ s creation , classifiers had proven themselves indispensable in the fight against spam , allowing email providers to move beyond simple keyword filters that sought to block mass emails about , say , “ Vi @ gra. ” By ingesting and comparing a huge collection of emails—some labeled as spam , some as not spam—­ a machine learning system could develop its own rubric for distinguishing between them . Once this classifier was “ trained , ” it would be set loose , analyzing incoming email and predicting the probability that each message should be sent to an inbox , a junk folder , or straight to hell . By the time machine learning experts began to arrive at Facebook , the list of questions that classifiers sought to answer had grown well past “ Is it spam ? , ” thanks in large part to people like LeCun . Zuckerberg was bullish on its future progress and its applications for Facebook . By 2016 , he was predicting that classifiers would surpass human capacities of perception , recognition , and comprehension within the next five to ten years , allowing the company to shut down misbehavior and make huge leaps in connecting the world . That prediction would prove more than a little optimistic . Even as techniques improved , data sets grew , and processing sped up , one drawback of machine learning persisted . The algorithms that the company produced stubbornly refused to explain themselves . Engineers could evaluate a classifier ’ s success by testing it to see what percentage of its judgment calls were accurate ( its “ precision ” ) and what portion of a thing it detected ( its “ recall ” ) . But because the system was teaching itself how to identify something based on a logic of its own design , when it erred , there was no human-­cognizable reason why . Sometimes mistakes would seem nonsensical . Other times they would be systematic in ways that reflected human error . Early in Facebook ’ s efforts to deploy a classifier to detect pornography , Arturo Bejar recalled , the system routinely tried to cull images of beds . Rather than learning to identify people screwing , the model had instead taught itself to recognize the furniture on which they most often did . The problem had an easy fix : engineers simply needed to train the model with more PG-­ rated mattress scenes . It made for a good joke—­ as long as you didn ’ t consider that the form of machine learning that the engineers had just screwed up was one of the most basic that Facebook was using . Similarly fundamental errors kept occurring , even as the company came to rely on far more advanced AI techniques to make far weightier and complex decisions than “ porn/not porn. ” The company was going all in on AI , both to determine what people should see , and also to solve any problems that might arise . There was no question that the computer science was dazzling and the gains concrete . But the speed , breadth , and scale of Face- book ’ s adoption of machine learning came at the cost of comprehensibility . Why did Facebook ’ s “ Pages You Might Like ” algorithm seem so focused on recommending certain topics ? How had a video snippet from a computer animation about dental implants ended up being seen a hundred million times ? And why did some news publishers consistently achieve virality when they just rewrote other outlets ’ stories ? Faced with these questions , Facebook ’ s Communications team would note that the company ’ s systems responded to people ’ s behavior and that there was no accounting for taste . These were difficult points to refute . They also obscured an uncomfortable fact : Facebook was achieving its growth in ways it didn ’ t fully understand . Within five years of announcing that it was beginning to use machine learning to recommend content and target ads , Facebook ’ s systems would rely so heavily on AI capable of training itself that , without the technology , Yann LeCun proudly declared , all that would be left of the company ’ s products would be “ dust . ”","['follow', 'excerpt', 'broken', 'code', 'facebook', 'fight', 'expose', 'harmful', 'secret', 'reprint', 'permission', 'doubleday', 'imprint', 'knopf', 'publishing', 'group', 'division', 'llc', 'copyright', '©', 'patent', 'office', 'receive', 'filing', 'automatically', 'generate', 'display', 'contain', 'information', 'relevant', 'user', 'user', 'social', 'network', 'rather', 'force', 'people', 'search', 'disparate', 'disorganized', 'content', 'item', 'interest', 'system', 'seek', 'generate', 'list', 'relevant', 'information', 'preferred', 'order', 'list', 'author', 'product', 'news', 'feed', 'idea', 'show', 'user', 'stream', 'activity', 'entirely', 'new—\xad', 'photo\xadsharing', 'website', 'experiment', 'it—\xad', 'change', 'massive', 'facebook', 'user', 'interact', 'site', 'mainly', 'notification', 'poke', 'look', 'friend', 'profile', 'launch', 'news', 'feed', 'user', 'get', 'constantly', 'update', 'stream', 'post', 'status', 'change', 'shift', 'come', 'shock', 'facebook', 'user', 'appreciate', 'activity', 'monitor', 'static', 'profile', 'mine', 'update', 'content', 'face', 'widespread', 'complaint', 'write', 'post', 'reassure', 'user', 'broadcast', 'rather', 'share', 'people', 'care', 'do—\xad', 'friend', 'title', 'calm', 'breathe', 'hear', 'hear', 'user', 'complaint', 'thing', 'listen', 'later', 'note', 'press', 'event', 'news', 'feed', 'instant', 'success', 'boost', 'activity', 'platform', 'connect', 'user', 'engagement', 'quickly', 'double', 'week', 'launch', 'member', 'affiliate', 'single', 'interest', 'first', 'time', 'cause', 'unite', 'many', 'people', 'petition', 'eradicate', 'stalkeresque', 'news', 'feed', 'opaque', 'system', 'user', 'revolt', 'hindsight', 'remarkably', 'simple', 'content', 'mostly', 'appear', 'reverse', 'chronological', 'order', 'manual', 'adjustment', 'make', 'ensure', 'people', 'see', 'popular', 'post', 'range', 'material', 'beginning', 'news', 'feed', 'ranking', 'turn', 'knob', 'say', 'fiddle', 'dial', 'work', 'well', 'enough', 'little', 'friend', 'list', 'grow', 'facebook', 'introduce', 'new', 'feature', 'ad', 'page', 'interest', 'group', 'entertainment', 'meme', 'begin', 'compete', 'post', 'friend', 'news', 'feed', 'facebook', 'need', 'ensure', 'user', 'log', 'see', 'good', 'friend', 'engagement', 'photo', 'ahead', 'cooking', 'page', 'popular', 'enchilada', 'recipe', 'first', 'effort', 'sort', 'eventually', 'brand', 'edgerank', 'simple', 'formula', 'prioritize', 'content', 'accord', 'principal', 'factor', 'post', 'age', 'amount', 'engagement', 'get', 'interconnection', 'user', 'poster', 'rough', 'attempt', 'translate', 'question', 'new', 'popular', 'care', 'math', 'dark', 'magic', 'play', 'user', 'revolt', 'idea', 'facebook', 'put', 'thumb', 'see', 'facebook', 'usage', 'metric', 'jump', 'board', 'platform', 'recommendation', 'system', 'still', 'infancy', 'dissonance', 'user', 'vocal', 'disapproval', 'avid', 'usage', 'lead', 'inescapable', 'conclusion', 'company', 'regular', 'people', 'opinion', 'mechanic', 'well', 'ignore', 'user', 'scream', 'stop', 'keep', 'go', 'work', 'dandy', 'company', 'look', 'move', 'crude', 'formula', 'recommend', 'content', 'base', 'machine', 'learn', 'branch', 'artificial', 'intelligence', 'focus', 'training', 'computer', 'design', 'decision\xad', 'making', 'algorithm', 'rather', 'programming', 'facebook', 'computer', 'rank', 'content', 'accord', 'simple', 'math', 'engineer', 'program', 'analyze', 'user', 'behavior', 'design', 'ranking', 'formula', 'people', 'see', 'result', 'constant', 'experimentation', 'platform', 'serve', 'predict', 'likely', 'generate', 'like', 'user', 'evaluate', 'result', 'real', 'time', 'grow', 'complexity', 'product', 'collection', 'user', 'datum', 'scale', 'world', 'never', 'see', 'facebook', 'still', 'know', 'enough', 'user', 'show', 'relevant', 'ad', 'brand', 'love', 'attention', 'buzz', 'get', 'create', 'content', 'facebook', 'find', 'company', 'pay', 'offering', 'compelling', 'general', 'motor', 'kill', 'entire', 'facebook', 'advertising', 'budget', 'prominent', 'digital', 'advertising', 'executive', 'declare', 'facebook', 'ad', 'fundamentally', 'bad', 'perform', 'ad', 'unit', 'web', 'fix', 'problem', 'fall', 'team', 'run', 'spaniard', 'grow', 'quiñonero', 'live', 'work', 'artificial', 'intelligence', 'friend', 'scatter', 'begin', 'talk', 'excitedly', 'social', 'media–\xad', 'drive', 'protest', 'machine', 'learn', 'technique', 'use', 'optimize', 'bing', 'search', 'ad', 'clear', 'application', 'social', 'network', 'people', 'use', 'overthrow', 'autocratic', 'state', 'nearly', 'topple', 'several', 'join', 'facebook', 'arab', 'spring', 'quiñonero', 'say', 'quiñonero', 'find', 'way', 'facebook', 'build', 'product', 'nearly', 'revolutionary', 'result', 'invite', 'friend', 'tour', 'menlo', 'park', 'campus', 'shocked', 'look', 'shoulder', 'engineer', 'make', 'significant', 'unsupervised', 'update', 'code', 'confirm', 'much', 'fast', 'company', 'move', 'quiñonero', 'receive', 'facebook', 'job', 'offer', 'week', 'later', 'quiñonero', 'begin', 'work', 'ad', 'timing', 'hardly', 'well', 'advance', 'machine', 'learning', 'raw', 'computing', 'speed', 'allow', 'platform', 'pigeonhole', 'user', 'demographic', 'niche', 'single', 'heterosexual', 'woman', 'late', 'twenty', 'interested', 'camping', 'salsa', 'dance', 'spot', 'correlation', 'click', 'use', 'information', 'guess', 'ad', 'find', 'relevant', 'begin', 'random', 'guess', 'maximize', 'odd', 'click', 'system', 'learn', 'hit', 'miss', 'refine', 'model', 'predict', 'ad', 'good', 'shot', 'success', 'hardly', 'omniscient—\xad', 'recommend', 'ad', 'regularly', 'inexplicable', 'bar', 'success', 'digital', 'advertising', 'low', 'percent', 'user', 'click', 'ad', 'triumph', 'billion', 'ad', 'serve', 'day', 'tweak', 'produce', 'even', 'modest', 'gain', 'bring', 'ten', 'hundred', 'million', 'dollar', 'revenue', 'quiñonero', 'team', 'find', 'churn', 'alteration', 'tell', 'team', 'go', 'fast', 'ship', 'week', 'say', 'consumer', 'get', 'first', 'sampling', 'meat', 'make', 'lab', 'rapid', 'pace', 'make', 'sense', 'team', 'improve', 'revenue', 'people', 'feel', 'platform', 'better\xad', 'target', 'ad', 'mean', 'facebook', 'make', 'money', 'user', 'increase', 'ad', 'load', 'much', 'go', 'wrong', 'pitch', 'denture', 'cream', 'teenager', 'die', 'advertising', 'beachhead', 'machine', 'learn', 'facebook', 'soon', 'want', 'piece', 'action', 'product', 'executive', 'task', 'increase', 'number', 'facebook', 'group', 'join', 'friend', 'add', 'post', 'make', 'appeal', 'obvious', 'quiñonero', 'technique', 'increase', 'often', 'user', 'engage', 'ad', 'increase', 'often', 'user', 'engage', 'else', 'platform', 'team', 'responsible', 'rank', 'recommend', 'content', 'rush', 'overhaul', 'system', 'fast', 'set', 'explosion', 'complexity', 'product', 'employee', 'find', 'big', 'gain', 'often', 'come', 'deliberate', 'initiative', 'simple', 'futzing', 'around', 'rather', 'redesign', 'algorithm', 'slow', 'engineer', 'score', 'big', 'quick', 'dirty', 'machine', 'learn', 'experiment', 'amount', 'throw', 'hundred', 'variant', 'exist', 'algorithm', 'wall', 'see', 'version', 'stuck—\xad', 'perform', 'well', 'user', 'necessarily', 'know', 'variable', 'matter', 'outperform', 'say', 'predict', 'likelihood', 'commenting', 'keep', 'fiddle', 'machine', 'learning', 'model', 'produce', 'statistically', 'outperform', 'exist', 'one', 'good', 'enough', 'hard', 'conceive', 'approach', 'building', 'system', 'embody', 'slogan', 'move', 'fast', 'break', 'thing', 'want', 'woo', 'french', 'computer', 'scientist', 'specialize', 'deep', 'learning', 'mean', 'construction', 'computer', 'system', 'capable', 'process', 'information', 'way', 'inspire', 'human', 'thinking', 'already', 'renowne', 'create', 'foundational', 'ai', 'technique', 'make', 'facial', 'recognition', 'possible', 'lecun', 'put', 'charge', 'division', 'aim', 'put', 'facebook', 'vanguard', 'fundamental', 'research', 'artificial', 'intelligence', 'follow', 'success', 'ad', 'quiñonero', 'give', 'equally', 'formidable', 'task', 'push', 'machine', 'learn', 'company', 'bloodstream', 'fast', 'possible', 'initial', 'staff', 'team', 'responsible', 'build', 'new', 'core', 'machine', 'learning', 'tool', 'make', 'available', 'part', 'company—\xad', 'grow', 'year', 'hire', 'still', 'nowhere', 'near', 'large', 'enough', 'assist', 'product', 'team', 'want', 'machine', 'learning', 'help', 'skill', 'build', 'model', 'scratch', 'specialized', 'engineer', 'readily', 'pick', 'increase', 'supply', 'machine', 'learn', 'phds', 'throw', 'money', 'solution', 'build', 'fb', 'learner', 'sort', 'paint', 'number', 'version', 'machine', 'learn', 'package', 'technique', 'template', 'use', 'engineer', 'quite', 'literally', 'understand', 'fb', 'learner', 'machine', 'learn', 'facebook', 'service', 'wordpress', 'build', 'website', 'render', 'need', 'muck', 'html', 'configure', 'server', 'unnecessary', 'rather', 'set', 'blog', 'however', 'engineer', 'question', 'mess', 'gut', 'rapidly', 'become', 'preeminent', 'global', 'communication', 'platform', 'many', 'facebook', 'aware', 'increase', 'concern', 'ai', 'company', 'wall', 'poorly', 'design', 'algorithm', 'mean', 'reward', 'good', 'healthcare', 'penalize', 'hospital', 'treat', 'sicker', 'patient', 'model', 'purport', 'quantify', 'parole', 'candidate', 'risk', 'reoffending', 'turn', 'bias', 'favor', 'keep', 'black', 'people', 'jail', 'issue', 'seem', 'remote', 'social', 'network', 'avid', 'user', 'fb', 'learner', 'later', 'describe', 'machine', 'learning', 'mass', 'diffusion', 'facebook', 'give', 'rocket', 'launcher', 'year\xad', 'old', 'engineer', 'time', 'quiñonero', 'company', 'speak', 'triumph', 'company', 'ai', 'algorithm', 'give', 'insatiable', 'habit', 'lie', 'hate', 'speech', 'man', 'build', 'fix', 'problem', 'engineer', 'team', 'even', 'little', 'expertise', 'build', 'run', 'experiment', 'ease', 'deploy', 'ai\xad', 'powered', 'product', 'production', 'fast', 'ever', 'facebook', 'announce', 'boasting', 'fb', 'learner', 'ingest', 'trillion', 'datum', 'point', 'user', 'behavior', 'day', 'engineer', 'run', 'experiment', 'month', 'sheer', 'amount', 'datum', 'facebook', 'collected—\xad', 'adtargete', 'result', 'good', 'user', 'regularly', 'suspect', 'wrongly', 'company', 'eavesdropping', 'offline', 'conversation', 'give', 'rise', 'claim', 'facebook', 'know', 'quite', 'correct', 'wonder', 'machine', 'learning', 'obscure', 'limit', 'facebook', 'recommendation', 'system', 'work', 'raw', 'correlation', 'user', 'behavior', 'identify', 'user', 'taste', 'interest', 'serve', 'content', 'base', 'news', 'tell', 'like', 'ice', 'skating', 'dirt', 'biking', 'hip\xad', 'hop', 'pop', 'explain', 'human', 'term', 'post', 'appear', 'feed', 'inexplicability', 'obvious', 'drawback', 'machine', 'base', 'recommendation', 'system', 'speak', 'deep', 'faith', 'data', 'code', 'personalization', 'free', 'human', 'limitation', 'error', 'bias', 'algorithm', 'capable', 'believe', 'unparalleled', 'objectivity—\xad', 'perhaps', 'important', 'efficiency', 'separate', 'strain', 'machine', 'learning', 'work', 'devote', 'figure', 'content', 'actually', 'post', 'facebook', 'recommend', 'know', 'classifier', 'system', 'train', 'perform', 'pattern', 'recognition', 'vast', 'datum', 'set', 'year', 'creation', 'classifier', 'prove', 'indispensable', 'fight', 'spam', 'allow', 'email', 'provider', 'move', 'simple', 'keyword', 'filter', 'seek', 'block', 'mass', 'email', 'say', 'vi', 'gra', 'ingest', 'compare', 'huge', 'collection', 'email', 'label', 'spam', 'spam—\xad', 'machine', 'learning', 'system', 'develop', 'rubric', 'distinguish', 'classifier', 'train', 'set', 'loose', 'analyze', 'incoming', 'email', 'predict', 'probability', 'message', 'send', 'inbox', 'junk', 'folder', 'straight', 'hell', 'time', 'machine', 'learn', 'expert', 'begin', 'arrive', 'facebook', 'list', 'question', 'classifier', 'seek', 'answer', 'grow', 'well', 'past', 'spam', 'thank', 'large', 'part', 'people', 'bullish', 'future', 'progress', 'application', 'facebook', 'predict', 'classifier', 'surpass', 'human', 'capacity', 'perception', 'recognition', 'comprehension', 'next', 'year', 'allow', 'company', 'shut', 'misbehavior', 'make', 'huge', 'leap', 'connect', 'world', 'prediction', 'prove', 'little', 'optimistic', 'even', 'technique', 'improve', 'datum', 'set', 'grow', 'processing', 'speed', 'drawback', 'machine', 'learning', 'persist', 'algorithm', 'company', 'produce', 'stubbornly', 'refuse', 'explain', 'engineer', 'evaluate', 'classifier', 'success', 'test', 'see', 'percentage', 'judgment', 'call', 'accurate', 'precision', 'portion', 'thing', 'detect', 'recall', 'system', 'teach', 'identify', 'base', 'logic', 'design', 'err', 'human\xadcognizable', 'reason', 'sometimes', 'mistake', 'seem', 'nonsensical', 'time', 'systematic', 'way', 'reflect', 'human', 'error', 'early', 'effort', 'deploy', 'classifier', 'detect', 'pornography', 'arturo', 'recall', 'system', 'routinely', 'try', 'cull', 'image', 'bed', 'rather', 'learn', 'identify', 'people', 'screw', 'model', 'instead', 'teach', 'recognize', 'furniture', 'often', 'problem', 'easy', 'fix', 'engineer', 'simply', 'need', 'train', 'model', 'pg\xad', 'rate', 'mattress', 'scene', 'make', 'good', 'joke—\xad', 'long', 'consider', 'form', 'machine', 'learn', 'engineer', 'screw', 'basic', 'facebook', 'use', 'similarly', 'fundamental', 'error', 'keep', 'occur', 'even', 'company', 'come', 'rely', 'far', 'advanced', 'ai', 'technique', 'make', 'far', 'weighty', 'complex', 'decision', 'pornnot', 'porn', 'company', 'go', 'ai', 'determine', 'people', 'see', 'also', 'solve', 'problem', 'arise', 'question', 'computer', 'science', 'dazzle', 'gain', 'concrete', 'speed', 'breadth', 'scale', 'face', 'book', 'adoption', 'machine', 'learning', 'come', 'cost', 'comprehensibility', 'facebook', 'page', 'like', 'seem', 'focused', 'recommend', 'certain', 'topic', 'video', 'snippet', 'computer', 'animation', 'dental', 'implant', 'end', 'see', 'time', 'news', 'publisher', 'consistently', 'achieve', 'virality', 'rewrote', 'outlet', 'story', 'face', 'question', 'facebook', 'communication', 'team', 'note', 'company', 'system', 'respond', 'people', 'behavior', 'accounting', 'taste', 'difficult', 'point', 'refute', 'also', 'obscure', 'uncomfortable', 'fact', 'facebook', 'achieve', 'growth', 'way', 'fully', 'understand', 'year', 'announce', 'begin', 'use', 'machine', 'learning', 'recommend', 'content', 'target', 'ad', 'system', 'rely', 'heavily', 'ai', 'capable', 'train', 'technology', 'lecun', 'proudly', 'declare', 'leave', 'company', 'product', 'dust']","<p>In an excerpt from <em>Broken Code: Inside Facebook and the Fight to Expose its Harmful Secrets</em><em>, Jeff Horwitz reveals how the company came to rely on artificial intelligence.</em></p>
"
Augmenting the realities of work,https://www.technologyreview.com/2023/11/29/1083726/augmenting-the-realities-of-work/,2023-11-29,"In association withJPMorgan Chase &Co Imagine an integrated workplace with 3D visualizations that augment presentations, interactive and accelerated onboarding, and controlled training simulations. This is the future of immersive technology that global head of Immersive Technology Research at JPMorgan Chase, Blair MacIntyre is working to build. Augmented reality (AR) and virtual reality (VR) technologies can blend physical and digital dimensions together and infuse new innovations and efficiencies into business and customer experiences. ""These technologies can offer newer ways of collaborating over distance both synchronously and asynchronously than we can get with the traditional work technologies that we use right now,"" says MacIntyre. ""It's these new ways to collaborate, ways of using the environment and space in new and interesting ways that will hopefully offer new value and change the way we work."" Many enterprises are integrating VR into business practices like video conference calls. But having some participants in a virtual world and some sidelined creates imbalances in the employee experience. MacIntyre's team is looking for ways to use AR/VR technologies that can be additive, like 3D data visualizations that enhance financial forecasting within a bank, not ones that overhaul entire experiences. Although the potential of AR/VR is quickly evolving, it's unlikely that customers’ interactions or workplace environments will be entirely moved to the virtual world anytime soon. Rather, MacIntyre's immersive technology research looks to infuse efficiencies into existing practices. ""It's thinking about how the technologies integrate and how we can add value where there is value and not trying to replace everything we do with these technologies,"" MacIntyre says. AI can help remove some of the tedium from immersive technologies that have made them impractical for widespread enterprise use in the past. Using VR technology in the workplace may prohibit taking notes and having access to traditional input devices and files. AI tools can take and transcribe notes and fill in any other gaps to help remove that friction and eliminate redundancies. Connected Internet of things (IoT) devices are also key to enabling AR/VR technologies. To create a valuable immersive experience, MacIntyre says, it's imperative to know as much about the surrounding world of the user as well as their needs, habits, and preferences. ""If we can figure out more ways of enabling people to work together in a distributed way, we can start enabling more people to participate meaningfully in a wider variety of jobs,"" says MacIntyre. This episode of Business Lab is produced in association with JPMorgan Chase. Laurel: From MIT Technology Review, I'm Laurel Ruma, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is emerging technologies, specifically, immersive technologies like augmented and virtual reality. Keeping up with technology trends may be a challenge for most enterprises, but it's a critical way to think about future possibilities from product to customer service to employee experience. Augmented and virtual realities aren't necessarily new, but when it comes to applying them beyond gaming, it's a brave new world.Two words for you: emerging realities.My guest is Blair MacIntyre, who is the global head of Immersive Technology Research at JPMorgan Chase.This podcast is produced in association with JPMorgan Chase.Welcome, Blair. Blair MacIntyre: Thank you. It's great to be here. Laurel: Well, let's do a little bit of context setting. Your career has been focused on researching and exploring immersive technology, including software and design tools, privacy and ethics, and game and experience design. So what brought you to JPMorgan Chase, and could you describe your current role? Blair: So before joining the firm, I had spent the last 23 years as a professor at Georgia Tech and Northeastern University. During that time, as you say, I explored a lot of ways that we can both create things with these technologies, immersive technologies and also, what they might be useful for and what the impacts on people in society and how we experience life are. But as these technologies have become more real, moved out of the lab, starting to see real products from real companies, we have this opportunity to actually see how they might be useful in practice and to have, for me, an impact on how these technologies will be deployed and used that goes beyond the traditional impact that professors might have. So beyond writing papers, beyond teaching students. That's what brought me to the firm, and so my current role is, really, to explore that, to understand all the different ways that immersive technology could impact the firm and its customers. Right? So we think about not just customer-facing and not just products, but also employees and their experience as well. Laurel: That's really interesting. So why does JPMorgan Chase have a dedicated immersive technology focus in its global technology applied research division, and what are the primary goals of your team's research within finance and large enterprises as a whole? Blair: That's a great question. So JPMorgan Chase has a fairly wide variety of research going on within the company. There's large efforts in AI/ML, in quantum computing, blockchain. So they're interested in looking at all of the range of new technologies and how they might impact the firm and our customers, and immersive technologies represent one of those technologies that could over time have a relatively large impact, I think, especially on the employee experience and how we interact with our customers. So they really want to have a group of people focusing on, really, looking both in the near and long term, and thinking about how we can leverage the technology now and how we might be able to leverage it down the road, and not just how we can, but what we should not do. Right? So we're interested in understanding of these applications that are being proposed or people are imagining could be used. Which ones actually have value to the company, and which ones may not actually have value in practice? Laurel: So when people think of immersive technologies like augmented reality and virtual reality, AR and VR, many think of headsets or smartphone apps for gaming and retail shopping experiences. Could you give an overview of the state of immersive technology today and what use cases you find to be the most innovative and interesting in your research? Blair: So, as you say, I think many people think about smartphones, and we've seen, at least in movies and TV shows, head mounts of various kinds. The market, I would divide it right now into the two parts, the handheld phone and tablet experience. So you can do augmented reality now, and that really translates to we take the camera feed, and we can overlay computer graphics on it to do things like see what something you might want to buy looks like in your living room or do, in an enterprise situation, remote maintenance assistance where I can take my phone, point it at a piece of technology, and a remote expert could draw on it or help me do something with it. There’s the phone-based things, and we carry these things in our pockets all the time, and they're relatively cheap. So there's a lot of opportunities when it's appropriate to use those, but the big downside of those devices is that you have to hold them in your hands, so if you wanted to try to put information all around you, you would have to hold the device up and look around, which is uncomfortable and awkward. So that is where the head mount displays come in. So either virtual reality displays which, right now, many of us think about computer games and education as use cases in the consumer world or augmented reality displays. These sorts of displays now let us do the same kind of things we might do with our phones, but we can do it without our hands having to hold something so we can be doing whatever work it was we wanted to do, right? Repairing the equipment, taking notes, working with things in the world around us, and we can have information spread all around us, which I think is the big advantage of head mounts. So many of the things people imagine when they think about augmented reality in particular involve this serendipitous access to information. I'm walking into a conference room, and I see sort of my notes and information about the people I'm meeting there and the materials from our last meeting, whatever it is, or I'm walking down the street, and I see advertising or other kinds of, say, tourism information, but those things only work if the device is out of mind. If I can put it on, and then go about my life, I'm not going to walk into a conference room, and hold up a phone, and look at everybody through it. So that, I think, is the big difference. You could implement the same sorts of applications on both the handheld devices and the head-worn devices, but the two different form factors are going to make very different applications appropriate for those two sorts of technologies.On the virtual reality side, we're at the point now where the displays we can buy are light enough and comfortable enough that we could wear them for half an hour, a couple hours without discomfort. So a lot of the applications that people imagine there, I think the most popular things that people have done research on and that I see having a near-term impact in the enterprise are immersive training applications where you can get into a situation rather than, say, watching a video or a little click-through presentation as part of your annual training. You could really be in an experience and hopefully learn more from it. So I think those sorts of experiences where we're totally immersed and focused is where virtual reality comes in.The big thing that I think is most exciting about head-worn displays in particular where we can wear them while we're doing work as opposed to just having these ephemeral experiences with a phone is the opportunity to do things together, to collaborate. So I might want to look at a map on a table and see a bunch of data floating above the map, but it would be better if you and our other colleagues were around the table with me, and we can all see the same things, or if we want to take a training experience, I could be in there getting my training experience, but maybe someone else is joining me and being able to both offer feedback or guidance and so on. Essentially, when I think about these technologies, I think about the parallels to how we do work regularly, right? We generally collaborate with people. We might grab a colleague and have them look at our laptop to show them something. I might send someone something on my phone, and then we can talk about it. So much of what we do involves interactions with other people and with the data that we are doing our job with that anything we do with these immersive technologies is really going to have to mimic that and give us the ability to do our real work in these immersive spaces with the people that we normally work with. Laurel: Well, speaking of working with people, how can the scale of an institution like JPMorgan Chase help propel this research forward in immersive technology, and what opportunities does it provide that are otherwise limited in a traditional university or startup research environment? Blair: I think it comes down to a few different things. On one hand, we have the access to people who are really doing the things that we want to build technologies to help with. Right? So if I wanted to look how I could use immersive visualization of data to help people in human resources do planning or help people who are doing financial modeling look at the data in new and interesting ways, now I could actually do the research in conjunction with the real people who do that work. Right? So I've already and I've been at the firm for a little over a year, and many conversations we've had were either we've had an idea or somebody has come to us with an idea. Through the course of the conversations, relatively quickly, we hone in on things that are much more sophisticated, much more powerful than what we might have thought of at a university where we didn't have that sort of direct access to people doing the work.On the other hand, if we actually build something, we can actually test it with the same people, which is an amazing opportunity. Right? When I go to a conference, we’re going to put 20 people who actually represent the real users of those systems. So, for me, that's where I think the big opportunity of doing research in an enterprise is, is building solutions for the real people of that enterprise and being able to test it with those people. Laurel: Recent years have actually changed what customers and employees expect from enterprises as well, like omnichannel retail experiences. So immersive technologies can be used to bridge gaps between physical and virtual environments as you were saying earlier. What are the different opportunities that AR and VR can offer enterprises, and how can these technologies be used to improve employee and customer experience? Blair: So I alluded back to some of that in previous answers. I think the biggest opportunities have to do with how employees within the organization can do new things together, can interact, and also how companies can interact with customers. Now, we're not going to move all of our interactions with our customers into the virtual world, or the metaverse, or whatever you want to call it nowadays anytime soon. Right? But I think there are opportunities for customers who are interested in those technologies, and comfortable with them, and excited by them to get new kinds of experiences and new ways of interacting with our firm or other firms than you could get with webpages and in-person meetings. The other big opportunity I think is as we move to a more hybrid work environment and a distributed work environment, so a company like JPMorgan Chase is huge and spread around the world. We have over 300,000 employees now in most countries around the world. There might be groups of people, but they're connected together through video right now. These technologies, I think, can offer new ways of collaborating over distance both synchronously and asynchronously than we can get with the traditional work technologies that we use right now. So it's those new ways to collaborate, ways of using the environment and space in new and interesting ways that is going to, hopefully, offer new value and change the way we work. Laurel: Yeah, and staying on that topic, we can't really have a discussion about technology without talking about AI which is another evolving, increasingly popular technology. So that's being used by many enterprises to reduce redundancies and automate repetitive tasks. In this way, how can immersive technology provide value to people in their everyday work with the help of AI? Blair: So I think the big opportunity that AI brings to immersive technologies is helping ease a lot of the tedium and burden that may have prevented these technologies from being practical in the past, and this could happen in a variety of ways. When I'm in a virtual reality experience, I don't have access to a keyboard, I don't have access to traditional input devices, I don't have necessarily the same sorts of access to my files, and so on. With a lot of the new AI technologies that are coming around, I can start relying on the computer to take notes. I can have new ways of pulling up information that I otherwise wouldn't have access to. So, I think AI reducing the friction of using these technologies is a huge opportunity, and the research community is actively looking at that because friction has been one of the big problems with these technologies up till now. Laurel: So, other than AI, what are other emerging technologies that can aid in immersive technology research and development? Blair: So, aside from AI, if we step back and look at all of the emerging technologies as a whole and how they complement each other, I think we can see new opportunities. So, in our research, we work closely with people doing computer vision and other sort of sensing research to understand the world. We work closely with people looking at internet of things and connected devices because at a 10,000-foot level, all of these technologies are based on the idea of understanding, sensing the world, understanding what people are doing in it, understanding what people's needs might be, and then somehow providing information to them or actuating things in the world, displaying stuff on walls or displays. From that viewpoint, immersive technologies are primarily one way of displaying things in a new and interesting way and getting input from people, knowing what people want to do, allowing them to interact with data. But in order to do that, they need to know as much about the world around the user as possible, the structure of it, but also, who's there, what we are doing, and so on. So all of these other technologies, especially the Internet of things (IoT) and other forms and ways of sensing what's happening in the world are very complimentary and together can create new sorts of experiences that neither could do alone. Laurel: So what are some of the challenges, but also, possible opportunities in your research that contrast the future potential of AR and VR to where the technology is today? Blair: So I think one of the big limitations of technology today is that most of the experiences are very siloed and disconnected from everything else we do. During the pandemic, many of us experimented with how we could have conferences online in various ways, right? A lot of companies, small companies and larger companies, started looking at how you could create immersive meetings and big group experiences using virtual reality technology, but all of those experiences that people created were these closed systems that you couldn't bring things into. So one of the things we're really interested in is how we stop thinking about creating new kinds of experiences and new ways of doing things, and instead think about how do we add these technologies to our existing work practices to enhance them in some way. So, for example. Right now, we do video meetings. It would be more interesting for some people to be able to join those meetings, say, in VR. Companies have experimented with that, but most of the experiments that people are doing assume that everyone is going to move into virtual reality, or we’re going to bring, say, the people in as a little video wall on the side of a big virtual reality room, making them second class citizens. I'm really interested and my team is interested in how we can start incorporating technologies like this while keeping everyone a first-class participant in these meetings. As one example, a lot of the systems that large enterprises build, and we're no different, are web-based right now. So if, let's say, I have a system to do financial forecasting, you could imagine there's a bunch of those at a bank, and it's a web-based system, I'm really interested in how do we add the ability for people to go into a virtual reality or augmented reality experience, say, a 3D visualization of some kind of data at the moment they want to do it, do the work that they want to do, invite colleagues in to discuss things, and then go back to the work as it was always done on a desktop web browser.So that idea of thinking of these technologies as a capability, a feature instead of a new whole application and way of doing things permeates all the work we're doing. When I look down the road at where this can go, I see in, say, let's say, two to five years, I see people with displays maybe sitting on their desk. They have their tablet and their phone, and they might also have another display or two sitting there. They're doing their work, and at different times, they might be in a video chat, they might pick up a head mount and put it on to do different things, but it's all integrated. I'm really interested in how we connect these together and reduce friction. Right? If it takes you four or five minutes to move your work into a VR experience, nobody is going to do it because it just is too problematic. So it's that. It's thinking about how the technologies integrate and how we can add value where there is value and not trying to replace everything we do with these technologies. Laurel: So to stay on that future focus, how do you foresee the immersive technology landscape entirely evolving over the next decade, and how will your research enable those changes? Blair: So, at some level, it's really hard to answer that question. Right? So if I think back 10 years to where immersive technologies were, it would have been inconceivable for us to imagine the videos that are coming out. So, at some level, I can say, ""Well, I have no idea where we're going to be in 10 years."" On the other hand, it's pretty safe to imagine the kinds of technologies that we're experimenting with now just getting better, and more comfortable, and more easy to integrate into work. So I think the landscape is going to evolve in the near term to be more amenable to work. Especially for augmented reality, the threshold that these devices would have to get to such that a lot of people would be willing to wear them all the time while they're walking down the street, playing sports, doing whatever, that's a very high bar because it has to be small, it has to be light, it has to be cheap, it has to have a battery that lasts all day, etcetera, etcetera. On the other hand, in the enterprise, in any business situation, it's easy to imagine the scenario I described. It's sitting on my desk, I pick it up, I put it on, I take it off. In the medium term after that, I think we will see more consumer applications as people start solving more of the problems that are preventing people from wearing these devices for longer periods of time. Right? It's not just size, and battery power, and comfort, it's also things like optics. Right? A lot of people — not a lot, but say, let's say 10%, 15% of people might experience headaches, or nausea, or other kinds of discomfort when they wear a VR display as they're currently built, and a lot of that has to do with the fact that the optics that you're looking at when you're putting this display are built in a way that makes it hard to comfortably focus at objects at different distances away from you without getting into the nitty-gritty details. For many of us, that's fine. We can deal with the slight problems. But for some people, it's problematic.So as we figure out how to solve problems like that, more people can wear them, and more people can use them. I think that's a really critical issue for not just consumers, but for the enterprise because if we think about a future where more of our business applications and the kind of way we work are done with technologies like this, these technologies have to be accessible to everybody. Right? If that 10% or 15% of people get headaches and feel nauseous wearing this device, you've now disenfranchised a pretty significant portion of your workforce, but I think those can be solved, and so we need to be thinking about how we can enable everybody to use them.On the other hand, technologies like this can enfranchise more people, where right now, working remotely, working in a distributed sense is hard. For many kinds of work, it's difficult to do remotely. If we can figure out more ways of enabling people to work together in a distributed way, we can start enabling more people to participate meaningfully in a wider variety of jobs. Laurel: Blair, that was fantastic. It's so interesting. I really appreciate your perspective and sharing it here with us on the Business Lab. Blair: It was great to be here. I enjoyed talking to you. Laurel: That was Blair MacIntyre, the global head of Immersive Technology Research at JPMorgan Chase, who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review.That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the global director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. This podcast is for informational purposes only and it is not intended as legal, tax, financial, investment, accounting or regulatory advice. Opinions expressed herein are the personal views of the individual(s) and do not represent the views of JPMorgan Chase & Co. The accuracy of any statements, linked resources, reported findings or quotations are not the responsibility of JPMorgan Chase & Co.","In association withJPMorgan Chase & Co Imagine an integrated workplace with 3D visualizations that augment presentations , interactive and accelerated onboarding , and controlled training simulations . This is the future of immersive technology that global head of Immersive Technology Research at JPMorgan Chase , Blair MacIntyre is working to build . Augmented reality ( AR ) and virtual reality ( VR ) technologies can blend physical and digital dimensions together and infuse new innovations and efficiencies into business and customer experiences . `` These technologies can offer newer ways of collaborating over distance both synchronously and asynchronously than we can get with the traditional work technologies that we use right now , '' says MacIntyre . `` It 's these new ways to collaborate , ways of using the environment and space in new and interesting ways that will hopefully offer new value and change the way we work . '' Many enterprises are integrating VR into business practices like video conference calls . But having some participants in a virtual world and some sidelined creates imbalances in the employee experience . MacIntyre 's team is looking for ways to use AR/VR technologies that can be additive , like 3D data visualizations that enhance financial forecasting within a bank , not ones that overhaul entire experiences . Although the potential of AR/VR is quickly evolving , it 's unlikely that customers ’ interactions or workplace environments will be entirely moved to the virtual world anytime soon . Rather , MacIntyre 's immersive technology research looks to infuse efficiencies into existing practices . `` It 's thinking about how the technologies integrate and how we can add value where there is value and not trying to replace everything we do with these technologies , '' MacIntyre says . AI can help remove some of the tedium from immersive technologies that have made them impractical for widespread enterprise use in the past . Using VR technology in the workplace may prohibit taking notes and having access to traditional input devices and files . AI tools can take and transcribe notes and fill in any other gaps to help remove that friction and eliminate redundancies . Connected Internet of things ( IoT ) devices are also key to enabling AR/VR technologies . To create a valuable immersive experience , MacIntyre says , it 's imperative to know as much about the surrounding world of the user as well as their needs , habits , and preferences . `` If we can figure out more ways of enabling people to work together in a distributed way , we can start enabling more people to participate meaningfully in a wider variety of jobs , '' says MacIntyre . This episode of Business Lab is produced in association with JPMorgan Chase . Laurel : From MIT Technology Review , I 'm Laurel Ruma , and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is emerging technologies , specifically , immersive technologies like augmented and virtual reality . Keeping up with technology trends may be a challenge for most enterprises , but it 's a critical way to think about future possibilities from product to customer service to employee experience . Augmented and virtual realities are n't necessarily new , but when it comes to applying them beyond gaming , it 's a brave new world.Two words for you : emerging realities.My guest is Blair MacIntyre , who is the global head of Immersive Technology Research at JPMorgan Chase.This podcast is produced in association with JPMorgan Chase.Welcome , Blair . Blair MacIntyre : Thank you . It 's great to be here . Laurel : Well , let 's do a little bit of context setting . Your career has been focused on researching and exploring immersive technology , including software and design tools , privacy and ethics , and game and experience design . So what brought you to JPMorgan Chase , and could you describe your current role ? Blair : So before joining the firm , I had spent the last 23 years as a professor at Georgia Tech and Northeastern University . During that time , as you say , I explored a lot of ways that we can both create things with these technologies , immersive technologies and also , what they might be useful for and what the impacts on people in society and how we experience life are . But as these technologies have become more real , moved out of the lab , starting to see real products from real companies , we have this opportunity to actually see how they might be useful in practice and to have , for me , an impact on how these technologies will be deployed and used that goes beyond the traditional impact that professors might have . So beyond writing papers , beyond teaching students . That 's what brought me to the firm , and so my current role is , really , to explore that , to understand all the different ways that immersive technology could impact the firm and its customers . Right ? So we think about not just customer-facing and not just products , but also employees and their experience as well . Laurel : That 's really interesting . So why does JPMorgan Chase have a dedicated immersive technology focus in its global technology applied research division , and what are the primary goals of your team 's research within finance and large enterprises as a whole ? Blair : That 's a great question . So JPMorgan Chase has a fairly wide variety of research going on within the company . There 's large efforts in AI/ML , in quantum computing , blockchain . So they 're interested in looking at all of the range of new technologies and how they might impact the firm and our customers , and immersive technologies represent one of those technologies that could over time have a relatively large impact , I think , especially on the employee experience and how we interact with our customers . So they really want to have a group of people focusing on , really , looking both in the near and long term , and thinking about how we can leverage the technology now and how we might be able to leverage it down the road , and not just how we can , but what we should not do . Right ? So we 're interested in understanding of these applications that are being proposed or people are imagining could be used . Which ones actually have value to the company , and which ones may not actually have value in practice ? Laurel : So when people think of immersive technologies like augmented reality and virtual reality , AR and VR , many think of headsets or smartphone apps for gaming and retail shopping experiences . Could you give an overview of the state of immersive technology today and what use cases you find to be the most innovative and interesting in your research ? Blair : So , as you say , I think many people think about smartphones , and we 've seen , at least in movies and TV shows , head mounts of various kinds . The market , I would divide it right now into the two parts , the handheld phone and tablet experience . So you can do augmented reality now , and that really translates to we take the camera feed , and we can overlay computer graphics on it to do things like see what something you might want to buy looks like in your living room or do , in an enterprise situation , remote maintenance assistance where I can take my phone , point it at a piece of technology , and a remote expert could draw on it or help me do something with it . There ’ s the phone-based things , and we carry these things in our pockets all the time , and they 're relatively cheap . So there 's a lot of opportunities when it 's appropriate to use those , but the big downside of those devices is that you have to hold them in your hands , so if you wanted to try to put information all around you , you would have to hold the device up and look around , which is uncomfortable and awkward . So that is where the head mount displays come in . So either virtual reality displays which , right now , many of us think about computer games and education as use cases in the consumer world or augmented reality displays . These sorts of displays now let us do the same kind of things we might do with our phones , but we can do it without our hands having to hold something so we can be doing whatever work it was we wanted to do , right ? Repairing the equipment , taking notes , working with things in the world around us , and we can have information spread all around us , which I think is the big advantage of head mounts . So many of the things people imagine when they think about augmented reality in particular involve this serendipitous access to information . I 'm walking into a conference room , and I see sort of my notes and information about the people I 'm meeting there and the materials from our last meeting , whatever it is , or I 'm walking down the street , and I see advertising or other kinds of , say , tourism information , but those things only work if the device is out of mind . If I can put it on , and then go about my life , I 'm not going to walk into a conference room , and hold up a phone , and look at everybody through it . So that , I think , is the big difference . You could implement the same sorts of applications on both the handheld devices and the head-worn devices , but the two different form factors are going to make very different applications appropriate for those two sorts of technologies.On the virtual reality side , we 're at the point now where the displays we can buy are light enough and comfortable enough that we could wear them for half an hour , a couple hours without discomfort . So a lot of the applications that people imagine there , I think the most popular things that people have done research on and that I see having a near-term impact in the enterprise are immersive training applications where you can get into a situation rather than , say , watching a video or a little click-through presentation as part of your annual training . You could really be in an experience and hopefully learn more from it . So I think those sorts of experiences where we 're totally immersed and focused is where virtual reality comes in.The big thing that I think is most exciting about head-worn displays in particular where we can wear them while we 're doing work as opposed to just having these ephemeral experiences with a phone is the opportunity to do things together , to collaborate . So I might want to look at a map on a table and see a bunch of data floating above the map , but it would be better if you and our other colleagues were around the table with me , and we can all see the same things , or if we want to take a training experience , I could be in there getting my training experience , but maybe someone else is joining me and being able to both offer feedback or guidance and so on . Essentially , when I think about these technologies , I think about the parallels to how we do work regularly , right ? We generally collaborate with people . We might grab a colleague and have them look at our laptop to show them something . I might send someone something on my phone , and then we can talk about it . So much of what we do involves interactions with other people and with the data that we are doing our job with that anything we do with these immersive technologies is really going to have to mimic that and give us the ability to do our real work in these immersive spaces with the people that we normally work with . Laurel : Well , speaking of working with people , how can the scale of an institution like JPMorgan Chase help propel this research forward in immersive technology , and what opportunities does it provide that are otherwise limited in a traditional university or startup research environment ? Blair : I think it comes down to a few different things . On one hand , we have the access to people who are really doing the things that we want to build technologies to help with . Right ? So if I wanted to look how I could use immersive visualization of data to help people in human resources do planning or help people who are doing financial modeling look at the data in new and interesting ways , now I could actually do the research in conjunction with the real people who do that work . Right ? So I 've already and I 've been at the firm for a little over a year , and many conversations we 've had were either we 've had an idea or somebody has come to us with an idea . Through the course of the conversations , relatively quickly , we hone in on things that are much more sophisticated , much more powerful than what we might have thought of at a university where we did n't have that sort of direct access to people doing the work.On the other hand , if we actually build something , we can actually test it with the same people , which is an amazing opportunity . Right ? When I go to a conference , we ’ re going to put 20 people who actually represent the real users of those systems . So , for me , that 's where I think the big opportunity of doing research in an enterprise is , is building solutions for the real people of that enterprise and being able to test it with those people . Laurel : Recent years have actually changed what customers and employees expect from enterprises as well , like omnichannel retail experiences . So immersive technologies can be used to bridge gaps between physical and virtual environments as you were saying earlier . What are the different opportunities that AR and VR can offer enterprises , and how can these technologies be used to improve employee and customer experience ? Blair : So I alluded back to some of that in previous answers . I think the biggest opportunities have to do with how employees within the organization can do new things together , can interact , and also how companies can interact with customers . Now , we 're not going to move all of our interactions with our customers into the virtual world , or the metaverse , or whatever you want to call it nowadays anytime soon . Right ? But I think there are opportunities for customers who are interested in those technologies , and comfortable with them , and excited by them to get new kinds of experiences and new ways of interacting with our firm or other firms than you could get with webpages and in-person meetings . The other big opportunity I think is as we move to a more hybrid work environment and a distributed work environment , so a company like JPMorgan Chase is huge and spread around the world . We have over 300,000 employees now in most countries around the world . There might be groups of people , but they 're connected together through video right now . These technologies , I think , can offer new ways of collaborating over distance both synchronously and asynchronously than we can get with the traditional work technologies that we use right now . So it 's those new ways to collaborate , ways of using the environment and space in new and interesting ways that is going to , hopefully , offer new value and change the way we work . Laurel : Yeah , and staying on that topic , we ca n't really have a discussion about technology without talking about AI which is another evolving , increasingly popular technology . So that 's being used by many enterprises to reduce redundancies and automate repetitive tasks . In this way , how can immersive technology provide value to people in their everyday work with the help of AI ? Blair : So I think the big opportunity that AI brings to immersive technologies is helping ease a lot of the tedium and burden that may have prevented these technologies from being practical in the past , and this could happen in a variety of ways . When I 'm in a virtual reality experience , I do n't have access to a keyboard , I do n't have access to traditional input devices , I do n't have necessarily the same sorts of access to my files , and so on . With a lot of the new AI technologies that are coming around , I can start relying on the computer to take notes . I can have new ways of pulling up information that I otherwise would n't have access to . So , I think AI reducing the friction of using these technologies is a huge opportunity , and the research community is actively looking at that because friction has been one of the big problems with these technologies up till now . Laurel : So , other than AI , what are other emerging technologies that can aid in immersive technology research and development ? Blair : So , aside from AI , if we step back and look at all of the emerging technologies as a whole and how they complement each other , I think we can see new opportunities . So , in our research , we work closely with people doing computer vision and other sort of sensing research to understand the world . We work closely with people looking at internet of things and connected devices because at a 10,000-foot level , all of these technologies are based on the idea of understanding , sensing the world , understanding what people are doing in it , understanding what people 's needs might be , and then somehow providing information to them or actuating things in the world , displaying stuff on walls or displays . From that viewpoint , immersive technologies are primarily one way of displaying things in a new and interesting way and getting input from people , knowing what people want to do , allowing them to interact with data . But in order to do that , they need to know as much about the world around the user as possible , the structure of it , but also , who 's there , what we are doing , and so on . So all of these other technologies , especially the Internet of things ( IoT ) and other forms and ways of sensing what 's happening in the world are very complimentary and together can create new sorts of experiences that neither could do alone . Laurel : So what are some of the challenges , but also , possible opportunities in your research that contrast the future potential of AR and VR to where the technology is today ? Blair : So I think one of the big limitations of technology today is that most of the experiences are very siloed and disconnected from everything else we do . During the pandemic , many of us experimented with how we could have conferences online in various ways , right ? A lot of companies , small companies and larger companies , started looking at how you could create immersive meetings and big group experiences using virtual reality technology , but all of those experiences that people created were these closed systems that you could n't bring things into . So one of the things we 're really interested in is how we stop thinking about creating new kinds of experiences and new ways of doing things , and instead think about how do we add these technologies to our existing work practices to enhance them in some way . So , for example . Right now , we do video meetings . It would be more interesting for some people to be able to join those meetings , say , in VR . Companies have experimented with that , but most of the experiments that people are doing assume that everyone is going to move into virtual reality , or we ’ re going to bring , say , the people in as a little video wall on the side of a big virtual reality room , making them second class citizens . I 'm really interested and my team is interested in how we can start incorporating technologies like this while keeping everyone a first-class participant in these meetings . As one example , a lot of the systems that large enterprises build , and we 're no different , are web-based right now . So if , let 's say , I have a system to do financial forecasting , you could imagine there 's a bunch of those at a bank , and it 's a web-based system , I 'm really interested in how do we add the ability for people to go into a virtual reality or augmented reality experience , say , a 3D visualization of some kind of data at the moment they want to do it , do the work that they want to do , invite colleagues in to discuss things , and then go back to the work as it was always done on a desktop web browser.So that idea of thinking of these technologies as a capability , a feature instead of a new whole application and way of doing things permeates all the work we 're doing . When I look down the road at where this can go , I see in , say , let 's say , two to five years , I see people with displays maybe sitting on their desk . They have their tablet and their phone , and they might also have another display or two sitting there . They 're doing their work , and at different times , they might be in a video chat , they might pick up a head mount and put it on to do different things , but it 's all integrated . I 'm really interested in how we connect these together and reduce friction . Right ? If it takes you four or five minutes to move your work into a VR experience , nobody is going to do it because it just is too problematic . So it 's that . It 's thinking about how the technologies integrate and how we can add value where there is value and not trying to replace everything we do with these technologies . Laurel : So to stay on that future focus , how do you foresee the immersive technology landscape entirely evolving over the next decade , and how will your research enable those changes ? Blair : So , at some level , it 's really hard to answer that question . Right ? So if I think back 10 years to where immersive technologies were , it would have been inconceivable for us to imagine the videos that are coming out . So , at some level , I can say , `` Well , I have no idea where we 're going to be in 10 years . '' On the other hand , it 's pretty safe to imagine the kinds of technologies that we 're experimenting with now just getting better , and more comfortable , and more easy to integrate into work . So I think the landscape is going to evolve in the near term to be more amenable to work . Especially for augmented reality , the threshold that these devices would have to get to such that a lot of people would be willing to wear them all the time while they 're walking down the street , playing sports , doing whatever , that 's a very high bar because it has to be small , it has to be light , it has to be cheap , it has to have a battery that lasts all day , etcetera , etcetera . On the other hand , in the enterprise , in any business situation , it 's easy to imagine the scenario I described . It 's sitting on my desk , I pick it up , I put it on , I take it off . In the medium term after that , I think we will see more consumer applications as people start solving more of the problems that are preventing people from wearing these devices for longer periods of time . Right ? It 's not just size , and battery power , and comfort , it 's also things like optics . Right ? A lot of people — not a lot , but say , let 's say 10 % , 15 % of people might experience headaches , or nausea , or other kinds of discomfort when they wear a VR display as they 're currently built , and a lot of that has to do with the fact that the optics that you 're looking at when you 're putting this display are built in a way that makes it hard to comfortably focus at objects at different distances away from you without getting into the nitty-gritty details . For many of us , that 's fine . We can deal with the slight problems . But for some people , it 's problematic.So as we figure out how to solve problems like that , more people can wear them , and more people can use them . I think that 's a really critical issue for not just consumers , but for the enterprise because if we think about a future where more of our business applications and the kind of way we work are done with technologies like this , these technologies have to be accessible to everybody . Right ? If that 10 % or 15 % of people get headaches and feel nauseous wearing this device , you 've now disenfranchised a pretty significant portion of your workforce , but I think those can be solved , and so we need to be thinking about how we can enable everybody to use them.On the other hand , technologies like this can enfranchise more people , where right now , working remotely , working in a distributed sense is hard . For many kinds of work , it 's difficult to do remotely . If we can figure out more ways of enabling people to work together in a distributed way , we can start enabling more people to participate meaningfully in a wider variety of jobs . Laurel : Blair , that was fantastic . It 's so interesting . I really appreciate your perspective and sharing it here with us on the Business Lab . Blair : It was great to be here . I enjoyed talking to you . Laurel : That was Blair MacIntyre , the global head of Immersive Technology Research at JPMorgan Chase , who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review.That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the global director of Insights , the custom publishing division of MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can find us in print , on the web , and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com.This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff . This podcast is for informational purposes only and it is not intended as legal , tax , financial , investment , accounting or regulatory advice . Opinions expressed herein are the personal views of the individual ( s ) and do not represent the views of JPMorgan Chase & Co . The accuracy of any statements , linked resources , reported findings or quotations are not the responsibility of JPMorgan Chase & Co .","['association', 'co', 'imagine', 'integrate', 'workplace', '3d', 'visualization', 'augment', 'presentation', 'interactive', 'accelerate', 'onboarding', 'control', 'training', 'simulation', 'future', 'immersive', 'technology', 'global', 'head', 'immersive', 'technology', 'research', 'work', 'build', 'augment', 'reality', 'ar', 'virtual', 'reality', 'technology', 'blend', 'physical', 'digital', 'dimension', 'together', 'infuse', 'new', 'innovation', 'efficiency', 'business', 'customer', 'experience', 'technology', 'offer', 'new', 'way', 'collaborate', 'distance', 'synchronously', 'asynchronously', 'get', 'traditional', 'work', 'technology', 'use', 'right', 'say', 'new', 'way', 'collaborate', 'way', 'use', 'environment', 'space', 'new', 'interesting', 'way', 'hopefully', 'offer', 'new', 'value', 'change', 'way', 'work', 'many', 'enterprise', 'integrate', 'business', 'practice', 'video', 'conference', 'call', 'participant', 'virtual', 'world', 'sideline', 'create', 'imbalance', 'employee', 'experience', 'team', 'look', 'way', 'use', 'arvr', 'technology', 'additive', '3d', 'datum', 'visualization', 'enhance', 'financial', 'forecasting', 'bank', 'one', 'overhaul', 'entire', 'experience', 'potential', 'arvr', 'quickly', 'evolve', 'unlikely', 'customer', 'interaction', 'workplace', 'environment', 'entirely', 'move', 'virtual', 'world', 'anytime', 'soon', 'rather', 'immersive', 'technology', 'research', 'look', 'infuse', 'efficiency', 'exist', 'practice', 'think', 'technology', 'integrate', 'add', 'value', 'value', 'try', 'replace', 'technology', 'macintyre', 'say', 'help', 'remove', 'tedium', 'immersive', 'technology', 'make', 'impractical', 'widespread', 'enterprise', 'use', 'past', 'use', 'technology', 'workplace', 'prohibit', 'take', 'note', 'access', 'traditional', 'input', 'device', 'file', 'tool', 'take', 'transcribe', 'note', 'fill', 'gap', 'help', 'remove', 'friction', 'eliminate', 'redundancy', 'connect', 'internet', 'thing', 'device', 'also', 'key', 'enable', 'arvr', 'technology', 'create', 'valuable', 'immersive', 'experience', 'say', 'imperative', 'know', 'much', 'surround', 'world', 'user', 'well', 'need', 'habit', 'preference', 'figure', 'way', 'enable', 'people', 'work', 'together', 'distribute', 'way', 'start', 'enable', 'people', 'participate', 'meaningfully', 'wide', 'variety', 'job', 'say', 'episode', 'business', 'lab', 'produce', 'association', 'laurel', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'today', 'emerge', 'technology', 'specifically', 'immersive', 'technology', 'augment', 'virtual', 'reality', 'keep', 'technology', 'trend', 'challenge', 'enterprise', 'critical', 'way', 'think', 'future', 'possibility', 'product', 'customer', 'service', 'employee', 'experience', 'augment', 'virtual', 'reality', 'necessarily', 'new', 'come', 'apply', 'game', 'brave', 'new', 'word', 'emerge', 'realitiesmy', 'guest', 'global', 'head', 'immersive', 'technology', 'research', 'produce', 'association', 'thank', 'great', 'laurel', 'well', 'let', 'little', 'bit', 'context', 'set', 'career', 'focus', 'research', 'explore', 'immersive', 'technology', 'include', 'software', 'design', 'tool', 'privacy', 'ethic', 'game', 'experience', 'design', 'bring', 'describe', 'current', 'role', 'join', 'firm', 'spend', 'last', 'year', 'professor', 'northeastern', 'time', 'say', 'explore', 'lot', 'way', 'create', 'thing', 'technology', 'immersive', 'technology', 'also', 'useful', 'impact', 'people', 'society', 'experience', 'life', 'technology', 'become', 'real', 'move', 'lab', 'start', 'see', 'real', 'product', 'real', 'company', 'opportunity', 'actually', 'see', 'useful', 'practice', 'impact', 'technology', 'deploy', 'use', 'go', 'traditional', 'impact', 'professor', 'write', 'paper', 'teach', 'student', 'bring', 'firm', 'current', 'role', 'really', 'explore', 'understand', 'different', 'way', 'immersive', 'technology', 'impact', 'firm', 'customer', 'right', 'think', 'customerface', 'product', 'also', 'employee', 'experience', 'well', 'laurel', 'really', 'interesting', 'dedicated', 'immersive', 'technology', 'focus', 'global', 'technology', 'apply', 'research', 'division', 'primary', 'goal', 'team', 'research', 'finance', 'large', 'enterprise', 'whole', 'great', 'question', 'fairly', 'wide', 'variety', 'research', 'go', 'company', 'large', 'effort', 'aiml', 'computing', 'interested', 'look', 'range', 'new', 'technology', 'impact', 'firm', 'customer', 'immersive', 'technology', 'represent', 'technology', 'time', 'relatively', 'large', 'impact', 'think', 'especially', 'employee', 'experience', 'interact', 'customer', 'really', 'want', 'group', 'people', 'focus', 'really', 'look', 'near', 'long', 'term', 'think', 'leverage', 'technology', 'able', 'leverage', 'road', 'right', 'interested', 'understanding', 'application', 'propose', 'people', 'imagine', 'use', 'one', 'actually', 'value', 'company', 'one', 'actually', 'value', 'practice', 'laurel', 'people', 'think', 'immersive', 'technology', 'augment', 'reality', 'virtual', 'reality', 'many', 'think', 'headset', 'smartphone', 'app', 'gaming', 'retail', 'shopping', 'experience', 'give', 'overview', 'state', 'immersive', 'technology', 'today', 'use', 'case', 'find', 'innovative', 'interesting', 'research', 'say', 'think', 'many', 'people', 'think', 'smartphone', 'see', 'least', 'movie', 'tv', 'show', 'head', 'mount', 'various', 'kind', 'market', 'divide', 'right', 'part', 'handheld', 'phone', 'tablet', 'experience', 'augment', 'reality', 'really', 'translate', 'take', 'camera', 'feed', 'overlay', 'computer', 'graphic', 'thing', 'see', 'want', 'buy', 'look', 'living', 'room', 'enterprise', 'situation', 'remote', 'maintenance', 'assistance', 'take', 'phone', 'point', 'piece', 'technology', 'remote', 'expert', 'draw', 'help', 'phonebase', 'thing', 'carry', 'thing', 'pocket', 'time', 'relatively', 'cheap', 'lot', 'opportunity', 'appropriate', 'use', 'big', 'downside', 'device', 'hold', 'hand', 'want', 'try', 'put', 'information', 'hold', 'device', 'look', 'uncomfortable', 'awkward', 'head', 'display', 'come', 'virtual', 'reality', 'display', 'right', 'many', 'think', 'computer', 'game', 'education', 'use', 'case', 'consumer', 'world', 'augment', 'reality', 'display', 'sort', 'display', 'let', 'kind', 'thing', 'phone', 'hand', 'hold', 'work', 'want', 'right', 'repair', 'equipment', 'take', 'note', 'work', 'thing', 'world', 'information', 'spread', 'think', 'big', 'advantage', 'head', 'mount', 'many', 'thing', 'people', 'imagine', 'think', 'augmented', 'reality', 'particular', 'involve', 'serendipitous', 'access', 'information', 'walk', 'conference', 'room', 'see', 'sort', 'note', 'information', 'people', 'meet', 'material', 'last', 'meeting', 'walk', 'street', 'see', 'advertising', 'kind', 'say', 'tourism', 'information', 'thing', 'work', 'device', 'mind', 'put', 'go', 'life', 'go', 'walk', 'conference', 'room', 'hold', 'phone', 'look', 'think', 'big', 'difference', 'implement', 'sort', 'application', 'handheld', 'device', 'device', 'different', 'form', 'factor', 'go', 'make', 'different', 'application', 'appropriate', 'sort', 'technologieson', 'virtual', 'reality', 'side', 'point', 'display', 'buy', 'light', 'enough', 'comfortable', 'enough', 'wear', 'hour', 'couple', 'hour', 'discomfort', 'lot', 'application', 'people', 'imagine', 'think', 'popular', 'thing', 'people', 'research', 'see', 'nearterm', 'impact', 'enterprise', 'immersive', 'training', 'application', 'get', 'situation', 'rather', 'say', 'watch', 'video', 'little', 'clickthrough', 'presentation', 'part', 'annual', 'training', 'really', 'experience', 'hopefully', 'learn', 'think', 'sort', 'experience', 'totally', 'immerse', 'focus', 'virtual', 'reality', 'come', 'inthe', 'big', 'thing', 'think', 'exciting', 'headworn', 'display', 'particular', 'wear', 'work', 'oppose', 'ephemeral', 'experience', 'phone', 'opportunity', 'thing', 'together', 'collaborate', 'want', 'look', 'map', 'table', 'see', 'bunch', 'datum', 'float', 'map', 'well', 'colleague', 'table', 'see', 'thing', 'want', 'take', 'training', 'experience', 'get', 'training', 'experience', 'maybe', 'else', 'join', 'able', 'offer', 'feedback', 'guidance', 'essentially', 'think', 'technology', 'think', 'parallel', 'work', 'regularly', 'right', 'generally', 'collaborate', 'people', 'grab', 'colleague', 'look', 'laptop', 'show', 'send', 'phone', 'talk', 'much', 'involve', 'interaction', 'people', 'datum', 'job', 'immersive', 'technology', 'really', 'go', 'mimic', 'give', 'ability', 'real', 'work', 'immersive', 'space', 'people', 'normally', 'work', 'laurel', 'well', 'speak', 'work', 'people', 'scale', 'institution', 'help', 'propel', 'research', 'forward', 'immersive', 'technology', 'opportunity', 'provide', 'otherwise', 'limit', 'traditional', 'university', 'startup', 'research', 'environment', 'think', 'come', 'different', 'thing', 'hand', 'access', 'people', 'really', 'thing', 'want', 'build', 'technology', 'help', 'right', 'want', 'look', 'use', 'immersive', 'visualization', 'datum', 'help', 'people', 'human', 'resource', 'planning', 'help', 'people', 'financial', 'modeling', 'look', 'datum', 'new', 'interesting', 'way', 'actually', 'research', 'conjunction', 'real', 'people', 'work', 'right', 'already', 'firm', 'little', 'year', 'many', 'conversation', 'idea', 'come', 'idea', 'course', 'conversation', 'relatively', 'quickly', 'hone', 'thing', 'much', 'sophisticated', 'much', 'powerful', 'think', 'university', 'sort', 'direct', 'access', 'people', 'workon', 'hand', 'actually', 'build', 'actually', 'test', 'people', 'amazing', 'opportunity', 'right', 'go', 'conference', 'go', 'put', 'people', 'actually', 'represent', 'real', 'user', 'system', 'think', 'big', 'opportunity', 'research', 'enterprise', 'build', 'solution', 'real', 'people', 'enterprise', 'able', 'test', 'people', 'laurel', 'recent', 'year', 'actually', 'change', 'customer', 'employee', 'expect', 'enterprise', 'well', 'omnichannel', 'retail', 'experience', 'immersive', 'technology', 'use', 'bridge', 'gap', 'physical', 'virtual', 'environment', 'say', 'early', 'different', 'opportunity', 'ar', 'offer', 'enterprise', 'technology', 'use', 'improve', 'employee', 'customer', 'experience', 'allude', 'back', 'previous', 'answer', 'think', 'big', 'opportunity', 'employee', 'organization', 'new', 'thing', 'together', 'interact', 'also', 'company', 'interact', 'customer', 'go', 'move', 'interaction', 'customer', 'virtual', 'world', 'metaverse', 'want', 'call', 'nowadays', 'anytime', 'soon', 'right', 'think', 'opportunity', 'customer', 'interested', 'technology', 'comfortable', 'excite', 'get', 'new', 'kind', 'experience', 'new', 'way', 'interact', 'firm', 'firm', 'get', 'webpage', 'meeting', 'big', 'opportunity', 'think', 'move', 'hybrid', 'work', 'environment', 'distribute', 'work', 'environment', 'company', 'huge', 'spread', 'world', 'employee', 'country', 'world', 'group', 'people', 'connect', 'together', 'video', 'right', 'technology', 'think', 'offer', 'new', 'way', 'collaborate', 'distance', 'synchronously', 'asynchronously', 'get', 'traditional', 'work', 'technology', 'use', 'right', 'new', 'way', 'collaborate', 'way', 'use', 'environment', 'space', 'new', 'interesting', 'way', 'go', 'hopefully', 'offer', 'new', 'value', 'change', 'way', 'work', 'laurel', 'stay', 'topic', 'really', 'discussion', 'technology', 'talk', 'ai', 'evolve', 'increasingly', 'popular', 'technology', 'use', 'many', 'enterprise', 'reduce', 'redundancy', 'automate', 'repetitive', 'task', 'way', 'immersive', 'technology', 'provide', 'value', 'people', 'everyday', 'work', 'help', 'ai', 'think', 'big', 'opportunity', 'bring', 'immersive', 'technology', 'ease', 'lot', 'tedium', 'burden', 'prevent', 'technology', 'practical', 'past', 'happen', 'variety', 'way', 'virtual', 'reality', 'experience', 'access', 'keyboard', 'access', 'traditional', 'input', 'device', 'necessarily', 'sort', 'access', 'file', 'lot', 'new', 'technology', 'come', 'start', 'rely', 'computer', 'take', 'note', 'new', 'way', 'pull', 'information', 'otherwise', 'access', 'think', 'reduce', 'friction', 'use', 'technology', 'huge', 'opportunity', 'research', 'community', 'actively', 'look', 'friction', 'big', 'problem', 'technology', 'ai', 'emerge', 'technology', 'aid', 'immersive', 'technology', 'research', 'development', 'aside', 'ai', 'step', 'back', 'look', 'emerge', 'technology', 'whole', 'complement', 'think', 'see', 'new', 'opportunity', 'research', 'work', 'closely', 'people', 'computer', 'vision', 'sort', 'sense', 'research', 'understand', 'world', 'work', 'closely', 'people', 'look', 'internet', 'thing', 'connect', 'device', '10000foot', 'level', 'technology', 'base', 'idea', 'understand', 'sense', 'world', 'understand', 'people', 'understand', 'people', 'need', 'somehow', 'provide', 'information', 'actuate', 'thing', 'world', 'display', 'stuff', 'wall', 'display', 'viewpoint', 'immersive', 'technology', 'primarily', 'way', 'display', 'thing', 'new', 'interesting', 'way', 'get', 'input', 'people', 'know', 'people', 'want', 'allow', 'interact', 'datum', 'order', 'need', 'know', 'much', 'world', 'user', 'possible', 'structure', 'also', 'technology', 'especially', 'internet', 'thing', 'iot', 'form', 'way', 'sense', 'happen', 'world', 'complimentary', 'together', 'create', 'new', 'sort', 'experience', 'alone', 'laurel', 'challenge', 'also', 'possible', 'opportunity', 'research', 'contrast', 'future', 'potential', 'ar', 'technology', 'today', 'think', 'big', 'limitation', 'technology', 'today', 'experience', 'siloe', 'disconnect', 'else', 'pandemic', 'many', 'experiment', 'conference', 'online', 'various', 'way', 'right', 'lot', 'company', 'small', 'company', 'large', 'company', 'start', 'look', 'create', 'immersive', 'meeting', 'big', 'group', 'experience', 'use', 'virtual', 'reality', 'technology', 'experience', 'people', 'create', 'closed', 'system', 'bring', 'thing', 'thing', 'really', 'interested', 'stop', 'think', 'create', 'new', 'kind', 'experience', 'new', 'way', 'thing', 'instead', 'think', 'add', 'technology', 'exist', 'work', 'practice', 'enhance', 'way', 'example', 'right', 'video', 'meeting', 'interesting', 'people', 'able', 'join', 'meeting', 'say', 'company', 'experiment', 'experiment', 'people', 'assume', 'go', 'move', 'virtual', 'reality', 'go', 'bring', 'say', 'people', 'little', 'video', 'wall', 'side', 'big', 'virtual', 'reality', 'room', 'make', 'second', 'class', 'citizen', 'really', 'interested', 'team', 'interested', 'start', 'incorporate', 'technology', 'keep', 'firstclass', 'participant', 'meeting', 'example', 'lot', 'system', 'large', 'enterprise', 'build', 'different', 'webbase', 'right', 'let', 'say', 'system', 'financial', 'forecasting', 'imagine', 'bunch', 'bank', 'webbase', 'system', 'really', 'interested', 'add', 'ability', 'people', 'go', 'virtual', 'reality', 'augment', 'reality', 'experience', 'say', '3d', 'visualization', 'kind', 'datum', 'moment', 'want', 'work', 'want', 'invite', 'colleague', 'discuss', 'thing', 'go', 'back', 'work', 'always', 'desktop', 'web', 'browserso', 'idea', 'thinking', 'technology', 'capability', 'feature', 'instead', 'new', 'whole', 'application', 'way', 'thing', 'permeate', 'work', 'look', 'road', 'go', 'see', 'say', 'let', 'say', 'year', 'see', 'people', 'display', 'maybe', 'sit', 'desk', 'tablet', 'phone', 'also', 'display', 'sit', 'work', 'different', 'time', 'video', 'chat', 'pick', 'head', 'mount', 'put', 'different', 'thing', 'integrate', 'really', 'interested', 'connect', 'together', 'reduce', 'friction', 'right', 'take', 'minute', 'move', 'work', 'experience', 'go', 'problematic', 'think', 'technology', 'integrate', 'add', 'value', 'value', 'try', 'replace', 'technology', 'laurel', 'stay', 'future', 'focus', 'foresee', 'immersive', 'technology', 'landscape', 'entirely', 'evolve', 'next', 'decade', 'research', 'enable', 'change', 'level', 'really', 'hard', 'answer', 'question', 'right', 'think', 'back', 'year', 'immersive', 'technology', 'inconceivable', 'imagine', 'video', 'come', 'level', 'say', 'well', 'idea', 'go', 'year', 'hand', 'pretty', 'safe', 'imagine', 'kind', 'technology', 'experiment', 'get', 'well', 'comfortable', 'easy', 'integrate', 'work', 'think', 'landscape', 'go', 'evolve', 'near', 'term', 'amenable', 'work', 'especially', 'augment', 'reality', 'threshold', 'device', 'get', 'lot', 'people', 'willing', 'wear', 'time', 'walk', 'street', 'play', 'sport', 'high', 'bar', 'small', 'light', 'cheap', 'battery', 'last', 'day', 'etcetera', 'etcetera', 'hand', 'enterprise', 'business', 'situation', 'easy', 'imagine', 'scenario', 'describe', 'sit', 'desk', 'pick', 'put', 'take', 'medium', 'term', 'think', 'see', 'consumer', 'application', 'people', 'start', 'solve', 'problem', 'prevent', 'people', 'wear', 'device', 'long', 'period', 'time', 'right', 'size', 'battery', 'power', 'comfort', 'also', 'thing', 'optic', 'right', 'lot', 'people', 'lot', 'say', 'let', 'say', 'people', 'experience', 'headache', 'nausea', 'kind', 'discomfort', 'wear', 'vr', 'display', 'currently', 'build', 'lot', 'fact', 'optic', 'look', 'put', 'display', 'build', 'way', 'make', 'hard', 'comfortably', 'focus', 'object', 'different', 'distance', 'away', 'get', 'nittygritty', 'detail', 'many', 'fine', 'deal', 'slight', 'problem', 'people', 'problematicso', 'figure', 'solve', 'problem', 'people', 'wear', 'people', 'use', 'think', 'really', 'critical', 'issue', 'consumer', 'enterprise', 'think', 'future', 'business', 'application', 'kind', 'way', 'work', 'technology', 'technology', 'accessible', 'right', 'people', 'get', 'headache', 'feel', 'nauseous', 'wear', 'device', 'disenfranchise', 'pretty', 'significant', 'portion', 'workforce', 'think', 'solve', 'need', 'think', 'enable', 'use', 'themon', 'hand', 'technology', 'enfranchise', 'people', 'right', 'work', 'remotely', 'work', 'distribute', 'sense', 'hard', 'many', 'kind', 'work', 'difficult', 'remotely', 'figure', 'way', 'enable', 'people', 'work', 'together', 'distribute', 'way', 'start', 'enable', 'people', 'participate', 'meaningfully', 'wide', 'variety', 'job', 'fantastic', 'interesting', 'really', 'appreciate', 'perspective', 'share', 'business', 'lab', 'great', 'enjoy', 'talk', 'laurel', 'global', 'head', 'immersive', 'technology', 'research', 'speak', 'home', 'mit', 'mit', 'technology', 'episode', 'business', 'lab', 'host', 'global', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff', 'podcast', 'informational', 'purpose', 'intend', 'legal', 'tax', 'financial', 'investment', 'accounting', 'regulatory', 'advice', 'opinion', 'express', 'herein', 'personal', 'view', 'individual', 'represent', 'view', 'co', 'accuracy', 'statement', 'link', 'resource', 'report', 'finding', 'quotation', 'responsibility', 'co']","Imagine an integrated workplace with 3D visualizations that augment presentations, interactive and accelerated onboarding, and controlled training simulations. This is the future of immersive technology that global head of Immersive Technology Research at JPMorgan Chase, Blair MacIntyre is working to build. Augmented reality (AR) and virtual reality (VR) technologies can blend physical and digital dimensions…"
Customer experience horizons,https://www.technologyreview.com/2023/11/09/1083068/customer-experience-horizons/,2023-11-09,"In partnership withGenesys Customer experience (CX) is a leading driver of brand loyalty and organizational performance. According to NTT’s State of CX 2023 report, 92% of CEOs believe improvements in CX directly impact their improved productivity, and customer brand advocacy. They also recognize that the quality of their employee experience (EX) is critical to success. The real potential for transforming business, according to 95% of CEOs, is bringing customer and employee experience improvements together into one end-to-end strategy. This, they anticipate, will deliver revenue growth, business agility, and resilience. To succeed, organizations need to reimagine what’s possible with customer and employee experience and understand horizon trends that will affect their business. This MIT Technology Review Insights report explores the strategies and technologies that will transform customer experience and contact center employee experience in the years ahead. It is based on nearly two dozen interviews with customer experience leaders, conducted between December 2022 and April 2023. The interviews explored the future of customer experience and employee experience and the role of the contact center as a strategic driver of business value. The main findings of this report are as follows: Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withGenesys Customer experience ( CX ) is a leading driver of brand loyalty and organizational performance . According to NTT ’ s State of CX 2023 report , 92 % of CEOs believe improvements in CX directly impact their improved productivity , and customer brand advocacy . They also recognize that the quality of their employee experience ( EX ) is critical to success . The real potential for transforming business , according to 95 % of CEOs , is bringing customer and employee experience improvements together into one end-to-end strategy . This , they anticipate , will deliver revenue growth , business agility , and resilience . To succeed , organizations need to reimagine what ’ s possible with customer and employee experience and understand horizon trends that will affect their business . This MIT Technology Review Insights report explores the strategies and technologies that will transform customer experience and contact center employee experience in the years ahead . It is based on nearly two dozen interviews with customer experience leaders , conducted between December 2022 and April 2023 . The interviews explored the future of customer experience and employee experience and the role of the contact center as a strategic driver of business value . The main findings of this report are as follows : Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'customer', 'experience', 'cx', 'lead', 'driver', 'brand', 'loyalty', 'organizational', 'performance', 'accord', 'state', 'report', 'believe', 'improvement', 'directly', 'impact', 'improve', 'productivity', 'customer', 'brand', 'advocacy', 'also', 'recognize', 'quality', 'employee', 'experience', 'ex', 'critical', 'success', 'real', 'potential', 'transform', 'business', 'accord', 'bring', 'customer', 'employee', 'experience', 'improvement', 'together', 'endtoend', 'strategy', 'anticipate', 'deliver', 'revenue', 'growth', 'business', 'agility', 'resilience', 'succeed', 'organization', 'need', 'reimagine', 'possible', 'customer', 'employee', 'experience', 'understand', 'horizon', 'trend', 'affect', 'business', 'mit', 'technology', 'review', 'insight', 'report', 'explore', 'strategy', 'technology', 'transform', 'customer', 'experience', 'contact', 'center', 'employee', 'experience', 'year', 'ahead', 'base', 'nearly', 'dozen', 'interview', 'customer', 'experience', 'leader', 'conduct', 'interview', 'explore', 'future', 'customer', 'experience', 'employee', 'experience', 'role', 'contact', 'center', 'strategic', 'driver', 'business', 'value', 'main', 'finding', 'report', 'follow', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Customer experience (CX) is a leading driver of brand loyalty and organizational performance. According to NTT’s State of CX 2023 report, 92% of CEOs believe improvements in CX directly impact their improved productivity, and customer brand advocacy. They also recognize that the quality of their employee experience (EX) is critical to success. The real potential…"
AI gains momentum in core manufacturing services functions,https://www.technologyreview.com/2023/11/02/1080614/ai-gains-momentum-in-core-manufacturing-services-functions/,2023-11-02,"In partnership withDataiku When considering the potential for AI systems to change manufacturing, Ritu Jyoti, global AI research lead at market-intelligence firm IDC, points to windmill manufacturers. To improve windmills before AI, she says, the company analyzed data from observing a functioning prototype, a process that took weeks. Now, the manufacturer has dramatically shortened the process using a digital twin—a digital model of the operational windmill—using machine learning (ML) and AI to create and simulate improvements. “Sometimes it was impossible and physically challenging for them to even go and get all the measurements, so they used drones and AI technologies to generate a digital win,” Jyoti says. This manufacturer now sees this AI/ML technology as essential. ""Because if they’re not doing it, they’re not going to be relevant,” she says. Disruption in manufacturing and the supply chain has pushed businesses toward digital transformation as they seek ways to stay competitive. For manufacturers, these disruptions—along with the advent of AI—present opportunities to make manufacturing more efficient, safer, and sustainable.  Companies can use AI to streamline processes and fight downtime, adopt robotics that promote safety and speed, allow AI to detect anomalies quickly through computer vision, and develop AI systems to process vast volumes of data to identify patterns and predict customer needs. “In manufacturing, the biggest benefits come when people from the business are able to work together with data experts, using data and AI to get insights, ultimately taking actions to improve their processes,” says Pierre Goutorbe, AI solutions director for energy and manufacturing at Dataiku. “The more workers get familiar with AI and use it on a daily basis, the more we’ll see the benefit from it,” he says. Between supply-chain disruptions and worker shortages, the manufacturing sector has been innovating to stay ahead in the global marketplace. However, a June 2023 study by Dataiku and Databricks found manufacturing lags behind other industries, with about a quarter (24%) of companies still at the exploration or experimentation stage in terms of AI adoption, while only about one-fifth (19%) of companies across all other industries are still in this beginning stage. Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.  ","In partnership withDataiku When considering the potential for AI systems to change manufacturing , Ritu Jyoti , global AI research lead at market-intelligence firm IDC , points to windmill manufacturers . To improve windmills before AI , she says , the company analyzed data from observing a functioning prototype , a process that took weeks . Now , the manufacturer has dramatically shortened the process using a digital twin—a digital model of the operational windmill—using machine learning ( ML ) and AI to create and simulate improvements . “ Sometimes it was impossible and physically challenging for them to even go and get all the measurements , so they used drones and AI technologies to generate a digital win , ” Jyoti says . This manufacturer now sees this AI/ML technology as essential . `` Because if they ’ re not doing it , they ’ re not going to be relevant , ” she says . Disruption in manufacturing and the supply chain has pushed businesses toward digital transformation as they seek ways to stay competitive . For manufacturers , these disruptions—along with the advent of AI—present opportunities to make manufacturing more efficient , safer , and sustainable . Companies can use AI to streamline processes and fight downtime , adopt robotics that promote safety and speed , allow AI to detect anomalies quickly through computer vision , and develop AI systems to process vast volumes of data to identify patterns and predict customer needs . “ In manufacturing , the biggest benefits come when people from the business are able to work together with data experts , using data and AI to get insights , ultimately taking actions to improve their processes , ” says Pierre Goutorbe , AI solutions director for energy and manufacturing at Dataiku . “ The more workers get familiar with AI and use it on a daily basis , the more we ’ ll see the benefit from it , ” he says . Between supply-chain disruptions and worker shortages , the manufacturing sector has been innovating to stay ahead in the global marketplace . However , a June 2023 study by Dataiku and Databricks found manufacturing lags behind other industries , with about a quarter ( 24 % ) of companies still at the exploration or experimentation stage in terms of AI adoption , while only about one-fifth ( 19 % ) of companies across all other industries are still in this beginning stage . Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withdataiku', 'consider', 'potential', 'system', 'change', 'manufacture', 'ritu', 'research', 'lead', 'marketintelligence', 'firm', 'idc', 'point', 'windmill', 'manufacturer', 'improve', 'windmill', 'say', 'company', 'analyze', 'datum', 'observe', 'function', 'prototype', 'process', 'take', 'week', 'manufacturer', 'dramatically', 'shorten', 'process', 'use', 'digital', 'twin', 'digital', 'model', 'operational', 'windmill', 'use', 'machine', 'learning', 'ml', 'ai', 'create', 'simulate', 'improvement', 'sometimes', 'impossible', 'physically', 'challenge', 'even', 'go', 'get', 'measurement', 'use', 'drone', 'ai', 'technology', 'generate', 'digital', 'win', 'say', 'manufacturer', 'see', 'aiml', 'technology', 'essential', 'go', 'relevant', 'say', 'disruption', 'manufacturing', 'supply', 'chain', 'push', 'business', 'digital', 'transformation', 'seek', 'way', 'stay', 'competitive', 'manufacturer', 'disruption', 'advent', 'ai', 'present', 'opportunity', 'make', 'manufacturing', 'efficient', 'safe', 'sustainable', 'company', 'use', 'ai', 'streamline', 'process', 'fight', 'downtime', 'adopt', 'robotic', 'promote', 'safety', 'speed', 'allow', 'ai', 'detect', 'anomaly', 'quickly', 'computer', 'vision', 'develop', 'system', 'process', 'vast', 'volume', 'datum', 'identify', 'pattern', 'predict', 'customer', 'need', 'manufacture', 'big', 'benefit', 'come', 'people', 'business', 'able', 'work', 'together', 'data', 'expert', 'use', 'datum', 'ai', 'get', 'insight', 'ultimately', 'take', 'action', 'improve', 'process', 'say', 'ai', 'solution', 'director', 'energy', 'manufacturing', 'dataiku', 'worker', 'get', 'familiar', 'ai', 'use', 'daily', 'basis', 'see', 'benefit', 'say', 'supplychain', 'disruption', 'worker', 'shortage', 'manufacturing', 'sector', 'innovate', 'stay', 'ahead', 'global', 'marketplace', 'however', 'study', 'dataiku', 'databrick', 'find', 'manufacture', 'lag', 'industry', 'quarter', 'company', 'still', 'exploration', 'experimentation', 'stage', 'term', 'ai', 'adoption', 'onefifth', 'company', 'industry', 'still', 'begin', 'stage', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","When considering the potential for AI systems to change manufacturing, Ritu Jyoti, global AI research lead at market-intelligence firm IDC, points to windmill manufacturers. To improve windmills before AI, she says, the company analyzed data from observing a functioning prototype, a process that took weeks. Now, the manufacturer has dramatically shortened the process using a…"
AI-powered 6G networks will reshape digital interactions,https://www.technologyreview.com/2023/10/26/1082028/ai-powered-6g-networks-will-reshape-digital-interactions/,2023-10-26,"In partnership withVivo Sixth-generation (6G) mobile networks, underpinned by artificial intelligence (AI), are poised to combine communication and computing in a hyperconnected world of digital and physical experiences that will transform daily lives, experts predict. “In the past, we talked about internet of things, but with 6G, we talk about intelligent or smart internet of things,” says Qin Fei, president of communications research institute at Vivo, a Chinese mobile phone maker that has stepped up R&D efforts into 6G since 2020. Communication and tech companies are already planning for 6G wireless networks, even though 5G has yet to be fully rolled out globally. With improved data latency, security, reliability, and the ability to process massive volumes of global data in real time, experts like Qin believe 6G is set to transform our leisure and work. Among the new use cases for 6G networks envisioned by Vivo are mixed reality, holographic and multi-sensory communication, interactive 3D virtual digital humans, collaborative robots, and automated driving. There are expectations for 6G to be deployed by 2030. The UN’s telecoms agency, International Telecommunication Union (ITU), has stated it plans to finish the initial 6G standardization process no later than the year 2030. Optimized by AI technologies, experts expect 6G to have a bigger impact than 5G for two reasons. One, because it will enable the convergence of computing and mobile communications. Two, because it will integrate digital and physical realms and introduce new sensory experiences for users. Qin says that “6G will provide super communication and ubiquitous information, and converge computing services, thus being the base for an interconnected and converged physical and digital world.” Capgemini agrees—predicting that 6G networks will enable immersive, ubiquitous, and sensory digital experiences on a massive scale. This will make it possible for 6G applications to “sense” their surroundings, and thereby turn the network into “our sixth sense”, according to a report by the consultancy. As each generation of wireless networks becomes increasingly complex, they rely on other technologies to harness their power and make them easier to run. 6G is expected to be one of the first AI-native networks, where AI is embedded in the networking equipment. This will enable the network to learn and manage itself, be more autonomous, and make it cheaper to run. “When we are designing the 6G network, we're going to use AI technology in designing the air interface and also in managing the 6G network,” says Qin. Machine learning and AI-based network automation will be crucial to simplify network management and optimization. “The 6G network with AI inside is like a very good student,” he adds. “The 6G network will self-train, self-learn, and it will actually grow as a student to become more and more powerful.” Although 6G standards and specifications are still under development, experts agree that it will be a leapfrog technology, thanks to its higher speed (estimates vary, but 6G could be between 10 times, 50 times, to 100 times faster than 5G) and significantly reduced latency; improved connectivity, security, and reliability; and an ability to integrate digital and physical versions of the world. “For 5G, it’s mainly a communication technology—that’s its core,” says Qin. “But for 6G, besides enhanced communications technology, it also includes computing, as well as other relevant services.” Another benefit is wider geographical coverage than 5G— 6G will cover the whole planet and connect all kinds of machines, he adds. Qin says 6G networks will also popularize the use of digital twins—virtual replicas of products or processes used to predict how the physical entities will perform in the real world. This will be possible due to 6G networks’ enhanced connectivity, stronger sensing capability, and capacity to collect massive amounts of data. According to Qin: “We could have more powerful connectivity and sensing capability, so we could install more sensors in the physical world and collect a massive amount of data about this world. With this data we could build models to rebuild the world in the digital arena.” Vivo believes 6G will support dozens or hundreds of new services in a wide range of industries. The company is developing prototype 6G mobile technologies based on three trends—communication plus sensing; communication plus computing; and communication plus AI. For example, Vivo is developing a prototype that can collect users’ biometric data to monitor their health while they are asleep. According to this technological vision, a person’s bedside phone could become a medical monitoring device. “If there is any health issue or abnormal behavior happening with respiration, then [the phone] could send an alert to the hospital,” says Qin. Vivo also sees virtual and mixed reality glasses as another potential application for 6G that could revolutionize video streaming by making it a more compelling and immersive experience. Current AR glasses have limited computing power, says Qin. “Therefore, it needs to connect as a kind of edge device to the cloud so it could provide better experiences for the users.” 6G will also support self-driving or autonomous cars. “I believe autonomous driving will be very popular after 2030 and be supported by 6G,” says Qin. “Driverless cars need to really gather all kinds of data, for example, about the ambient environment, about road conditions and even the adjacent cars in order to make informed decisions [such as] whether it should speed up or break. 6G can provide the computing power and network.” Although 5G mobile networks have yet to live up to initial expectations, most experts agree that 6G has the potential to deliver major advances in connectivity and computing power. However, like any complex and powerful new technology, 6G also faces challenges, including network capacity and energy consumption. Getting to the 6G era requires an increase in network capacity. Finding the right telecommunications spectrum to support its rollout is crucial. It has not been finalized but 6.4 to 15 gigahertz is under consideration for 6G. “We think that the spectrum for 6G should be on the lower spectrum, like 6.4 to 7.1 gigahertz, because the lower band electromagnetic wave physically has much better coverage and penetration characteristics,” says Qin. Minimizing 6G’s energy consumption and carbon emissions is another major task. 6G networks will have vastly more computing demands than 5G. Suppliers and users will need to cooperate to minimize energy use. According to a report by GSMA, which represents global telecom operators, energy-saving techniques (such as AI-driven sleep states and lithium-ion batteries) may help to make 6G more energy efficient. Ultimately, 6G will only succeed if it delivers great experiences and services for consumers and businesses, says Qin. “We should avoid overdesign of 6G network, and we should really collaborate with different verticals.” Problems faced by 5G networks—for example, bottlenecks in other technologies needed to support new terminals for 5G, such as material sciences for augmented reality equipment—should be lessons for 6G development, he adds. ""We hope that we can avoid these problems. That means we need the whole ecosystem to collaborate, to jointly develop 6G infrastructure, mobile terminals, and applications.” This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withVivo Sixth-generation ( 6G ) mobile networks , underpinned by artificial intelligence ( AI ) , are poised to combine communication and computing in a hyperconnected world of digital and physical experiences that will transform daily lives , experts predict . “ In the past , we talked about internet of things , but with 6G , we talk about intelligent or smart internet of things , ” says Qin Fei , president of communications research institute at Vivo , a Chinese mobile phone maker that has stepped up R & D efforts into 6G since 2020 . Communication and tech companies are already planning for 6G wireless networks , even though 5G has yet to be fully rolled out globally . With improved data latency , security , reliability , and the ability to process massive volumes of global data in real time , experts like Qin believe 6G is set to transform our leisure and work . Among the new use cases for 6G networks envisioned by Vivo are mixed reality , holographic and multi-sensory communication , interactive 3D virtual digital humans , collaborative robots , and automated driving . There are expectations for 6G to be deployed by 2030 . The UN ’ s telecoms agency , International Telecommunication Union ( ITU ) , has stated it plans to finish the initial 6G standardization process no later than the year 2030 . Optimized by AI technologies , experts expect 6G to have a bigger impact than 5G for two reasons . One , because it will enable the convergence of computing and mobile communications . Two , because it will integrate digital and physical realms and introduce new sensory experiences for users . Qin says that “ 6G will provide super communication and ubiquitous information , and converge computing services , thus being the base for an interconnected and converged physical and digital world. ” Capgemini agrees—predicting that 6G networks will enable immersive , ubiquitous , and sensory digital experiences on a massive scale . This will make it possible for 6G applications to “ sense ” their surroundings , and thereby turn the network into “ our sixth sense ” , according to a report by the consultancy . As each generation of wireless networks becomes increasingly complex , they rely on other technologies to harness their power and make them easier to run . 6G is expected to be one of the first AI-native networks , where AI is embedded in the networking equipment . This will enable the network to learn and manage itself , be more autonomous , and make it cheaper to run . “ When we are designing the 6G network , we 're going to use AI technology in designing the air interface and also in managing the 6G network , ” says Qin . Machine learning and AI-based network automation will be crucial to simplify network management and optimization . “ The 6G network with AI inside is like a very good student , ” he adds . “ The 6G network will self-train , self-learn , and it will actually grow as a student to become more and more powerful. ” Although 6G standards and specifications are still under development , experts agree that it will be a leapfrog technology , thanks to its higher speed ( estimates vary , but 6G could be between 10 times , 50 times , to 100 times faster than 5G ) and significantly reduced latency ; improved connectivity , security , and reliability ; and an ability to integrate digital and physical versions of the world . “ For 5G , it ’ s mainly a communication technology—that ’ s its core , ” says Qin . “ But for 6G , besides enhanced communications technology , it also includes computing , as well as other relevant services. ” Another benefit is wider geographical coverage than 5G— 6G will cover the whole planet and connect all kinds of machines , he adds . Qin says 6G networks will also popularize the use of digital twins—virtual replicas of products or processes used to predict how the physical entities will perform in the real world . This will be possible due to 6G networks ’ enhanced connectivity , stronger sensing capability , and capacity to collect massive amounts of data . According to Qin : “ We could have more powerful connectivity and sensing capability , so we could install more sensors in the physical world and collect a massive amount of data about this world . With this data we could build models to rebuild the world in the digital arena. ” Vivo believes 6G will support dozens or hundreds of new services in a wide range of industries . The company is developing prototype 6G mobile technologies based on three trends—communication plus sensing ; communication plus computing ; and communication plus AI . For example , Vivo is developing a prototype that can collect users ’ biometric data to monitor their health while they are asleep . According to this technological vision , a person ’ s bedside phone could become a medical monitoring device . “ If there is any health issue or abnormal behavior happening with respiration , then [ the phone ] could send an alert to the hospital , ” says Qin . Vivo also sees virtual and mixed reality glasses as another potential application for 6G that could revolutionize video streaming by making it a more compelling and immersive experience . Current AR glasses have limited computing power , says Qin . “ Therefore , it needs to connect as a kind of edge device to the cloud so it could provide better experiences for the users. ” 6G will also support self-driving or autonomous cars . “ I believe autonomous driving will be very popular after 2030 and be supported by 6G , ” says Qin . “ Driverless cars need to really gather all kinds of data , for example , about the ambient environment , about road conditions and even the adjacent cars in order to make informed decisions [ such as ] whether it should speed up or break . 6G can provide the computing power and network. ” Although 5G mobile networks have yet to live up to initial expectations , most experts agree that 6G has the potential to deliver major advances in connectivity and computing power . However , like any complex and powerful new technology , 6G also faces challenges , including network capacity and energy consumption . Getting to the 6G era requires an increase in network capacity . Finding the right telecommunications spectrum to support its rollout is crucial . It has not been finalized but 6.4 to 15 gigahertz is under consideration for 6G . “ We think that the spectrum for 6G should be on the lower spectrum , like 6.4 to 7.1 gigahertz , because the lower band electromagnetic wave physically has much better coverage and penetration characteristics , ” says Qin . Minimizing 6G ’ s energy consumption and carbon emissions is another major task . 6G networks will have vastly more computing demands than 5G . Suppliers and users will need to cooperate to minimize energy use . According to a report by GSMA , which represents global telecom operators , energy-saving techniques ( such as AI-driven sleep states and lithium-ion batteries ) may help to make 6G more energy efficient . Ultimately , 6G will only succeed if it delivers great experiences and services for consumers and businesses , says Qin . “ We should avoid overdesign of 6G network , and we should really collaborate with different verticals. ” Problems faced by 5G networks—for example , bottlenecks in other technologies needed to support new terminals for 5G , such as material sciences for augmented reality equipment—should be lessons for 6G development , he adds . `` We hope that we can avoid these problems . That means we need the whole ecosystem to collaborate , to jointly develop 6G infrastructure , mobile terminals , and applications. ” This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'sixthgeneration', 'g', 'mobile', 'network', 'underpin', 'artificial', 'intelligence', 'poise', 'combine', 'communication', 'compute', 'hyperconnected', 'world', 'digital', 'physical', 'experience', 'transform', 'daily', 'life', 'expert', 'predict', 'past', 'talk', 'internet', 'thing', 'g', 'talk', 'intelligent', 'smart', 'internet', 'thing', 'say', 'president', 'chinese', 'mobile', 'phone', 'maker', 'step', 'r', 'effort', 'g', 'communication', 'tech', 'company', 'already', 'plan', 'g', 'wireless', 'network', 'even', 'g', 'yet', 'fully', 'roll', 'globally', 'improve', 'data', 'latency', 'security', 'reliability', 'ability', 'process', 'massive', 'volume', 'global', 'datum', 'real', 'time', 'expert', 'believe', 'g', 'set', 'transform', 'leisure', 'work', 'new', 'use', 'case', 'g', 'network', 'envision', 'vivo', 'mixed', 'reality', 'holographic', 'multisensory', 'communication', 'interactive', '3d', 'virtual', 'digital', 'human', 'collaborative', 'robot', 'automate', 'driving', 'expectation', 'g', 'deploy', 'telecom', 'agency', 'international', 'telecommunication', 'state', 'plan', 'finish', 'initial', 'g', 'standardization', 'process', 'later', 'year', 'optimize', 'technology', 'expert', 'expect', 'g', 'big', 'impact', 'g', 'reason', 'enable', 'convergence', 'computing', 'mobile', 'communication', 'integrate', 'digital', 'physical', 'realm', 'introduce', 'new', 'sensory', 'experience', 'user', 'qin', 'say', 'g', 'provide', 'super', 'communication', 'ubiquitous', 'information', 'converge', 'computing', 'service', 'thus', 'base', 'interconnected', 'converge', 'physical', 'digital', 'world', 'agree', 'predict', 'g', 'network', 'enable', 'immersive', 'ubiquitous', 'sensory', 'digital', 'experience', 'massive', 'scale', 'make', 'possible', 'g', 'application', 'sense', 'surrounding', 'thereby', 'turn', 'network', 'sixth', 'sense', 'accord', 'report', 'consultancy', 'generation', 'wireless', 'network', 'become', 'increasingly', 'complex', 'rely', 'technology', 'harness', 'power', 'make', 'easy', 'run', 'g', 'expect', 'first', 'ainative', 'network', 'embed', 'networking', 'equipment', 'enable', 'network', 'learn', 'manage', 'autonomous', 'make', 'cheap', 'run', 'design', 'g', 'network', 'go', 'use', 'ai', 'technology', 'design', 'air', 'interface', 'also', 'manage', 'g', 'network', 'say', 'machine', 'learning', 'aibased', 'network', 'automation', 'crucial', 'simplify', 'network', 'management', 'optimization', 'g', 'network', 'inside', 'good', 'student', 'add', 'g', 'network', 'selftrain', 'selflearn', 'actually', 'grow', 'student', 'become', 'powerful', 'g', 'standard', 'specification', 'still', 'development', 'expert', 'agree', 'leapfrog', 'technology', 'thank', 'high', 'speed', 'estimate', 'vary', 'g', 'time', 'time', 'time', 'fast', 'g', 'significantly', 'reduce', 'latency', 'improve', 'connectivity', 'security', 'reliability', 'ability', 'integrate', 'digital', 'physical', 'version', 'world', 'g', 'mainly', 'communication', 'technology', 'core', 'say', 'g', 'enhanced', 'communication', 'technology', 'also', 'include', 'compute', 'well', 'relevant', 'service', 'benefit', 'wide', 'geographical', 'coverage', 'g', 'g', 'cover', 'whole', 'planet', 'connect', 'kind', 'machine', 'add', 'qin', 'say', 'g', 'network', 'also', 'popularize', 'use', 'digital', 'twin', 'virtual', 'replica', 'product', 'process', 'use', 'predict', 'physical', 'entity', 'perform', 'real', 'world', 'possible', 'due', 'g', 'network', 'enhance', 'connectivity', 'strong', 'sensing', 'capability', 'capacity', 'collect', 'massive', 'amount', 'datum', 'accord', 'powerful', 'connectivity', 'sensing', 'capability', 'install', 'sensor', 'physical', 'world', 'collect', 'massive', 'amount', 'datum', 'world', 'datum', 'build', 'model', 'rebuild', 'world', 'digital', 'arena', 'vivo', 'believe', 'g', 'support', 'dozen', 'hundred', 'new', 'service', 'wide', 'range', 'industry', 'company', 'develop', 'prototype', 'g', 'mobile', 'technology', 'base', 'trend', 'communication', 'sensing', 'communication', 'computing', 'communication', 'ai', 'example', 'vivo', 'develop', 'prototype', 'collect', 'user', 'biometric', 'datum', 'monitor', 'health', 'asleep', 'accord', 'technological', 'vision', 'person', 'bedside', 'phone', 'become', 'medical', 'monitoring', 'device', 'health', 'issue', 'abnormal', 'behavior', 'happen', 'respiration', 'phone', 'send', 'alert', 'hospital', 'say', 'vivo', 'also', 'see', 'virtual', 'mixed', 'reality', 'glass', 'potential', 'application', 'g', 'revolutionize', 'video', 'streaming', 'make', 'compelling', 'immersive', 'experience', 'current', 'ar', 'glass', 'limit', 'computing', 'power', 'say', 'therefore', 'need', 'connect', 'kind', 'edge', 'device', 'cloud', 'provide', 'well', 'experience', 'user', 'g', 'also', 'support', 'selfdriving', 'autonomous', 'car', 'believe', 'autonomous', 'driving', 'popular', 'support', 'g', 'say', 'driverless', 'car', 'need', 'really', 'gather', 'kind', 'datum', 'example', 'ambient', 'environment', 'road', 'condition', 'even', 'adjacent', 'car', 'order', 'make', 'informed', 'decision', 'speed', 'break', 'g', 'provide', 'computing', 'power', 'network', 'g', 'mobile', 'network', 'yet', 'live', 'initial', 'expectation', 'expert', 'agree', 'g', 'potential', 'deliver', 'major', 'advance', 'connectivity', 'computing', 'power', 'however', 'complex', 'powerful', 'new', 'technology', 'g', 'also', 'face', 'challenge', 'include', 'network', 'capacity', 'energy', 'consumption', 'get', 'g', 'era', 'require', 'increase', 'network', 'capacity', 'find', 'right', 'telecommunication', 'spectrum', 'support', 'rollout', 'crucial', 'finalize', 'gigahertz', 'consideration', 'g', 'think', 'spectrum', 'g', 'low', 'spectrum', 'gigahertz', 'low', 'band', 'electromagnetic', 'wave', 'physically', 'much', 'well', 'coverage', 'penetration', 'characteristic', 'say', 'minimize', 'g', 'energy', 'consumption', 'carbon', 'emission', 'major', 'task', 'g', 'network', 'vastly', 'computing', 'demand', 'g', 'supplier', 'user', 'need', 'cooperate', 'minimize', 'energy', 'use', 'accord', 'report', 'gsma', 'represent', 'global', 'telecom', 'operator', 'energysave', 'technique', 'sleep', 'state', 'lithiumion', 'battery', 'help', 'make', 'energy', 'efficient', 'ultimately', 'g', 'succeed', 'deliver', 'great', 'experience', 'service', 'consumer', 'business', 'say', 'avoid', 'overdesign', 'g', 'network', 'really', 'collaborate', 'different', 'vertical', 'problem', 'face', 'g', 'network', 'example', 'bottleneck', 'technology', 'need', 'support', 'new', 'terminal', 'g', 'material', 'science', 'augment', 'reality', 'equipment', 'lesson', 'g', 'development', 'add', 'hope', 'avoid', 'problem', 'mean', 'need', 'whole', 'ecosystem', 'collaborate', 'jointly', 'develop', 'g', 'infrastructure', 'mobile', 'terminal', 'application', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Sixth-generation (6G) mobile networks, underpinned by artificial intelligence (AI), are poised to combine communication and computing in a hyperconnected world of digital and physical experiences that will transform daily lives, experts predict. “In the past, we talked about internet of things, but with 6G, we talk about intelligent or smart internet of things,” says Qin…"
Enabling enterprise growth with data intelligence,https://www.technologyreview.com/2023/10/19/1081876/enabling-enterprise-growth-with-data-intelligence/,2023-10-19,"In partnership withHitachi Vantara  Data — how it’s stored and managed — has become a key competitive differentiator. As global data continues to grow exponentially, organizations face many hurdles between piling up historical data, real-time data streams from IoT sensors, and building data-driven supply chains. Senior vice president of product engineering at Hitachi Vantara, Bharti Patel sees these challenges as an opportunity to create a better data strategy. “Before enterprises can become data-driven, they must first become data intelligent,"" says Patel. ""That means knowing more about the data you have, whether you need to keep it or not, or where it should reside to derive the most value out of it."" Patel stresses that the data journey begins with data planning that includes all stakeholders from CIOs and CTOs to business users. Patel describes universal data intelligence as enterprises having the ability to gain better insights from data streams and meet increasing demands for transparency by offering seamless access to data and insights no matter where it resides. Building this intelligence means building a data infrastructure that is scalable, secure, cost-effective, and socially responsible. The public cloud is often lauded as a way for enterprises to innovate with agility at scale while on premises infrastructures are viewed as less accessible and user friendly. But while data streams continue to grow, IT budgets are not and Patel notes that many organizations that use the cloud are facing cost challenges. Combating this, says Patel, means finding the best of both worlds of both on-prem and cloud environments in private data centers to keep costs low but insights flowing. Looking ahead, Patel foresees a future of total automation. Today, data resides in many places from the minds of experts to documentation to IT support tickets, making it impossible for one person to be able to analyze all that data and glean meaningful insights. “As we go into the future, we'll see more manual operations converted into automated operations,"" says Patel. ""First, we'll see humans in the loop, and eventually we'll see a trend towards fully autonomous data centers."" This episode of Business Lab is produced in partnership with Hitachi Vantara. Laurel Ruma: From MIT Technology Review, I'm Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is building better data infrastructures. Doing just the basics with data can be difficult, but when it comes to scaling and adopting emerging technologies, it's crucial to organize data, tear down data silos, and focus on how data infrastructure, which is so often in the background, comes to the front of your data strategy.Two words for you: data intelligence.My guest is Bharti Patel. Bharti is a senior vice president of product engineering at Hitachi Vantara. This episode of Business Lab is sponsored by Hitachi Vantara. Welcome, Bharti. Bharti Patel: Hey, thank you Laurel. Nice to be with you again. Laurel: So let's start off with kind of giving some context to this discussion. As global data continues to grow exponentially, according to IDC, it's projected to double between 2022 and 2026. Enterprises face many hurdles to becoming data-driven. These hurdles include, but aren't of course limited to, piles of historical data, new real-time data streams, and supply chains becoming more data-driven. How should enterprises be evaluating their data strategies? And what are the markers of a strong data infrastructure? Bharti: Yeah, Laurel, I can't agree more with you here. Data is growing exponentially, and as per one of the studies that we conducted recently where we talked to about 1,200 CIOs and CTOs from about 12 countries, then we have more proof for it that data is almost going to double every two to three years. And I think what's more interesting here is that data is going to grow, but their budgets are not going to grow in the same proportion. So instead of worrying about it, I want to tackle this problem differently. I want to look at how we convert this challenge into an opportunity by deriving value out of this deal. So let's talk a little more about this in the context of what's happening in the industry today. I'm sure everyone by now has heard about generative AI and why generative AI or gen AI is a buzzword. AI has been there in the industry forever. However, what has changed recently is ChatGPT has exposed the power of AI to common people right from school going kids to grandparents by providing a very simple natural language interface. And just to talk a little bit more about ChatGPT, it is the fastest growing app in the industry. It touched 100 million users in just about two months. And what has changed because of this very fast adoption is that this has got businesses interested in it. Everyone wants to see how to unleash the power of generative AI. In fact, according to McKinsey, they're saying it's like it's going to add about $2.6 trillion to $4.4 trillion to the global economy. That means we are talking about big numbers here, but everyone's talking about ChatGPT, but what is the science behind it? The science behind it is the large language models. And if you think of these large language models, they are AI models with billions or even trillions of parameters, and they are the science behind ChatGPT. However, to get most of these large language models or LLMs, they need to be fine-tuned because that means you're just relying on the public data. Then what you're getting, it means you're not getting first, you're not getting the information that you want, correct all the time. And of course there is a risk of people feeding bad data associated with it. So how do you make the most of it? And here actually comes your private data sets. So your proprietary data sets are very, very important here. And if you use this private data to fine-tune your models, I have no doubt in mind that it will create differentiation for you in the long run to remain competitive. So I think even with this, we're just scratching the surface here when it comes to gen AI. And what more needs to be thought about for enterprise adoption is all the features that are needed like explainability, traceability, quality, trustworthiness, reliability. So if you again look at all these parameters, actually data is again the centerpiece of everything here. And you have to harness this private data, you have to curate it, and you have to create the data sets that will give you the maximum return on investment. Now, before enterprises can become data-driven, I think they must first become data intelligent. And that means knowing more about the data you have, whether you need to keep it or not, or where it should reside to derive the most value out of it. And as I talk to more and more CIOs and CTOs, it is very evident that there's a lot of data out there and we need to find a way to fix the problem. Because that data may or may not be useful, but you are storing it, you are keeping it, and you are spending money on it. So that is definitely a problem that needs to be solved. Then back to your question of, what is the right infrastructure, what are some of the parameters of it? So in my mind, it needs to be nimble, it needs to be scalable, trusted, secured, cost-effective, and finally socially responsible. Laurel: That certainly gives us a lot of perspective, Bharti. So customers are demanding more access to data and enterprises also need to get better insights from the streams of data that they're accumulating. So could you describe what universal data intelligence is, and then how it relates to data infrastructure? Bharti: Universal data intelligence is the ability for businesses to offer seamless access to data and insights irrespective of where it resides. So basically we are talking about getting full insights into your data in a hybrid environment. Also, on the same lines, we also talk about our approach to infrastructure, which is a distributed approach. And what I mean by distributed is that you do as little data movement as possible because moving data from one place to another place is expensive. So what we are doing here at Hitachi Vantara, we are designing systems. Think of it as there is an elastic fabric that ties it all together and we are able to get insights from the data no matter where it resides in a very, very timely manner. And even this data could be in any format, from structured, unstructured, and it could be blocked to file to objects. And just to kind of give you an example of the same, recently we worked with the Arizona Department of Water Resources to simplify their data management strategy. They have data coming from more than 300,000 water resources like means we are talking about huge data sets here. And what we did there for them was we designed an intelligent data discovery and automation tool. And in fact, we completed this data discovery and the metadata cataloging and platform migration in just two weeks with minimal downtime. And we are hearing all the time from them that they are really happy with it and they're now able to understand, integrate, and analyze the data sets to meet the needs of their water users, their planners, and their decision makers. Laurel: So that's a great example. So data and how it's stored and managed is clearly a competitive differentiator as well. But although the amount of data is increasing, many budgets, as you mentioned, particularly IT budgets are not. So how can organizations navigate building a data infrastructure that's effective and cost-efficient? And then do you have another example of how to do more with less? Bharti: Yeah, I think that's a great question. And this goes back to having data intelligence as the first step to becoming data-driven and reaping the full benefits of the data. So I think it goes back to you needing to know what exists and why it exists. And all of it should be available to the decision makers and the people who are working on the data at their fingertips. Just to give an example here, suppose you have data that you're just retaining because you need to just retain it for legal purposes, and the likelihood of it being used is extremely, extremely low. So there's no point in storing that data on an expensive storage device. It makes sense to transfer that data to a low cost object storage. And at the same time, you might have the data that you need to access all the time. And speed is important. Low latency is important, and that kind of data needs to reside on fast NVMEs. And in fact, many of our customers do it all the time, and in fact in all the sectors. So what they do is they have their data, which through the policies, they constantly transfer from our highly, highly efficient file systems to object storage based on the policies. And it's like they still retain the pointers there in the file system and they're able to access it back in case they need it. Laurel: So the public cloud is often cited as a way for enterprises to scale, be more agile, and innovate while by contrast, legacy on-premises infrastructures are seen as less user-friendly and accessible. How accurate is this conception and how should enterprises approach data modernization and management of that data? Bharti: Yeah, I've got to admit here that the public cloud and the hyperscalers have raised the bar in terms of what is possible when it comes to innovation. However, we are also seeing and hearing from our customers that the cost is a concern there. And in fact, many of our customers, they move to cloud very fast and now they're facing the cost challenge. When their CIOs see the bills going exponentially up, they're asking like, ""Hey, well how could we keep it flat?"" That's where I think we see a big opportunity, how to provide the same experience that cloud provides in a private data center so that when customers are talking about partition of the data, we have something equivalent to offer. And here again, I have got to say that we want to address in a slightly different manner. I think we want to address it so that customers are able to take full advantage of the elasticity of the cloud, and also they're able to take full advantage of on-prem environments. And how we want to do it, we want to do it in such a way that it's almost in a seamless way, in a seamless manner. They can manage the data from their private data centers, doing the cloud and get the best from both worlds. Laurel: An interesting perspective there, but this also kind of requires different elements of the business to come in. So from a leadership perspective, what are some best practices that you've instituted or recommended to make that transition to better data management? Bharti: Yeah, I would say I think the data journey starts with data planning, and which should not be done in a siloed manner. And getting it right from the onset is extremely, extremely important. And what you need to do here is at the beginning of your data planning, you've got to get all the stakeholders together, whether it's your CIO, your business users, your CTOs. So this strategy should never be done in a siloed manner. And in fact, I do want to think about, highlight another aspect, which probably people don't do very much is how do you even bring your partners into the mix? In fact, I do have an example here. Prior to joining Hitachi Vantara, I was a CTO, an air purifier company. And as we were defining our data strategy, we were looking at our Salesforce data, we were looking at data in our NetSuite, we were looking at the customer tickets, and we were doing all this to see how we can drive marketing campaigns. And as I was looking at this data, I felt that something was totally missing. And in fact, what was missing was the weather data, which is not our data, which was third-party data. For us to design effective marketing campaigns, it was very important for us to have insights into this weather data. For example, if there are allergies in a particular region or if there are wildfires in a particular region. And that data was so important. So having a strategy where you are able to bring all stakeholders, all parts of data together and think from the beginning is the right thing to get started. Laurel: And with big hairy problems and goals, there's also this consideration that data centers contribute to an enterprise's carbon emissions. Thinking about partnerships and modernizing data management and everything we've talked about so far, how can enterprises meet sustainability goals while also modernizing their data infrastructure to accommodate all of their historical and real-time data, especially when it comes from, as you mentioned, so many different sources? Bharti: Yeah, I'm glad that you are bringing up this point because it's very important not to ignore this. And in fact, with all the gen AI and all the things that we are talking about, like one fine-tuning of one model can actually generate up to five times the carbon emissions that are possible from a passenger car in a lifetime. So we're talking about a huge, huge environmental effect here. And this particular topic is extremely important to Hitachi. And in fact, our goal is to go carbon-neutral with our operations by 2030 and across our value chain by 2050. And how we are addressing this problem here is kind of both on the hardware side and also on the software side. Right from the onset, we are designing our hardware, we are looking at end-to-end components to see what kind of carbon footprint it creates and how we could really minimize it. And in fact, once our hardware is ready, actually, it needs to pass through a very stringent set of energy certifications. And so that's on the hardware side. Now, on the software side, actually, I have just started this initiative where we are looking at how we can move to modern languages that are more likely to create less carbon footprint. And this is where we are looking at how we can replace our existing Java [code base] with Rust, wherever it makes sense. And again, this is a big problem we all need to think about and it cannot be solved overnight, but we have to constantly think about interface manner. Laurel: Well, certainly are impressive goals. How can emerging technologies like generative AI, as you were saying before, help push an organization into a next generation of data infrastructure systems, but then also help differentiate it from competitors? Bharti: Yeah, I want to take a kind of a two-pronged approach here. First, what I call is table stakes. So if you don't do it, you'll be completely wiped out. And these are simple things about how you automate certain things, how you create better customer experience. But in my mind, that's not enough. You got to think about what kind of disruptions you will create for yourself and for your customers. So a couple of ideas that we are working on here are the companions or copilots. And these are, think of them as AI agents in the data centers. And these agents actually help the data center environment from becoming more reactive to proactive. So basically these agents are running in your data center all the time and they're watching if there is a new patch available and if you should update to the new patch, or maybe there's a new white paper that has better insights to manage some of your resources. So this is like these agents are constantly acting in your data center. They are aware of what's going on on the internet based on how you have designed, and they're able to provide you with creative solutions. And I think that's going to be the disruption here, and that's something we are working on. Laurel: So looking to the future, what tools, technologies, or trends do you see emerging as more and more enterprises look to modernize their data infrastructure and really benefit from data intelligence? Bharti: Again, I'll go back to what I'm talking about, generative AI here, and I'll give an example. For one of our customers, we are managing their data center, and I'm also part of that channel where we see constant back and forth between the support and the engineering. The support is asking, ""Hey, this is what is happening, what should we be doing?"" So just think of it like a different scenario that you have all this and you were able to collect this data and feed it into the LLMs. When you're talking about this data, this data resides at several places. It resides in the heads of our experts. It is there in the documentation, it's there in the support tickets, it's there in logs, like life logs. It is there in the traces. So it's almost impossible for a human being to analyze this data and get meaningful insights. However, if we combine LLMs with the power of, say, knowledge graphs, vector databases, and other tools, it will be possible to analyze this data at the speed of light, and present the recommendation in front of the user through a very simple user interface. And in most cases, just via a very simple natural language interface. So I think that's a kind of a complete paradigm shift where you have so many sources that you need to constantly analyze versus having the full automation. And that's why I feel that these copilots will become an essential part of the data centers. In the beginning they'll help with the automation to deal with the problems prevalent in any data center like resource management and optimization, proactive problem determination, and resolution of the same. As we go into the future, we'll see more manual operations converted into automated operations. First, we'll see humans in the loop, and eventually we'll see a trend towards fully autonomous data centers. Laurel: Well, that is quite a future. Thank you very much for joining us today on the Business Lab. Bharti: Thank you, Laurel. Bye-bye. Laurel: That was Bharti Patel, who is the senior vice president of Product Marketing at Hitachi Vantara who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review.That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.","In partnership withHitachi Vantara Data — how it ’ s stored and managed — has become a key competitive differentiator . As global data continues to grow exponentially , organizations face many hurdles between piling up historical data , real-time data streams from IoT sensors , and building data-driven supply chains . Senior vice president of product engineering at Hitachi Vantara , Bharti Patel sees these challenges as an opportunity to create a better data strategy . “ Before enterprises can become data-driven , they must first become data intelligent , '' says Patel . `` That means knowing more about the data you have , whether you need to keep it or not , or where it should reside to derive the most value out of it . '' Patel stresses that the data journey begins with data planning that includes all stakeholders from CIOs and CTOs to business users . Patel describes universal data intelligence as enterprises having the ability to gain better insights from data streams and meet increasing demands for transparency by offering seamless access to data and insights no matter where it resides . Building this intelligence means building a data infrastructure that is scalable , secure , cost-effective , and socially responsible . The public cloud is often lauded as a way for enterprises to innovate with agility at scale while on premises infrastructures are viewed as less accessible and user friendly . But while data streams continue to grow , IT budgets are not and Patel notes that many organizations that use the cloud are facing cost challenges . Combating this , says Patel , means finding the best of both worlds of both on-prem and cloud environments in private data centers to keep costs low but insights flowing . Looking ahead , Patel foresees a future of total automation . Today , data resides in many places from the minds of experts to documentation to IT support tickets , making it impossible for one person to be able to analyze all that data and glean meaningful insights . “ As we go into the future , we 'll see more manual operations converted into automated operations , '' says Patel . `` First , we 'll see humans in the loop , and eventually we 'll see a trend towards fully autonomous data centers . '' This episode of Business Lab is produced in partnership with Hitachi Vantara . Laurel Ruma : From MIT Technology Review , I 'm Laurel Ruma and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is building better data infrastructures . Doing just the basics with data can be difficult , but when it comes to scaling and adopting emerging technologies , it 's crucial to organize data , tear down data silos , and focus on how data infrastructure , which is so often in the background , comes to the front of your data strategy.Two words for you : data intelligence.My guest is Bharti Patel . Bharti is a senior vice president of product engineering at Hitachi Vantara . This episode of Business Lab is sponsored by Hitachi Vantara . Welcome , Bharti . Bharti Patel : Hey , thank you Laurel . Nice to be with you again . Laurel : So let 's start off with kind of giving some context to this discussion . As global data continues to grow exponentially , according to IDC , it 's projected to double between 2022 and 2026 . Enterprises face many hurdles to becoming data-driven . These hurdles include , but are n't of course limited to , piles of historical data , new real-time data streams , and supply chains becoming more data-driven . How should enterprises be evaluating their data strategies ? And what are the markers of a strong data infrastructure ? Bharti : Yeah , Laurel , I ca n't agree more with you here . Data is growing exponentially , and as per one of the studies that we conducted recently where we talked to about 1,200 CIOs and CTOs from about 12 countries , then we have more proof for it that data is almost going to double every two to three years . And I think what 's more interesting here is that data is going to grow , but their budgets are not going to grow in the same proportion . So instead of worrying about it , I want to tackle this problem differently . I want to look at how we convert this challenge into an opportunity by deriving value out of this deal . So let 's talk a little more about this in the context of what 's happening in the industry today . I 'm sure everyone by now has heard about generative AI and why generative AI or gen AI is a buzzword . AI has been there in the industry forever . However , what has changed recently is ChatGPT has exposed the power of AI to common people right from school going kids to grandparents by providing a very simple natural language interface . And just to talk a little bit more about ChatGPT , it is the fastest growing app in the industry . It touched 100 million users in just about two months . And what has changed because of this very fast adoption is that this has got businesses interested in it . Everyone wants to see how to unleash the power of generative AI . In fact , according to McKinsey , they 're saying it 's like it 's going to add about $ 2.6 trillion to $ 4.4 trillion to the global economy . That means we are talking about big numbers here , but everyone 's talking about ChatGPT , but what is the science behind it ? The science behind it is the large language models . And if you think of these large language models , they are AI models with billions or even trillions of parameters , and they are the science behind ChatGPT . However , to get most of these large language models or LLMs , they need to be fine-tuned because that means you 're just relying on the public data . Then what you 're getting , it means you 're not getting first , you 're not getting the information that you want , correct all the time . And of course there is a risk of people feeding bad data associated with it . So how do you make the most of it ? And here actually comes your private data sets . So your proprietary data sets are very , very important here . And if you use this private data to fine-tune your models , I have no doubt in mind that it will create differentiation for you in the long run to remain competitive . So I think even with this , we 're just scratching the surface here when it comes to gen AI . And what more needs to be thought about for enterprise adoption is all the features that are needed like explainability , traceability , quality , trustworthiness , reliability . So if you again look at all these parameters , actually data is again the centerpiece of everything here . And you have to harness this private data , you have to curate it , and you have to create the data sets that will give you the maximum return on investment . Now , before enterprises can become data-driven , I think they must first become data intelligent . And that means knowing more about the data you have , whether you need to keep it or not , or where it should reside to derive the most value out of it . And as I talk to more and more CIOs and CTOs , it is very evident that there 's a lot of data out there and we need to find a way to fix the problem . Because that data may or may not be useful , but you are storing it , you are keeping it , and you are spending money on it . So that is definitely a problem that needs to be solved . Then back to your question of , what is the right infrastructure , what are some of the parameters of it ? So in my mind , it needs to be nimble , it needs to be scalable , trusted , secured , cost-effective , and finally socially responsible . Laurel : That certainly gives us a lot of perspective , Bharti . So customers are demanding more access to data and enterprises also need to get better insights from the streams of data that they 're accumulating . So could you describe what universal data intelligence is , and then how it relates to data infrastructure ? Bharti : Universal data intelligence is the ability for businesses to offer seamless access to data and insights irrespective of where it resides . So basically we are talking about getting full insights into your data in a hybrid environment . Also , on the same lines , we also talk about our approach to infrastructure , which is a distributed approach . And what I mean by distributed is that you do as little data movement as possible because moving data from one place to another place is expensive . So what we are doing here at Hitachi Vantara , we are designing systems . Think of it as there is an elastic fabric that ties it all together and we are able to get insights from the data no matter where it resides in a very , very timely manner . And even this data could be in any format , from structured , unstructured , and it could be blocked to file to objects . And just to kind of give you an example of the same , recently we worked with the Arizona Department of Water Resources to simplify their data management strategy . They have data coming from more than 300,000 water resources like means we are talking about huge data sets here . And what we did there for them was we designed an intelligent data discovery and automation tool . And in fact , we completed this data discovery and the metadata cataloging and platform migration in just two weeks with minimal downtime . And we are hearing all the time from them that they are really happy with it and they 're now able to understand , integrate , and analyze the data sets to meet the needs of their water users , their planners , and their decision makers . Laurel : So that 's a great example . So data and how it 's stored and managed is clearly a competitive differentiator as well . But although the amount of data is increasing , many budgets , as you mentioned , particularly IT budgets are not . So how can organizations navigate building a data infrastructure that 's effective and cost-efficient ? And then do you have another example of how to do more with less ? Bharti : Yeah , I think that 's a great question . And this goes back to having data intelligence as the first step to becoming data-driven and reaping the full benefits of the data . So I think it goes back to you needing to know what exists and why it exists . And all of it should be available to the decision makers and the people who are working on the data at their fingertips . Just to give an example here , suppose you have data that you 're just retaining because you need to just retain it for legal purposes , and the likelihood of it being used is extremely , extremely low . So there 's no point in storing that data on an expensive storage device . It makes sense to transfer that data to a low cost object storage . And at the same time , you might have the data that you need to access all the time . And speed is important . Low latency is important , and that kind of data needs to reside on fast NVMEs . And in fact , many of our customers do it all the time , and in fact in all the sectors . So what they do is they have their data , which through the policies , they constantly transfer from our highly , highly efficient file systems to object storage based on the policies . And it 's like they still retain the pointers there in the file system and they 're able to access it back in case they need it . Laurel : So the public cloud is often cited as a way for enterprises to scale , be more agile , and innovate while by contrast , legacy on-premises infrastructures are seen as less user-friendly and accessible . How accurate is this conception and how should enterprises approach data modernization and management of that data ? Bharti : Yeah , I 've got to admit here that the public cloud and the hyperscalers have raised the bar in terms of what is possible when it comes to innovation . However , we are also seeing and hearing from our customers that the cost is a concern there . And in fact , many of our customers , they move to cloud very fast and now they 're facing the cost challenge . When their CIOs see the bills going exponentially up , they 're asking like , `` Hey , well how could we keep it flat ? '' That 's where I think we see a big opportunity , how to provide the same experience that cloud provides in a private data center so that when customers are talking about partition of the data , we have something equivalent to offer . And here again , I have got to say that we want to address in a slightly different manner . I think we want to address it so that customers are able to take full advantage of the elasticity of the cloud , and also they 're able to take full advantage of on-prem environments . And how we want to do it , we want to do it in such a way that it 's almost in a seamless way , in a seamless manner . They can manage the data from their private data centers , doing the cloud and get the best from both worlds . Laurel : An interesting perspective there , but this also kind of requires different elements of the business to come in . So from a leadership perspective , what are some best practices that you 've instituted or recommended to make that transition to better data management ? Bharti : Yeah , I would say I think the data journey starts with data planning , and which should not be done in a siloed manner . And getting it right from the onset is extremely , extremely important . And what you need to do here is at the beginning of your data planning , you 've got to get all the stakeholders together , whether it 's your CIO , your business users , your CTOs . So this strategy should never be done in a siloed manner . And in fact , I do want to think about , highlight another aspect , which probably people do n't do very much is how do you even bring your partners into the mix ? In fact , I do have an example here . Prior to joining Hitachi Vantara , I was a CTO , an air purifier company . And as we were defining our data strategy , we were looking at our Salesforce data , we were looking at data in our NetSuite , we were looking at the customer tickets , and we were doing all this to see how we can drive marketing campaigns . And as I was looking at this data , I felt that something was totally missing . And in fact , what was missing was the weather data , which is not our data , which was third-party data . For us to design effective marketing campaigns , it was very important for us to have insights into this weather data . For example , if there are allergies in a particular region or if there are wildfires in a particular region . And that data was so important . So having a strategy where you are able to bring all stakeholders , all parts of data together and think from the beginning is the right thing to get started . Laurel : And with big hairy problems and goals , there 's also this consideration that data centers contribute to an enterprise 's carbon emissions . Thinking about partnerships and modernizing data management and everything we 've talked about so far , how can enterprises meet sustainability goals while also modernizing their data infrastructure to accommodate all of their historical and real-time data , especially when it comes from , as you mentioned , so many different sources ? Bharti : Yeah , I 'm glad that you are bringing up this point because it 's very important not to ignore this . And in fact , with all the gen AI and all the things that we are talking about , like one fine-tuning of one model can actually generate up to five times the carbon emissions that are possible from a passenger car in a lifetime . So we 're talking about a huge , huge environmental effect here . And this particular topic is extremely important to Hitachi . And in fact , our goal is to go carbon-neutral with our operations by 2030 and across our value chain by 2050 . And how we are addressing this problem here is kind of both on the hardware side and also on the software side . Right from the onset , we are designing our hardware , we are looking at end-to-end components to see what kind of carbon footprint it creates and how we could really minimize it . And in fact , once our hardware is ready , actually , it needs to pass through a very stringent set of energy certifications . And so that 's on the hardware side . Now , on the software side , actually , I have just started this initiative where we are looking at how we can move to modern languages that are more likely to create less carbon footprint . And this is where we are looking at how we can replace our existing Java [ code base ] with Rust , wherever it makes sense . And again , this is a big problem we all need to think about and it can not be solved overnight , but we have to constantly think about interface manner . Laurel : Well , certainly are impressive goals . How can emerging technologies like generative AI , as you were saying before , help push an organization into a next generation of data infrastructure systems , but then also help differentiate it from competitors ? Bharti : Yeah , I want to take a kind of a two-pronged approach here . First , what I call is table stakes . So if you do n't do it , you 'll be completely wiped out . And these are simple things about how you automate certain things , how you create better customer experience . But in my mind , that 's not enough . You got to think about what kind of disruptions you will create for yourself and for your customers . So a couple of ideas that we are working on here are the companions or copilots . And these are , think of them as AI agents in the data centers . And these agents actually help the data center environment from becoming more reactive to proactive . So basically these agents are running in your data center all the time and they 're watching if there is a new patch available and if you should update to the new patch , or maybe there 's a new white paper that has better insights to manage some of your resources . So this is like these agents are constantly acting in your data center . They are aware of what 's going on on the internet based on how you have designed , and they 're able to provide you with creative solutions . And I think that 's going to be the disruption here , and that 's something we are working on . Laurel : So looking to the future , what tools , technologies , or trends do you see emerging as more and more enterprises look to modernize their data infrastructure and really benefit from data intelligence ? Bharti : Again , I 'll go back to what I 'm talking about , generative AI here , and I 'll give an example . For one of our customers , we are managing their data center , and I 'm also part of that channel where we see constant back and forth between the support and the engineering . The support is asking , `` Hey , this is what is happening , what should we be doing ? '' So just think of it like a different scenario that you have all this and you were able to collect this data and feed it into the LLMs . When you 're talking about this data , this data resides at several places . It resides in the heads of our experts . It is there in the documentation , it 's there in the support tickets , it 's there in logs , like life logs . It is there in the traces . So it 's almost impossible for a human being to analyze this data and get meaningful insights . However , if we combine LLMs with the power of , say , knowledge graphs , vector databases , and other tools , it will be possible to analyze this data at the speed of light , and present the recommendation in front of the user through a very simple user interface . And in most cases , just via a very simple natural language interface . So I think that 's a kind of a complete paradigm shift where you have so many sources that you need to constantly analyze versus having the full automation . And that 's why I feel that these copilots will become an essential part of the data centers . In the beginning they 'll help with the automation to deal with the problems prevalent in any data center like resource management and optimization , proactive problem determination , and resolution of the same . As we go into the future , we 'll see more manual operations converted into automated operations . First , we 'll see humans in the loop , and eventually we 'll see a trend towards fully autonomous data centers . Laurel : Well , that is quite a future . Thank you very much for joining us today on the Business Lab . Bharti : Thank you , Laurel . Bye-bye . Laurel : That was Bharti Patel , who is the senior vice president of Product Marketing at Hitachi Vantara who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review.That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the director of Insights , the custom publishing division of MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can find us in print , on the web , and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com.This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'store', 'manage', 'become', 'key', 'competitive', 'differentiator', 'global', 'datum', 'continue', 'grow', 'exponentially', 'organization', 'face', 'many', 'hurdle', 'pile', 'historical', 'data', 'realtime', 'datum', 'stream', 'iot', 'sensor', 'build', 'datadriven', 'supply', 'chain', 'senior', 'vice', 'president', 'product', 'engineering', 'see', 'challenge', 'opportunity', 'create', 'well', 'datum', 'strategy', 'enterprise', 'become', 'datadriven', 'first', 'become', 'datum', 'intelligent', 'say', 'mean', 'know', 'datum', 'need', 'keep', 'reside', 'derive', 'value', 'stress', 'datum', 'journey', 'begin', 'datum', 'planning', 'include', 'stakeholder', 'cio', 'cto', 'business', 'user', 'describe', 'universal', 'datum', 'intelligence', 'enterprise', 'ability', 'gain', 'well', 'insight', 'datum', 'stream', 'meet', 'increase', 'demand', 'transparency', 'offer', 'seamless', 'access', 'datum', 'insight', 'matter', 'reside', 'build', 'intelligence', 'mean', 'build', 'datum', 'infrastructure', 'scalable', 'secure', 'costeffective', 'socially', 'responsible', 'public', 'cloud', 'often', 'laud', 'way', 'enterprise', 'innovate', 'agility', 'scale', 'premise', 'infrastructure', 'view', 'less', 'accessible', 'user', 'friendly', 'datum', 'stream', 'continue', 'grow', 'budget', 'patel', 'note', 'many', 'organization', 'use', 'cloud', 'face', 'cost', 'challenge', 'combat', 'say', 'mean', 'find', 'good', 'world', 'cloud', 'environment', 'private', 'datum', 'center', 'keep', 'cost', 'low', 'insight', 'flow', 'look', 'ahead', 'foresee', 'future', 'total', 'automation', 'today', 'datum', 'reside', 'many', 'place', 'mind', 'expert', 'documentation', 'support', 'ticket', 'make', 'impossible', 'person', 'able', 'analyze', 'datum', 'glean', 'meaningful', 'insight', 'go', 'future', 'see', 'manual', 'operation', 'convert', 'automate', 'operation', 'say', 'first', 'see', 'human', 'loop', 'eventually', 'see', 'trend', 'fully', 'autonomous', 'datum', 'center', 'episode', 'business', 'lab', 'produce', 'partnership', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'today', 'build', 'well', 'datum', 'infrastructure', 'basic', 'datum', 'difficult', 'come', 'scale', 'adopt', 'emerge', 'technology', 'crucial', 'organize', 'datum', 'tear', 'data', 'silo', 'focus', 'datum', 'infrastructure', 'often', 'background', 'come', 'front', 'datum', 'strategytwo', 'word', 'datum', 'intelligencemy', 'guest', 'patel', 'bharti', 'senior', 'vice', 'president', 'product', 'engineering', 'episode', 'business', 'lab', 'sponsor', 'welcome', 'bharti', 'thank', 'nice', 'laurel', 'let', 'start', 'kind', 'give', 'context', 'discussion', 'global', 'datum', 'continue', 'grow', 'exponentially', 'accord', 'project', 'double', 'enterprise', 'face', 'many', 'hurdle', 'become', 'datadriven', 'hurdle', 'include', 'course', 'limit', 'pile', 'historical', 'datum', 'new', 'realtime', 'datum', 'stream', 'supply', 'chain', 'become', 'datadriven', 'enterprise', 'evaluate', 'datum', 'strategy', 'marker', 'strong', 'data', 'infrastructure', 'bharti', 'laurel', 'agree', 'datum', 'grow', 'exponentially', 'study', 'conduct', 'recently', 'talk', 'cio', 'cto', 'country', 'proof', 'data', 'almost', 'go', 'double', 'year', 'think', 'interesting', 'data', 'go', 'grow', 'budget', 'go', 'grow', 'proportion', 'instead', 'worry', 'want', 'tackle', 'problem', 'differently', 'want', 'look', 'convert', 'challenge', 'opportunity', 'derive', 'value', 'deal', 'let', 'talk', 'little', 'context', 'happen', 'industry', 'today', 'sure', 'hear', 'generative', 'generative', 'ai', 'buzzword', 'industry', 'forever', 'however', 'change', 'recently', 'chatgpt', 'expose', 'power', 'ai', 'common', 'people', 'right', 'school', 'go', 'kid', 'grandparent', 'provide', 'simple', 'natural', 'language', 'interface', 'talk', 'little', 'bit', 'chatgpt', 'fast', 'grow', 'app', 'industry', 'touch', 'user', 'month', 'change', 'fast', 'adoption', 'get', 'business', 'interested', 'want', 'see', 'unleash', 'power', 'generative', 'ai', 'fact', 'accord', 'mckinsey', 'say', 'go', 'add', 'global', 'economy', 'mean', 'talk', 'big', 'number', 'talk', 'chatgpt', 'science', 'science', 'large', 'language', 'model', 'think', 'large', 'language', 'model', 'model', 'billion', 'even', 'trillion', 'parameter', 'science', 'chatgpt', 'however', 'get', 'large', 'language', 'model', 'llm', 'need', 'finetune', 'mean', 'rely', 'public', 'datum', 'get', 'mean', 'get', 'first', 'get', 'information', 'want', 'correct', 'time', 'course', 'risk', 'people', 'feed', 'bad', 'datum', 'associate', 'make', 'actually', 'come', 'private', 'data', 'set', 'proprietary', 'data', 'set', 'important', 'use', 'private', 'datum', 'finetune', 'model', 'doubt', 'mind', 'create', 'differentiation', 'long', 'run', 'remain', 'competitive', 'think', 'even', 'scratch', 'surface', 'come', 'ai', 'need', 'think', 'enterprise', 'adoption', 'feature', 'need', 'explainability', 'traceability', 'quality', 'trustworthiness', 'reliability', 'look', 'parameter', 'actually', 'datum', 'centerpiece', 'harness', 'private', 'datum', 'curate', 'create', 'datum', 'set', 'give', 'maximum', 'return', 'investment', 'enterprise', 'become', 'datadriven', 'think', 'first', 'become', 'datum', 'intelligent', 'mean', 'know', 'datum', 'need', 'keep', 'reside', 'derive', 'value', 'talk', 'cio', 'cto', 'evident', 'lot', 'datum', 'need', 'find', 'way', 'fix', 'problem', 'datum', 'useful', 'store', 'keep', 'spend', 'money', 'definitely', 'problem', 'need', 'solve', 'back', 'question', 'right', 'infrastructure', 'parameter', 'mind', 'need', 'nimble', 'need', 'scalable', 'trust', 'secure', 'costeffective', 'finally', 'socially', 'responsible', 'laurel', 'certainly', 'give', 'lot', 'perspective', 'bharti', 'customer', 'demand', 'access', 'datum', 'enterprise', 'also', 'need', 'get', 'well', 'insight', 'stream', 'datum', 'accumulate', 'describe', 'universal', 'datum', 'intelligence', 'relate', 'data', 'infrastructure', 'bharti', 'universal', 'datum', 'intelligence', 'ability', 'business', 'offer', 'seamless', 'access', 'datum', 'insight', 'irrespective', 'reside', 'basically', 'talk', 'get', 'full', 'insight', 'datum', 'hybrid', 'environment', 'also', 'line', 'also', 'talk', 'approach', 'infrastructure', 'distribute', 'approach', 'mean', 'distribute', 'little', 'datum', 'movement', 'possible', 'move', 'datum', 'place', 'place', 'expensive', 'design', 'system', 'think', 'elastic', 'fabric', 'tie', 'together', 'able', 'get', 'insight', 'datum', 'matter', 'reside', 'timely', 'manner', 'even', 'datum', 'format', 'structured', 'unstructured', 'block', 'file', 'object', 'kind', 'give', 'example', 'recently', 'work', 'water', 'resource', 'simplify', 'data', 'management', 'strategy', 'datum', 'come', 'water', 'resource', 'mean', 'talk', 'huge', 'data', 'set', 'design', 'intelligent', 'datum', 'discovery', 'automation', 'tool', 'fact', 'complete', 'data', 'discovery', 'metadata', 'catalog', 'platform', 'migration', 'week', 'minimal', 'downtime', 'hear', 'time', 'really', 'happy', 'able', 'understand', 'integrate', 'analyze', 'datum', 'set', 'meet', 'need', 'water', 'user', 'planner', 'decision', 'maker', 'laurel', 'great', 'example', 'datum', 'store', 'manage', 'clearly', 'competitive', 'differentiator', 'well', 'amount', 'datum', 'increase', 'many', 'budget', 'mention', 'particularly', 'budget', 'organization', 'navigate', 'build', 'datum', 'infrastructure', 'effective', 'costefficient', 'example', 'less', 'bharti', 'think', 'great', 'question', 'go', 'back', 'datum', 'intelligence', 'first', 'step', 'become', 'datadriven', 'reap', 'full', 'benefit', 'datum', 'think', 'go', 'back', 'need', 'know', 'exist', 'exist', 'available', 'decision', 'maker', 'people', 'work', 'datum', 'fingertip', 'give', 'example', 'suppose', 'datum', 'retain', 'need', 'retain', 'legal', 'purpose', 'likelihood', 'use', 'extremely', 'extremely', 'low', 'point', 'store', 'datum', 'expensive', 'storage', 'device', 'make', 'sense', 'transfer', 'datum', 'low', 'cost', 'object', 'storage', 'time', 'datum', 'need', 'access', 'time', 'speed', 'important', 'low', 'latency', 'important', 'kind', 'datum', 'need', 'reside', 'fast', 'nvme', 'fact', 'many', 'customer', 'time', 'fact', 'sector', 'datum', 'policy', 'constantly', 'transfer', 'highly', 'highly', 'efficient', 'file', 'system', 'object', 'storage', 'base', 'policy', 'still', 'retain', 'pointer', 'file', 'system', 'able', 'access', 'back', 'case', 'need', 'public', 'cloud', 'often', 'cite', 'way', 'enterprise', 'scale', 'agile', 'innovate', 'contrast', 'legacy', 'onpremise', 'infrastructure', 'see', 'less', 'userfriendly', 'accessible', 'accurate', 'conception', 'enterprise', 'approach', 'modernization', 'management', 'datum', 'get', 'admit', 'public', 'cloud', 'hyperscaler', 'raise', 'bar', 'term', 'possible', 'come', 'innovation', 'however', 'also', 'see', 'hear', 'customer', 'cost', 'concern', 'fact', 'many', 'customer', 'move', 'cloud', 'fast', 'face', 'cost', 'challenge', 'cio', 'see', 'bill', 'go', 'exponentially', 'ask', 'keep', 'flat', 'think', 'see', 'big', 'opportunity', 'provide', 'experience', 'cloud', 'provide', 'private', 'datum', 'center', 'customer', 'talk', 'partition', 'data', 'equivalent', 'offer', 'get', 'say', 'want', 'address', 'slightly', 'different', 'manner', 'think', 'want', 'address', 'customer', 'able', 'take', 'full', 'advantage', 'elasticity', 'cloud', 'also', 'able', 'take', 'full', 'advantage', 'environment', 'want', 'want', 'way', 'almost', 'seamless', 'way', 'seamless', 'manner', 'manage', 'datum', 'private', 'data', 'center', 'cloud', 'get', 'good', 'world', 'laurel', 'interesting', 'perspective', 'also', 'kind', 'require', 'different', 'element', 'business', 'come', 'leadership', 'perspective', 'good', 'practice', 'institute', 'recommend', 'make', 'transition', 'well', 'datum', 'management', 'say', 'think', 'datum', 'journey', 'start', 'datum', 'planning', 'siloe', 'manner', 'get', 'right', 'onset', 'extremely', 'extremely', 'important', 'need', 'beginning', 'datum', 'plan', 'get', 'get', 'stakeholder', 'together', 'cio', 'business', 'user', 'cto', 'strategy', 'never', 'siloe', 'manner', 'fact', 'want', 'think', 'highlight', 'aspect', 'probably', 'people', 'much', 'even', 'bring', 'partner', 'mix', 'fact', 'example', 'prior', 'join', 'cto', 'air', 'purifi', 'company', 'define', 'datum', 'strategy', 'look', 'salesforce', 'datum', 'look', 'datum', 'netsuite', 'look', 'customer', 'ticket', 'see', 'drive', 'marketing', 'campaign', 'look', 'datum', 'feel', 'totally', 'miss', 'fact', 'miss', 'weather', 'datum', 'datum', 'datum', 'design', 'effective', 'marketing', 'campaign', 'important', 'insight', 'weather', 'datum', 'example', 'allergy', 'particular', 'region', 'wildfire', 'particular', 'region', 'datum', 'important', 'strategy', 'able', 'bring', 'stakeholder', 'part', 'datum', 'together', 'think', 'beginning', 'right', 'thing', 'start', 'laurel', 'big', 'hairy', 'problem', 'goal', 'also', 'consideration', 'datum', 'center', 'contribute', 'enterprise', 'carbon', 'emission', 'think', 'partnership', 'modernize', 'datum', 'management', 'talk', 'far', 'enterprise', 'meet', 'sustainability', 'goal', 'also', 'modernize', 'datum', 'infrastructure', 'accommodate', 'historical', 'realtime', 'datum', 'especially', 'come', 'mention', 'many', 'different', 'source', 'bharti', 'glad', 'bring', 'point', 'important', 'ignore', 'fact', 'thing', 'talk', 'finetuning', 'model', 'actually', 'generate', 'time', 'carbon', 'emission', 'possible', 'passenger', 'car', 'lifetime', 'talk', 'huge', 'huge', 'environmental', 'effect', 'particular', 'topic', 'extremely', 'important', 'fact', 'goal', 'go', 'carbonneutral', 'operation', 'value', 'chain', 'address', 'problem', 'kind', 'hardware', 'side', 'also', 'software', 'side', 'right', 'onset', 'design', 'hardware', 'look', 'endtoend', 'component', 'see', 'kind', 'carbon', 'footprint', 'create', 'really', 'minimize', 'fact', 'hardware', 'ready', 'actually', 'need', 'pass', 'stringent', 'set', 'energy', 'certification', 'hardware', 'side', 'software', 'side', 'actually', 'start', 'initiative', 'look', 'move', 'modern', 'language', 'likely', 'create', 'less', 'carbon', 'footprint', 'look', 'replace', 'exist', 'code', 'base', 'rust', 'make', 'sense', 'big', 'problem', 'need', 'think', 'solve', 'overnight', 'constantly', 'think', 'interface', 'manner', 'laurel', 'well', 'certainly', 'impressive', 'goal', 'emerge', 'technology', 'generative', 'ai', 'say', 'push', 'organization', 'next', 'generation', 'datum', 'infrastructure', 'system', 'also', 'help', 'differentiate', 'competitor', 'want', 'take', 'kind', 'twopronged', 'approach', 'first', 'call', 'table', 'stake', 'completely', 'wipe', 'simple', 'thing', 'automate', 'certain', 'thing', 'create', 'well', 'customer', 'experience', 'mind', 'enough', 'got', 'think', 'kind', 'disruption', 'create', 'customer', 'couple', 'idea', 'work', 'companion', 'copilot', 'think', 'agent', 'datum', 'center', 'agent', 'actually', 'help', 'datum', 'center', 'environment', 'become', 'reactive', 'proactive', 'basically', 'agent', 'run', 'datum', 'center', 'time', 'watch', 'new', 'patch', 'available', 'update', 'new', 'patch', 'maybe', 'new', 'white', 'paper', 'well', 'insight', 'manage', 'resource', 'agent', 'constantly', 'act', 'datum', 'center', 'aware', 'go', 'internet', 'base', 'design', 'able', 'provide', 'creative', 'solution', 'think', 'go', 'disruption', 'work', 'laurel', 'look', 'future', 'tool', 'technology', 'trend', 'see', 'emerge', 'enterprise', 'look', 'modernize', 'datum', 'infrastructure', 'really', 'benefit', 'data', 'intelligence', 'bharti', 'go', 'back', 'talk', 'generative', 'ai', 'give', 'example', 'customer', 'manage', 'datum', 'center', 'also', 'part', 'channel', 'see', 'constant', 'back', 'forth', 'support', 'engineering', 'support', 'ask', 'happen', 'think', 'different', 'scenario', 'able', 'collect', 'datum', 'feed', 'llm', 'talk', 'datum', 'datum', 'reside', 'several', 'place', 'reside', 'head', 'expert', 'documentation', 'support', 'ticket', 'log', 'life', 'log', 'trace', 'almost', 'impossible', 'human', 'analyze', 'datum', 'get', 'meaningful', 'insight', 'however', 'combine', 'llm', 'power', 'say', 'knowledge', 'graph', 'vector', 'database', 'tool', 'possible', 'analyze', 'datum', 'speed', 'light', 'present', 'recommendation', 'front', 'user', 'simple', 'user', 'interface', 'case', 'simple', 'natural', 'language', 'interface', 'think', 'kind', 'complete', 'paradigm', 'shift', 'many', 'source', 'need', 'constantly', 'analyze', 'full', 'automation', 'feel', 'copilot', 'become', 'essential', 'part', 'datum', 'center', 'beginning', 'help', 'automation', 'deal', 'problem', 'prevalent', 'datum', 'center', 'resource', 'management', 'optimization', 'proactive', 'problem', 'determination', 'resolution', 'go', 'future', 'see', 'manual', 'operation', 'convert', 'automate', 'operation', 'first', 'see', 'human', 'loop', 'eventually', 'see', 'trend', 'fully', 'autonomous', 'data', 'center', 'laurel', 'well', 'future', 'thank', 'much', 'join', 'today', 'business', 'lab', 'thank', 'laurel', 'byebye', 'laurel', 'bharti', 'senior', 'vice', 'president', 'product', 'marketing', 'speak', 'home', 'mit', 'mit', 'technology', 'episode', 'business', 'lab', 'host', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Data — how it’s stored and managed — has become a key competitive differentiator. As global data continues to grow exponentially, organizations face many hurdles between piling up historical data, real-time data streams from IoT sensors, and building data-driven supply chains. Senior vice president of product engineering at Hitachi Vantara, Bharti Patel sees these challenges…"
Optimizing platforms offers customers and stakeholders a better way to bank,https://www.technologyreview.com/2023/09/27/1079913/optimizing-platforms-offers-customers-and-stakeholders-a-better-way-to-bank/,2023-09-27,"In association withJPMorgan Chase & Co. When it comes to banking, whether it’s personal, business, or private, customer experience is everything. Building new technologies and platforms, employing them at scale, and optimizing workflows is especially critical for any large bank looking to meet evolving customer and internal stakeholder demands for faster and more personalized ways of doing business. Institutions like JPMorgan Chase are implementing best practices, cost efficient cloud migration, and emerging AI and machine learning (ML) tools to build better ways to bank, says Head of Managed Accounts, Client Onboarding and Client Services Technology at J. P. Morgan Private Bank, Vrinda Menon. Menon stresses that it is critical that technologists stay very focused on the business impact of the software and tools they develop. “We coach our teams that success and innovation does not come from rebuilding something that somebody has already built, but instead from leveraging it and taking the next leap with additional features upon it to create high impact business outcomes,” says Menon. At JPMorgan Chase, technologists are encouraged, where possible, to see the bigger picture and solve for the larger pattern rather than just the singular problem at hand. To reduce redundancies and automate tasks, Menon and her team focus on data and measurements that indicate where emerging technologies like AI and machine learning could enhance processes like onboarding or transaction processing at scale.  AI/ML have become commonplace across many industries with private banking being no exception, says Menon. At a base level, AI/ML can extract data from documents, classify information, analyze data smartly and detect issues and outliers across a wide range of use cases. But Menon is looking to the near future when AI/ML can help proactively predict client needs based on various signals. For example, a private banking client that has recently been married may ask their bank for a title change. Using the client’s data in context and this new request, AI/ML tools could proactively help bankers identify additional things to ask this client, such as need to change beneficiaries or the possibility to optimize taxes by considering jointly filed taxes. “You have an opportunity to be more proactive and think about it holistically so you can address their needs before they even come to you to ask for that level of engagement and detail,” says Menon. This episode of Business Lab is produced in association with JPMorgan Chase.  Laurel Ruma: From MIT Technology Review. I'm Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is investing in building great experiences. A number of people benefit from enterprise investment in emerging and new technologies, including customers who want better, faster, and newer ways of doing business. But internal stakeholders want the same investment in better tools and systems to build those fast and new ways of doing business. Balancing both needs is possible.Two words for you: optimizing platforms.Today we're talking with Vrinda Menon, the chief technology officer of Managed Accounts, Client Onboarding and Client Services at JPMorgan Private Bank.This podcast is produced in association with JPMorgan Chase.Welcome, Vrinda.Vrinda Menon: Thank you so much, Laurel. I'm looking forward to this discussion.Laurel: Great. So, let's start with how often people think of JPMorgan Chase. They likely associate the company with personal banking, ATMs and credit cards, but could you describe what services the private bank provides and how operations and client services have evolved and transformed since you began your role at JPMorgan Chase? Vrinda: Sure. JPMorgan Chase indeed does far more than personal banking, credit cards and ATMs. The private bank of JPMorgan Chase is often referred to as the crown jewel of our franchise. We service our high net worth clients and ultra-high net worth clients across the globe. We provide them services like investment management, trust and estate planning, banking services, brokerage services, customized lending, etc., just to name a few. And in terms of what has transformed in the recent years since I joined, I would say that we've become far more tech savvy as an organization, and this is thanks and no small measure to new leadership as well in operations and client services. I think three things have changed very dramatically since I've joined. The first is culture. In my first few months, I spent a week doing the job of an operations analyst. And in doing that I started to understand firsthand the painful manual work that people were subject to and feeling like they did not have the permission to have things changed for them. But working off that and actually connecting with a lot more people at the ground who are doing these types of activities, we worked with them to make those changes and make them see light at the end of the tunnel. And then suddenly the demand for more change and demand for more automation started building as a groundswell energy with support from our partners in operations and services. Now, routine, repetitive, mundane, mind-numbing work is not an option at the table. It's become a thing of the past. And secondly, what we've done also is we've grown an army of citizen developers who really have access to tools and technologies where they can do quick automation without having to depend on broader programs and broader pieces of technology. We've also done something super interesting, which is, over the past three years we've taken every new analyst in the private bank and trained them on Python. And so, they've started to see the benefits of doing things themselves. So, culture change I think has been one of the biggest things that we've achieved in the past few years since I joined. Second, we built a whole set of capabilities, we call them common capabilities. Things like how do you configure new workflows? How do you make decisions using spreadsheets and decision models versus coding it into systems? So,  you can configure it, you can modify it, and you can do things more effectively. And then tools like checklists, which can be again put into systems and automated in a few minutes, in many cases. Today, we have millions of tasks and millions of decisions being executed through these capabilities, which has suddenly game-changed our ability to provide automation at scale. And last but not least, AI and machine learning, it now plays an important role in the underpinnings of everything that we do in operations and client services. For example, we do a lot of process analytics. We do load balancing. So, when a client calls, which agent or which group of people do we direct that client call to so that they can actually service the client most effectively. In the space of payments, we do a lot with machine learning. Fraud detection is another, and I will say that I'm so glad we've had the time to invest and think through all of these foundational capabilities. So, we are now poised and ready to take on the next big leap of changes that are right now at our fingertips, especially in the evolving world of AI and machine learning and of course the public cloud.Laurel: Excellent. Yeah, you've certainly outlined the diversity of the firm's offerings. So, when building new technologies and platforms, what are some of the working methodologies and practices that you employ to build at scale and then optimize those workflows? Vrinda: Yeah, as I said before, the private bank has a lot of offerings, but then amplify that with all the other offerings that JPMorgan Chase, the franchise has, a commercial bank, a corporate and investment bank, a consumer and community bank, and many of our clients cross all of these lines of business. It brings a lot of benefits, but it also has complexities. And one of the things that I obsess personally over is how do we simplify things, not add to the complexity? Second is a mantra of reuse. Don't reinvent because it's easy for technologists to look at a piece of software and say, ""That's great, but I can build something better."" Instead, the three things that I ask people to focus on and our organization collectively with our partners focus on is first of all, look at the business outcome. We coach our teams that success and innovation does not come from rebuilding something that somebody has already built, but instead from leveraging it and taking the next leap with additional features upon it to create high impact business outcomes. So, focusing on outcome number one. Second, if you are given a problem, try and look at it from a bigger picture to see whether you can solve the pattern instead of that specific problem. So, I'll give you an example. We built a chatbot called Casey. It's one of the most loved products in our private bank right now. And Casey doesn't do anything really complex, but what it does is solves a very common pattern, which is ask a few simple questions, get the inputs, join this with data services and join this with execution services and complete the task. And we have hundreds of thousands of tasks that Casey performs every single day. And one of them, especially a very simple functionality, the client wants a bank reference letter. Casey is called upon to do that thousands of times a month. And what used to take three or four hours to produce now takes like a few seconds. So, it suddenly changes the outcome, changes productivity, and changes the happiness of people who are doing things that you know they themselves felt was mundane. So, solving the pattern, again, important. And last but not least, focusing on data is the other thing that's helped us. Nothing can be improved if you don't measure it. So, to give you an example of processes, the first thing we did was pick the most complex processes and mapped them out. We understood each step in the process, we understood the purpose of each step in the process, the time taken in each step, we started to question, do you really need this approval from this person? We observed that for the past six months, not one single thing has been rejected. So, is that even a meaningful approval to begin with? Questioning if that process could be enhanced with AI, could AI automatically say, ""Yes, please approve,"" or ""There's a risk in this do not approve,"" or ""It's okay, it needs a human review."" And then making those changes in our systems and flows and then obsessively measuring the impact of those changes. All of these have given us a lot of benefits. And I would say we've made significant progress just with these three principles of focus on outcome, focus on solving the pattern and focus on data and measurements in areas like client onboarding, in areas like maintaining client data, et cetera. So, this has been very helpful for us because in a bank like ours, scale is super important.Laurel: Yeah, that's a really great explanation. So, when new challenges do come along, like moving to the public cloud, how do you balance the opportunities of that scale, but also computing power and resources within the cost of the actual investment? How do you ensure that the shifts to the cloud are actually both financially and operationally efficient? Vrinda: Great question. So obviously every technologist in the world is super excited with the advent of the public cloud. It gives us the powers of agility, economies of scale. We at JPMorgan Chase are able to leverage world class evolving capabilities at our fingertips. We have the ability also to partner with talented technologies at the cloud providers and many service providers that we work with that have advanced solutions that are available first on the public cloud. We are eager to get our hands on those. But with that comes a lot of responsibility because as a bank, we have to worry about security, client data, privacy, resilience, how are we going to operate in a multi-cloud environment because some data has to remain on-prem in our private cloud. So, there's a lot of complexity, and we have engineers across the board who think a lot about this, and their day and night jobs are to try and figure this out. As we think about moving to the public cloud in my area, I personally spend time thinking in depth about how we could build architectures that are financially efficient. And the reason I bring that up is because traditionally as we think about data centers where our hardware and software has been hosted, developers and architects haven't had to worry about costs because you start with sizing the infrastructure, you order that infrastructure, it's captive, it remains in the data center, and you can expand it, but it's a one-time cost each time that you upgrade. With the cloud, that situation changes dramatically. It's both an opportunity but also a risk. So, a financial lens then becomes super important right at the outset. Let me give you a couple of examples of what I mean. Developers in the public cloud have a lot of power, and with that power comes responsibility. So, I'm a developer and my application is not working right now because there's some issue. I have the ability to actually spin up additional processes. I have the ability to spin up additional environments, all of which attract costs, and if I don't control and manage that, the cost could quickly pile up. Data storage, again, we had fixed storage, we could expand it periodically in the data centers, but in the public cloud, you have choices. You can say data that's going to be slowly accessed versus data that's going to be accessed frequently to be stored in different types of storage with different costs as a result. Now think about something like a financial ledger where you have retention requirements of let's say 20 years. The cost could quickly pile up if you store it in the wrong type of storage. So, there's an opportunity to optimize cost there, and if you ignore it and you've not kept an eye on it, you could actually have costs that are just not required. To do this right, we have to ask developers, architects, and our engineers to not just think about the best performance, the most optimal resilience, but also think about cost as a fundamental aspect of how we look at architectures. So, this for me is a huge area of focus, starting with awareness for our people, training our people, thinking about architecture patterns, solution patterns, tooling, measurements, so that we completely stay on top of this and become more effective and more efficient in how we get to the public cloud. While the journey is exciting, I want to make sure that as we land there, we land safely and optimally from a cost standpoint. Laurel: And especially in your position, thinking about how technology will affect the firm years and the future is critical. Therefore, as emerging technologies like AI and machine learning become more commonplace across industries, could you offer an example of how you're using them in the areas that you cover?Vrinda: Yeah, certainly. And we use AI/ML at many levels of complexity. So let me start with the base case. AI/ML, especially in operations and client services, starts with can I get data from documents? Can I OCR those documents, which is optical character recognition? Can I get information out of it, can I classify it? Can I perform analytics on it? So that's the base case. On top of that, as you look at data, for example, payments data or data of transactions, and let's say human beings are scanning them for issues or outliers, outlier detection techniques with AI/ML, they are also table stakes now, and many of our systems do that. But as you move on to the next level of prediction, what we've been able to do is start to build up models where say the client is calling. The client has all these types of cases in progress right now. What could they be calling about in addition to this? The client expressed sentiment about something that they were not happy with two weeks ago. Is it likely that they're calling about this? Can I have that information at the fingertips of the client service agent so they can look at it and respond as soon as the client asks for something? And think about the next stage of evolution, which is, the client came to us and said, ""Change my title because I just got married."" Typically, in a transactional kind of activity, you would respond to the client and fix the title from let's say, Ms. to Mrs. if that's what they asked you to do. But imagine if when they came to do that, we said to them, here's 10 other things that you should possibly think of now that you said you've got married. Congratulations. Do you want to address your beneficiaries? Do you want to change something in tax planning? Do you want to change the type of tax calculations that you do because you want to optimize now that you're married, and you and your spouse could be filing jointly? Again, not that the client would choose to change those things, but you have an opportunity to be more proactive and think about it holistically so you can address their needs before they even come to you asking for that level of engagement and detail. We also exploit a lot of AI/ML capabilities and in client onboarding to get better data to start to predict what data is right and start to predict risk. And our next leap, I believe strongly, and I'm super excited about this area of large language models, which I think are going to offer us exponential possibilities, not just in JPMorgan Chase, but as you can see in the world right now with technologies like ChatGPT, OpenAI's technologies, as well as any of the other publicly available large language models that are being developed every single day. Laurel: Well, it's clear that AI offers great opportunities for optimizing platforms and transformations. Could you describe the process of how JPMorgan Chase decided to create dedicated teams for AI and machine learning, and how did you build out those teams? Vrinda: Yeah, certainly. At JPMorgan Chase, we've been cultivating the mindset for some years now to think AI-first while hiring people. And we also leverage the best talent in the industry, and we've hired a lot of people in our research divisions as well to work on AI/ML. We've got thousands, several thousand technologists focused on AI. For me personally, in 2020, during the first months of the pandemic, I decided that I needed to see more AI/ML activity across my areas. So, I did what I called the “Summer of AI/ML,” and this was a fully immersive program that ran over 12 weeks with training for our people, and it was not full-time. So, they would dial in for a couple of hours, get trained on an AI/ML concept and some techniques, and then they would continue that and practice that for the week. Then we had ideation sessions with our users for a couple of weeks and then a hackathon and some brilliant ideas came out of it. But when I stepped back and looked at this whole thing and the results of it, a few months later, I realized that many of the ideas had not reached the final destination into production. And in thinking a little more deeply about that, I understood that we had a problem. The problem was as follows, while AI is a great thing and everybody appreciates it, until AI becomes ingrained in everybody's brain as the first thing to think about, there's always going to be a healthy tension between choosing the next best feature on a product, which is very deterministic. If you say, add this button here or add these features using conventional technologies like Java versus game-changing the product using AI, which is a little bit more of a risk, the results are not always predictable, and it requires experimentation and R&D. And so, when you have a choice of incremental changes that are deterministic and changes that are more probabilistic, people tend to take the most certain answer. And so, I decided that I needed to build out a focused, dedicated team of data scientists who were just going to obsess about solving problems in the space of data science and embed them across the products that we were building. And now the results are starting to show for themselves because the work they've done is phenomenal and the demand on them is growing every single day to the point where I've grown the team and the value that they're providing is also measured and visible to the broader organization.Laurel: So, in JPMorgan Chase's client services, customer experience is clearly a driving force. How do you ensure that your teams are providing clients, especially those high-net-worth private clients that have high expectations of service with services that then meet their banking and account management needs? Vrinda: So, we obsess over customer experience starting from the CEO down to every single employee. I have three tenets for my team. Number one is client experience, the second is user experience, and third is engineering excellence. And they know that a lot of us are measured by how well we service our clients. So, in the private bank specifically, in addition to reviewing our core capabilities like our case management system, our voice recognition systems, our fraud capture systems, all of that, we continuously analyze data received from client surveys, data received through every single interaction that we have with our client across all channels. So, whether it be a voice channel, whether it be emails, whether it be things that the client types in our websites, the places that they access, and our models just do not look at sentiment, they also look at client experience. And as they look at experience, the things that we are trying to understand are, first of all, how's the client feeling in this interaction? But more important is client one and client two and client three feeling the same thing about a particular aspect of our process, and do we need to change that process as a result, or is there more training that needs to be provided to our agents because we are not able to fully satisfy this category of requests? And by doing that continuously and analyzing it, and back to the point that I made earlier, by measuring it constantly, we are able to say, first of all, how was the experience to begin with? How is the experience now and after making these changes on these training programs or these fixes in our systems, how is that experience showing? And some of the other things we are able to do are look at experiences over a period of time. So, for example, the client came to us last year and their experience based on the measurements that we did was at a certain level, they continue to interact with us over a period of months. Has it gone up? Has it gone down? How is that needle trending? How do we take that to superb? And we've been able to figure out these in ways that we've been able to prevent complaints, for example, and get to a point where things are escalated to the right people in the organization, especially in the servicing space where we are able to triage and manage these things more effectively because we are a high- touch client business, and we need to make sure that our clients are extremely happy with us. Laurel: Oh yeah, absolutely. And sort of like another phase or idea when we're thinking about customer experience and customer services, building a workforce that can respond to it. So here we're going to talk a bit about how we promote diversity, which has been a tenet of your career, and you currently sit on the board of the Transition Network, which is a nonprofit that empowers a diverse network of women throughout career transitions. So, at JPMorgan Chase, how do you grow talent and improve representation across the company? And then how does that help build better customer experience? Vrinda: Sure, that's a great question. I certainly am very passionate about diversity, and during the past 15 years of my career, I've spent a lot of time supporting diversity. In my prior firm, I was co-head of the Asian Professional Network. Then subsequently for the past three years, I've been a board member at the Transition Network, which is all about women in transition. Meaning as they grow out of their careers into retirement and into other stages of life, how do we help them transition? And then here at JPMorgan Chase, I'm the sponsor for what is called the Take It Forward initiative, which is an initiative that supports 15,000 women technologists. JPMorgan Chase, as you know, does a broad range of activities in the area of diversity across all kinds of business resource groups, and we invest a lot of time and energy. But specifically, for the Take It Forward initiative that I sponsor, it plays a key role in helping these 15,000 women technologists continuously enhance their leadership skills, grow their technical skills, build their confidence, develop networks, learn from senior sponsors and mentors, grow their careers, all of which makes their work experience very enriching. When I hear things like I'm motivated, I get new energy interacting with these senior women, I trust my personal power more. I'm confident to negotiate with my manager for a better role; I feel confident that I can discuss my compensation. It makes me really happy, and especially when they say I stay in JPMorgan Chase because of Take It Forward, it brings tears to my eyes. It's really one of the most amazing volunteer driven initiatives in this organization. And a lot of people pour in passion, energy, time to make it succeed. And this initiative has won many awards as well externally. I strongly believe all of these efforts are critical as it changes when people's experiences change and when they're happy, what they do becomes that much more effective. And it changes how we work internally, how we present ourselves externally and game changes our business outcomes. I've seen that in problem solving meetings. When you evaluate risk and you bring in people from diverse backgrounds, some are more risk averse, some are more risk taking, and you suddenly see that dynamic play out and the outcome becomes much different from what it would've been if you didn't have all those people in the mix. So overall, I strongly believe in this, and I've seen it play out in every single firm that I've ever worked at. So that's my take on diversity and how it helps us. Laurel: Well, it certainly is important work, especially as it ties so tightly with the firm's own ethos. So Vrinda, looking forward, how do you envision the future of private banking and client management? As we see emerging technologies become more prevalent and enterprises start to shift their infrastructure to the public cloud? Vrinda: As I mentioned earlier, I see the next set of emerging technologies taking the world on a super exciting ride, and I think it's going to be as transformational as the advent of the world wide web. Just take the example of large language models. The areas that are most likely to be first disrupted will be any work that involves content creation because that is table stakes for a large language model. As I expand that to my work, the rest of my work, client services and operations and many other areas that require repetitive work and large-scale interpretation and synthesis of information, that's again, table stakes for large language models. Expand that now to the next evolution, which is agents that are now the emerging technology with large language models. When agents are provided with a suite of tools and they can use reasoning like humans to decide which tool to execute based on input, that'll game change the whole thing I was talking about earlier on workflow and task execution and operationally intense activities in the organization. When I look at myself as a software developer in the areas of code generation, code testing, code correction, test data generation, just to name a few, all of those are going to be game changed. So not just the work that our users do in the private bank, but the work that we do as technologists in the private bank. A lot of that is going to game change dramatically. And then you add on the next level, which is problem solving. Large language models are continuously being trained on all subjects ever known to humans. And that for me is the most fascinating part of this. It's like hundreds of thousands of brains that are working together on a diverse set of subjects. So, imagine a model that's been trained on a domain like medicine or aerospace or defense, and then trying to bring all of that brainpower together to solve a problem in finance. That truly to me is the ultimate gold standard of problem solving. We talked about diverse people in the room coming with different experiences but imagine models that have been trained. You suddenly have breadth, depth, range of diverse knowledge that could never have been contemplated at that scale. And in order to do all of this, obviously one of the key underpinnings is the public cloud and being able to spin up compute as quickly as possible to do complex calculations and then spin it down when you don't need it, which is where the public cloud becomes super important. So, all I can say in conclusion is I think this is an amazing time to be in technology, and I just cannot wait to see how we further step up our game in the coming months and years, and things are moving almost at the speed of light now. Every single day, new papers get published and new ideas are coming out building on top of some of the exponential technologies that we are seeing in the world today. Laurel: Oh, that's fantastic. Vrinda, thank you so much for being on the Business Lab today.Vrinda: Thank you so much, Laurel. It's my pleasure. I really enjoyed speaking with you and thank you for your thoughtful questions. They were super interesting. Laurel: That was Vrinda Menon, the chief technology officer of Managed Accounts, Client Onboarding and Client Services at J.P. Morgan Private Bank, who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review overlooking the Charles River.That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. This podcast is for informational purposes only and it is not intended as legal, tax, financial, investment, accounting or regulatory advice. Opinions expressed herein are the personal views of the individual(s) and do not represent the views of JPMorgan Chase & Co. The accuracy of any statements, linked resources, reported findings or quotations are not the responsibility of JPMorgan Chase & Co.","In association withJPMorgan Chase & Co . When it comes to banking , whether it ’ s personal , business , or private , customer experience is everything . Building new technologies and platforms , employing them at scale , and optimizing workflows is especially critical for any large bank looking to meet evolving customer and internal stakeholder demands for faster and more personalized ways of doing business . Institutions like JPMorgan Chase are implementing best practices , cost efficient cloud migration , and emerging AI and machine learning ( ML ) tools to build better ways to bank , says Head of Managed Accounts , Client Onboarding and Client Services Technology at J. P. Morgan Private Bank , Vrinda Menon . Menon stresses that it is critical that technologists stay very focused on the business impact of the software and tools they develop . “ We coach our teams that success and innovation does not come from rebuilding something that somebody has already built , but instead from leveraging it and taking the next leap with additional features upon it to create high impact business outcomes , ” says Menon . At JPMorgan Chase , technologists are encouraged , where possible , to see the bigger picture and solve for the larger pattern rather than just the singular problem at hand . To reduce redundancies and automate tasks , Menon and her team focus on data and measurements that indicate where emerging technologies like AI and machine learning could enhance processes like onboarding or transaction processing at scale . AI/ML have become commonplace across many industries with private banking being no exception , says Menon . At a base level , AI/ML can extract data from documents , classify information , analyze data smartly and detect issues and outliers across a wide range of use cases . But Menon is looking to the near future when AI/ML can help proactively predict client needs based on various signals . For example , a private banking client that has recently been married may ask their bank for a title change . Using the client ’ s data in context and this new request , AI/ML tools could proactively help bankers identify additional things to ask this client , such as need to change beneficiaries or the possibility to optimize taxes by considering jointly filed taxes . “ You have an opportunity to be more proactive and think about it holistically so you can address their needs before they even come to you to ask for that level of engagement and detail , ” says Menon . This episode of Business Lab is produced in association with JPMorgan Chase . Laurel Ruma : From MIT Technology Review . I 'm Laurel Ruma and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is investing in building great experiences . A number of people benefit from enterprise investment in emerging and new technologies , including customers who want better , faster , and newer ways of doing business . But internal stakeholders want the same investment in better tools and systems to build those fast and new ways of doing business . Balancing both needs is possible.Two words for you : optimizing platforms.Today we 're talking with Vrinda Menon , the chief technology officer of Managed Accounts , Client Onboarding and Client Services at JPMorgan Private Bank.This podcast is produced in association with JPMorgan Chase.Welcome , Vrinda.Vrinda Menon : Thank you so much , Laurel . I 'm looking forward to this discussion.Laurel : Great . So , let 's start with how often people think of JPMorgan Chase . They likely associate the company with personal banking , ATMs and credit cards , but could you describe what services the private bank provides and how operations and client services have evolved and transformed since you began your role at JPMorgan Chase ? Vrinda : Sure . JPMorgan Chase indeed does far more than personal banking , credit cards and ATMs . The private bank of JPMorgan Chase is often referred to as the crown jewel of our franchise . We service our high net worth clients and ultra-high net worth clients across the globe . We provide them services like investment management , trust and estate planning , banking services , brokerage services , customized lending , etc. , just to name a few . And in terms of what has transformed in the recent years since I joined , I would say that we 've become far more tech savvy as an organization , and this is thanks and no small measure to new leadership as well in operations and client services . I think three things have changed very dramatically since I 've joined . The first is culture . In my first few months , I spent a week doing the job of an operations analyst . And in doing that I started to understand firsthand the painful manual work that people were subject to and feeling like they did not have the permission to have things changed for them . But working off that and actually connecting with a lot more people at the ground who are doing these types of activities , we worked with them to make those changes and make them see light at the end of the tunnel . And then suddenly the demand for more change and demand for more automation started building as a groundswell energy with support from our partners in operations and services . Now , routine , repetitive , mundane , mind-numbing work is not an option at the table . It 's become a thing of the past . And secondly , what we 've done also is we 've grown an army of citizen developers who really have access to tools and technologies where they can do quick automation without having to depend on broader programs and broader pieces of technology . We 've also done something super interesting , which is , over the past three years we 've taken every new analyst in the private bank and trained them on Python . And so , they 've started to see the benefits of doing things themselves . So , culture change I think has been one of the biggest things that we 've achieved in the past few years since I joined . Second , we built a whole set of capabilities , we call them common capabilities . Things like how do you configure new workflows ? How do you make decisions using spreadsheets and decision models versus coding it into systems ? So , you can configure it , you can modify it , and you can do things more effectively . And then tools like checklists , which can be again put into systems and automated in a few minutes , in many cases . Today , we have millions of tasks and millions of decisions being executed through these capabilities , which has suddenly game-changed our ability to provide automation at scale . And last but not least , AI and machine learning , it now plays an important role in the underpinnings of everything that we do in operations and client services . For example , we do a lot of process analytics . We do load balancing . So , when a client calls , which agent or which group of people do we direct that client call to so that they can actually service the client most effectively . In the space of payments , we do a lot with machine learning . Fraud detection is another , and I will say that I 'm so glad we 've had the time to invest and think through all of these foundational capabilities . So , we are now poised and ready to take on the next big leap of changes that are right now at our fingertips , especially in the evolving world of AI and machine learning and of course the public cloud.Laurel : Excellent . Yeah , you 've certainly outlined the diversity of the firm 's offerings . So , when building new technologies and platforms , what are some of the working methodologies and practices that you employ to build at scale and then optimize those workflows ? Vrinda : Yeah , as I said before , the private bank has a lot of offerings , but then amplify that with all the other offerings that JPMorgan Chase , the franchise has , a commercial bank , a corporate and investment bank , a consumer and community bank , and many of our clients cross all of these lines of business . It brings a lot of benefits , but it also has complexities . And one of the things that I obsess personally over is how do we simplify things , not add to the complexity ? Second is a mantra of reuse . Do n't reinvent because it 's easy for technologists to look at a piece of software and say , `` That 's great , but I can build something better . '' Instead , the three things that I ask people to focus on and our organization collectively with our partners focus on is first of all , look at the business outcome . We coach our teams that success and innovation does not come from rebuilding something that somebody has already built , but instead from leveraging it and taking the next leap with additional features upon it to create high impact business outcomes . So , focusing on outcome number one . Second , if you are given a problem , try and look at it from a bigger picture to see whether you can solve the pattern instead of that specific problem . So , I 'll give you an example . We built a chatbot called Casey . It 's one of the most loved products in our private bank right now . And Casey does n't do anything really complex , but what it does is solves a very common pattern , which is ask a few simple questions , get the inputs , join this with data services and join this with execution services and complete the task . And we have hundreds of thousands of tasks that Casey performs every single day . And one of them , especially a very simple functionality , the client wants a bank reference letter . Casey is called upon to do that thousands of times a month . And what used to take three or four hours to produce now takes like a few seconds . So , it suddenly changes the outcome , changes productivity , and changes the happiness of people who are doing things that you know they themselves felt was mundane . So , solving the pattern , again , important . And last but not least , focusing on data is the other thing that 's helped us . Nothing can be improved if you do n't measure it . So , to give you an example of processes , the first thing we did was pick the most complex processes and mapped them out . We understood each step in the process , we understood the purpose of each step in the process , the time taken in each step , we started to question , do you really need this approval from this person ? We observed that for the past six months , not one single thing has been rejected . So , is that even a meaningful approval to begin with ? Questioning if that process could be enhanced with AI , could AI automatically say , `` Yes , please approve , '' or `` There 's a risk in this do not approve , '' or `` It 's okay , it needs a human review . '' And then making those changes in our systems and flows and then obsessively measuring the impact of those changes . All of these have given us a lot of benefits . And I would say we 've made significant progress just with these three principles of focus on outcome , focus on solving the pattern and focus on data and measurements in areas like client onboarding , in areas like maintaining client data , et cetera . So , this has been very helpful for us because in a bank like ours , scale is super important.Laurel : Yeah , that 's a really great explanation . So , when new challenges do come along , like moving to the public cloud , how do you balance the opportunities of that scale , but also computing power and resources within the cost of the actual investment ? How do you ensure that the shifts to the cloud are actually both financially and operationally efficient ? Vrinda : Great question . So obviously every technologist in the world is super excited with the advent of the public cloud . It gives us the powers of agility , economies of scale . We at JPMorgan Chase are able to leverage world class evolving capabilities at our fingertips . We have the ability also to partner with talented technologies at the cloud providers and many service providers that we work with that have advanced solutions that are available first on the public cloud . We are eager to get our hands on those . But with that comes a lot of responsibility because as a bank , we have to worry about security , client data , privacy , resilience , how are we going to operate in a multi-cloud environment because some data has to remain on-prem in our private cloud . So , there 's a lot of complexity , and we have engineers across the board who think a lot about this , and their day and night jobs are to try and figure this out . As we think about moving to the public cloud in my area , I personally spend time thinking in depth about how we could build architectures that are financially efficient . And the reason I bring that up is because traditionally as we think about data centers where our hardware and software has been hosted , developers and architects have n't had to worry about costs because you start with sizing the infrastructure , you order that infrastructure , it 's captive , it remains in the data center , and you can expand it , but it 's a one-time cost each time that you upgrade . With the cloud , that situation changes dramatically . It 's both an opportunity but also a risk . So , a financial lens then becomes super important right at the outset . Let me give you a couple of examples of what I mean . Developers in the public cloud have a lot of power , and with that power comes responsibility . So , I 'm a developer and my application is not working right now because there 's some issue . I have the ability to actually spin up additional processes . I have the ability to spin up additional environments , all of which attract costs , and if I do n't control and manage that , the cost could quickly pile up . Data storage , again , we had fixed storage , we could expand it periodically in the data centers , but in the public cloud , you have choices . You can say data that 's going to be slowly accessed versus data that 's going to be accessed frequently to be stored in different types of storage with different costs as a result . Now think about something like a financial ledger where you have retention requirements of let 's say 20 years . The cost could quickly pile up if you store it in the wrong type of storage . So , there 's an opportunity to optimize cost there , and if you ignore it and you 've not kept an eye on it , you could actually have costs that are just not required . To do this right , we have to ask developers , architects , and our engineers to not just think about the best performance , the most optimal resilience , but also think about cost as a fundamental aspect of how we look at architectures . So , this for me is a huge area of focus , starting with awareness for our people , training our people , thinking about architecture patterns , solution patterns , tooling , measurements , so that we completely stay on top of this and become more effective and more efficient in how we get to the public cloud . While the journey is exciting , I want to make sure that as we land there , we land safely and optimally from a cost standpoint . Laurel : And especially in your position , thinking about how technology will affect the firm years and the future is critical . Therefore , as emerging technologies like AI and machine learning become more commonplace across industries , could you offer an example of how you 're using them in the areas that you cover ? Vrinda : Yeah , certainly . And we use AI/ML at many levels of complexity . So let me start with the base case . AI/ML , especially in operations and client services , starts with can I get data from documents ? Can I OCR those documents , which is optical character recognition ? Can I get information out of it , can I classify it ? Can I perform analytics on it ? So that 's the base case . On top of that , as you look at data , for example , payments data or data of transactions , and let 's say human beings are scanning them for issues or outliers , outlier detection techniques with AI/ML , they are also table stakes now , and many of our systems do that . But as you move on to the next level of prediction , what we 've been able to do is start to build up models where say the client is calling . The client has all these types of cases in progress right now . What could they be calling about in addition to this ? The client expressed sentiment about something that they were not happy with two weeks ago . Is it likely that they 're calling about this ? Can I have that information at the fingertips of the client service agent so they can look at it and respond as soon as the client asks for something ? And think about the next stage of evolution , which is , the client came to us and said , `` Change my title because I just got married . '' Typically , in a transactional kind of activity , you would respond to the client and fix the title from let 's say , Ms. to Mrs. if that 's what they asked you to do . But imagine if when they came to do that , we said to them , here 's 10 other things that you should possibly think of now that you said you 've got married . Congratulations . Do you want to address your beneficiaries ? Do you want to change something in tax planning ? Do you want to change the type of tax calculations that you do because you want to optimize now that you 're married , and you and your spouse could be filing jointly ? Again , not that the client would choose to change those things , but you have an opportunity to be more proactive and think about it holistically so you can address their needs before they even come to you asking for that level of engagement and detail . We also exploit a lot of AI/ML capabilities and in client onboarding to get better data to start to predict what data is right and start to predict risk . And our next leap , I believe strongly , and I 'm super excited about this area of large language models , which I think are going to offer us exponential possibilities , not just in JPMorgan Chase , but as you can see in the world right now with technologies like ChatGPT , OpenAI 's technologies , as well as any of the other publicly available large language models that are being developed every single day . Laurel : Well , it 's clear that AI offers great opportunities for optimizing platforms and transformations . Could you describe the process of how JPMorgan Chase decided to create dedicated teams for AI and machine learning , and how did you build out those teams ? Vrinda : Yeah , certainly . At JPMorgan Chase , we 've been cultivating the mindset for some years now to think AI-first while hiring people . And we also leverage the best talent in the industry , and we 've hired a lot of people in our research divisions as well to work on AI/ML . We 've got thousands , several thousand technologists focused on AI . For me personally , in 2020 , during the first months of the pandemic , I decided that I needed to see more AI/ML activity across my areas . So , I did what I called the “ Summer of AI/ML , ” and this was a fully immersive program that ran over 12 weeks with training for our people , and it was not full-time . So , they would dial in for a couple of hours , get trained on an AI/ML concept and some techniques , and then they would continue that and practice that for the week . Then we had ideation sessions with our users for a couple of weeks and then a hackathon and some brilliant ideas came out of it . But when I stepped back and looked at this whole thing and the results of it , a few months later , I realized that many of the ideas had not reached the final destination into production . And in thinking a little more deeply about that , I understood that we had a problem . The problem was as follows , while AI is a great thing and everybody appreciates it , until AI becomes ingrained in everybody 's brain as the first thing to think about , there 's always going to be a healthy tension between choosing the next best feature on a product , which is very deterministic . If you say , add this button here or add these features using conventional technologies like Java versus game-changing the product using AI , which is a little bit more of a risk , the results are not always predictable , and it requires experimentation and R & D . And so , when you have a choice of incremental changes that are deterministic and changes that are more probabilistic , people tend to take the most certain answer . And so , I decided that I needed to build out a focused , dedicated team of data scientists who were just going to obsess about solving problems in the space of data science and embed them across the products that we were building . And now the results are starting to show for themselves because the work they 've done is phenomenal and the demand on them is growing every single day to the point where I 've grown the team and the value that they 're providing is also measured and visible to the broader organization.Laurel : So , in JPMorgan Chase 's client services , customer experience is clearly a driving force . How do you ensure that your teams are providing clients , especially those high-net-worth private clients that have high expectations of service with services that then meet their banking and account management needs ? Vrinda : So , we obsess over customer experience starting from the CEO down to every single employee . I have three tenets for my team . Number one is client experience , the second is user experience , and third is engineering excellence . And they know that a lot of us are measured by how well we service our clients . So , in the private bank specifically , in addition to reviewing our core capabilities like our case management system , our voice recognition systems , our fraud capture systems , all of that , we continuously analyze data received from client surveys , data received through every single interaction that we have with our client across all channels . So , whether it be a voice channel , whether it be emails , whether it be things that the client types in our websites , the places that they access , and our models just do not look at sentiment , they also look at client experience . And as they look at experience , the things that we are trying to understand are , first of all , how 's the client feeling in this interaction ? But more important is client one and client two and client three feeling the same thing about a particular aspect of our process , and do we need to change that process as a result , or is there more training that needs to be provided to our agents because we are not able to fully satisfy this category of requests ? And by doing that continuously and analyzing it , and back to the point that I made earlier , by measuring it constantly , we are able to say , first of all , how was the experience to begin with ? How is the experience now and after making these changes on these training programs or these fixes in our systems , how is that experience showing ? And some of the other things we are able to do are look at experiences over a period of time . So , for example , the client came to us last year and their experience based on the measurements that we did was at a certain level , they continue to interact with us over a period of months . Has it gone up ? Has it gone down ? How is that needle trending ? How do we take that to superb ? And we 've been able to figure out these in ways that we 've been able to prevent complaints , for example , and get to a point where things are escalated to the right people in the organization , especially in the servicing space where we are able to triage and manage these things more effectively because we are a high- touch client business , and we need to make sure that our clients are extremely happy with us . Laurel : Oh yeah , absolutely . And sort of like another phase or idea when we 're thinking about customer experience and customer services , building a workforce that can respond to it . So here we 're going to talk a bit about how we promote diversity , which has been a tenet of your career , and you currently sit on the board of the Transition Network , which is a nonprofit that empowers a diverse network of women throughout career transitions . So , at JPMorgan Chase , how do you grow talent and improve representation across the company ? And then how does that help build better customer experience ? Vrinda : Sure , that 's a great question . I certainly am very passionate about diversity , and during the past 15 years of my career , I 've spent a lot of time supporting diversity . In my prior firm , I was co-head of the Asian Professional Network . Then subsequently for the past three years , I 've been a board member at the Transition Network , which is all about women in transition . Meaning as they grow out of their careers into retirement and into other stages of life , how do we help them transition ? And then here at JPMorgan Chase , I 'm the sponsor for what is called the Take It Forward initiative , which is an initiative that supports 15,000 women technologists . JPMorgan Chase , as you know , does a broad range of activities in the area of diversity across all kinds of business resource groups , and we invest a lot of time and energy . But specifically , for the Take It Forward initiative that I sponsor , it plays a key role in helping these 15,000 women technologists continuously enhance their leadership skills , grow their technical skills , build their confidence , develop networks , learn from senior sponsors and mentors , grow their careers , all of which makes their work experience very enriching . When I hear things like I 'm motivated , I get new energy interacting with these senior women , I trust my personal power more . I 'm confident to negotiate with my manager for a better role ; I feel confident that I can discuss my compensation . It makes me really happy , and especially when they say I stay in JPMorgan Chase because of Take It Forward , it brings tears to my eyes . It 's really one of the most amazing volunteer driven initiatives in this organization . And a lot of people pour in passion , energy , time to make it succeed . And this initiative has won many awards as well externally . I strongly believe all of these efforts are critical as it changes when people 's experiences change and when they 're happy , what they do becomes that much more effective . And it changes how we work internally , how we present ourselves externally and game changes our business outcomes . I 've seen that in problem solving meetings . When you evaluate risk and you bring in people from diverse backgrounds , some are more risk averse , some are more risk taking , and you suddenly see that dynamic play out and the outcome becomes much different from what it would 've been if you did n't have all those people in the mix . So overall , I strongly believe in this , and I 've seen it play out in every single firm that I 've ever worked at . So that 's my take on diversity and how it helps us . Laurel : Well , it certainly is important work , especially as it ties so tightly with the firm 's own ethos . So Vrinda , looking forward , how do you envision the future of private banking and client management ? As we see emerging technologies become more prevalent and enterprises start to shift their infrastructure to the public cloud ? Vrinda : As I mentioned earlier , I see the next set of emerging technologies taking the world on a super exciting ride , and I think it 's going to be as transformational as the advent of the world wide web . Just take the example of large language models . The areas that are most likely to be first disrupted will be any work that involves content creation because that is table stakes for a large language model . As I expand that to my work , the rest of my work , client services and operations and many other areas that require repetitive work and large-scale interpretation and synthesis of information , that 's again , table stakes for large language models . Expand that now to the next evolution , which is agents that are now the emerging technology with large language models . When agents are provided with a suite of tools and they can use reasoning like humans to decide which tool to execute based on input , that 'll game change the whole thing I was talking about earlier on workflow and task execution and operationally intense activities in the organization . When I look at myself as a software developer in the areas of code generation , code testing , code correction , test data generation , just to name a few , all of those are going to be game changed . So not just the work that our users do in the private bank , but the work that we do as technologists in the private bank . A lot of that is going to game change dramatically . And then you add on the next level , which is problem solving . Large language models are continuously being trained on all subjects ever known to humans . And that for me is the most fascinating part of this . It 's like hundreds of thousands of brains that are working together on a diverse set of subjects . So , imagine a model that 's been trained on a domain like medicine or aerospace or defense , and then trying to bring all of that brainpower together to solve a problem in finance . That truly to me is the ultimate gold standard of problem solving . We talked about diverse people in the room coming with different experiences but imagine models that have been trained . You suddenly have breadth , depth , range of diverse knowledge that could never have been contemplated at that scale . And in order to do all of this , obviously one of the key underpinnings is the public cloud and being able to spin up compute as quickly as possible to do complex calculations and then spin it down when you do n't need it , which is where the public cloud becomes super important . So , all I can say in conclusion is I think this is an amazing time to be in technology , and I just can not wait to see how we further step up our game in the coming months and years , and things are moving almost at the speed of light now . Every single day , new papers get published and new ideas are coming out building on top of some of the exponential technologies that we are seeing in the world today . Laurel : Oh , that 's fantastic . Vrinda , thank you so much for being on the Business Lab today.Vrinda : Thank you so much , Laurel . It 's my pleasure . I really enjoyed speaking with you and thank you for your thoughtful questions . They were super interesting . Laurel : That was Vrinda Menon , the chief technology officer of Managed Accounts , Client Onboarding and Client Services at J.P. Morgan Private Bank , who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review overlooking the Charles River.That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the director of Insights , the custom publishing division of MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can find us in print , on the web and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com.This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff . This podcast is for informational purposes only and it is not intended as legal , tax , financial , investment , accounting or regulatory advice . Opinions expressed herein are the personal views of the individual ( s ) and do not represent the views of JPMorgan Chase & Co . The accuracy of any statements , linked resources , reported findings or quotations are not the responsibility of JPMorgan Chase & Co .","['association', 'co', 'come', 'banking', 'personal', 'business', 'private', 'customer', 'experience', 'build', 'new', 'technology', 'platform', 'employ', 'scale', 'optimize', 'workflow', 'especially', 'critical', 'large', 'bank', 'look', 'meet', 'evolve', 'customer', 'internal', 'stakeholder', 'demand', 'fast', 'personalized', 'way', 'business', 'institution', 'implement', 'good', 'practice', 'cost', 'efficient', 'cloud', 'migration', 'emerge', 'ai', 'machine', 'learning', 'ml', 'tool', 'build', 'well', 'way', 'bank', 'say', 'head', 'manage', 'account', 'client', 'onboarding', 'client', 'service', 'technology', 'bank', 'vrinda', 'menon', 'menon', 'stress', 'critical', 'technologist', 'stay', 'focused', 'business', 'impact', 'software', 'tool', 'develop', 'coach', 'team', 'success', 'innovation', 'come', 'rebuild', 'already', 'build', 'instead', 'leverage', 'take', 'next', 'leap', 'additional', 'feature', 'create', 'high', 'impact', 'business', 'outcome', 'say', 'technologist', 'encourage', 'possible', 'see', 'big', 'picture', 'solve', 'large', 'pattern', 'rather', 'singular', 'problem', 'hand', 'reduce', 'redundancy', 'automate', 'task', 'menon', 'team', 'focus', 'datum', 'measurement', 'indicate', 'emerge', 'technology', 'ai', 'machine', 'learning', 'enhance', 'process', 'onboarding', 'transaction', 'processing', 'scale', 'aiml', 'become', 'commonplace', 'many', 'industry', 'private', 'banking', 'exception', 'say', 'base', 'level', 'aiml', 'extract', 'datum', 'document', 'classify', 'information', 'analyze', 'datum', 'smartly', 'detect', 'issue', 'outlier', 'wide', 'range', 'use', 'case', 'menon', 'look', 'near', 'future', 'aiml', 'proactively', 'predict', 'client', 'need', 'base', 'various', 'signal', 'example', 'private', 'banking', 'client', 'recently', 'marry', 'ask', 'bank', 'title', 'change', 'use', 'client', 'datum', 'context', 'new', 'request', 'aiml', 'tool', 'proactively', 'help', 'banker', 'identify', 'additional', 'thing', 'ask', 'client', 'need', 'change', 'beneficiary', 'possibility', 'optimize', 'taxis', 'consider', 'jointly', 'file', 'taxis', 'opportunity', 'proactive', 'think', 'holistically', 'address', 'need', 'even', 'come', 'ask', 'level', 'engagement', 'detail', 'say', 'episode', 'business', 'lab', 'produce', 'association', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'today', 'invest', 'build', 'great', 'experience', 'number', 'people', 'benefit', 'enterprise', 'investment', 'emerge', 'new', 'technology', 'include', 'customer', 'want', 'well', 'fast', 'new', 'way', 'business', 'internal', 'stakeholder', 'want', 'investment', 'well', 'tool', 'system', 'build', 'fast', 'new', 'way', 'business', 'balance', 'need', 'possibletwo', 'word', 'optimize', 'platformstoday', 'talk', 'vrinda', 'menon', 'chief', 'technology', 'officer', 'manage', 'account', 'client', 'onboarding', 'client', 'service', 'podcast', 'produce', 'association', 'vrindavrinda', 'thank', 'much', 'laurel', 'look', 'forward', 'discussionlaurel', 'great', 'let', 'start', 'often', 'people', 'think', 'likely', 'associate', 'company', 'personal', 'banking', 'credit', 'card', 'describe', 'service', 'private', 'bank', 'provide', 'operation', 'client', 'service', 'evolve', 'transform', 'begin', 'role', 'vrinda', 'sure', 'indeed', 'far', 'personal', 'banking', 'credit', 'card', 'private', 'often', 'refer', 'crown', 'jewel', 'franchise', 'service', 'high', 'net', 'worth', 'client', 'ultrahigh', 'net', 'worth', 'client', 'globe', 'provide', 'service', 'investment', 'management', 'trust', 'estate', 'planning', 'banking', 'service', 'brokerage', 'service', 'customize', 'lending', 'name', 'term', 'transform', 'recent', 'year', 'join', 'say', 'become', 'far', 'tech', 'savvy', 'organization', 'thank', 'small', 'measure', 'new', 'leadership', 'well', 'operation', 'client', 'service', 'think', 'thing', 'change', 'dramatically', 'join', 'first', 'culture', 'first', 'month', 'spend', 'week', 'job', 'operation', 'analyst', 'start', 'understand', 'firsthand', 'painful', 'manual', 'work', 'people', 'subject', 'feel', 'permission', 'thing', 'change', 'work', 'actually', 'connect', 'lot', 'people', 'ground', 'type', 'activity', 'work', 'make', 'change', 'make', 'see', 'light', 'end', 'tunnel', 'suddenly', 'demand', 'change', 'demand', 'automation', 'start', 'build', 'groundswell', 'energy', 'support', 'partner', 'operation', 'service', 'routine', 'repetitive', 'mundane', 'mindnumbe', 'work', 'option', 'table', 'become', 'thing', 'past', 'secondly', 'also', 'grow', 'army', 'citizen', 'developer', 'really', 'access', 'tool', 'technology', 'quick', 'automation', 'depend', 'broad', 'program', 'broad', 'piece', 'technology', 'also', 'super', 'interesting', 'past', 'year', 'take', 'new', 'analyst', 'private', 'bank', 'train', 'python', 'start', 'see', 'benefit', 'thing', 'culture', 'change', 'think', 'big', 'thing', 'achieve', 'past', 'year', 'join', 'second', 'build', 'whole', 'set', 'capability', 'call', 'common', 'capability', 'thing', 'configure', 'new', 'workflow', 'make', 'decision', 'use', 'spreadsheet', 'decision', 'model', 'code', 'system', 'configure', 'modify', 'thing', 'effectively', 'tool', 'checklist', 'put', 'system', 'automate', 'minute', 'many', 'case', 'today', 'million', 'task', 'million', 'decision', 'execute', 'capability', 'suddenly', 'gamechange', 'ability', 'provide', 'automation', 'scale', 'last', 'least', 'ai', 'machine', 'learn', 'play', 'important', 'role', 'underpinning', 'operation', 'client', 'service', 'example', 'lot', 'process', 'analytic', 'load', 'balance', 'client', 'call', 'agent', 'group', 'people', 'direct', 'client', 'call', 'actually', 'service', 'client', 'effectively', 'space', 'payment', 'lot', 'machine', 'learn', 'fraud', 'detection', 'say', 'glad', 'time', 'invest', 'think', 'foundational', 'capability', 'poise', 'ready', 'take', 'next', 'big', 'leap', 'change', 'right', 'fingertip', 'especially', 'evolve', 'world', 'machine', 'learning', 'course', 'public', 'cloudlaurel', 'excellent', 'certainly', 'outline', 'diversity', 'firm', 'offering', 'build', 'new', 'technology', 'platform', 'work', 'methodology', 'practice', 'employ', 'build', 'scale', 'optimize', 'workflow', 'vrinda', 'say', 'private', 'bank', 'lot', 'offering', 'amplify', 'offering', 'franchise', 'commercial', 'bank', 'corporate', 'investment', 'bank', 'consumer', 'community', 'bank', 'many', 'client', 'cross', 'line', 'business', 'bring', 'lot', 'benefit', 'also', 'complexity', 'thing', 'obsess', 'personally', 'simplify', 'thing', 'add', 'complexity', 'second', 'mantra', 'reuse', 'reinvent', 'easy', 'technologist', 'look', 'piece', 'software', 'say', 'great', 'build', 'well', 'instead', 'thing', 'ask', 'people', 'focus', 'organization', 'collectively', 'partner', 'focus', 'first', 'look', 'business', 'outcome', 'coach', 'team', 'success', 'innovation', 'come', 'rebuild', 'already', 'build', 'instead', 'leverage', 'take', 'next', 'leap', 'additional', 'feature', 'create', 'high', 'impact', 'business', 'outcome', 'focus', 'outcome', 'number', 'second', 'give', 'problem', 'try', 'look', 'big', 'picture', 'see', 'solve', 'pattern', 'instead', 'specific', 'problem', 'give', 'example', 'build', 'chatbot', 'call', 'casey', 'love', 'product', 'private', 'bank', 'right', 'casey', 'really', 'complex', 'solve', 'common', 'pattern', 'ask', 'simple', 'question', 'get', 'input', 'join', 'datum', 'service', 'join', 'execution', 'service', 'complete', 'task', 'hundred', 'thousand', 'task', 'casey', 'perform', 'single', 'day', 'especially', 'simple', 'functionality', 'client', 'want', 'bank', 'reference', 'letter', 'casey', 'call', 'thousand', 'time', 'month', 'use', 'take', 'hour', 'produce', 'take', 'second', 'suddenly', 'change', 'outcome', 'change', 'productivity', 'change', 'happiness', 'people', 'thing', 'know', 'feel', 'mundane', 'solve', 'pattern', 'important', 'last', 'least', 'focus', 'datum', 'thing', 'help', 'improve', 'measure', 'give', 'example', 'process', 'first', 'thing', 'pick', 'complex', 'process', 'map', 'understand', 'step', 'process', 'understand', 'purpose', 'step', 'process', 'time', 'take', 'step', 'start', 'question', 'really', 'need', 'approval', 'person', 'observe', 'past', 'month', 'single', 'thing', 'reject', 'even', 'meaningful', 'approval', 'begin', 'questioning', 'process', 'enhance', 'automatically', 'say', 'approve', 'risk', 'approve', 'need', 'human', 'review', 'make', 'change', 'system', 'flow', 'obsessively', 'measure', 'impact', 'change', 'give', 'lot', 'benefit', 'say', 'make', 'significant', 'progress', 'principle', 'focus', 'outcome', 'focus', 'solve', 'pattern', 'focus', 'datum', 'measurement', 'area', 'client', 'onboarding', 'area', 'maintain', 'client', 'datum', 'et', 'cetera', 'helpful', 'bank', 'scale', 'super', 'importantlaurel', 'really', 'great', 'explanation', 'new', 'challenge', 'come', 'move', 'public', 'cloud', 'balance', 'opportunity', 'scale', 'also', 'compute', 'power', 'resource', 'cost', 'actual', 'investment', 'ensure', 'shift', 'cloud', 'actually', 'financially', 'operationally', 'efficient', 'vrinda', 'great', 'question', 'obviously', 'technologist', 'world', 'super', 'excited', 'advent', 'public', 'cloud', 'give', 'power', 'agility', 'economy', 'scale', 'able', 'leverage', 'world', 'class', 'evolving', 'capability', 'fingertip', 'ability', 'also', 'partner', 'talented', 'technology', 'cloud', 'provider', 'many', 'service', 'provider', 'work', 'advanced', 'solution', 'available', 'first', 'public', 'cloud', 'eager', 'get', 'hand', 'come', 'lot', 'responsibility', 'bank', 'worry', 'security', 'client', 'datum', 'privacy', 'resilience', 'go', 'operate', 'multicloud', 'environment', 'datum', 'remain', 'private', 'cloud', 'lot', 'complexity', 'engineer', 'board', 'think', 'lot', 'day', 'night', 'job', 'try', 'figure', 'think', 'move', 'public', 'cloud', 'area', 'personally', 'spend', 'time', 'think', 'depth', 'build', 'architecture', 'financially', 'efficient', 'reason', 'bring', 'traditionally', 'think', 'datum', 'center', 'hardware', 'software', 'host', 'developer', 'architect', 'worry', 'cost', 'start', 'size', 'infrastructure', 'order', 'infrastructure', 'captive', 'remain', 'datum', 'center', 'expand', 'onetime', 'cost', 'time', 'upgrade', 'cloud', 'situation', 'change', 'dramatically', 'opportunity', 'also', 'risk', 'financial', 'lens', 'become', 'super', 'important', 'right', 'outset', 'let', 'give', 'couple', 'example', 'mean', 'developer', 'public', 'cloud', 'lot', 'power', 'power', 'come', 'responsibility', 'developer', 'application', 'work', 'right', 'issue', 'ability', 'actually', 'spin', 'additional', 'process', 'ability', 'spin', 'additional', 'environment', 'attract', 'cost', 'control', 'manage', 'cost', 'quickly', 'pile', 'datum', 'storage', 'fix', 'storage', 'expand', 'periodically', 'datum', 'center', 'public', 'cloud', 'choice', 'say', 'datum', 'go', 'slowly', 'access', 'datum', 'go', 'access', 'frequently', 'store', 'different', 'type', 'storage', 'different', 'cost', 'result', 'think', 'financial', 'ledger', 'retention', 'requirement', 'let', 'say', 'year', 'cost', 'quickly', 'pile', 'store', 'wrong', 'type', 'storage', 'opportunity', 'optimize', 'cost', 'ignore', 'keep', 'eye', 'actually', 'cost', 'require', 'right', 'ask', 'developer', 'architect', 'engineer', 'think', 'good', 'performance', 'optimal', 'resilience', 'also', 'think', 'cost', 'fundamental', 'aspect', 'look', 'architecture', 'huge', 'area', 'focus', 'start', 'awareness', 'people', 'train', 'people', 'think', 'architecture', 'pattern', 'solution', 'pattern', 'tool', 'measurement', 'completely', 'stay', 'top', 'become', 'effective', 'efficient', 'get', 'public', 'cloud', 'journey', 'exciting', 'want', 'make', 'sure', 'land', 'land', 'safely', 'optimally', 'cost', 'standpoint', 'laurel', 'especially', 'position', 'think', 'technology', 'affect', 'firm', 'year', 'future', 'critical', 'therefore', 'emerge', 'technology', 'ai', 'machine', 'learning', 'become', 'commonplace', 'industry', 'offer', 'example', 'use', 'area', 'cover', 'vrinda', 'certainly', 'use', 'aiml', 'many', 'level', 'complexity', 'let', 'start', 'base', 'case', 'aiml', 'especially', 'operation', 'client', 'service', 'start', 'get', 'datum', 'document', 'ocr', 'document', 'optical', 'character', 'recognition', 'get', 'information', 'classify', 'perform', 'analytic', 'base', 'case', 'top', 'look', 'datum', 'example', 'payment', 'datum', 'datum', 'transaction', 'let', 'say', 'human', 'scan', 'issue', 'outlier', 'outli', 'detection', 'technique', 'aiml', 'also', 'table', 'stake', 'many', 'system', 'move', 'next', 'level', 'prediction', 'able', 'start', 'build', 'model', 'say', 'client', 'call', 'client', 'type', 'case', 'progress', 'right', 'call', 'addition', 'client', 'express', 'sentiment', 'happy', 'week', 'ago', 'likely', 'call', 'information', 'fingertip', 'client', 'service', 'agent', 'look', 'respond', 'soon', 'client', 'ask', 'think', 'next', 'stage', 'evolution', 'client', 'come', 'say', 'change', 'title', 'marry', 'typically', 'transactional', 'kind', 'activity', 'respond', 'client', 'fix', 'title', 'let', 'say', 'ask', 'imagine', 'come', 'say', 'thing', 'possibly', 'think', 'say', 'get', 'marry', 'congratulation', 'want', 'address', 'beneficiary', 'want', 'change', 'tax', 'planning', 'want', 'change', 'type', 'tax', 'calculation', 'want', 'optimize', 'marry', 'spouse', 'file', 'jointly', 'client', 'choose', 'change', 'thing', 'opportunity', 'proactive', 'think', 'holistically', 'address', 'need', 'even', 'come', 'ask', 'level', 'engagement', 'detail', 'also', 'exploit', 'lot', 'aiml', 'capability', 'client', 'onboarde', 'get', 'well', 'datum', 'start', 'predict', 'datum', 'right', 'start', 'predict', 'risk', 'next', 'leap', 'believe', 'strongly', 'super', 'excited', 'area', 'large', 'language', 'model', 'think', 'go', 'offer', 'exponential', 'possibility', 'see', 'world', 'right', 'technology', 'chatgpt', 'technology', 'well', 'publicly', 'available', 'large', 'language', 'model', 'develop', 'single', 'day', 'laurel', 'well', 'clear', 'offer', 'great', 'opportunity', 'optimize', 'platform', 'transformation', 'describe', 'process', 'jpmorgan', 'decide', 'create', 'dedicated', 'team', 'ai', 'machine', 'learning', 'build', 'team', 'vrinda', 'certainly', 'cultivate', 'mindset', 'year', 'think', 'aifirst', 'hire', 'people', 'also', 'leverage', 'good', 'talent', 'industry', 'hire', 'lot', 'people', 'research', 'division', 'well', 'work', 'aiml', 'get', 'thousand', 'several', 'technologist', 'focus', 'ai', 'personally', 'first', 'month', 'pandemic', 'decide', 'need', 'see', 'aiml', 'activity', 'area', 'call', 'summer', 'aiml', 'fully', 'immersive', 'program', 'run', 'week', 'training', 'people', 'fulltime', 'dial', 'couple', 'hour', 'train', 'aiml', 'concept', 'technique', 'continue', 'practice', 'week', 'ideation', 'session', 'user', 'couple', 'week', 'hackathon', 'brilliant', 'idea', 'come', 'step', 'back', 'look', 'whole', 'thing', 'result', 'month', 'later', 'realize', 'many', 'idea', 'reach', 'final', 'destination', 'production', 'think', 'little', 'deeply', 'understand', 'problem', 'problem', 'follow', 'great', 'thing', 'appreciate', 'become', 'ingrained', 'brain', 'first', 'thing', 'think', 'always', 'go', 'healthy', 'tension', 'choose', 'next', 'good', 'feature', 'product', 'deterministic', 'say', 'add', 'button', 'add', 'feature', 'use', 'conventional', 'technology', 'java', 'gamechange', 'product', 'use', 'ai', 'little', 'bit', 'risk', 'result', 'always', 'predictable', 'require', 'experimentation', 'r', 'choice', 'incremental', 'change', 'deterministic', 'change', 'probabilistic', 'people', 'tend', 'take', 'certain', 'answer', 'decide', 'need', 'build', 'focused', 'dedicated', 'team', 'datum', 'scientist', 'go', 'obsess', 'solve', 'problem', 'space', 'datum', 'science', 'embed', 'product', 'build', 'result', 'start', 'show', 'work', 'phenomenal', 'demand', 'grow', 'single', 'day', 'point', 'grow', 'team', 'value', 'provide', 'also', 'measure', 'visible', 'broad', 'organizationlaurel', 'client', 'service', 'customer', 'experience', 'clearly', 'drive', 'force', 'ensure', 'team', 'provide', 'client', 'especially', 'highnetworth', 'private', 'client', 'high', 'expectation', 'service', 'service', 'meet', 'banking', 'account', 'management', 'need', 'vrinda', 'obsess', 'customer', 'experience', 'start', 'ceo', 'single', 'employee', 'tenet', 'team', 'number', 'client', 'experience', 'second', 'user', 'experience', 'third', 'engineer', 'excellence', 'know', 'lot', 'measure', 'well', 'service', 'client', 'private', 'bank', 'specifically', 'addition', 'review', 'core', 'capability', 'case', 'management', 'system', 'voice', 'recognition', 'system', 'fraud', 'capture', 'system', 'continuously', 'analyze', 'datum', 'receive', 'client', 'survey', 'datum', 'receive', 'single', 'interaction', 'client', 'channel', 'voice', 'channel', 'email', 'thing', 'client', 'type', 'website', 'place', 'access', 'model', 'look', 'sentiment', 'also', 'look', 'client', 'experience', 'look', 'experience', 'thing', 'try', 'understand', 'first', 'client', 'feeling', 'interaction', 'important', 'client', 'client', 'client', 'feel', 'thing', 'particular', 'aspect', 'process', 'need', 'change', 'process', 'result', 'training', 'need', 'provide', 'agent', 'able', 'fully', 'satisfy', 'category', 'request', 'continuously', 'analyze', 'back', 'point', 'make', 'early', 'measure', 'constantly', 'able', 'say', 'first', 'experience', 'begin', 'experience', 'make', 'change', 'training', 'program', 'fix', 'system', 'experience', 'show', 'thing', 'able', 'look', 'experience', 'period', 'time', 'example', 'client', 'come', 'last', 'year', 'experience', 'base', 'measurement', 'certain', 'level', 'continue', 'interact', 'period', 'month', 'go', 'go', 'needle', 'trend', 'take', 'superb', 'able', 'figure', 'way', 'able', 'prevent', 'complaint', 'example', 'get', 'point', 'thing', 'escalate', 'right', 'people', 'organization', 'especially', 'servicing', 'space', 'able', 'triage', 'manage', 'thing', 'effectively', 'high', 'touch', 'client', 'business', 'need', 'make', 'sure', 'client', 'extremely', 'happy', 'absolutely', 'sort', 'phase', 'idea', 'think', 'customer', 'experience', 'customer', 'service', 'build', 'workforce', 'respond', 'go', 'talk', 'bit', 'promote', 'diversity', 'tenet', 'career', 'currently', 'sit', 'board', 'transition', 'network', 'nonprofit', 'empower', 'diverse', 'network', 'woman', 'career', 'transition', 'grow', 'talent', 'improve', 'representation', 'company', 'help', 'build', 'well', 'customer', 'experience', 'vrinda', 'sure', 'great', 'question', 'certainly', 'passionate', 'diversity', 'past', 'year', 'career', 'spend', 'lot', 'time', 'support', 'diversity', 'prior', 'firm', 'cohead', 'asian', 'professional', 'network', 'subsequently', 'past', 'year', 'board', 'member', 'transition', 'network', 'woman', 'transition', 'meaning', 'grow', 'career', 'retirement', 'stage', 'life', 'help', 'transition', 'sponsor', 'call', 'take', 'forward', 'initiative', 'initiative', 'support', 'woman', 'technologist', 'know', 'broad', 'range', 'activity', 'area', 'diversity', 'kind', 'business', 'resource', 'group', 'invest', 'lot', 'time', 'energy', 'specifically', 'take', 'forward', 'initiative', 'sponsor', 'play', 'key', 'role', 'help', 'woman', 'technologist', 'continuously', 'enhance', 'leadership', 'skill', 'grow', 'technical', 'skill', 'build', 'confidence', 'develop', 'network', 'learn', 'senior', 'sponsor', 'mentor', 'grow', 'career', 'make', 'work', 'experience', 'enrich', 'hear', 'thing', 'motivate', 'get', 'new', 'energy', 'interact', 'senior', 'woman', 'trust', 'personal', 'power', 'confident', 'negotiate', 'manager', 'well', 'role', 'feel', 'confident', 'discuss', 'compensation', 'make', 'really', 'happy', 'especially', 'say', 'stay', 'take', 'forward', 'bring', 'tear', 'eye', 'really', 'amazing', 'volunteer', 'drive', 'initiative', 'organization', 'lot', 'people', 'pour', 'passion', 'energy', 'time', 'make', 'succeed', 'initiative', 'win', 'many', 'award', 'well', 'externally', 'strongly', 'believe', 'effort', 'critical', 'change', 'people', 'experience', 'change', 'happy', 'become', 'much', 'effective', 'change', 'work', 'internally', 'present', 'externally', 'game', 'change', 'business', 'outcome', 'see', 'problem', 'solve', 'meeting', 'evaluate', 'risk', 'bring', 'people', 'diverse', 'background', 'risk', 'averse', 'risk', 'take', 'suddenly', 'see', 'dynamic', 'play', 'outcome', 'become', 'much', 'different', 'people', 'mix', 'overall', 'strongly', 'believe', 'see', 'play', 'single', 'firm', 'ever', 'work', 'take', 'diversity', 'help', 'well', 'certainly', 'important', 'work', 'especially', 'tie', 'tightly', 'firm', 'ethos', 'vrinda', 'look', 'forward', 'envision', 'future', 'private', 'banking', 'client', 'management', 'see', 'emerge', 'technology', 'become', 'prevalent', 'enterprise', 'start', 'shift', 'infrastructure', 'public', 'cloud', 'vrinda', 'mention', 'early', 'see', 'next', 'set', 'emerge', 'technology', 'take', 'world', 'super', 'exciting', 'ride', 'think', 'go', 'transformational', 'advent', 'world', 'wide', 'web', 'take', 'example', 'large', 'language', 'model', 'area', 'likely', 'first', 'disrupt', 'work', 'involve', 'content', 'creation', 'table', 'stake', 'large', 'language', 'model', 'expand', 'work', 'rest', 'work', 'client', 'service', 'operation', 'many', 'area', 'require', 'repetitive', 'work', 'largescale', 'interpretation', 'synthesis', 'information', 'table', 'stake', 'large', 'language', 'model', 'expand', 'next', 'evolution', 'agent', 'emerge', 'technology', 'large', 'language', 'model', 'agent', 'provide', 'suite', 'tool', 'use', 'reasoning', 'human', 'decide', 'tool', 'execute', 'base', 'input', 'game', 'change', 'whole', 'thing', 'talk', 'early', 'workflow', 'task', 'execution', 'operationally', 'intense', 'activity', 'organization', 'look', 'software', 'developer', 'area', 'code', 'generation', 'code', 'testing', 'code', 'correction', 'test', 'datum', 'generation', 'name', 'go', 'game', 'change', 'work', 'user', 'private', 'bank', 'work', 'technologist', 'private', 'bank', 'lot', 'go', 'game', 'change', 'dramatically', 'add', 'next', 'level', 'problem', 'solve', 'large', 'language', 'model', 'continuously', 'train', 'subject', 'ever', 'know', 'human', 'fascinating', 'part', 'hundred', 'thousand', 'brain', 'work', 'together', 'diverse', 'set', 'subject', 'imagine', 'model', 'train', 'domain', 'medicine', 'aerospace', 'defense', 'try', 'bring', 'brainpower', 'together', 'solve', 'problem', 'finance', 'truly', 'ultimate', 'gold', 'standard', 'problem', 'solve', 'talk', 'diverse', 'people', 'room', 'come', 'different', 'experience', 'imagine', 'model', 'train', 'suddenly', 'breadth', 'depth', 'range', 'diverse', 'knowledge', 'never', 'contemplate', 'scale', 'order', 'obviously', 'key', 'underpinning', 'public', 'cloud', 'able', 'spin', 'compute', 'quickly', 'possible', 'complex', 'calculation', 'spin', 'need', 'public', 'cloud', 'become', 'super', 'important', 'say', 'conclusion', 'think', 'amazing', 'time', 'technology', 'wait', 'see', 'far', 'step', 'game', 'come', 'month', 'year', 'thing', 'move', 'almost', 'speed', 'light', 'single', 'day', 'new', 'paper', 'publish', 'new', 'idea', 'come', 'building', 'top', 'exponential', 'technology', 'see', 'world', 'today', 'laurel', 'fantastic', 'vrinda', 'much', 'business', 'lab', 'thank', 'much', 'laurel', 'pleasure', 'really', 'enjoy', 'speak', 'thank', 'thoughtful', 'question', 'super', 'interesting', 'laurel', 'vrinda', 'menon', 'chief', 'technology', 'officer', 'manage', 'account', 'client', 'onboarding', 'client', 'service', 'speak', 'home', 'mit', 'mit', 'technology', 'review', 'overlook', 'charle', 'episode', 'business', 'lab', 'host', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff', 'podcast', 'informational', 'purpose', 'intend', 'legal', 'tax', 'financial', 'investment', 'accounting', 'regulatory', 'advice', 'opinion', 'express', 'herein', 'personal', 'view', 'individual', 'represent', 'view', 'co', 'accuracy', 'statement', 'link', 'resource', 'report', 'finding', 'quotation', 'responsibility', 'co']","When it comes to banking, whether it’s personal, business, or private, customer experience is everything. Building new technologies and platforms, employing them at scale, and optimizing workflows is especially critical for any large bank looking to meet evolving customer and internal stakeholder demands for faster and more personalized ways of doing business. Institutions like JPMorgan…"
2023 Global Cloud Ecosystem,https://www.technologyreview.com/2023/11/16/1078645/2023-global-cloud-ecosystem/,2023-11-16,"In partnership withInfosys Cobalt The cloud, fundamentally a tool for cost and resource efficiency, has long enabled companies and countries to organize around digital-first principles. It is an established capability that improves the bottom line for enterprises. However, maturity lags, and global standards are sorely needed. Cloud capabilities play a crucial role in accelerating the global economy’s next stage of digital transformation. Results from our 2023 Global Cloud Ecosystem survey of executives indicate there are two stages of cloud maturity globally: one where firms adopt cloud to achieve essential opex and capex cost reduction, and a second where firms link cloud investments to a positive business value. Respondents indicate the two are converging quickly. The key findings are as follows: Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withInfosys Cobalt The cloud , fundamentally a tool for cost and resource efficiency , has long enabled companies and countries to organize around digital-first principles . It is an established capability that improves the bottom line for enterprises . However , maturity lags , and global standards are sorely needed . Cloud capabilities play a crucial role in accelerating the global economy ’ s next stage of digital transformation . Results from our 2023 Global Cloud Ecosystem survey of executives indicate there are two stages of cloud maturity globally : one where firms adopt cloud to achieve essential opex and capex cost reduction , and a second where firms link cloud investments to a positive business value . Respondents indicate the two are converging quickly . The key findings are as follows : Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'cobalt', 'cloud', 'fundamentally', 'tool', 'cost', 'resource', 'efficiency', 'long', 'enable', 'company', 'country', 'organize', 'digitalfirst', 'principle', 'establish', 'capability', 'improve', 'bottom', 'line', 'enterprise', 'however', 'maturity', 'lag', 'global', 'standard', 'sorely', 'need', 'cloud', 'capability', 'play', 'crucial', 'role', 'accelerate', 'global', 'economy', 'next', 'stage', 'digital', 'transformation', 'result', 'global', 'cloud', 'ecosystem', 'survey', 'executive', 'indicate', 'stage', 'cloud', 'maturity', 'globally', 'one', 'firm', 'adopt', 'cloud', 'achieve', 'essential', 'opex', 'capex', 'cost', 'reduction', 'second', 'firm', 'link', 'cloud', 'investment', 'positive', 'business', 'value', 'respondent', 'indicate', 'converge', 'quickly', 'key', 'finding', 'follow', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","The cloud, fundamentally a tool for cost and resource efficiency, has long enabled companies and countries to organize around digital-first principles. It is an established capability that improves the bottom line for enterprises. However, maturity lags, and global standards are sorely needed. Cloud capabilities play a crucial role in accelerating the global economy’s next stage…"
Huawei’s 5G chip breakthrough needs a reality check,https://www.technologyreview.com/2023/11/15/1083413/huaweis-5g-chip-smartphone-sanction/,2023-11-15,"This story first appeared in China Report, MIT Technology Review’s newsletter about technology in China. Sign up to receive it in your inbox every Tuesday. This is going to be a BIG week for US-China relations: On Wednesday, Xi Jinping will sit down with Joe Biden in San Francisco and talk about military issues, trade, and more. It will be his first visit to the US in six years.  Well, so much has happened since 2017. A harmonious era in US-China relations ended; we’ve seen a trade war, a pandemic, an ongoing technology rivalry, an off-course spy balloon, and too many other tumultuous developments to list here.  It’s in this context that Huawei, the Chinese telecom and technology company, became something of a poster child for souring relations. It was one of the first Chinese tech companies to receive intense scrutiny and become a target for sanctions driven by national security concerns. In fact, many of the ways the US currently deploys sanctions in the US-China tech war are inspired by its success in curbing Huawei. (If you want to know more about the fight over semiconductors, read more from me here and here.) But it’d be a mistake to think Huawei has collapsed. Far from it, in fact. In August—at the same time US Commerce Secretary Gina Raimondo was visiting China—Huawei suddenly, without any public announcement, shocked the world when it started selling a new flagship phone, the Mate 60 Pro.  The big surprise here was that this phone uses a new 5G chip, even though the company has been blocked from sourcing 5G chips or working with chip factories outside of China since 2020. So regulators in DC and other China hawks panicked that the relatively advanced seven-nanometer chip proved the company had somehow circumvented sanctions.  But had it really? Researchers who broke down the chip believe that it seems to be entirely designed by Huawei and made in China. I’ve been wanting to understand what really happened here—and what it means more broadly for the ongoing competition over chips and other tech. So I recently spoke with Harish Krishnaswamy, a professor at Columbia University who studies telecommunications and chip designs.  The bottom line is that the Mate 60 Pro shows a manufacturing breakthrough on Huawei’s side that puts it back into the smartphone game. “It's clearly wrong to assume that they're not a player because of the sanctions,” Krishnaswamy told me. He explained that the designing of 5G chips is not necessarily difficult for Chinese researchers and companies, so it wouldn’t have been that hard for Huawei to create a 5G chip in the lab. What is much harder is mass-producing a 5G chip at great quality and reasonable costs so it can be used in a consumer product.  Producing advanced 5G chips “is just a very large engineering endeavor that very few companies can pull off,” he says. Only a handful of companies—like Qualcomm (American), MediaTek (Taiwanese), Samsung (South Korean), and HiSilicon (a chip company that is a Huawei subsidiary)—have successfully made 5G modem chips. Even tech giants like Intel and Apple have failed so far to develop in-house 5G chips.  So when the US sanctions blocked companies from supplying chips to Huawei for its phones, the Chinese manufacturer had no choice but to rely on HiSilicon alone.  Aggressive new US policies will be put to the test in 2023. They could ultimately fragment the global semiconductor industry. But the sanctions also cut HiSilicon off from the global network of factories (called “foundries” or “fabs” in the semiconductor industry) that are vital in manufacturing and testing the products. TSMC, the Taiwanese chip fab that was making chips for HiSilicon at the time, stopped supplying it in 2020. This further narrowed the options for Huawei. It could only turn to Chinese fabs, and that was a costly and time-consuming process. “Anytime you’ve designed a chip in a fab and then, for some reason, you cannot source that anymore and move to another company, just that process of redesigning, qualification, and ramping production takes at least three years,” says Krishnaswamy, who also owns a chip company that works closely with fabs.  Then, making things even more challenging for Huawei, Chinese chip fabs were later put under sanctions and now can’t access any cutting-edge chip-making technologies. So when Huawei made the smartphone with a new 5G chip earlier this year, Krishnaswamy says, the industry was surprised by how quickly it had been able to turn around a design, shift to a Chinese fab, manufacture the new chip, and get it to production rate, all while having to deal with significant revenue losses caused by sanctions. “For Huawei to show [that it is] able to do that is definitely impressive,” he says.  Indeed, since news came out that it was making its own 5G chips in Chinese fabs with minimum overseas input, the company has become a source of national pride for many in China. For them, it’s an example of how US sanctions don’t always work as intended. They may force Chinese companies to adapt, move production back to China, and catch up in areas of tech where they had lagged—and that can ultimately work out in China’s favor. Still, it may be too early for them to cheer, as there are other obstacles to clear before Huawei can become competitive in the high-end phone market again. “After any company shows proof of life—basically, that they have a chip—the long-term success is really governed by the ability to source [it] in large volumes and get the costs down over time,” Krishnaswamy says. Huawei is experiencing a reality check right now. A few months after the release of the new model, domestic enthusiasm has been hurt by the fact that it’s still very difficult for consumers to get a phone. They are back-ordered for months because of a supply shortage. In fact, just this past weekend, which was China’s equivalent of Black Friday, the top-selling phones were still from Apple and Xiaomi (another Chinese brand). At the end of the day, for any chance of regaining its dominant position in the smartphone market, Huawei needs to compete with Qualcomm and Samsung to reliably make better and cheaper chips, and that’s very difficult to pull off. The US chip blockade, which is consistently being expanded and strengthened, will only make it harder in the future—even if it hasn’t made it impossible thus far. Unless the Biden-Xi meeting produces some truly surprising results this week, the bitter winter of US-China relations may continue for years. In which case there are a lot more battles ahead for Chinese companies like Huawei, and one breakthrough in chip manufacturing won’t be enough.  Do you think Huawei still has a chance in the global smartphone market? Let me know your thoughts at zeyi@technologyreview.com. 1. Xi wanted to dine with American business leaders in San Francisco before he met with Biden. The White House rejected the proposal, part of a lengthy back-and-forth over the visit’s agenda. (Wall Street Journal $) 2. A Chinese university tapped Hikvision, the surveillance camera company, to build a smart campus project last year. It includes an alert system that flags ethnic minority students who choose to fast during Ramadan. (IPVM) 3. As Huawei builds up its Harmony Operating System to replace the now-embargoed Android system, Chinese tech companies are quickly recruiting developers who understand HarmonyOS. (South China Morning Post $) 4. The Nepali government decided to ban TikTok, claiming that the app has spread discord and disturbed social harmony. (Nepali Times) 5. After Chinese hobbyist drones were heavily used in the war in Ukraine, the Israeli military is now stockpiling them to use in its war. (Wall Street Journal $) 6. A new bipartisan bill in Congress seeks to block the US government from doing business with Tether, the issuer of a major stablecoin called USDT, because Tether’s parent company is based in Hong Kong. (Coindesk) 7. Baidu ordered 1,600 AI chips from Huawei as an alternative to Nvidia’s A100 chip to use in servers, showing how Chinese companies cope with the bottleneck on computing power. (Reuters $)  8. The founder of a major Chinese game-streaming website is apparently being held by the police. It could be related to an investigation of porn and gambling content on the platform. (Financial Times $) This year, many Chinese smartphone users found themselves pestered by a new form of advertising: whenever they shook their phone, intentionally or not, the e-commerce app would automatically open.  It turns out this is the latest marketing trick that exploits the gyroscope function in modern-day phones, which detects movement to calculate steps or find directions. Late last year, some Chinese tech companies came up with an industry standard on the technical specifications for how to open a new app when movement is detected. It quickly started being deployed to spam users with ads, and it has drawn widespread complaints. According to the Chinese publication Time Finance, Apple has become aware of the phenomenon and instructed several top Chinese apps to ban the practice starting this month. But it’s hard to say whether that’s enough to prevent more apps from following suit. A traveler ordered food delivery to his hotel room in China but never got it. Eventually, he received a call asking for help—the robot that was carrying his food had tripped and couldn’t get up. Maybe robots will eventually rule the world, but fortunately, not today.  Iâ€™ve been thinking a lot about this æœ‹å‹åœˆ latelyâ€¦ the guy ordered delivery to the room at a hotel, called down and concierge said it had already been sent up, but eventually he gets a call from the delivery robot asking for helpâ€¦ #china #robotwars pic.twitter.com/Miy6Ibyvku","This story first appeared in China Report , MIT Technology Review ’ s newsletter about technology in China . Sign up to receive it in your inbox every Tuesday . This is going to be a BIG week for US-China relations : On Wednesday , Xi Jinping will sit down with Joe Biden in San Francisco and talk about military issues , trade , and more . It will be his first visit to the US in six years . Well , so much has happened since 2017 . A harmonious era in US-China relations ended ; we ’ ve seen a trade war , a pandemic , an ongoing technology rivalry , an off-course spy balloon , and too many other tumultuous developments to list here . It ’ s in this context that Huawei , the Chinese telecom and technology company , became something of a poster child for souring relations . It was one of the first Chinese tech companies to receive intense scrutiny and become a target for sanctions driven by national security concerns . In fact , many of the ways the US currently deploys sanctions in the US-China tech war are inspired by its success in curbing Huawei . ( If you want to know more about the fight over semiconductors , read more from me here and here . ) But it ’ d be a mistake to think Huawei has collapsed . Far from it , in fact . In August—at the same time US Commerce Secretary Gina Raimondo was visiting China—Huawei suddenly , without any public announcement , shocked the world when it started selling a new flagship phone , the Mate 60 Pro . The big surprise here was that this phone uses a new 5G chip , even though the company has been blocked from sourcing 5G chips or working with chip factories outside of China since 2020 . So regulators in DC and other China hawks panicked that the relatively advanced seven-nanometer chip proved the company had somehow circumvented sanctions . But had it really ? Researchers who broke down the chip believe that it seems to be entirely designed by Huawei and made in China . I ’ ve been wanting to understand what really happened here—and what it means more broadly for the ongoing competition over chips and other tech . So I recently spoke with Harish Krishnaswamy , a professor at Columbia University who studies telecommunications and chip designs . The bottom line is that the Mate 60 Pro shows a manufacturing breakthrough on Huawei ’ s side that puts it back into the smartphone game . “ It 's clearly wrong to assume that they 're not a player because of the sanctions , ” Krishnaswamy told me . He explained that the designing of 5G chips is not necessarily difficult for Chinese researchers and companies , so it wouldn ’ t have been that hard for Huawei to create a 5G chip in the lab . What is much harder is mass-producing a 5G chip at great quality and reasonable costs so it can be used in a consumer product . Producing advanced 5G chips “ is just a very large engineering endeavor that very few companies can pull off , ” he says . Only a handful of companies—like Qualcomm ( American ) , MediaTek ( Taiwanese ) , Samsung ( South Korean ) , and HiSilicon ( a chip company that is a Huawei subsidiary ) —have successfully made 5G modem chips . Even tech giants like Intel and Apple have failed so far to develop in-house 5G chips . So when the US sanctions blocked companies from supplying chips to Huawei for its phones , the Chinese manufacturer had no choice but to rely on HiSilicon alone . Aggressive new US policies will be put to the test in 2023 . They could ultimately fragment the global semiconductor industry . But the sanctions also cut HiSilicon off from the global network of factories ( called “ foundries ” or “ fabs ” in the semiconductor industry ) that are vital in manufacturing and testing the products . TSMC , the Taiwanese chip fab that was making chips for HiSilicon at the time , stopped supplying it in 2020 . This further narrowed the options for Huawei . It could only turn to Chinese fabs , and that was a costly and time-consuming process . “ Anytime you ’ ve designed a chip in a fab and then , for some reason , you can not source that anymore and move to another company , just that process of redesigning , qualification , and ramping production takes at least three years , ” says Krishnaswamy , who also owns a chip company that works closely with fabs . Then , making things even more challenging for Huawei , Chinese chip fabs were later put under sanctions and now can ’ t access any cutting-edge chip-making technologies . So when Huawei made the smartphone with a new 5G chip earlier this year , Krishnaswamy says , the industry was surprised by how quickly it had been able to turn around a design , shift to a Chinese fab , manufacture the new chip , and get it to production rate , all while having to deal with significant revenue losses caused by sanctions . “ For Huawei to show [ that it is ] able to do that is definitely impressive , ” he says . Indeed , since news came out that it was making its own 5G chips in Chinese fabs with minimum overseas input , the company has become a source of national pride for many in China . For them , it ’ s an example of how US sanctions don ’ t always work as intended . They may force Chinese companies to adapt , move production back to China , and catch up in areas of tech where they had lagged—and that can ultimately work out in China ’ s favor . Still , it may be too early for them to cheer , as there are other obstacles to clear before Huawei can become competitive in the high-end phone market again . “ After any company shows proof of life—basically , that they have a chip—the long-term success is really governed by the ability to source [ it ] in large volumes and get the costs down over time , ” Krishnaswamy says . Huawei is experiencing a reality check right now . A few months after the release of the new model , domestic enthusiasm has been hurt by the fact that it ’ s still very difficult for consumers to get a phone . They are back-ordered for months because of a supply shortage . In fact , just this past weekend , which was China ’ s equivalent of Black Friday , the top-selling phones were still from Apple and Xiaomi ( another Chinese brand ) . At the end of the day , for any chance of regaining its dominant position in the smartphone market , Huawei needs to compete with Qualcomm and Samsung to reliably make better and cheaper chips , and that ’ s very difficult to pull off . The US chip blockade , which is consistently being expanded and strengthened , will only make it harder in the future—even if it hasn ’ t made it impossible thus far . Unless the Biden-Xi meeting produces some truly surprising results this week , the bitter winter of US-China relations may continue for years . In which case there are a lot more battles ahead for Chinese companies like Huawei , and one breakthrough in chip manufacturing won ’ t be enough . Do you think Huawei still has a chance in the global smartphone market ? Let me know your thoughts at zeyi @ technologyreview.com . 1 . Xi wanted to dine with American business leaders in San Francisco before he met with Biden . The White House rejected the proposal , part of a lengthy back-and-forth over the visit ’ s agenda . ( Wall Street Journal $ ) 2 . A Chinese university tapped Hikvision , the surveillance camera company , to build a smart campus project last year . It includes an alert system that flags ethnic minority students who choose to fast during Ramadan . ( IPVM ) 3 . As Huawei builds up its Harmony Operating System to replace the now-embargoed Android system , Chinese tech companies are quickly recruiting developers who understand HarmonyOS . ( South China Morning Post $ ) 4 . The Nepali government decided to ban TikTok , claiming that the app has spread discord and disturbed social harmony . ( Nepali Times ) 5 . After Chinese hobbyist drones were heavily used in the war in Ukraine , the Israeli military is now stockpiling them to use in its war . ( Wall Street Journal $ ) 6 . A new bipartisan bill in Congress seeks to block the US government from doing business with Tether , the issuer of a major stablecoin called USDT , because Tether ’ s parent company is based in Hong Kong . ( Coindesk ) 7 . Baidu ordered 1,600 AI chips from Huawei as an alternative to Nvidia ’ s A100 chip to use in servers , showing how Chinese companies cope with the bottleneck on computing power . ( Reuters $ ) 8 . The founder of a major Chinese game-streaming website is apparently being held by the police . It could be related to an investigation of porn and gambling content on the platform . ( Financial Times $ ) This year , many Chinese smartphone users found themselves pestered by a new form of advertising : whenever they shook their phone , intentionally or not , the e-commerce app would automatically open . It turns out this is the latest marketing trick that exploits the gyroscope function in modern-day phones , which detects movement to calculate steps or find directions . Late last year , some Chinese tech companies came up with an industry standard on the technical specifications for how to open a new app when movement is detected . It quickly started being deployed to spam users with ads , and it has drawn widespread complaints . According to the Chinese publication Time Finance , Apple has become aware of the phenomenon and instructed several top Chinese apps to ban the practice starting this month . But it ’ s hard to say whether that ’ s enough to prevent more apps from following suit . A traveler ordered food delivery to his hotel room in China but never got it . Eventually , he received a call asking for help—the robot that was carrying his food had tripped and couldn ’ t get up . Maybe robots will eventually rule the world , but fortunately , not today . Iâ€™ve been thinking a lot about this æœ‹å‹åœˆ latelyâ€¦ the guy ordered delivery to the room at a hotel , called down and concierge said it had already been sent up , but eventually he gets a call from the delivery robot asking for helpâ€¦ # china # robotwars pic.twitter.com/Miy6Ibyvku","['story', 'first', 'appear', 'mit', 'technology', 'review', 'newsletter', 'technology', 'sign', 'receive', 'inbox', 'go', 'big', 'week', 'relation', 'jinping', 'sit', 'talk', 'military', 'issue', 'trade', 'first', 'visit', 'year', 'well', 'much', 'happen', 'harmonious', 'era', 'relation', 'end', 'see', 'trade', 'war', 'pandemic', 'ongoing', 'technology', 'rivalry', 'offcourse', 'spy', 'balloon', 'many', 'tumultuous', 'development', 'list', 'context', 'huawei', 'chinese', 'telecom', 'technology', 'company', 'become', 'poster', 'child', 'sour', 'relation', 'first', 'chinese', 'tech', 'company', 'receive', 'intense', 'scrutiny', 'become', 'target', 'sanction', 'drive', 'national', 'security', 'concern', 'fact', 'many', 'way', 'currently', 'deploy', 'sanction', 'war', 'inspire', 'success', 'curb', 'huawei', 'want', 'know', 'fight', 'semiconductor', 'read', 'mistake', 'think', 'collapse', 'far', 'fact', 'time', 'visit', 'suddenly', 'public', 'announcement', 'shock', 'world', 'start', 'sell', 'new', 'flagship', 'phone', 'mate', 'pro', 'big', 'surprise', 'phone', 'use', 'new', 'g', 'chip', 'even', 'company', 'block', 'source', 'g', 'chip', 'work', 'chip', 'factory', 'outside', 'regulator', 'panic', 'relatively', 'advanced', 'sevennanometer', 'chip', 'prove', 'company', 'somehow', 'circumvent', 'sanction', 'really', 'researcher', 'break', 'chip', 'believe', 'seem', 'entirely', 'design', 'make', 'want', 'understand', 'really', 'happen', 'mean', 'broadly', 'ongoing', 'competition', 'chip', 'tech', 'recently', 'speak', 'harish', 'krishnaswamy', 'professor', 'study', 'telecommunication', 'chip', 'design', 'bottom', 'line', 'mate', 'show', 'manufacturing', 'breakthrough', 'side', 'put', 'back', 'smartphone', 'game', 'clearly', 'wrong', 'assume', 'player', 'sanction', 'tell', 'explain', 'designing', 'g', 'chip', 'necessarily', 'difficult', 'chinese', 'researcher', 'company', 'hard', 'huawei', 'create', 'g', 'chip', 'lab', 'much', 'hard', 'massproduce', 'g', 'chip', 'great', 'quality', 'reasonable', 'cost', 'use', 'consumer', 'product', 'produce', 'advanced', 'g', 'chip', 'large', 'engineering', 'endeavor', 'company', 'pull', 'say', 'handful', 'company', 'qualcomm', 'south', 'korean', 'chip', 'company', 'subsidiary', 'successfully', 'make', 'g', 'modem', 'chip', 'even', 'tech', 'giant', 'apple', 'fail', 'far', 'develop', 'inhouse', 'g', 'chip', 'sanction', 'block', 'company', 'supply', 'chip', 'phone', 'chinese', 'manufacturer', 'choice', 'rely', 'alone', 'aggressive', 'new', 'policy', 'put', 'test', 'ultimately', 'fragment', 'global', 'semiconductor', 'industry', 'sanction', 'also', 'cut', 'global', 'network', 'factory', 'call', 'foundry', 'fab', 'semiconductor', 'industry', 'vital', 'manufacturing', 'test', 'product', 'taiwanese', 'chip', 'make', 'chip', 'hisilicon', 'time', 'stop', 'supply', 'far', 'narrow', 'option', 'turn', 'chinese', 'fab', 'costly', 'timeconsuming', 'process', 'anytime', 'design', 'chip', 'fab', 'reason', 'source', 'anymore', 'move', 'company', 'process', 'redesign', 'qualification', 'ramp', 'production', 'take', 'least', 'year', 'say', 'also', 'chip', 'company', 'work', 'closely', 'fab', 'make', 'thing', 'even', 'challenging', 'chinese', 'chip', 'fab', 'later', 'put', 'sanction', 'access', 'cuttingedge', 'chipmake', 'technology', 'make', 'smartphone', 'new', 'g', 'chip', 'early', 'year', 'krishnaswamy', 'say', 'industry', 'surprise', 'quickly', 'able', 'turn', 'design', 'shift', 'manufacture', 'new', 'chip', 'get', 'production', 'rate', 'deal', 'significant', 'revenue', 'loss', 'cause', 'sanction', 'show', 'able', 'definitely', 'impressive', 'say', 'indeed', 'news', 'come', 'make', 'g', 'chip', 'chinese', 'fab', 'minimum', 'overseas', 'input', 'company', 'become', 'source', 'national', 'pride', 'many', 'example', 'sanction', 'always', 'work', 'intend', 'force', 'chinese', 'company', 'adapt', 'move', 'production', 'back', 'catch', 'area', 'tech', 'lag', 'ultimately', 'work', 'favor', 'still', 'early', 'cheer', 'obstacle', 'clear', 'huawei', 'become', 'competitive', 'highend', 'phone', 'market', 'company', 'show', 'proof', 'life', 'basically', 'chip', 'longterm', 'success', 'really', 'govern', 'ability', 'source', 'large', 'volume', 'get', 'cost', 'time', 'krishnaswamy', 'say', 'experience', 'reality', 'check', 'right', 'month', 'release', 'new', 'model', 'domestic', 'enthusiasm', 'hurt', 'fact', 'still', 'difficult', 'consumer', 'get', 'phone', 'backordere', 'month', 'supply', 'shortage', 'fact', 'past', 'weekend', 'equivalent', 'black', 'topselle', 'phone', 'still', 'apple', 'xiaomi', 'chinese', 'brand', 'end', 'day', 'chance', 'regain', 'dominant', 'position', 'smartphone', 'market', 'need', 'compete', 'qualcomm', 'reliably', 'make', 'well', 'cheap', 'chip', 'difficult', 'pull', 'chip', 'blockade', 'consistently', 'expand', 'strengthen', 'make', 'hard', 'future', 'even', 'make', 'impossible', 'thus', 'far', 'bidenxi', 'meeting', 'produce', 'truly', 'surprising', 'result', 'week', 'bitter', 'winter', 'relation', 'continue', 'year', 'case', 'lot', 'battle', 'ahead', 'chinese', 'company', 'breakthrough', 'chip', 'manufacturing', 'win', 'enough', 'think', 'still', 'chance', 'global', 'smartphone', 'market', 'let', 'know', 'thought', 'want', 'dine', 'american', 'business', 'leader', 'meet', 'biden', 'reject', 'proposal', 'part', 'lengthy', 'backandforth', 'visit', 'chinese', 'university', 'tap', 'hikvision', 'surveillance', 'camera', 'company', 'build', 'smart', 'campus', 'project', 'last', 'year', 'include', 'alert', 'system', 'flag', 'ethnic', 'minority', 'student', 'choose', 'fast', 'ipvm', 'build', 'harmony', 'operating', 'system', 'replace', 'android', 'system', 'chinese', 'tech', 'company', 'quickly', 'recruit', 'developer', 'understand', 'harmonyos', 'morning', 'post', 'nepali', 'government', 'decide', 'ban', 'tiktok', 'claim', 'app', 'spread', 'discord', 'disturb', 'social', 'harmony', 'time', 'hobbyist', 'drone', 'heavily', 'use', 'war', 'israeli', 'military', 'stockpile', 'use', 'war', 'new', 'bipartisan', 'bill', 'seek', 'block', 'government', 'business', 'tether', 'issuer', 'major', 'stablecoin', 'call', 'tether', 'parent', 'company', 'base', 'coindesk', 'baidu', 'order', 'ai', 'chip', 'alternative', 'chip', 'use', 'server', 'show', 'chinese', 'company', 'cope', 'bottleneck', 'compute', 'power', 'reuter', 'founder', 'major', 'chinese', 'gamestreaming', 'website', 'apparently', 'hold', 'police', 'relate', 'investigation', 'porn', 'gambling', 'content', 'platform', 'financial', 'time', 'year', 'many', 'chinese', 'smartphone', 'user', 'find', 'pester', 'new', 'form', 'advertising', 'shake', 'phone', 'intentionally', 'ecommerce', 'automatically', 'open', 'turn', 'late', 'marketing', 'trick', 'exploit', 'gyroscope', 'function', 'modernday', 'phone', 'detect', 'movement', 'calculate', 'step', 'find', 'direction', 'late', 'last', 'year', 'chinese', 'tech', 'company', 'come', 'industry', 'standard', 'technical', 'specification', 'open', 'new', 'app', 'movement', 'detect', 'quickly', 'start', 'deploy', 'spam', 'user', 'ad', 'draw', 'widespread', 'complaint', 'accord', 'chinese', 'publication', 'time', 'finance', 'apple', 'become', 'aware', 'phenomenon', 'instruct', 'several', 'top', 'chinese', 'app', 'ban', 'practice', 'start', 'month', 'hard', 'say', 'enough', 'prevent', 'app', 'follow', 'suit', 'traveler', 'order', 'food', 'delivery', 'hotel', 'room', 'never', 'get', 'eventually', 'receive', 'call', 'ask', 'help', 'robot', 'carry', 'food', 'trip', 'get', 'maybe', 'robot', 'eventually', 'rule', 'world', 'fortunately', 'today', 'iâ€', 'think', 'lot', 'guy', 'order', 'delivery', 'room', 'hotel', 'call', 'concierge', 'say', 'already', 'send', 'eventually', 'get', 'call', 'delivery', 'robot', 'ask', 'robotwar']","<p>A self-made chip puts Huawei back in the smartphone game, but US sanctions are still hurting the company.</p>
"
The grassroots push to digitize India’s most precious documents,https://www.technologyreview.com/2023/10/25/1081572/india-digitize-rare-documents-internet-archive-library-servants-of-knowledge/,2023-10-25,"On a bright sunny day in August, in a second-floor room at the Gandhi Bhavan Museum in Bengaluru, workers sit in front of five giant tabletop scanners, lining up books and flipping pages with foot pedals. The museum building houses the largest reference library for Gandhian philosophy in the state of Karnataka, and over the next year, the large assortment of books—including the collected works of Mahatma Gandhi, a translation of his autobiography, Experiments with Truth, into the Kannada language, and other rare items—will be digitized and their metadata recorded before they join the Servants of Knowledge (SoK) collection on the Internet Archive.  This digitization push is just the latest for the SoK, which was established about four years ago with a volunteer effort to preserve hard-to-find resources. It has since expanded to include partnerships with various libraries and archives throughout India. Today, the SoK collection is a searchable library of books, speeches, magazines, newspapers, palm leaf manuscripts, audio, and film from and about India in over 15 languages. The collection is a truly open digital library containing public-domain and out-of-copyright works on science, literature, law, politics, history, religion, music, and folklore, among many other topics. All content is open access, searchable, downloadable, and accessible to visually challenged people using text-to-speech tools. Volunteers and staff continue to expand the collection, scanning about 1.4 million pages per month in various locations across Bengaluru, and more collaborations are in the works. The collection is an effort to make up for the scarcity of library resources in India. There are about 50,000 public-funded libraries in this country of over 1.4 billion people, according to the Raja Rammohun Roy Library Foundation, a group established by the Indian government to promote the public-library movement there. Village and tribal libraries may contain just a few thousand books, compared with a median 77,000 books in each state’s central library and 24,000 in every district library, according to a 2018 report by the foundation. Some libraries have lost their collections to fire. A number of books have been ruined by neglect. Others have gone missing. Some people are finding their accounts permanently blocked Moreover, most public libraries aren’t freely accessible to the public. “Getting access to many of our public libraries is so difficult, and after a point people will give up asking for access. That’s the case in many of our public-funded educational institutes too,” says Arul George Scaria, an associate professor at the National Law School of India University Bengaluru, who studies intellectual-property law. One of the best ways to liberate access to these libraries, he says, is through digitization. Technologist Omshivaprakash H L felt the acute lack of such resources when he needed references for writing Wikipedia articles in Kannada, a southwestern Indian language. Around 2019, he heard that Carl Malamud, who runs Public Resource, a registered US charity, was already archiving books like Gandhi’s Hind Swaraj collection on Indian self-rule and works of the Indian government in the public domain. “I also knew that he used to buy a lot of these books from secondhand bookstores and take them to the US to get them digitized,” says Omshivaprakash.  Public Resource had been working with the Indian Academy of Sciences, Bengaluru, to digitize its books using a scanner provided by the Internet Archive, but the efforts had tapered off. Omshivaprakash proposed engaging community members to help. During the weekends, these volunteers began scanning some of the books Omshivaprakash had and that Malamud had bought. “Carl really understood the idea of community collaboration, the idea of local language technology that we needed, and the kind of impact we were creating,” Omshivaprakash says. The scanners use a V-shaped cradle to hold the books and two DSLR cameras to capture the pages in high resolution. The device is based on the Internet Archive’s scanner but was reengineered by Omshivaprakash and manufactured in India at a lower cost. Each worker can scan about 800 pages an hour.  The more crucial parts of the operation happen after the scan: volunteers make sure to apply accurate metadata to make the scans findable on the Internet Archive, and optical character recognition, which has been fine-tuned to work better for a range of Indian language scripts, makes the text searchable and accessible through text-to-speech programs. Public Resource funds the SoK project, and Omshivaprakash manages the operation, with the help of staff and volunteers. Collaborators have come through social media and word of mouth. For instance, a community member and Kannada teacher named Chaya Acharya approached Omshivaprakash with newspaper clippings of work by her grandfather, the renowned journalist and writer Pavem Acharya, who wrote articles on science and social issues as well as satirical essays. Unexpectedly, she found more articles by her grandfather in the existing Servants of Knowledge collection. “Simply by searching his name, I got many more articles from the archive,” she says. She began collecting copies of Kasturi, a prominent Kannada monthly magazine that Pavem Acharya had edited from 1952 to early 1975, and gave them to Omshivaprakash for digitizing. The old issues of the magazine contain rare writings and translations by popular Kannada authors, such as Indirabai by Gulavadi Venkata Rao, regarded as the first modern novel in Kannada, and a Kannada translation of Edgar Allan Poe’s famous short story “The Gold-Bug.” This is all part of a vision of a public library on the internet as “a bottom-up, grassroots thing,” Malamud says. “It’s a bunch of people teaching each other. We just want to keep scanning and making [these materials] available to people. It’s not a grand goal or single aim.  “It’s what we do for a living,” he says. “We have done it for years, and we are gonna keep doing it for years.” Ananya is a freelance science and technology journalist based in Bengaluru, India. ","On a bright sunny day in August , in a second-floor room at the Gandhi Bhavan Museum in Bengaluru , workers sit in front of five giant tabletop scanners , lining up books and flipping pages with foot pedals . The museum building houses the largest reference library for Gandhian philosophy in the state of Karnataka , and over the next year , the large assortment of books—including the collected works of Mahatma Gandhi , a translation of his autobiography , Experiments with Truth , into the Kannada language , and other rare items—will be digitized and their metadata recorded before they join the Servants of Knowledge ( SoK ) collection on the Internet Archive . This digitization push is just the latest for the SoK , which was established about four years ago with a volunteer effort to preserve hard-to-find resources . It has since expanded to include partnerships with various libraries and archives throughout India . Today , the SoK collection is a searchable library of books , speeches , magazines , newspapers , palm leaf manuscripts , audio , and film from and about India in over 15 languages . The collection is a truly open digital library containing public-domain and out-of-copyright works on science , literature , law , politics , history , religion , music , and folklore , among many other topics . All content is open access , searchable , downloadable , and accessible to visually challenged people using text-to-speech tools . Volunteers and staff continue to expand the collection , scanning about 1.4 million pages per month in various locations across Bengaluru , and more collaborations are in the works . The collection is an effort to make up for the scarcity of library resources in India . There are about 50,000 public-funded libraries in this country of over 1.4 billion people , according to the Raja Rammohun Roy Library Foundation , a group established by the Indian government to promote the public-library movement there . Village and tribal libraries may contain just a few thousand books , compared with a median 77,000 books in each state ’ s central library and 24,000 in every district library , according to a 2018 report by the foundation . Some libraries have lost their collections to fire . A number of books have been ruined by neglect . Others have gone missing . Some people are finding their accounts permanently blocked Moreover , most public libraries aren ’ t freely accessible to the public . “ Getting access to many of our public libraries is so difficult , and after a point people will give up asking for access . That ’ s the case in many of our public-funded educational institutes too , ” says Arul George Scaria , an associate professor at the National Law School of India University Bengaluru , who studies intellectual-property law . One of the best ways to liberate access to these libraries , he says , is through digitization . Technologist Omshivaprakash H L felt the acute lack of such resources when he needed references for writing Wikipedia articles in Kannada , a southwestern Indian language . Around 2019 , he heard that Carl Malamud , who runs Public Resource , a registered US charity , was already archiving books like Gandhi ’ s Hind Swaraj collection on Indian self-rule and works of the Indian government in the public domain . “ I also knew that he used to buy a lot of these books from secondhand bookstores and take them to the US to get them digitized , ” says Omshivaprakash . Public Resource had been working with the Indian Academy of Sciences , Bengaluru , to digitize its books using a scanner provided by the Internet Archive , but the efforts had tapered off . Omshivaprakash proposed engaging community members to help . During the weekends , these volunteers began scanning some of the books Omshivaprakash had and that Malamud had bought . “ Carl really understood the idea of community collaboration , the idea of local language technology that we needed , and the kind of impact we were creating , ” Omshivaprakash says . The scanners use a V-shaped cradle to hold the books and two DSLR cameras to capture the pages in high resolution . The device is based on the Internet Archive ’ s scanner but was reengineered by Omshivaprakash and manufactured in India at a lower cost . Each worker can scan about 800 pages an hour . The more crucial parts of the operation happen after the scan : volunteers make sure to apply accurate metadata to make the scans findable on the Internet Archive , and optical character recognition , which has been fine-tuned to work better for a range of Indian language scripts , makes the text searchable and accessible through text-to-speech programs . Public Resource funds the SoK project , and Omshivaprakash manages the operation , with the help of staff and volunteers . Collaborators have come through social media and word of mouth . For instance , a community member and Kannada teacher named Chaya Acharya approached Omshivaprakash with newspaper clippings of work by her grandfather , the renowned journalist and writer Pavem Acharya , who wrote articles on science and social issues as well as satirical essays . Unexpectedly , she found more articles by her grandfather in the existing Servants of Knowledge collection . “ Simply by searching his name , I got many more articles from the archive , ” she says . She began collecting copies of Kasturi , a prominent Kannada monthly magazine that Pavem Acharya had edited from 1952 to early 1975 , and gave them to Omshivaprakash for digitizing . The old issues of the magazine contain rare writings and translations by popular Kannada authors , such as Indirabai by Gulavadi Venkata Rao , regarded as the first modern novel in Kannada , and a Kannada translation of Edgar Allan Poe ’ s famous short story “ The Gold-Bug. ” This is all part of a vision of a public library on the internet as “ a bottom-up , grassroots thing , ” Malamud says . “ It ’ s a bunch of people teaching each other . We just want to keep scanning and making [ these materials ] available to people . It ’ s not a grand goal or single aim . “ It ’ s what we do for a living , ” he says . “ We have done it for years , and we are gon na keep doing it for years. ” Ananya is a freelance science and technology journalist based in Bengaluru , India .","['bright', 'sunny', 'day', 'secondfloor', 'room', 'worker', 'sit', 'front', 'giant', 'tabletop', 'scanner', 'line', 'book', 'flip', 'page', 'foot', 'pedal', 'museum', 'building', 'house', 'large', 'reference', 'library', 'gandhian', 'philosophy', 'state', 'next', 'year', 'large', 'assortment', 'book', 'include', 'collect', 'work', 'translation', 'autobiography', 'experiment', 'truth', 'language', 'rare', 'item', 'digitize', 'metadata', 'record', 'join', 'servant', 'knowledge', 'collection', 'internet', 'archive', 'digitization', 'push', 'late', 'sok', 'establish', 'year', 'ago', 'volunteer', 'effort', 'preserve', 'hardtofind', 'resource', 'expand', 'include', 'partnership', 'various', 'library', 'archive', 'today', 'collection', 'searchable', 'library', 'book', 'speech', 'magazine', 'newspaper', 'palm', 'leaf', 'manuscript', 'audio', 'film', 'language', 'collection', 'truly', 'open', 'digital', 'library', 'contain', 'publicdomain', 'outofcopyright', 'work', 'science', 'literature', 'law', 'politic', 'history', 'religion', 'music', 'folklore', 'many', 'topic', 'content', 'open', 'access', 'searchable', 'downloadable', 'accessible', 'visually', 'challenge', 'people', 'use', 'texttospeech', 'tool', 'volunteer', 'staff', 'continue', 'expand', 'collection', 'scan', 'page', 'month', 'various', 'location', 'bengaluru', 'collaboration', 'work', 'collection', 'effort', 'make', 'scarcity', 'library', 'resource', 'publicfunde', 'library', 'country', 'people', 'accord', 'raja', 'group', 'establish', 'indian', 'government', 'promote', 'publiclibrary', 'movement', 'village', 'tribal', 'library', 'contain', 'book', 'compare', 'median', 'book', 'state', 'central', 'library', 'district', 'library', 'accord', 'report', 'foundation', 'library', 'lose', 'collection', 'fire', 'number', 'book', 'ruin', 'neglect', 'go', 'miss', 'people', 'find', 'account', 'permanently', 'block', 'moreover', 'public', 'library', 'freely', 'accessible', 'public', 'get', 'access', 'many', 'public', 'library', 'difficult', 'point', 'people', 'give', 'ask', 'access', 'case', 'many', 'publicfunde', 'educational', 'institute', 'say', 'associate', 'professor', 'national', 'law', 'school', 'study', 'law', 'good', 'way', 'liberate', 'access', 'library', 'say', 'digitization', 'technologist', 'omshivaprakash', 'l', 'feel', 'acute', 'lack', 'resource', 'need', 'reference', 'write', 'wikipedia', 'article', 'southwestern', 'indian', 'language', 'hear', 'run', 'public', 'resource', 'register', 'charity', 'already', 'archive', 'book', 'hind', 'swaraj', 'collection', 'indian', 'selfrule', 'work', 'indian', 'government', 'public', 'domain', 'also', 'know', 'use', 'buy', 'lot', 'book', 'secondhand', 'bookstore', 'take', 'get', 'digitize', 'say', 'omshivaprakash', 'public', 'resource', 'work', 'bengaluru', 'digitize', 'book', 'use', 'scanner', 'provide', 'internet', 'archive', 'effort', 'taper', 'omshivaprakash', 'propose', 'engage', 'community', 'member', 'help', 'weekend', 'volunteer', 'begin', 'scan', 'book', 'omshivaprakash', 'malamud', 'buy', 'really', 'understand', 'idea', 'community', 'collaboration', 'idea', 'local', 'language', 'technology', 'need', 'kind', 'impact', 'create', 'omshivaprakash', 'say', 'scanner', 'use', 'vshape', 'cradle', 'hold', 'book', 'dslr', 'camera', 'capture', 'page', 'high', 'resolution', 'device', 'base', 'internet', 'archive', 'scanner', 'reengineere', 'omshivaprakash', 'manufacture', 'low', 'cost', 'worker', 'scan', 'page', 'hour', 'crucial', 'part', 'operation', 'happen', 'scan', 'volunteer', 'make', 'sure', 'apply', 'accurate', 'make', 'scan', 'findable', 'internet', 'archive', 'optical', 'character', 'recognition', 'finetune', 'work', 'well', 'range', 'indian', 'language', 'script', 'make', 'text', 'searchable', 'accessible', 'program', 'public', 'resource', 'fund', 'sok', 'project', 'omshivaprakash', 'manage', 'operation', 'help', 'staff', 'volunteer', 'collaborator', 'come', 'social', 'medium', 'word', 'mouth', 'instance', 'community', 'member', 'teacher', 'name', 'acharya', 'approach', 'omshivaprakash', 'newspaper', 'clipping', 'work', 'grandfather', 'renowned', 'journalist', 'writer', 'acharya', 'write', 'article', 'science', 'social', 'issue', 'well', 'satirical', 'essay', 'unexpectedly', 'find', 'article', 'grandfather', 'exist', 'servant', 'knowledge', 'collection', 'simply', 'search', 'name', 'get', 'many', 'article', 'archive', 'say', 'begin', 'collect', 'copy', 'prominent', 'monthly', 'magazine', 'pavem', 'acharya', 'edit', 'early', 'give', 'omshivaprakash', 'digitize', 'old', 'issue', 'magazine', 'contain', 'rare', 'writing', 'translation', 'popular', 'author', 'indirabai', 'regard', 'first', 'modern', 'novel', 'translation', 'famous', 'short', 'story', 'part', 'vision', 'public', 'library', 'internet', 'bottomup', 'grassroots', 'thing', 'malamud', 'say', 'bunch', 'people', 'teach', 'want', 'keep', 'scan', 'make', 'material', 'available', 'people', 'grand', 'goal', 'single', 'aim', 'living', 'say', 'year', 'go', 'keep', 'year', 'freelance', 'science', 'technology', 'journalist', 'base']","<p>The Servants of Knowledge collection on the Internet Archive is an effort to make up for the lack of library resources in India.</p>
"
How this Turing Award–winning researcher became a legendary academic advisor,https://www.technologyreview.com/2023/10/24/1081478/manuel-blum-theoretical-computer-science-turing-award-academic-advisor/,2023-10-24,"Every academic field has its superstars. But a rare few achieve superstardom not just by demonstrating individual excellence but also by consistently producing future superstars. A notable example of such a legendary doctoral advisor is the Princeton physicist John Archibald Wheeler. A dissertation was once written about his mentorship, and he advised Richard Feynman, Kip Thorne, Hugh Everett (who proposed the “many worlds” theory of quantum mechanics), and a host of others who could collectively staff a top-tier physics department. In ecology, there is Bob Paine, who discovered that certain “keystone species” have an outsize impact on the environment and started a lineage of influential ecologists. And in journalism, there is John McPhee, who has taught generations of accomplished journalists at Princeton since 1975.  Computer science has its own such figure: Manuel Blum, who won the 1995 Turing Award—the Nobel Prize of computer science. Blum’s métier is theoretical computer science, a field that often escapes the general public’s radar. But you certainly have come across one of Blum’s creations: the “Completely Automated Public Turing test to tell Computers and Humans Apart,” better known as thecaptcha—a test designed to distinguish humans from bots online. Scientists hope to leverage mRNA for a bevy of vaccines and therapeutics.  “I don’t know what his secret has been. But he has been a tremendously successful advisor,” says Michael Sipser, a theoretical computer scientist at MIT who was advised by Blum, referring to the “extraordinary number of PhD students” who have worked with him and then gone on to make an impact in the field. “It is extraordinary in the literal sense of that word—outside the ordinary.” Three of Blum’s students have also won Turing Awards; many have received other high honors in theoretical computer science, such as the Gödel Prize and the Knuth Prize; and more than 20 hold professorships at top computer science departments. There are five, for example, at MIT and three at Carnegie Mellon University (where there were four until one left to found Duolingo).  Blum is also distinguished by the great plurality of subfields that his students work in. When Mor Harchol-Balter, a professor of computer science at Carnegie Mellon, arrived at the University of California, Berkeley, as a PhD student, she quickly realized that she wanted to work with him. “Manuel was warm, smiling, and just immediately emanated kindness,” Harchol-Balter told me. Her specialty, queueing theory, had little overlap with Blum’s, but he took her on. “Every professor I know, if you start working on what’s way out of their area, they would tell you to go find somebody else,” she said. “Not Manuel.”   A few months ago, as I was reading about some of the most significant yet counterintuitive ideas in modern theoretical computer science, I realized that the vast majority of the researchers responsible for that work had been advised by Blum. I wondered whether there might be some formula to his success. Of course, it’s presumptuous to think such an intimately human process can be distilled into an algorithm. However, conversations with his students gave me a sense of his approach and revealed consistent themes. Many spoke warmly of him: I often heard some version of “I could talk about Manuel all day” or “Manuel is my favorite topic of conversation.” The finer points of mentorship aside, what I learned was at least proof that kindness can beget greatness.  Manuel Blum is married to Lenore Blum, an accomplished mathematician and computer scientist, who has also been at the forefront of promoting diversity in math and computing (among other things, she founded America’s first computer science department at a women’s college and helped CMU’s computer science department achieve 50-50 gender parity). They are both now emeritus professors at CMU and Manuel Blum is an emeritus professor at UC Berkeley; they split their time between the two coasts.  One day in August, I joined the couple for breakfast at their house in Pittsburgh. Breezy in his manner, Blum, at 85, still has a schoolboy’s smile and frequently erupts into a resonant laugh; he is charismatic in a way typical of people who are utterly oblivious to their charisma. (When he says “WON-derful,” which he frequently does, you can practically hear “WON” in all caps.)  The Blums, who recently celebrated their 62nd anniversary, still shuttlecock research ideas, enthuse over emails from their former students, and complete each other’s memories—some dating from their life in Venezuela, where they met as kids.  Manuel Blum was born in 1938 in Caracas to Jewish parents who had moved from Romania. His first language was German, which his parents spoke at home. But when they moved to the Bronx, his family realized that people did not want to hear German spoken. The year was 1942, and the country was at war. After switching to Spanish at home, he quickly lost his fluency in German. But when he had to learn English for school, he soon forgot Spanish as well. At one point, Blum says, he was listening to both languages but found himself understanding neither. “I remember thinking to myself, ‘Very interesting—I don’t have a language. I couldn’t express myself through language. How was it that I was able to think?’” he told me. In a lucid moment of metacognition—an act that befits a future theorist of abstract concepts—he realized: You don’t need language to think. “He is completely original and goes off and does what he thinks is interesting and important. And often it turns out to be something really significant.” Likely because of his language difficulties, Blum’s second-grade teacher warned his mother that while he might manage to complete high school, he might not go to college. “But I wanted to be smarter. So I asked my father, ‘What can I do to get smarter?’” His father answered that if he understood how the brain works, he could be smart. The conversation marked the inception of Blum’s interest in studying consciousness (something he and Lenore Blum now research full-time, often assisted by their son, the computer scientist Avrim Blum).  Blum was ultimately accepted to MIT, but he struggled the first year, until a friend noticed that his approach to studying physics—owing to Blum’s training at a military academy he went to before college—was heavy on memorization. Blum recalls his friend saying, “You don’t memorize. You memorize only ‘F = ma’ and a few things like that. When you need a formula, you derive it.” Soon, his grades started climbing. “I went from being a Xerox machine to being a thinker. I really enjoyed thinking,” he says. To pursue his interest in the brain, Blum took a course that involved reading multiple volumes of the standard edition of Freud’s works. But they didn’t offer much in the way of satisfactory answers. Then his professor told him that he should introduce himself to Warren S. McCulloch, known for very early research on neural networks and pioneering work in cybernetics. Blum read some of McCulloch’s papers and was able to prove a couple of theorems in mathematical biophysics, and McCulloch took him on in his MIT lab. “A wonderful person. A magnanimous person. Anything I wanted to do, he was supportive,” Blum says.  McCulloch’s lab focused on both the rigorous mathematical work of modeling the neuron and the experimental process of studying the brain to understand how it functions. But what Blum couldn’t study in the lab was consciousness. The topic was taboo at the time. Many felt that subjective mental phenomena weren’t fit for scientific inquiry, and there were few tools available in any case. (The fMRI, for example, which is an imaging technique that maps brain activity, wouldn’t be developed until 1990.)  Blum would revisit the topic occasionally as he transitioned away from electrical engineering to mathematics and computer science in graduate school. As he pursued his graduate work at MIT, he became captivated by a branch of theoretical computer science known as recursive function theory—now more commonly referred to as computability theory—and began searching for a thesis advisor. Soon, he found Marvin Minsky, the mathematician and computer scientist, who was a pioneer of artificial intelligence. Minsky (who had an office full of mechanical hands) often dropped by McCulloch’s lab to demonstrate his new machines and discuss mathematical problems.  After studying computational complexity and computability for his thesis, Blum received his PhD in 1964. At the time, computational complexity theory represented the hinterlands of computer science. It wasn’t until 1971 that Stephen Cook formulated the foundational question of the field, “P vs. NP”—which essentially asks whether every problem whose solution can be checked quickly can also be solved quickly.  But Blum found a productive home in Berkeley’s electrical engineering and computer science department. At MIT, he had helped form the contours of computational complexity theory. At Berkeley, he showed how this highly abstract field could also have useful applications in areas such as cryptography and program checking—a method that uses an algorithm to verify the correctness of a computer program. The kinds of questions Blum poses read like paradoxes and have a somewhat playful quality, making complexity theory and cryptography sound almost like a subgenre of sci-fi. “He is completely original and goes off and does what he thinks is interesting and important. And often it turns out to be something really significant,” Sipser told me.  In his seminal paper “Coin Flipping by Telephone,” the question that he poses is: “Alice and Bob want to flip a coin by telephone. (They have just divorced, live in different cities, and want to decide who gets the car.)” Let’s say that Alice calls “heads” and Bob says she lost; how does she trust that he is being truthful? And how could Bob trust Alice if the situation were reversed? What sounds like a riddle addresses a fundamental problem in cryptography: How can two parties engage in trustworthy exchanges over a communication channel in such a way that neither party can cheat?  Blum showed that this can be achieved using the concept of “commitment.” In a simplified analogy, the idea is that Alice gives Bob a locked box with her prediction inside, but without the key. This prevents Alice from altering her prediction and stops Bob from discovering Alice’s guess prematurely. Once Bob tosses the coin, Alice hands over the key to open the box.  When you ask Blum about the secrets of good mentorship, he reacts with a sheepish head scratch, attributing his students’ success to their own talents. “Students come up with wonderful ideas, and people don’t realize how wonderful they are. The only thing I can say is that, more than most, I really enjoy the ideas that the students have,” he told me. “I have learned from each of them.”  His response left me puzzled, especially after I heard from his students that Blum never criticized their ideas or prescribed research directions. Offering full autonomy and boundless encouragement sounded wonderful in theory, but I was mystified as to how it worked in practice—how did students receive the occasional course correction or hyper-specific advice that is often essential in academic pursuits? Still, it’s not that he was dodging my question. He is not so much a magician who refuses to give away his tricks as one who is himself astonished by what has been conjured around him. One thing I came to understand about Blum’s advising style is that when he says “Students are here to teach me,” he truly means it, with all that entails. While it’s easy to pay lip service to the principle of “treating a student as a colleague,” Ryan Williams, a professor of computer science at MIT who studied with Blum, told me that working together made him really feel like one. What this means, in concrete terms, is that Blum imparted to his students a sense of pedagogical responsibility: he was really expecting to learn from them at every weekly meeting, which in turn meant they had to understand their ideas to the bone.  “During my first few months of working with him, I thought he was testing me. And then I realized that was just him,” Russell Impagliazzo, a professor of computer science at the University of California, San Diego, told me. “You had to learn how to say things so that Manuel could understand them. And that’s the most valuable skill that he gives his students, like the skill of learning to swim by being thrown into a pool: the ability to translate what you’re saying into more concrete terms. This skill proves invaluable when you are teaching a class or writing a grant proposal.” Former students describe Blum as unwaveringly positive, saying he had other ways besides criticism to steer them away from dead ends. “He is always smiling, but you can see he smiles wider when he likes something. And oh, we wanted that big smile,” says Ronitt Rubinfeld, a professor of electrical engineering and computer science at MIT. What would it be like to have someone like Blum in your corner? What kinds of audacious ideas can take root when someone listens to you with absolutely no judgment? Behind the general positivity, Rubinfeld says, is a fine taste for interesting ideas. Students could trust they were being guided in the right direction. Come up with a boring idea? Blum, who is known for his terrible memory, would have mostly forgotten it by your next meeting.  When Harchol-Balter was in graduate school, she says, Blum never told her what to work on and instead guided her by means of questions: “Manuel is fantastic at asking questions. Manuel excels at asking questions.” Blum also “really makes sure that each student has a special area to develop,” Lenore Blum told me. “I don’t think he’s asked a student to ever do the next iteration of someone else’s work,” she said. “But he’ll say, ‘Work with me, and we’ll do something brand new.’” Working on a new idea is risky. But Blum’s encouragement, coupled with his track record of spotting fruitful lines of inquiry, gave his students confidence to keep going in bold directions while enduring criticism and self-doubt. “There’s a huge difference [between] Manuel’s advising style and everyone else’s in the world,” says Impagliazzo. “Manuel’s advising style is simply to listen to you and make you seem really, really important. Like what you’re doing is the most amazing thing in the world.”  Harchol-Balter says this is the magic she is now trying to emulate with her students. “Whenever I had an idea, whatever it was, he somehow made me feel like this was the most brilliant idea that had ever been invented,” she remembers. She felt that every idea could be “a multimillion-dollar breakthrough,” which allowed her to stay committed to her line of research, undeterred by external influences or trends. “He creates this feeling of supreme confidence—not just confidence, but like, ‘You. Are. Brilliant,’” she adds. “Having somebody beside you all those six years, when you’re feeling the most vulnerable, constantly boosting your confidence … It’s amazing. And that’s why his students are so great.” Astronomy is leading the way in making science more accessible through sonification—and the results sound amazing. Excellence in academia, as in many other fields, is about both what you do and how you do it. You need to identify a promising topic and have the technical ability to execute it. A technically flawless idea without original insight can be trivial; a radically original idea without proper execution might never fully develop, while a bold idea powered by misplaced confidence could hit a dead end.  The psychological reassurance students get from Blum may come in part from his superhuman level of aplomb. “He never seems stressed out,” says his son, Avrim Blum. “In the real world, there are deadlines and stresses, but he never showed any of that. At least I never saw it.” I’m still awed by his ability to mask inner turbulence—something that affects everyone—so well that it remains invisible even to his closest observers, including his own son. It’s a source of stability that students can rely on throughout their graduate studies. “I was more comfortable and more relaxed in grad school because I felt like he had things under control for me,” Williams told me. “If there were any difficulties, he would help. He had my back. He was going to sort things out.”  Speaking with Blum’s students, I felt a pang of jealousy. What would it be like to have someone like Blum in your corner during your most vulnerable moments? And how many direct criticisms you’ve faced could have been reformulated into questions? What kinds of audacious ideas can take root when someone listens to you with absolutely no judgment?  But even as Blum’s students claim they are still bewildered by the “magic” and “mystery” of their advisor’s approach, they have become accomplished teachers and advisors in their own right. Umesh Vazirani, a theoretical computer scientist at Berkeley, told me that he has thought a lot about Blum’s secrets. He said the essence can be expressed this way: “You respect every student, and you let them develop into whatever they want to be.” Vazirani, who has advised a number of superstars in the field himself, believes that in education, “the most important thing is not to break anything. Cause no damage.” The potency of the Blumian approach to advising isn’t domain specific, as illustrated by George Saunders’s reflections on his writing teacher, Tobias Wolff. Writing teachers have “so much power,” Saunders has written: They could mock us, disregard us, use us to prop themselves up. But ourteachers, if they are good, instead do something almost holy, which we neverforget: they take us seriously. They accept us as new members of the guild.They tolerate the under-wonderful stories we write, the dopy things we say, ourshaky-legged aesthetic theories, our posturing, because they have been therethemselves.  We say: I think I might be a writer.  They say: Good for you. Proceed.   On my last day in Pittsburgh, I noticed a photo of Blum’s old advisor, Warren S. McCulloch, behind Blum’s desk in his home office. It was in a prominent place where someone else might’ve chosen to display a family heirloom or showcase an autographed photo of himself shaking a president’s hand. (McCulloch died in 1969, only a few years after Blum began his professorship.) Out of curiosity, I pointed out the photo’s prominent position. “Yes, because he is always with me,” Blum replied. “Warren was Manuel’s spiritual father in every way,” added Lenore. As I made my way back to the airport, I remembered a book called Surviving Death, by the philosopher Mark Johnston. In the book, Johnston postulates that a good person could “quite literally” survive death by redirecting self-interest toward the well-being of future people. This forfeiture doesn’t spell the dissolution of the self but, rather, the expansion of it, allowing the person to live on in the “onward rush of humankind.” A line from the book unfolded, with a time-release effect, in my head: “Every time a baby is born, a good person acquires a new face.”  Behind every one of Blum’s knowing smiles, it may well have been McCulloch himself, nodding, imparting a blessing: “Wonderful idea. Proceed.”  Sheon Han is a writer based in Palo Alto, California.","Every academic field has its superstars . But a rare few achieve superstardom not just by demonstrating individual excellence but also by consistently producing future superstars . A notable example of such a legendary doctoral advisor is the Princeton physicist John Archibald Wheeler . A dissertation was once written about his mentorship , and he advised Richard Feynman , Kip Thorne , Hugh Everett ( who proposed the “ many worlds ” theory of quantum mechanics ) , and a host of others who could collectively staff a top-tier physics department . In ecology , there is Bob Paine , who discovered that certain “ keystone species ” have an outsize impact on the environment and started a lineage of influential ecologists . And in journalism , there is John McPhee , who has taught generations of accomplished journalists at Princeton since 1975 . Computer science has its own such figure : Manuel Blum , who won the 1995 Turing Award—the Nobel Prize of computer science . Blum ’ s métier is theoretical computer science , a field that often escapes the general public ’ s radar . But you certainly have come across one of Blum ’ s creations : the “ Completely Automated Public Turing test to tell Computers and Humans Apart , ” better known as thecaptcha—a test designed to distinguish humans from bots online . Scientists hope to leverage mRNA for a bevy of vaccines and therapeutics . “ I don ’ t know what his secret has been . But he has been a tremendously successful advisor , ” says Michael Sipser , a theoretical computer scientist at MIT who was advised by Blum , referring to the “ extraordinary number of PhD students ” who have worked with him and then gone on to make an impact in the field . “ It is extraordinary in the literal sense of that word—outside the ordinary. ” Three of Blum ’ s students have also won Turing Awards ; many have received other high honors in theoretical computer science , such as the Gödel Prize and the Knuth Prize ; and more than 20 hold professorships at top computer science departments . There are five , for example , at MIT and three at Carnegie Mellon University ( where there were four until one left to found Duolingo ) . Blum is also distinguished by the great plurality of subfields that his students work in . When Mor Harchol-Balter , a professor of computer science at Carnegie Mellon , arrived at the University of California , Berkeley , as a PhD student , she quickly realized that she wanted to work with him . “ Manuel was warm , smiling , and just immediately emanated kindness , ” Harchol-Balter told me . Her specialty , queueing theory , had little overlap with Blum ’ s , but he took her on . “ Every professor I know , if you start working on what ’ s way out of their area , they would tell you to go find somebody else , ” she said . “ Not Manuel. ” A few months ago , as I was reading about some of the most significant yet counterintuitive ideas in modern theoretical computer science , I realized that the vast majority of the researchers responsible for that work had been advised by Blum . I wondered whether there might be some formula to his success . Of course , it ’ s presumptuous to think such an intimately human process can be distilled into an algorithm . However , conversations with his students gave me a sense of his approach and revealed consistent themes . Many spoke warmly of him : I often heard some version of “ I could talk about Manuel all day ” or “ Manuel is my favorite topic of conversation. ” The finer points of mentorship aside , what I learned was at least proof that kindness can beget greatness . Manuel Blum is married to Lenore Blum , an accomplished mathematician and computer scientist , who has also been at the forefront of promoting diversity in math and computing ( among other things , she founded America ’ s first computer science department at a women ’ s college and helped CMU ’ s computer science department achieve 50-50 gender parity ) . They are both now emeritus professors at CMU and Manuel Blum is an emeritus professor at UC Berkeley ; they split their time between the two coasts . One day in August , I joined the couple for breakfast at their house in Pittsburgh . Breezy in his manner , Blum , at 85 , still has a schoolboy ’ s smile and frequently erupts into a resonant laugh ; he is charismatic in a way typical of people who are utterly oblivious to their charisma . ( When he says “ WON-derful , ” which he frequently does , you can practically hear “ WON ” in all caps . ) The Blums , who recently celebrated their 62nd anniversary , still shuttlecock research ideas , enthuse over emails from their former students , and complete each other ’ s memories—some dating from their life in Venezuela , where they met as kids . Manuel Blum was born in 1938 in Caracas to Jewish parents who had moved from Romania . His first language was German , which his parents spoke at home . But when they moved to the Bronx , his family realized that people did not want to hear German spoken . The year was 1942 , and the country was at war . After switching to Spanish at home , he quickly lost his fluency in German . But when he had to learn English for school , he soon forgot Spanish as well . At one point , Blum says , he was listening to both languages but found himself understanding neither . “ I remember thinking to myself , ‘ Very interesting—I don ’ t have a language . I couldn ’ t express myself through language . How was it that I was able to think ? ’ ” he told me . In a lucid moment of metacognition—an act that befits a future theorist of abstract concepts—he realized : You don ’ t need language to think . “ He is completely original and goes off and does what he thinks is interesting and important . And often it turns out to be something really significant. ” Likely because of his language difficulties , Blum ’ s second-grade teacher warned his mother that while he might manage to complete high school , he might not go to college . “ But I wanted to be smarter . So I asked my father , ‘ What can I do to get smarter ? ’ ” His father answered that if he understood how the brain works , he could be smart . The conversation marked the inception of Blum ’ s interest in studying consciousness ( something he and Lenore Blum now research full-time , often assisted by their son , the computer scientist Avrim Blum ) . Blum was ultimately accepted to MIT , but he struggled the first year , until a friend noticed that his approach to studying physics—owing to Blum ’ s training at a military academy he went to before college—was heavy on memorization . Blum recalls his friend saying , “ You don ’ t memorize . You memorize only ‘ F = ma ’ and a few things like that . When you need a formula , you derive it. ” Soon , his grades started climbing . “ I went from being a Xerox machine to being a thinker . I really enjoyed thinking , ” he says . To pursue his interest in the brain , Blum took a course that involved reading multiple volumes of the standard edition of Freud ’ s works . But they didn ’ t offer much in the way of satisfactory answers . Then his professor told him that he should introduce himself to Warren S. McCulloch , known for very early research on neural networks and pioneering work in cybernetics . Blum read some of McCulloch ’ s papers and was able to prove a couple of theorems in mathematical biophysics , and McCulloch took him on in his MIT lab . “ A wonderful person . A magnanimous person . Anything I wanted to do , he was supportive , ” Blum says . McCulloch ’ s lab focused on both the rigorous mathematical work of modeling the neuron and the experimental process of studying the brain to understand how it functions . But what Blum couldn ’ t study in the lab was consciousness . The topic was taboo at the time . Many felt that subjective mental phenomena weren ’ t fit for scientific inquiry , and there were few tools available in any case . ( The fMRI , for example , which is an imaging technique that maps brain activity , wouldn ’ t be developed until 1990 . ) Blum would revisit the topic occasionally as he transitioned away from electrical engineering to mathematics and computer science in graduate school . As he pursued his graduate work at MIT , he became captivated by a branch of theoretical computer science known as recursive function theory—now more commonly referred to as computability theory—and began searching for a thesis advisor . Soon , he found Marvin Minsky , the mathematician and computer scientist , who was a pioneer of artificial intelligence . Minsky ( who had an office full of mechanical hands ) often dropped by McCulloch ’ s lab to demonstrate his new machines and discuss mathematical problems . After studying computational complexity and computability for his thesis , Blum received his PhD in 1964 . At the time , computational complexity theory represented the hinterlands of computer science . It wasn ’ t until 1971 that Stephen Cook formulated the foundational question of the field , “ P vs. NP ” —which essentially asks whether every problem whose solution can be checked quickly can also be solved quickly . But Blum found a productive home in Berkeley ’ s electrical engineering and computer science department . At MIT , he had helped form the contours of computational complexity theory . At Berkeley , he showed how this highly abstract field could also have useful applications in areas such as cryptography and program checking—a method that uses an algorithm to verify the correctness of a computer program . The kinds of questions Blum poses read like paradoxes and have a somewhat playful quality , making complexity theory and cryptography sound almost like a subgenre of sci-fi . “ He is completely original and goes off and does what he thinks is interesting and important . And often it turns out to be something really significant , ” Sipser told me . In his seminal paper “ Coin Flipping by Telephone , ” the question that he poses is : “ Alice and Bob want to flip a coin by telephone . ( They have just divorced , live in different cities , and want to decide who gets the car . ) ” Let ’ s say that Alice calls “ heads ” and Bob says she lost ; how does she trust that he is being truthful ? And how could Bob trust Alice if the situation were reversed ? What sounds like a riddle addresses a fundamental problem in cryptography : How can two parties engage in trustworthy exchanges over a communication channel in such a way that neither party can cheat ? Blum showed that this can be achieved using the concept of “ commitment. ” In a simplified analogy , the idea is that Alice gives Bob a locked box with her prediction inside , but without the key . This prevents Alice from altering her prediction and stops Bob from discovering Alice ’ s guess prematurely . Once Bob tosses the coin , Alice hands over the key to open the box . When you ask Blum about the secrets of good mentorship , he reacts with a sheepish head scratch , attributing his students ’ success to their own talents . “ Students come up with wonderful ideas , and people don ’ t realize how wonderful they are . The only thing I can say is that , more than most , I really enjoy the ideas that the students have , ” he told me . “ I have learned from each of them. ” His response left me puzzled , especially after I heard from his students that Blum never criticized their ideas or prescribed research directions . Offering full autonomy and boundless encouragement sounded wonderful in theory , but I was mystified as to how it worked in practice—how did students receive the occasional course correction or hyper-specific advice that is often essential in academic pursuits ? Still , it ’ s not that he was dodging my question . He is not so much a magician who refuses to give away his tricks as one who is himself astonished by what has been conjured around him . One thing I came to understand about Blum ’ s advising style is that when he says “ Students are here to teach me , ” he truly means it , with all that entails . While it ’ s easy to pay lip service to the principle of “ treating a student as a colleague , ” Ryan Williams , a professor of computer science at MIT who studied with Blum , told me that working together made him really feel like one . What this means , in concrete terms , is that Blum imparted to his students a sense of pedagogical responsibility : he was really expecting to learn from them at every weekly meeting , which in turn meant they had to understand their ideas to the bone . “ During my first few months of working with him , I thought he was testing me . And then I realized that was just him , ” Russell Impagliazzo , a professor of computer science at the University of California , San Diego , told me . “ You had to learn how to say things so that Manuel could understand them . And that ’ s the most valuable skill that he gives his students , like the skill of learning to swim by being thrown into a pool : the ability to translate what you ’ re saying into more concrete terms . This skill proves invaluable when you are teaching a class or writing a grant proposal. ” Former students describe Blum as unwaveringly positive , saying he had other ways besides criticism to steer them away from dead ends . “ He is always smiling , but you can see he smiles wider when he likes something . And oh , we wanted that big smile , ” says Ronitt Rubinfeld , a professor of electrical engineering and computer science at MIT . What would it be like to have someone like Blum in your corner ? What kinds of audacious ideas can take root when someone listens to you with absolutely no judgment ? Behind the general positivity , Rubinfeld says , is a fine taste for interesting ideas . Students could trust they were being guided in the right direction . Come up with a boring idea ? Blum , who is known for his terrible memory , would have mostly forgotten it by your next meeting . When Harchol-Balter was in graduate school , she says , Blum never told her what to work on and instead guided her by means of questions : “ Manuel is fantastic at asking questions . Manuel excels at asking questions. ” Blum also “ really makes sure that each student has a special area to develop , ” Lenore Blum told me . “ I don ’ t think he ’ s asked a student to ever do the next iteration of someone else ’ s work , ” she said . “ But he ’ ll say , ‘ Work with me , and we ’ ll do something brand new. ’ ” Working on a new idea is risky . But Blum ’ s encouragement , coupled with his track record of spotting fruitful lines of inquiry , gave his students confidence to keep going in bold directions while enduring criticism and self-doubt . “ There ’ s a huge difference [ between ] Manuel ’ s advising style and everyone else ’ s in the world , ” says Impagliazzo . “ Manuel ’ s advising style is simply to listen to you and make you seem really , really important . Like what you ’ re doing is the most amazing thing in the world. ” Harchol-Balter says this is the magic she is now trying to emulate with her students . “ Whenever I had an idea , whatever it was , he somehow made me feel like this was the most brilliant idea that had ever been invented , ” she remembers . She felt that every idea could be “ a multimillion-dollar breakthrough , ” which allowed her to stay committed to her line of research , undeterred by external influences or trends . “ He creates this feeling of supreme confidence—not just confidence , but like , ‘ You . Are . Brilliant , ’ ” she adds . “ Having somebody beside you all those six years , when you ’ re feeling the most vulnerable , constantly boosting your confidence … It ’ s amazing . And that ’ s why his students are so great. ” Astronomy is leading the way in making science more accessible through sonification—and the results sound amazing . Excellence in academia , as in many other fields , is about both what you do and how you do it . You need to identify a promising topic and have the technical ability to execute it . A technically flawless idea without original insight can be trivial ; a radically original idea without proper execution might never fully develop , while a bold idea powered by misplaced confidence could hit a dead end . The psychological reassurance students get from Blum may come in part from his superhuman level of aplomb . “ He never seems stressed out , ” says his son , Avrim Blum . “ In the real world , there are deadlines and stresses , but he never showed any of that . At least I never saw it. ” I ’ m still awed by his ability to mask inner turbulence—something that affects everyone—so well that it remains invisible even to his closest observers , including his own son . It ’ s a source of stability that students can rely on throughout their graduate studies . “ I was more comfortable and more relaxed in grad school because I felt like he had things under control for me , ” Williams told me . “ If there were any difficulties , he would help . He had my back . He was going to sort things out. ” Speaking with Blum ’ s students , I felt a pang of jealousy . What would it be like to have someone like Blum in your corner during your most vulnerable moments ? And how many direct criticisms you ’ ve faced could have been reformulated into questions ? What kinds of audacious ideas can take root when someone listens to you with absolutely no judgment ? But even as Blum ’ s students claim they are still bewildered by the “ magic ” and “ mystery ” of their advisor ’ s approach , they have become accomplished teachers and advisors in their own right . Umesh Vazirani , a theoretical computer scientist at Berkeley , told me that he has thought a lot about Blum ’ s secrets . He said the essence can be expressed this way : “ You respect every student , and you let them develop into whatever they want to be. ” Vazirani , who has advised a number of superstars in the field himself , believes that in education , “ the most important thing is not to break anything . Cause no damage. ” The potency of the Blumian approach to advising isn ’ t domain specific , as illustrated by George Saunders ’ s reflections on his writing teacher , Tobias Wolff . Writing teachers have “ so much power , ” Saunders has written : They could mock us , disregard us , use us to prop themselves up . But ourteachers , if they are good , instead do something almost holy , which we neverforget : they take us seriously . They accept us as new members of the guild.They tolerate the under-wonderful stories we write , the dopy things we say , ourshaky-legged aesthetic theories , our posturing , because they have been therethemselves . We say : I think I might be a writer . They say : Good for you . Proceed . On my last day in Pittsburgh , I noticed a photo of Blum ’ s old advisor , Warren S. McCulloch , behind Blum ’ s desk in his home office . It was in a prominent place where someone else might ’ ve chosen to display a family heirloom or showcase an autographed photo of himself shaking a president ’ s hand . ( McCulloch died in 1969 , only a few years after Blum began his professorship . ) Out of curiosity , I pointed out the photo ’ s prominent position . “ Yes , because he is always with me , ” Blum replied . “ Warren was Manuel ’ s spiritual father in every way , ” added Lenore . As I made my way back to the airport , I remembered a book called Surviving Death , by the philosopher Mark Johnston . In the book , Johnston postulates that a good person could “ quite literally ” survive death by redirecting self-interest toward the well-being of future people . This forfeiture doesn ’ t spell the dissolution of the self but , rather , the expansion of it , allowing the person to live on in the “ onward rush of humankind. ” A line from the book unfolded , with a time-release effect , in my head : “ Every time a baby is born , a good person acquires a new face. ” Behind every one of Blum ’ s knowing smiles , it may well have been McCulloch himself , nodding , imparting a blessing : “ Wonderful idea . Proceed. ” Sheon Han is a writer based in Palo Alto , California .","['academic', 'field', 'superstar', 'rare', 'achieve', 'superstardom', 'demonstrate', 'individual', 'excellence', 'also', 'consistently', 'produce', 'future', 'superstar', 'notable', 'example', 'legendary', 'doctoral', 'advisor', 'dissertation', 'write', 'mentorship', 'advise', 'thorne', 'propose', 'many', 'world', 'theory', 'quantum', 'mechanic', 'host', 'collectively', 'staff', 'toptier', 'physics', 'department', 'ecology', 'paine', 'discover', 'certain', 'specie', 'outsize', 'impact', 'environment', 'start', 'lineage', 'influential', 'ecologist', 'journalism', 'teach', 'generation', 'accomplished', 'journalist', 'computer', 'science', 'figure', 'win', 'ture', 'award', 'computer', 'science', 'métier', 'theoretical', 'computer', 'science', 'field', 'often', 'escape', 'general', 'public', 'radar', 'certainly', 'come', 'blum', 'creation', 'completely', 'automate', 'public', 'ture', 'test', 'tell', 'computer', 'human', 'apart', 'well', 'know', 'test', 'design', 'distinguish', 'human', 'bot', 'online', 'scientist', 'hope', 'leverage', 'mrna', 'bevy', 'vaccine', 'therapeutic', 'know', 'secret', 'tremendously', 'successful', 'advisor', 'say', 'theoretical', 'computer', 'scientist', 'advise', 'blum', 'refer', 'extraordinary', 'number', 'phd', 'student', 'work', 'go', 'make', 'impact', 'field', 'extraordinary', 'literal', 'sense', 'word', 'ordinary', 'blum', 'student', 'also', 'win', 'ture', 'award', 'many', 'receive', 'high', 'honor', 'theoretical', 'computer', 'science', 'gödel', 'hold', 'professorship', 'top', 'computer', 'science', 'department', 'example', 'mit', 'leave', 'find', 'duolingo', 'blum', 'also', 'distinguish', 'great', 'plurality', 'subfield', 'student', 'work', 'harcholbalter', 'professor', 'computer', 'science', 'carnegie', 'mellon', 'arrive', 'phd', 'student', 'quickly', 'realize', 'want', 'work', 'manuel', 'warm', 'smile', 'immediately', 'emanated', 'kindness', 'harcholbalter', 'tell', 'specialty', 'queue', 'theory', 'little', 'overlap', 'blum', 'take', 'professor', 'know', 'start', 'work', 'way', 'area', 'tell', 'go', 'find', 'else', 'say', 'manuel', 'month', 'ago', 'read', 'significant', 'counterintuitive', 'idea', 'modern', 'theoretical', 'computer', 'science', 'realize', 'vast', 'majority', 'researcher', 'responsible', 'work', 'advise', 'wonder', 'formula', 'success', 'course', 'presumptuous', 'think', 'intimately', 'human', 'process', 'distil', 'however', 'conversation', 'student', 'give', 'sense', 'approach', 'reveal', 'consistent', 'theme', 'many', 'speak', 'warmly', 'often', 'hear', 'version', 'talk', 'manuel', 'day', 'manuel', 'favorite', 'topic', 'conversation', 'fine', 'point', 'mentorship', 'aside', 'learn', 'least', 'proof', 'kindness', 'beget', 'greatness', 'manuel', 'married', 'lenore', 'blum', 'accomplished', 'mathematician', 'computer', 'scientist', 'also', 'forefront', 'promote', 'diversity', 'math', 'compute', 'thing', 'found', 'first', 'computer', 'woman', 'college', 'help', 'cmu', 'computer', 'department', 'achieve', 'gender', 'parity', 'emeritus', 'professor', 'cmu', 'manuel', 'emeritus', 'professor', 'split', 'time', 'coast', 'day', 'join', 'couple', 'breakfast', 'house', 'breezy', 'manner', 'blum', 'still', 'schoolboy', 'smile', 'frequently', 'erupt', 'resonant', 'laugh', 'charismatic', 'way', 'typical', 'people', 'utterly', 'oblivious', 'charisma', 'say', 'wonderful', 'frequently', 'practically', 'win', 'cap', 'blum', 'recently', 'celebrate', '62nd', 'anniversary', 'still', 'shuttlecock', 'research', 'idea', 'enthuse', 'email', 'former', 'student', 'complete', 'memory', 'date', 'life', 'meet', 'kid', 'bear', 'caraca', 'jewish', 'parent', 'move', 'first', 'language', 'german', 'parent', 'speak', 'home', 'move', 'bronx', 'family', 'realize', 'people', 'want', 'hear', 'german', 'speak', 'year', 'country', 'war', 'switch', 'spanish', 'home', 'quickly', 'lose', 'fluency', 'german', 'learn', 'school', 'soon', 'forget', 'spanish', 'well', 'point', 'blum', 'say', 'listen', 'language', 'find', 'understand', 'remember', 'think', 'interesting', 'language', 'express', 'language', 'able', 'think', 'tell', 'lucid', 'moment', 'metacognition', 'act', 'befit', 'future', 'theorist', 'abstract', 'concept', 'realize', 'need', 'language', 'think', 'completely', 'original', 'go', 'think', 'interesting', 'important', 'often', 'turn', 'really', 'significant', 'likely', 'language', 'difficulty', 'blum', 'secondgrade', 'teacher', 'warn', 'mother', 'manage', 'complete', 'high', 'school', 'go', 'college', 'want', 'smart', 'ask', 'father', 'get', 'smart', 'father', 'answer', 'understand', 'brain', 'work', 'smart', 'conversation', 'mark', 'inception', 'blum', 'interest', 'study', 'consciousness', 'lenore', 'blum', 'research', 'fulltime', 'often', 'assist', 'son', 'computer', 'scientist', 'blum', 'ultimately', 'accept', 'mit', 'struggle', 'first', 'year', 'friend', 'notice', 'approach', 'study', 'physic', 'owe', 'blum', 'training', 'military', 'academy', 'go', 'college', 'heavy', 'memorization', 'recall', 'friend', 'say', 'memorize', 'memorize', 'thing', 'need', 'formula', 'derive', 'soon', 'grade', 'start', 'climb', 'go', 'xerox', 'machine', 'thinker', 'really', 'enjoy', 'think', 'say', 'pursue', 'interest', 'brain', 'blum', 'take', 'course', 'involve', 'read', 'multiple', 'volume', 'freud', 'work', 'offer', 'much', 'way', 'satisfactory', 'answer', 'professor', 'tell', 'introduce', 'know', 'early', 'research', 'neural', 'network', 'pioneer', 'work', 'cybernetic', 'blum', 'read', 'paper', 'able', 'prove', 'couple', 'theorem', 'mathematical', 'biophysic', 'take', 'mit', 'lab', 'wonderful', 'person', 'magnanimous', 'person', 'want', 'supportive', 'blum', 'say', 'lab', 'focus', 'rigorous', 'mathematical', 'work', 'model', 'neuron', 'experimental', 'process', 'study', 'brain', 'understand', 'function', 'study', 'lab', 'consciousness', 'topic', 'taboo', 'time', 'many', 'feel', 'subjective', 'mental', 'phenomena', 'fit', 'scientific', 'inquiry', 'tool', 'available', 'case', 'fmri', 'example', 'imaging', 'technique', 'map', 'brain', 'activity', 'develop', 'blum', 'revisit', 'topic', 'occasionally', 'transition', 'away', 'electrical', 'engineering', 'mathematic', 'computer', 'science', 'graduate', 'school', 'pursue', 'graduate', 'work', 'mit', 'captivate', 'branch', 'theoretical', 'computer', 'science', 'know', 'recursive', 'function', 'theory', 'commonly', 'refer', 'computability', 'theory', 'begin', 'search', 'thesis', 'advisor', 'soon', 'find', 'minsky', 'mathematician', 'computer', 'scientist', 'pioneer', 'artificial', 'intelligence', 'minsky', 'office', 'full', 'mechanical', 'hand', 'often', 'drop', 'lab', 'demonstrate', 'new', 'machine', 'discuss', 'mathematical', 'problem', 'study', 'computational', 'complexity', 'computability', 'thesis', 'blum', 'receive', 'phd', 'time', 'computational', 'complexity', 'theory', 'represent', 'hinterland', 'computer', 'science', 'formulate', 'foundational', 'question', 'field', 'p', 'vs', 'np', 'essentially', 'ask', 'problem', 'solution', 'check', 'quickly', 'also', 'solve', 'quickly', 'find', 'productive', 'home', 'electrical', 'engineering', 'computer', 'help', 'form', 'contour', 'computational', 'complexity', 'theory', 'show', 'highly', 'abstract', 'field', 'also', 'useful', 'application', 'area', 'cryptography', 'program', 'checking', 'method', 'use', 'verify', 'correctness', 'computer', 'program', 'kind', 'question', 'blum', 'pose', 'read', 'paradox', 'somewhat', 'playful', 'quality', 'make', 'complexity', 'theory', 'cryptography', 'sound', 'almost', 'subgenre', 'scifi', 'completely', 'original', 'go', 'think', 'interesting', 'important', 'often', 'turn', 'really', 'significant', 'sipser', 'tell', 'seminal', 'paper', 'coin', 'flipping', 'telephone', 'question', 'pose', 'want', 'flip', 'coin', 'telephone', 'divorce', 'live', 'different', 'city', 'want', 'decide', 'get', 'car', 'let', 'say', 'call', 'head', 'say', 'lose', 'trust', 'truthful', 'alice', 'situation', 'reverse', 'sound', 'riddle', 'address', 'fundamental', 'problem', 'cryptography', 'party', 'engage', 'trustworthy', 'exchange', 'communication', 'channel', 'way', 'party', 'cheat', 'blum', 'show', 'achieve', 'use', 'concept', 'commitment', 'simplified', 'analogy', 'idea', 'give', 'locked', 'box', 'prediction', 'key', 'prevent', 'alice', 'alter', 'prediction', 'stop', 'discover', 'guess', 'prematurely', 'toss', 'coin', 'alice', 'hand', 'key', 'open', 'box', 'ask', 'blum', 'secret', 'good', 'mentorship', 'react', 'sheepish', 'head', 'scratch', 'attribute', 'student', 'success', 'talent', 'student', 'come', 'wonderful', 'idea', 'realize', 'wonderful', 'thing', 'say', 'really', 'enjoy', 'idea', 'student', 'tell', 'learn', 'response', 'leave', 'puzzle', 'especially', 'hear', 'student', 'blum', 'never', 'criticize', 'idea', 'prescribe', 'research', 'direction', 'offer', 'full', 'autonomy', 'boundless', 'encouragement', 'sound', 'wonderful', 'theory', 'mystify', 'work', 'practice', 'student', 'receive', 'occasional', 'course', 'correction', 'hyperspecific', 'advice', 'often', 'essential', 'academic', 'pursuit', 'still', 'dodge', 'question', 'much', 'magician', 'refuse', 'give', 'trick', 'astonish', 'conjure', 'thing', 'come', 'understand', 'advise', 'style', 'say', 'student', 'teach', 'truly', 'mean', 'entail', 'easy', 'pay', 'lip', 'service', 'principle', 'treat', 'student', 'colleague', 'professor', 'computer', 'science', 'mit', 'study', 'blum', 'tell', 'work', 'together', 'make', 'really', 'feel', 'mean', 'concrete', 'term', 'blum', 'impart', 'student', 'sense', 'pedagogical', 'responsibility', 'really', 'expect', 'learn', 'weekly', 'meeting', 'turn', 'mean', 'understand', 'idea', 'bone', 'first', 'month', 'work', 'think', 'test', 'realize', 'professor', 'computer', 'science', 'tell', 'learn', 'say', 'thing', 'manuel', 'understand', 'valuable', 'skill', 'give', 'student', 'skill', 'learn', 'swim', 'throw', 'pool', 'ability', 'translate', 'say', 'concrete', 'term', 'skill', 'prove', 'invaluable', 'teach', 'class', 'write', 'grant', 'proposal', 'former', 'student', 'describe', 'blum', 'unwaveringly', 'positive', 'say', 'way', 'criticism', 'steer', 'away', 'dead', 'end', 'always', 'smile', 'see', 'smile', 'wide', 'like', 'want', 'big', 'smile', 'say', 'ronitt', 'rubinfeld', 'professor', 'electrical', 'engineering', 'computer', 'science', 'mit', 'blum', 'corner', 'kind', 'audacious', 'idea', 'take', 'root', 'listen', 'absolutely', 'judgment', 'general', 'positivity', 'rubinfeld', 'say', 'fine', 'taste', 'interesting', 'idea', 'student', 'trust', 'guide', 'right', 'direction', 'come', 'boring', 'idea', 'blum', 'know', 'terrible', 'memory', 'mostly', 'forget', 'next', 'meeting', 'harcholbalter', 'graduate', 'school', 'say', 'never', 'tell', 'work', 'instead', 'guide', 'mean', 'question', 'manuel', 'fantastic', 'ask', 'question', 'manuel', 'excel', 'ask', 'question', 'blum', 'also', 'really', 'make', 'sure', 'student', 'special', 'area', 'develop', 'lenore', 'blum', 'tell', 'think', 'ask', 'student', 'ever', 'next', 'iteration', 'else', 'work', 'say', 'say', 'work', 'brand', 'new', 'work', 'new', 'idea', 'risky', 'encouragement', 'couple', 'track', 'record', 'spot', 'fruitful', 'line', 'inquiry', 'give', 'student', 'confidence', 'keep', 'go', 'bold', 'direction', 'endure', 'criticism', 'selfdoubt', 'huge', 'difference', 'manuel', 'advise', 'style', 'else', 'world', 'say', 'advise', 'style', 'simply', 'listen', 'make', 'seem', 'really', 'really', 'important', 'amazing', 'thing', 'world', 'harcholbalter', 'say', 'magic', 'try', 'emulate', 'student', 'idea', 'somehow', 'make', 'feel', 'brilliant', 'idea', 'ever', 'invent', 'remember', 'feel', 'idea', 'multimilliondollar', 'breakthrough', 'allow', 'stay', 'committed', 'line', 'research', 'undeterred', 'external', 'influence', 'trend', 'create', 'feeling', 'supreme', 'confidence', 'confidence', 'brilliant', 'add', 'year', 'feel', 'vulnerable', 'constantly', 'boost', 'confidence', 'amazing', 'student', 'great', 'astronomy', 'lead', 'way', 'make', 'science', 'accessible', 'sonification', 'result', 'sound', 'amazing', 'excellence', 'academia', 'many', 'field', 'need', 'identify', 'promising', 'topic', 'technical', 'ability', 'execute', 'technically', 'flawless', 'idea', 'original', 'insight', 'trivial', 'radically', 'original', 'idea', 'proper', 'execution', 'never', 'fully', 'develop', 'bold', 'idea', 'power', 'misplaced', 'confidence', 'hit', 'dead', 'end', 'psychological', 'reassurance', 'student', 'get', 'blum', 'come', 'part', 'superhuman', 'level', 'aplomb', 'never', 'seem', 'stress', 'say', 'son', 'avrim', 'real', 'world', 'deadline', 'stress', 'never', 'show', 'least', 'never', 'see', 'still', 'awe', 'ability', 'mask', 'inner', 'turbulence', 'affect', 'well', 'remain', 'invisible', 'even', 'close', 'observer', 'include', 'son', 'source', 'stability', 'student', 'rely', 'graduate', 'study', 'comfortable', 'relaxed', 'grad', 'school', 'feel', 'thing', 'control', 'tell', 'difficulty', 'help', 'back', 'go', 'sort', 'thing', 'speak', 'student', 'feel', 'pang', 'jealousy', 'blum', 'corner', 'vulnerable', 'moment', 'many', 'direct', 'criticism', 'face', 'reformulate', 'question', 'kind', 'audacious', 'idea', 'take', 'root', 'listen', 'absolutely', 'judgment', 'even', 'blum', 'student', 'claim', 'still', 'bewilder', 'magic', 'mystery', 'advisor', 'approach', 'become', 'accomplished', 'teacher', 'advisor', 'right', 'umesh', 'theoretical', 'computer', 'scientist', 'tell', 'think', 'lot', 'secret', 'say', 'essence', 'express', 'way', 'respect', 'student', 'let', 'develop', 'want', 'vazirani', 'advise', 'number', 'superstar', 'field', 'believe', 'education', 'important', 'thing', 'break', 'cause', 'damage', 'potency', 'blumian', 'approach', 'advise', 'domain', 'specific', 'illustrate', 'george', 'saunder', 'reflection', 'writing', 'teacher', 'tobia', 'wolff', 'writing', 'teacher', 'much', 'power', 'saunder', 'write', 'mock', 'disregard', 'use', 'prop', 'ourteacher', 'good', 'instead', 'almost', 'holy', 'neverforget', 'take', 'seriously', 'accept', 'new', 'member', 'guildthey', 'tolerate', 'underwonderful', 'story', 'write', 'dopy', 'thing', 'say', 'ourshakylegge', 'aesthetic', 'theory', 'posturing', 'therethemselve', 'say', 'think', 'writer', 'say', 'good', 'proceed', 'last', 'day', 'notice', 'photo', 'blum', 'old', 'advisor', 'desk', 'home', 'office', 'prominent', 'place', 'else', 'choose', 'display', 'family', 'heirloom', 'showcase', 'autographed', 'photo', 'shake', 'president', 'hand', 'die', 'year', 'begin', 'professorship', 'curiosity', 'point', 'photo', 'prominent', 'position', 'always', 'blum', 'reply', 'manuel', 'spiritual', 'father', 'way', 'add', 'lenore', 'make', 'way', 'back', 'airport', 'remember', 'book', 'call', 'survive', 'death', 'philosopher', 'book', 'postulate', 'good', 'person', 'quite', 'literally', 'survive', 'death', 'redirect', 'selfinter', 'wellbeing', 'future', 'people', 'forfeiture', 'spell', 'dissolution', 'self', 'rather', 'expansion', 'allow', 'person', 'live', 'onward', 'rush', 'humankind', 'line', 'book', 'unfold', 'timerelease', 'effect', 'head', 'time', 'baby', 'bear', 'good', 'person', 'acquire', 'new', 'face', 'blum', 'know', 'smile', 'well', 'nod', 'impart', 'blessing', 'wonderful', 'idea', 'proceed', 'sheon', 'writer', 'base']","<p>Theoretical computer scientist Manuel Blum has guided generations of graduate students into fruitful careers in the field.</p>
"
Death to captchas,https://www.technologyreview.com/2023/10/24/1081139/captchas-ai-websites-computing/,2023-10-24,"Earlier this year, HBO Max users hoping to sign in to the service had to pass an audio challenge in which they listened to a bunch of tunes and had to select the one with a repeating pattern. When I signed in to LinkedIn recently, it asked me to prove I’m human with an unusual puzzle. With a set of left and right buttons, I had to turn a 3D image of a pink dog until it faced the direction that a hand next to it was pointing.  Websites use these captchas (the name comes from “Completely Automated Public Turing test to tell Computers and Humans Apart”) to tell whether a user is human or machine. You’ve likely noticed they have only gotten more difficult and more involved. That’s because of what happens after we solve a captcha: the data from our efforts to label those blurry grids of traffic lights, text, or buses is used to train AI systems, which then get better at defeating captchas, tricking systems into thinking they are human.  The arms race between humans and machines has been progressing for a while. As early as 2016, researchers at Columbia University showed they could solve Google’s image captchas with 70% accuracy using off-the-shelf automated image recognition tools, the sort that could readily be used by bot designers.  Captchas have gotten more complex out of necessity. Because as AI gets more sophisticated, they’ve become less effective. By now, some captchas have gotten a little surreal. A company called hCaptcha recently tasked people with identifying an object that doesn’t exist—a “Yoko,” which seems to be an AI-generated yo-yo with a roughly snail-like appearance.   Tech firms and consumers alike feel it’s time for a change. For one thing, legacy captchas (which are still in use) just don’t work anymore: “Clicking images such as buses and street signs is outdated,” Ashish Jain, the CTO of Arkose Labs, the firm behind those LinkedIn and HBO captchas, told MIT Technology Review. “Bots have evolved, but legacy captchas haven’t.” Even more convoluted mini-games may not be enough to keep AI at bay. In one instance, a chatbot (guided by humans) pretended to be visually impaired and managed to hire a human to solve a captcha for it.  Mauro Migliardi, a professor of software engineering at the University of Padua, believes captcha designers will have to go a step further in order to stay ahead of machines. Because AIs can be trained to tackle any cognitive task, he says, we may need to transition to physical challenges, like requiring users to rotate their phones or move them in a certain way as they would in a video game.  It’s a practice that could introduce further errors into already error-prone models. That might solve some problems, but it would create others. The more complicated the challenge, the more cumbersome it is to do what you want to do on the web. And some approaches might shut some users out. “It’s actually really hard to build a challenge like this that is friendly to the whole human population,” Jess Leroy, senior director of product management at Google Cloud, wrote in an e-mail. “There are many reasons why something that may be obvious or easy to one person may be difficult to another.” Those include disabilities and cultural differences. In the long term, we may see captchas abandoned altogether. Companies such as Google and Cloudflare have already quietly switched to “invisible” challenges, which monitor online fingerprints of human behavior, like cursor motions or browsing behavior, to differentiate a person from a bot. If these sorts of signals convince the software you are human, you won’t have to solve a captcha.  This approach raises privacy concerns: such signals can allow advertisers and websites to track what you are doing online. An alternative could come from a coalition of companies, including Google, Fastly, Cloudflare, and Apple, that has developed a more privacy-friendly mechanism called Privacy Pass. Before we even open a browser and run into a captcha challenge, we perform numerous actions on our phones and computers—like unlocking them with our faces—that are hard for a bot to imitate. On a Privacy Pass–enabled website, our devices take all that information and attest for us—allowing us to skip the captcha altogether. This data never leaves your device and isn’t shared with the website. Apple calls these signatures Private Access Tokens (PATs) and already leaves the feature on by default on iPhones running at least iOS 16.  Most captcha providers, like hCaptcha and Cloudflare, now support PATs as well. Cloudflare’s CTO, John Graham-Cumming, said in July that more than half of requests from iOS devices used PATs. Leroy says that Google’s Chrome and Android teams are “working on similar technologies.”  But don’t expect captchas to disappear anytime soon. While Privacy Pass may prove a reliable alternative, captchas remain popular. Ting Wang, an information science and technology professor at Penn State University, predicts they will “continue to exist as a cheap, platform-­agnostic, and universal verification solution.” Shubham Agarwal is a freelance tech journalist. ","Earlier this year , HBO Max users hoping to sign in to the service had to pass an audio challenge in which they listened to a bunch of tunes and had to select the one with a repeating pattern . When I signed in to LinkedIn recently , it asked me to prove I ’ m human with an unusual puzzle . With a set of left and right buttons , I had to turn a 3D image of a pink dog until it faced the direction that a hand next to it was pointing . Websites use these captchas ( the name comes from “ Completely Automated Public Turing test to tell Computers and Humans Apart ” ) to tell whether a user is human or machine . You ’ ve likely noticed they have only gotten more difficult and more involved . That ’ s because of what happens after we solve a captcha : the data from our efforts to label those blurry grids of traffic lights , text , or buses is used to train AI systems , which then get better at defeating captchas , tricking systems into thinking they are human . The arms race between humans and machines has been progressing for a while . As early as 2016 , researchers at Columbia University showed they could solve Google ’ s image captchas with 70 % accuracy using off-the-shelf automated image recognition tools , the sort that could readily be used by bot designers . Captchas have gotten more complex out of necessity . Because as AI gets more sophisticated , they ’ ve become less effective . By now , some captchas have gotten a little surreal . A company called hCaptcha recently tasked people with identifying an object that doesn ’ t exist—a “ Yoko , ” which seems to be an AI-generated yo-yo with a roughly snail-like appearance . Tech firms and consumers alike feel it ’ s time for a change . For one thing , legacy captchas ( which are still in use ) just don ’ t work anymore : “ Clicking images such as buses and street signs is outdated , ” Ashish Jain , the CTO of Arkose Labs , the firm behind those LinkedIn and HBO captchas , told MIT Technology Review . “ Bots have evolved , but legacy captchas haven ’ t. ” Even more convoluted mini-games may not be enough to keep AI at bay . In one instance , a chatbot ( guided by humans ) pretended to be visually impaired and managed to hire a human to solve a captcha for it . Mauro Migliardi , a professor of software engineering at the University of Padua , believes captcha designers will have to go a step further in order to stay ahead of machines . Because AIs can be trained to tackle any cognitive task , he says , we may need to transition to physical challenges , like requiring users to rotate their phones or move them in a certain way as they would in a video game . It ’ s a practice that could introduce further errors into already error-prone models . That might solve some problems , but it would create others . The more complicated the challenge , the more cumbersome it is to do what you want to do on the web . And some approaches might shut some users out . “ It ’ s actually really hard to build a challenge like this that is friendly to the whole human population , ” Jess Leroy , senior director of product management at Google Cloud , wrote in an e-mail . “ There are many reasons why something that may be obvious or easy to one person may be difficult to another. ” Those include disabilities and cultural differences . In the long term , we may see captchas abandoned altogether . Companies such as Google and Cloudflare have already quietly switched to “ invisible ” challenges , which monitor online fingerprints of human behavior , like cursor motions or browsing behavior , to differentiate a person from a bot . If these sorts of signals convince the software you are human , you won ’ t have to solve a captcha . This approach raises privacy concerns : such signals can allow advertisers and websites to track what you are doing online . An alternative could come from a coalition of companies , including Google , Fastly , Cloudflare , and Apple , that has developed a more privacy-friendly mechanism called Privacy Pass . Before we even open a browser and run into a captcha challenge , we perform numerous actions on our phones and computers—like unlocking them with our faces—that are hard for a bot to imitate . On a Privacy Pass–enabled website , our devices take all that information and attest for us—allowing us to skip the captcha altogether . This data never leaves your device and isn ’ t shared with the website . Apple calls these signatures Private Access Tokens ( PATs ) and already leaves the feature on by default on iPhones running at least iOS 16 . Most captcha providers , like hCaptcha and Cloudflare , now support PATs as well . Cloudflare ’ s CTO , John Graham-Cumming , said in July that more than half of requests from iOS devices used PATs . Leroy says that Google ’ s Chrome and Android teams are “ working on similar technologies. ” But don ’ t expect captchas to disappear anytime soon . While Privacy Pass may prove a reliable alternative , captchas remain popular . Ting Wang , an information science and technology professor at Penn State University , predicts they will “ continue to exist as a cheap , platform-­agnostic , and universal verification solution. ” Shubham Agarwal is a freelance tech journalist .","['early', 'year', 'user', 'hope', 'sign', 'service', 'pass', 'audio', 'challenge', 'listen', 'bunch', 'tune', 'select', 'one', 'repeat', 'pattern', 'sign', 'recently', 'ask', 'prove', 'human', 'unusual', 'puzzle', 'set', 'left', 'right', 'button', 'turn', '3d', 'image', 'pink', 'dog', 'face', 'direction', 'hand', 'next', 'point', 'website', 'use', 'captchas', 'name', 'come', 'completely', 'automate', 'public', 'ture', 'test', 'tell', 'computer', 'human', 'apart', 'tell', 'user', 'human', 'machine', 'likely', 'notice', 'get', 'difficult', 'involved', 'happen', 'solve', 'captcha', 'datum', 'effort', 'label', 'blurry', 'grid', 'traffic', 'light', 'text', 'bus', 'use', 'train', 'system', 'get', 'well', 'defeat', 'captchas', 'trick', 'system', 'think', 'human', 'arm', 'race', 'human', 'machine', 'progress', 'early', 'researcher', 'show', 'solve', 'image', 'captcha', 'accuracy', 'use', 'automate', 'image', 'recognition', 'tool', 'sort', 'readily', 'use', 'bot', 'designer', 'captcha', 'get', 'complex', 'necessity', 'get', 'sophisticated', 'become', 'less', 'effective', 'captcha', 'get', 'little', 'surreal', 'company', 'call', 'recently', 'task', 'people', 'identify', 'object', 'exist', 'seem', 'aigenerated', 'yoyo', 'roughly', 'snaillike', 'appearance', 'tech', 'firm', 'consumer', 'alike', 'feel', 'time', 'change', 'thing', 'legacy', 'still', 'use', 'work', 'anymore', 'click', 'image', 'bus', 'street', 'sign', 'outdate', 'ashish', 'cto', 'firm', 'linkedin', 'tell', 'mit', 'technology', 'review', 'bot', 'evolve', 'even', 'convoluted', 'minigame', 'enough', 'keep', 'ai', 'bay', 'instance', 'chatbot', 'guide', 'human', 'pretend', 'visually', 'impair', 'manage', 'hire', 'human', 'solve', 'captcha', 'professor', 'software', 'engineering', 'believe', 'designer', 'go', 'step', 'far', 'order', 'stay', 'ahead', 'machine', 'train', 'tackle', 'cognitive', 'task', 'say', 'need', 'transition', 'physical', 'challenge', 'require', 'user', 'rotate', 'phone', 'move', 'certain', 'way', 'video', 'game', 'practice', 'introduce', 'error', 'already', 'errorprone', 'model', 'solve', 'problem', 'create', 'complicated', 'challenge', 'cumbersome', 'want', 'web', 'approach', 'shut', 'user', 'actually', 'really', 'hard', 'build', 'challenge', 'friendly', 'whole', 'human', 'population', 'jess', 'leroy', 'senior', 'director', 'product', 'management', 'write', 'email', 'many', 'reason', 'obvious', 'easy', 'person', 'difficult', 'include', 'disability', 'cultural', 'difference', 'long', 'term', 'see', 'captcha', 'abandon', 'altogether', 'company', 'already', 'quietly', 'switch', 'invisible', 'challenge', 'monitor', 'online', 'fingerprint', 'human', 'behavior', 'cursor', 'motion', 'browse', 'behavior', 'differentiate', 'person', 'bot', 'sort', 'signal', 'convince', 'software', 'human', 'win', 'solve', 'captcha', 'approach', 'raise', 'privacy', 'concern', 'signal', 'allow', 'advertiser', 'website', 'track', 'online', 'alternative', 'come', 'coalition', 'company', 'include', 'fastly', 'cloudflare', 'apple', 'develop', 'privacyfriendly', 'mechanism', 'call', 'privacy', 'pass', 'even', 'open', 'browser', 'run', 'captcha', 'challenge', 'perform', 'numerous', 'action', 'phone', 'computer', 'unlock', 'face', 'hard', 'bot', 'imitate', 'privacy', 'pass', 'enable', 'website', 'device', 'take', 'information', 'attest', 'allow', 'skip', 'captcha', 'altogether', 'datum', 'never', 'leave', 'device', 'share', 'website', 'apple', 'call', 'signature', 'private', 'access', 'token', 'pat', 'already', 'leave', 'feature', 'default', 'iphone', 'run', 'least', 'io', 'captcha', 'provider', 'hcaptcha', 'cloudflare', 'support', 'pat', 'well', 'cloudflare', 'say', 'half', 'request', 'io', 'device', 'use', 'pat', 'say', 'chrome', 'android', 'team', 'work', 'similar', 'technology', 'expect', 'captcha', 'disappear', 'anytime', 'soon', 'privacy', 'pass', 'prove', 'reliable', 'alternative', 'captcha', 'remain', 'popular', 'te', 'information', 'science', 'technology', 'professor', 'predict', 'continue', 'exist', 'cheap', 'platform\xadagnostic', 'universal', 'verification', 'solution', 'freelance', 'tech', 'journalist']","<p>Proving you’re human on websites is harder than ever—but alternative tests are gaining ground.</p>
"
Inside the quest for unbreakable encryption,https://www.technologyreview.com/2023/10/19/1081389/unbreakable-encryption-quantum-computers-cryptography-math-problems/,2023-10-19,"When we check email, log in to our bank accounts, or exchange messages on Signal, our passwords and credentials are protected through encryption, a locking scheme that uses secrets to disguise our data. It works like a cyber padlock: with the right key someone can unlock the data. Without it, they’ll have to resort to laborious brute-force methods, the digital equivalent of hacksaws and blowtorches. Our trust in online security is rooted in mathematics. Encryption schemes are built on families of math problems called one-way functions—calculations that are easy to carry out in one direction but almost impossible to solve efficiently from the other, even with a powerful computer. They’re sort of a computational equivalent of those road spikes found at the exits of airport car rental agencies. Drive in one direction and you barely notice. Hit reverse and you won’t get far (and will need new tires). There’s a problem, however. Although mathematicians suspect true one-way functions exist, they have yet to prove it. They haven’t proved that the thorny problems we do use are impossible, or even extremely impractical, to solve. Instead, it could just be that we haven’t yet found the appropriate mathematical means to take the problems apart. This conundrum haunts all encryption. Our data is secured by the fact that no one knows how to crack the schemes that protect it—at least not yet.  It’s not just today’s hackers we may need to worry about. Security experts have long warned of a threat that hasn’t yet materialized: quantum computers. In the future these machines could execute a program that quickly solves the math problems behind today’s state-of-the-art encryption. That threat puts personal financial, medical, and other information at risk. Hackers could steal today’s encrypted data and store it away, just waiting for the arrival of new technological lockpicks.  Computer scientists, mathematicians, and cryptographers are on a quest to find new encryption algorithms that can withstand attacks not only from today’s conventional computers but also from tomorrow’s quantum machines. What they want is a big, sticky math problem—something that’s robust enough to withstand attacks from classical and quantum computers but can still be easily implemented in cyberspace.  Unfortunately, no one has yet found a single type of problem that is provably hard for computers—classical or quantum—to solve. (In the world of cryptography, “hard” describes a problem whose solution requires an unreasonable number of steps or amount of computing power.) If one-way functions don’t exist, then cryptographers’ whack-a-mole process of finding flaws and developing ever stronger schemes to block clever hackers will persist indefinitely.  “The question of whether one-way functions exist is really the most important problem,” says Rafael Pass, a theoretical computer scientist at Tel Aviv University in Israel. It’s a conundrum that dates to the 1970s and the dawn of a research area now known as computational complexity theory. Over five decades, theorists and cryptographers have been looking for ways to establish whether such functions do exist. Perhaps the problems we hope or suspect are one-way are just easier, breakable ones in disguise.  Pass is exploring how one-way functions are connected to a raft of other open problems, a promising line of research that has drawn other theorists into the quest. At the same time, people focused on the practical side of cryptography are plowing ahead, hunting for new schemes that are—if not provably hard—seemingly strong enough to hold up against quantum computers.  Computer scientists find themselves at a curious crossroads, unsure of whether post-quantum algorithms are truly unassailable—or just believed to be so. For the last seven years, the job of finding the best candidates has been spearheaded by the National Institute of Standards and Technology (NIST), the US government body charged with collecting, testing, and standardizing cryptographic algorithms for public use. NIST has been running dozens of potential “post-­quantum” algorithms through a gauntlet of tests and making them available for outside testing. The process has winnowed the field to a few finalists, and in August NIST announced that one called CRYSTALS-Kyber, which takes an approach believed to be robust enough to counter quantum attacks, will be the first to be officially recommended for public use by 2024. After that, companies and governments will adopt the algorithm for encrypting data.  Will it hold up? The answer will help determine the trajectory of cybersecurity in the near term. But it’s far from settled: history suggests that our faith in unbreakability has often been misplaced, and over the years, seemingly impenetrable encryption candidates have fallen to surprisingly simple attacks. Computer scientists find themselves at a curious crossroads, unsure of whether post-quantum algorithms are truly unassailable—or just believed to be so. It’s a distinction at the heart of modern encryption security.  Securing secret messages hasn’t always been tied to difficult math problems; until recently, cryptography was barely mathematical at all. In ancient Greece, military leaders encoded messages using a scytale, a cylindrical device that revealed a hidden message when a strip of seemingly jumbled text was wound around it. Centuries later, Roman historians described a code, often attributed to Julius Caesar, that involved shifting letters in a message three spots up in the alphabet; for example, a d would be written as an a.  Companies are moving away from setting qubit records in favor of practical hardware and long-term goals. In history as in our modern world, secret codes were frequently broken. In the 16th century, during the decades she spent imprisoned by her cousin Queen Elizabeth I, Mary, Queen of Scots, used elaborate, symbol-based ciphers to encode hundreds of letters, most of which were aimed at securing her freedom and regaining the throne. She didn’t prevail: Elizabeth I’s team of spies and codebreakers intercepted, decoded, and copied the letters. In the one that sealed her fate, Mary approved of a plan to assassinate Elizabeth with six words: “sett the six gentlemen to woork.” In response, Elizabeth eventually ordered her cousin beheaded in 1587. In 1932, codebreakers in Poland cracked the code for Germany’s early Enigma machine, invented at the end of World War I. They later shared their intel with British codebreakers, who cracked a more advanced version of Enigma during World War II. Pass, the theoretical computer scientist in Tel Aviv, half-jokingly refers to all time before the 1970s as the “dark age of cryptography.” “Cryptography wasn’t really a scientific field,” he says. “It was more like artist versus attackers. You needed to have [artistic] skills to invent an encryption scheme. And then it would get deployed until some clever person would figure out how to break it. And it was just going on and on like that.”  That changed, Pass says, in November 1976, when cryptographers Whitfield Diffie and Martin Hellman, at Stanford, described a novel way for two people to devise a key that only they knew—one they could then use to pass secret messages. Crucially, they wouldn’t have to meet to do it. This was a groundbreaking notion. Previously, both sender and receiver had to physically possess a key for encoding and decoding. To decrypt a message encoded with the Enigma machine, for example, a recipient needed a key sheet that revealed the initial encryption settings. The secret to the Diffie-Hellman strategy was for two people to build the key using a straightforward mathematical problem that’s easy to compute in one direction and laborious in the other. Here’s how it works: The two people who want to communicate secretly, usually designated Alice and Bob in these setups, each pick a secret number. Then, together, they agree on a pair of numbers that they share publicly (one is a big prime, and the other is called the base). Each of them next carries out a series of mathematical operations to combine those private numbers with the prime and the base.  Then they exchange the results, and they each carry out another series of mathematical operations on the new numbers. In the end, both Alice and Bob will have done the same operations on the same numbers—just not in the same order—and arrived at the same answer. The digits of that answer become the encryption. And an eavesdropper who intercepts the transmission—often nicknamed Eve—won’t be able to easily unravel the mathematical jumble without knowing at least one of the private numbers. She could start testing numbers in a brute-force approach, but that would require an unreasonable amount of calculation.  The complicated problem that Eve would have to solve is called finding a discrete logarithm. The Diffie-Hellman approach is still used today—to secure some VPNs, for example—and is integral to some post-quantum schemes. In their paper, Diffie and Hellman noted that there was no existing algorithm capable of solving the discrete log problem in a reasonable amount of time. There still isn’t. They went on to introduce, for the first time, the notion of one-way functions as a basis for secure cryptography. Today, secure online interactions that involve authentication or digital signatures, for example, are based on that general idea. But without mathematical proof that the problems they rely on are one-way functions, the possibility remains that someone might discover an efficient scheme for cracking them.  Today, online transactions begin with a kind of digital handshake, and the security of that handshake is often guaranteed by another math problem that’s presumed to be difficult. The most popular encryption scheme used today was introduced in 1977 by a trio of young computer scientists who were energized by Diffie and Hellman’s 1976 paper. They called their approach RSA, after the last names of the scientists (Ron Rivest, Adi Shamir, and Leonard Adleman).  RSA, which is based on the difficulty of finding prime factors relative to the ease of multiplying them together, is a bit different from the Diffie-Hellman approach. Diffie-Hellman is a shared secret: it allows two users to devise a key over an insecure channel (like the internet), and that key is used to disguise messages. In RSA, Alice uses Bob’s key—based on big prime numbers—to encrypt a message that only he can unlock. RSA can secure data sent from one person to another.   It quickly became one of the most popular public-key encryption methods. It’s easy to use and adapt. Over time, as new algorithms have emerged that can factor faster, and computers have become more powerful, NIST has recommended using larger and larger numbers for security. The numbers are represented in binary form with 1s and 0s, and these binary digits are better known as “bits.” The number 13, for example, is written in binary as 1101, which has four bits. NIST currently recommends using a key represented by at least 2,048 bits—which corresponds to a number with over 600 digits. (To date, the largest number that has been factored into two primes was made up of 250 digits, and the process took nearly 3,000 hours of computing time.) That’s a strength of RSA—even if it’s not uncrackable, it’s been easy to keep upping the ante, making it computationally impractical to break.  In 1994, however, a threat of a different type emerged when the American mathematician Peter Shor, then at Bell Labs, devised an algorithm for quantum computers that could solve the factoring problem in a reasonable amount of time. (It was a double threat: his approach could also conquer the discrete log problem in the Diffie-Hellman approach.)  Shor’s paper ignited excitement and anxiety among those who wanted to build quantum computers and those who recognized the threat it posed to cybersecurity. Fortunately for cryptographers, not just any quantum computer would do.  The last three decades of cybersecurity have played out like an increasingly intricate game, with researchers perpetually building and breaking—or attempting to break—new candidates. A few years back, researchers at Google and the KTH Royal Institute of Technology, in Sweden, estimated that it would take a quantum computer composed of 20 million quantum bits, or qubits, some eight hours to break today’s 2,048-bit RSA security. Current state-of-the-art machines are nowhere close to that size: the largest quantum computer to date, built by IBM, debuted last year with 433 qubits. Whether or not RSA can be considered at immediate risk of a quantum attack depends largely on whom you ask, says computer scientist Ted Shorter, who cofounded the cybersecurity company Keyfactor. He sees a cultural divide between the theorists who study the mathematics of encryption and the cryptographers who work in implementation. To some, the end seems nigh. “You talk to a theoretical computer scientist and they’re like, Yes, RSA is done, because they can imagine it,” Shorter says. For them, he adds, the existence of Shor’s algorithm points to the end of encryption as we know it.  Many cryptographers who are implementing real-world security systems are less concerned about the quantum future than they are about today’s cleverest hackers. After all, people have been trying to factor efficiently for thousands of years, and now the only known method requires a computer that doesn’t exist.  Thomas Decru, a cryptographer at KU Leuven in Belgium, says the quantum threat must be taken seriously, but it’s hard to know if RSA will fall to quantum computers in five years or longer—or never. “As long as quantum computers do not exist, everything you say about them is speculative, in a way,” he says. Pass is more certain about the threat: “It’s safe to say that the existence of this quantum algorithm means there are cracks in the problem, right?”  But we have to be ready for anything, says Lily Chen, a mathematician who manages NIST’s Cryptographic Technology Group and works on the ongoing effort to produce post-quantum encryption standards. Whether they arrive in three years or 30, quantum computers loom on the horizon, and RSA, Diffie-Hellman, and other encryption schemes may be left vulnerable.  Finding a quantum-resistant cryptographic scheme isn’t easy. Without a mathematical problem that is computationally hard, the last three decades of cybersecurity have played out like an increasingly intricate game, with researchers perpetually building and breaking—or attempting to break—new candidates.  This push and pull has already emerged in the NIST post-quantum program. In February 2022, cryptographers found a fatal flaw in Rainbow, an algorithm that had survived three rounds of NIST’s analysis. A few months later, after the NIST list had been winnowed again, Decru and his KU Leuven colleague Wouter Castryck announced that they’d broken another finalist, an algorithm called SIKE.  SIKE, which was developed a few years ago, was the brainchild of a collaboration among researchers and engineers at Amazon, Microsoft, the University of Versailles, and elsewhere. It is based on a special mathematical map, called an isogeny, that is made up of connections between elliptic curves. These maps can be turned into an encryption for communication, and outsiders can’t eavesdrop without knowing the maps. When quantum computers become powerful enough, they could theoretically crack the encryption algorithms that keep us safe. The race is on to find new ones. At Leuven, Decru and Castryck devise ways to use these so-called isogenies to build new, faster encryption approaches. They broke the most difficult version of SIKE in just a few hours of computing time using an ordinary desktop computer. (Since then, other groups have found ways to do it even faster.) What’s more, Decru and Castryck did it almost accidentally, and only a few weeks after SIKE had been declared an alternate NIST finalist. “We weren’t trying to break it at all,” insists Decru. “We just tried to generalize it.”  Chen says the case of SIKE—and Rainbow before it—illustrates a real-world tension that drives efforts to find quantum-proof algorithms. On one hand, she says, “you have to find a problem which is hard for both quantum computers and classical computers.” On the other is implementation: transforming that hard problem into one that can be used in a real-world cryptographic system. Even with today’s well-defined problems, Shorter says, it’s very difficult to predict and prevent every loophole in every operating system and device on the market today. “And then there’s interoperability testing and certifications and other tests,” he says, “to make sure they are not only implemented correctly, but also securely.”   The mathematical problem SIKE is based on seems computationally hard because there are so many different maps that could be constructed between curves. It may even be a one-way problem—and therefore quantum-proof. The flaw was in the design, which revealed too much of the transmitted information. Decru and Castryck cracked it because they inadvertently found a way to expose enough connecting points to give away the entire thing.  Other schemes have fared better. The first post-quantum encryption algorithm to be standardized, CRYSTALS-Kyber, delivers security through an approach that involves problems on lattices, mathematical objectsthat can be modeled as arrays of points. (There are five main families of post-quantum cryptographic methods. Isogeny and lattice approaches are two of them.)  CRYSTALS-Kyber is a general encryption scheme, like RSA, that can be used for tasks like securing online communication. Three other approved algorithms are designed to authenticate digital signatures, ensuring that digital documents haven’t been fraudulently signed. NIST plans to standardize these by spring 2024. Another three (it was four until SIKE was broken) could also be standardized in the next few years, as long as they survive further rounds of scrutiny. But unless mathematicians can prove whether one-way functions exist, says Pass, the patterns that have always characterized cryptography will continue. “We’re back to this cat-and-mouse game, where it’s a game between algorithm designers proposing new candidate constructions and other designers trying to break them,” he says. Unless, of course, he—or someone in his field—can come up with an implementable, provably one-way function to settle the matter of encryption forever.  Until that time, cryptographers will remain in a messy limbo in which convincingly robust encryption schemes can be trusted—but only until they can’t.  The perfect math problem could take us out of this limbo, but it can’t be some sticky mess cooked up by an armchair algebraist over a long weekend. It must strike a balance between math and cryptography, with computational hardness on one side and easy implementation on the other. Stray too far from either of those properties, and it becomes vulnerable—if not now, then in the future. Hanging in the balance is the past, present, and future security of everyone’s data, everywhere. No pressure.  Stephen Ornes is a science writer based in Nashville. ","When we check email , log in to our bank accounts , or exchange messages on Signal , our passwords and credentials are protected through encryption , a locking scheme that uses secrets to disguise our data . It works like a cyber padlock : with the right key someone can unlock the data . Without it , they ’ ll have to resort to laborious brute-force methods , the digital equivalent of hacksaws and blowtorches . Our trust in online security is rooted in mathematics . Encryption schemes are built on families of math problems called one-way functions—calculations that are easy to carry out in one direction but almost impossible to solve efficiently from the other , even with a powerful computer . They ’ re sort of a computational equivalent of those road spikes found at the exits of airport car rental agencies . Drive in one direction and you barely notice . Hit reverse and you won ’ t get far ( and will need new tires ) . There ’ s a problem , however . Although mathematicians suspect true one-way functions exist , they have yet to prove it . They haven ’ t proved that the thorny problems we do use are impossible , or even extremely impractical , to solve . Instead , it could just be that we haven ’ t yet found the appropriate mathematical means to take the problems apart . This conundrum haunts all encryption . Our data is secured by the fact that no one knows how to crack the schemes that protect it—at least not yet . It ’ s not just today ’ s hackers we may need to worry about . Security experts have long warned of a threat that hasn ’ t yet materialized : quantum computers . In the future these machines could execute a program that quickly solves the math problems behind today ’ s state-of-the-art encryption . That threat puts personal financial , medical , and other information at risk . Hackers could steal today ’ s encrypted data and store it away , just waiting for the arrival of new technological lockpicks . Computer scientists , mathematicians , and cryptographers are on a quest to find new encryption algorithms that can withstand attacks not only from today ’ s conventional computers but also from tomorrow ’ s quantum machines . What they want is a big , sticky math problem—something that ’ s robust enough to withstand attacks from classical and quantum computers but can still be easily implemented in cyberspace . Unfortunately , no one has yet found a single type of problem that is provably hard for computers—classical or quantum—to solve . ( In the world of cryptography , “ hard ” describes a problem whose solution requires an unreasonable number of steps or amount of computing power . ) If one-way functions don ’ t exist , then cryptographers ’ whack-a-mole process of finding flaws and developing ever stronger schemes to block clever hackers will persist indefinitely . “ The question of whether one-way functions exist is really the most important problem , ” says Rafael Pass , a theoretical computer scientist at Tel Aviv University in Israel . It ’ s a conundrum that dates to the 1970s and the dawn of a research area now known as computational complexity theory . Over five decades , theorists and cryptographers have been looking for ways to establish whether such functions do exist . Perhaps the problems we hope or suspect are one-way are just easier , breakable ones in disguise . Pass is exploring how one-way functions are connected to a raft of other open problems , a promising line of research that has drawn other theorists into the quest . At the same time , people focused on the practical side of cryptography are plowing ahead , hunting for new schemes that are—if not provably hard—seemingly strong enough to hold up against quantum computers . Computer scientists find themselves at a curious crossroads , unsure of whether post-quantum algorithms are truly unassailable—or just believed to be so . For the last seven years , the job of finding the best candidates has been spearheaded by the National Institute of Standards and Technology ( NIST ) , the US government body charged with collecting , testing , and standardizing cryptographic algorithms for public use . NIST has been running dozens of potential “ post-­quantum ” algorithms through a gauntlet of tests and making them available for outside testing . The process has winnowed the field to a few finalists , and in August NIST announced that one called CRYSTALS-Kyber , which takes an approach believed to be robust enough to counter quantum attacks , will be the first to be officially recommended for public use by 2024 . After that , companies and governments will adopt the algorithm for encrypting data . Will it hold up ? The answer will help determine the trajectory of cybersecurity in the near term . But it ’ s far from settled : history suggests that our faith in unbreakability has often been misplaced , and over the years , seemingly impenetrable encryption candidates have fallen to surprisingly simple attacks . Computer scientists find themselves at a curious crossroads , unsure of whether post-quantum algorithms are truly unassailable—or just believed to be so . It ’ s a distinction at the heart of modern encryption security . Securing secret messages hasn ’ t always been tied to difficult math problems ; until recently , cryptography was barely mathematical at all . In ancient Greece , military leaders encoded messages using a scytale , a cylindrical device that revealed a hidden message when a strip of seemingly jumbled text was wound around it . Centuries later , Roman historians described a code , often attributed to Julius Caesar , that involved shifting letters in a message three spots up in the alphabet ; for example , a d would be written as an a . Companies are moving away from setting qubit records in favor of practical hardware and long-term goals . In history as in our modern world , secret codes were frequently broken . In the 16th century , during the decades she spent imprisoned by her cousin Queen Elizabeth I , Mary , Queen of Scots , used elaborate , symbol-based ciphers to encode hundreds of letters , most of which were aimed at securing her freedom and regaining the throne . She didn ’ t prevail : Elizabeth I ’ s team of spies and codebreakers intercepted , decoded , and copied the letters . In the one that sealed her fate , Mary approved of a plan to assassinate Elizabeth with six words : “ sett the six gentlemen to woork. ” In response , Elizabeth eventually ordered her cousin beheaded in 1587 . In 1932 , codebreakers in Poland cracked the code for Germany ’ s early Enigma machine , invented at the end of World War I . They later shared their intel with British codebreakers , who cracked a more advanced version of Enigma during World War II . Pass , the theoretical computer scientist in Tel Aviv , half-jokingly refers to all time before the 1970s as the “ dark age of cryptography. ” “ Cryptography wasn ’ t really a scientific field , ” he says . “ It was more like artist versus attackers . You needed to have [ artistic ] skills to invent an encryption scheme . And then it would get deployed until some clever person would figure out how to break it . And it was just going on and on like that. ” That changed , Pass says , in November 1976 , when cryptographers Whitfield Diffie and Martin Hellman , at Stanford , described a novel way for two people to devise a key that only they knew—one they could then use to pass secret messages . Crucially , they wouldn ’ t have to meet to do it . This was a groundbreaking notion . Previously , both sender and receiver had to physically possess a key for encoding and decoding . To decrypt a message encoded with the Enigma machine , for example , a recipient needed a key sheet that revealed the initial encryption settings . The secret to the Diffie-Hellman strategy was for two people to build the key using a straightforward mathematical problem that ’ s easy to compute in one direction and laborious in the other . Here ’ s how it works : The two people who want to communicate secretly , usually designated Alice and Bob in these setups , each pick a secret number . Then , together , they agree on a pair of numbers that they share publicly ( one is a big prime , and the other is called the base ) . Each of them next carries out a series of mathematical operations to combine those private numbers with the prime and the base . Then they exchange the results , and they each carry out another series of mathematical operations on the new numbers . In the end , both Alice and Bob will have done the same operations on the same numbers—just not in the same order—and arrived at the same answer . The digits of that answer become the encryption . And an eavesdropper who intercepts the transmission—often nicknamed Eve—won ’ t be able to easily unravel the mathematical jumble without knowing at least one of the private numbers . She could start testing numbers in a brute-force approach , but that would require an unreasonable amount of calculation . The complicated problem that Eve would have to solve is called finding a discrete logarithm . The Diffie-Hellman approach is still used today—to secure some VPNs , for example—and is integral to some post-quantum schemes . In their paper , Diffie and Hellman noted that there was no existing algorithm capable of solving the discrete log problem in a reasonable amount of time . There still isn ’ t . They went on to introduce , for the first time , the notion of one-way functions as a basis for secure cryptography . Today , secure online interactions that involve authentication or digital signatures , for example , are based on that general idea . But without mathematical proof that the problems they rely on are one-way functions , the possibility remains that someone might discover an efficient scheme for cracking them . Today , online transactions begin with a kind of digital handshake , and the security of that handshake is often guaranteed by another math problem that ’ s presumed to be difficult . The most popular encryption scheme used today was introduced in 1977 by a trio of young computer scientists who were energized by Diffie and Hellman ’ s 1976 paper . They called their approach RSA , after the last names of the scientists ( Ron Rivest , Adi Shamir , and Leonard Adleman ) . RSA , which is based on the difficulty of finding prime factors relative to the ease of multiplying them together , is a bit different from the Diffie-Hellman approach . Diffie-Hellman is a shared secret : it allows two users to devise a key over an insecure channel ( like the internet ) , and that key is used to disguise messages . In RSA , Alice uses Bob ’ s key—based on big prime numbers—to encrypt a message that only he can unlock . RSA can secure data sent from one person to another . It quickly became one of the most popular public-key encryption methods . It ’ s easy to use and adapt . Over time , as new algorithms have emerged that can factor faster , and computers have become more powerful , NIST has recommended using larger and larger numbers for security . The numbers are represented in binary form with 1s and 0s , and these binary digits are better known as “ bits. ” The number 13 , for example , is written in binary as 1101 , which has four bits . NIST currently recommends using a key represented by at least 2,048 bits—which corresponds to a number with over 600 digits . ( To date , the largest number that has been factored into two primes was made up of 250 digits , and the process took nearly 3,000 hours of computing time . ) That ’ s a strength of RSA—even if it ’ s not uncrackable , it ’ s been easy to keep upping the ante , making it computationally impractical to break . In 1994 , however , a threat of a different type emerged when the American mathematician Peter Shor , then at Bell Labs , devised an algorithm for quantum computers that could solve the factoring problem in a reasonable amount of time . ( It was a double threat : his approach could also conquer the discrete log problem in the Diffie-Hellman approach . ) Shor ’ s paper ignited excitement and anxiety among those who wanted to build quantum computers and those who recognized the threat it posed to cybersecurity . Fortunately for cryptographers , not just any quantum computer would do . The last three decades of cybersecurity have played out like an increasingly intricate game , with researchers perpetually building and breaking—or attempting to break—new candidates . A few years back , researchers at Google and the KTH Royal Institute of Technology , in Sweden , estimated that it would take a quantum computer composed of 20 million quantum bits , or qubits , some eight hours to break today ’ s 2,048-bit RSA security . Current state-of-the-art machines are nowhere close to that size : the largest quantum computer to date , built by IBM , debuted last year with 433 qubits . Whether or not RSA can be considered at immediate risk of a quantum attack depends largely on whom you ask , says computer scientist Ted Shorter , who cofounded the cybersecurity company Keyfactor . He sees a cultural divide between the theorists who study the mathematics of encryption and the cryptographers who work in implementation . To some , the end seems nigh . “ You talk to a theoretical computer scientist and they ’ re like , Yes , RSA is done , because they can imagine it , ” Shorter says . For them , he adds , the existence of Shor ’ s algorithm points to the end of encryption as we know it . Many cryptographers who are implementing real-world security systems are less concerned about the quantum future than they are about today ’ s cleverest hackers . After all , people have been trying to factor efficiently for thousands of years , and now the only known method requires a computer that doesn ’ t exist . Thomas Decru , a cryptographer at KU Leuven in Belgium , says the quantum threat must be taken seriously , but it ’ s hard to know if RSA will fall to quantum computers in five years or longer—or never . “ As long as quantum computers do not exist , everything you say about them is speculative , in a way , ” he says . Pass is more certain about the threat : “ It ’ s safe to say that the existence of this quantum algorithm means there are cracks in the problem , right ? ” But we have to be ready for anything , says Lily Chen , a mathematician who manages NIST ’ s Cryptographic Technology Group and works on the ongoing effort to produce post-quantum encryption standards . Whether they arrive in three years or 30 , quantum computers loom on the horizon , and RSA , Diffie-Hellman , and other encryption schemes may be left vulnerable . Finding a quantum-resistant cryptographic scheme isn ’ t easy . Without a mathematical problem that is computationally hard , the last three decades of cybersecurity have played out like an increasingly intricate game , with researchers perpetually building and breaking—or attempting to break—new candidates . This push and pull has already emerged in the NIST post-quantum program . In February 2022 , cryptographers found a fatal flaw in Rainbow , an algorithm that had survived three rounds of NIST ’ s analysis . A few months later , after the NIST list had been winnowed again , Decru and his KU Leuven colleague Wouter Castryck announced that they ’ d broken another finalist , an algorithm called SIKE . SIKE , which was developed a few years ago , was the brainchild of a collaboration among researchers and engineers at Amazon , Microsoft , the University of Versailles , and elsewhere . It is based on a special mathematical map , called an isogeny , that is made up of connections between elliptic curves . These maps can be turned into an encryption for communication , and outsiders can ’ t eavesdrop without knowing the maps . When quantum computers become powerful enough , they could theoretically crack the encryption algorithms that keep us safe . The race is on to find new ones . At Leuven , Decru and Castryck devise ways to use these so-called isogenies to build new , faster encryption approaches . They broke the most difficult version of SIKE in just a few hours of computing time using an ordinary desktop computer . ( Since then , other groups have found ways to do it even faster . ) What ’ s more , Decru and Castryck did it almost accidentally , and only a few weeks after SIKE had been declared an alternate NIST finalist . “ We weren ’ t trying to break it at all , ” insists Decru . “ We just tried to generalize it. ” Chen says the case of SIKE—and Rainbow before it—illustrates a real-world tension that drives efforts to find quantum-proof algorithms . On one hand , she says , “ you have to find a problem which is hard for both quantum computers and classical computers. ” On the other is implementation : transforming that hard problem into one that can be used in a real-world cryptographic system . Even with today ’ s well-defined problems , Shorter says , it ’ s very difficult to predict and prevent every loophole in every operating system and device on the market today . “ And then there ’ s interoperability testing and certifications and other tests , ” he says , “ to make sure they are not only implemented correctly , but also securely. ” The mathematical problem SIKE is based on seems computationally hard because there are so many different maps that could be constructed between curves . It may even be a one-way problem—and therefore quantum-proof . The flaw was in the design , which revealed too much of the transmitted information . Decru and Castryck cracked it because they inadvertently found a way to expose enough connecting points to give away the entire thing . Other schemes have fared better . The first post-quantum encryption algorithm to be standardized , CRYSTALS-Kyber , delivers security through an approach that involves problems on lattices , mathematical objectsthat can be modeled as arrays of points . ( There are five main families of post-quantum cryptographic methods . Isogeny and lattice approaches are two of them . ) CRYSTALS-Kyber is a general encryption scheme , like RSA , that can be used for tasks like securing online communication . Three other approved algorithms are designed to authenticate digital signatures , ensuring that digital documents haven ’ t been fraudulently signed . NIST plans to standardize these by spring 2024 . Another three ( it was four until SIKE was broken ) could also be standardized in the next few years , as long as they survive further rounds of scrutiny . But unless mathematicians can prove whether one-way functions exist , says Pass , the patterns that have always characterized cryptography will continue . “ We ’ re back to this cat-and-mouse game , where it ’ s a game between algorithm designers proposing new candidate constructions and other designers trying to break them , ” he says . Unless , of course , he—or someone in his field—can come up with an implementable , provably one-way function to settle the matter of encryption forever . Until that time , cryptographers will remain in a messy limbo in which convincingly robust encryption schemes can be trusted—but only until they can ’ t . The perfect math problem could take us out of this limbo , but it can ’ t be some sticky mess cooked up by an armchair algebraist over a long weekend . It must strike a balance between math and cryptography , with computational hardness on one side and easy implementation on the other . Stray too far from either of those properties , and it becomes vulnerable—if not now , then in the future . Hanging in the balance is the past , present , and future security of everyone ’ s data , everywhere . No pressure . Stephen Ornes is a science writer based in Nashville .","['check', 'email', 'log', 'bank', 'account', 'exchange', 'message', 'signal', 'password', 'credential', 'protect', 'encryption', 'lock', 'scheme', 'use', 'secret', 'disguise', 'datum', 'work', 'cyber', 'padlock', 'right', 'key', 'unlock', 'datum', 'resort', 'laborious', 'bruteforce', 'method', 'digital', 'equivalent', 'hacksaw', 'blowtorch', 'trust', 'online', 'security', 'root', 'mathematics', 'encryption', 'scheme', 'build', 'family', 'math', 'problem', 'call', 'oneway', 'function', 'calculation', 'easy', 'carry', 'direction', 'almost', 'impossible', 'solve', 'efficiently', 'even', 'powerful', 'computer', 'sort', 'computational', 'equivalent', 'road', 'spike', 'find', 'exit', 'airport', 'car', 'rental', 'agency', 'drive', 'direction', 'barely', 'notice', 'hit', 'reverse', 'win', 'get', 'far', 'need', 'new', 'tire', 'problem', 'however', 'mathematician', 'suspect', 'true', 'oneway', 'function', 'exist', 'yet', 'prove', 'prove', 'thorny', 'problem', 'use', 'impossible', 'even', 'extremely', 'impractical', 'solve', 'instead', 'yet', 'find', 'appropriate', 'mathematical', 'mean', 'take', 'problem', 'apart', 'conundrum', 'haunt', 'encryption', 'datum', 'secure', 'fact', 'one', 'know', 'crack', 'scheme', 'protect', 'least', 'yet', 'today', 'hacker', 'need', 'worry', 'security', 'expert', 'long', 'warn', 'threat', 'yet', 'materialize', 'quantum', 'computer', 'future', 'machine', 'execute', 'program', 'quickly', 'solve', 'math', 'problem', 'today', 'stateoftheart', 'encryption', 'threat', 'put', 'personal', 'financial', 'medical', 'information', 'risk', 'hacker', 'steal', 'today', 'encrypt', 'datum', 'store', 'away', 'wait', 'arrival', 'new', 'technological', 'lockpick', 'computer', 'scientist', 'mathematician', 'cryptographer', 'quest', 'find', 'new', 'encryption', 'algorithm', 'withstand', 'attack', 'today', 'conventional', 'computer', 'also', 'tomorrow', 'quantum', 'machine', 'want', 'big', 'sticky', 'math', 'problem', 'robust', 'enough', 'withstand', 'attack', 'classical', 'quantum', 'computer', 'still', 'easily', 'implement', 'cyberspace', 'unfortunately', 'one', 'yet', 'find', 'single', 'type', 'problem', 'provably', 'hard', 'computer', 'classical', 'quantum', 'solve', 'world', 'cryptography', 'hard', 'describe', 'problem', 'solution', 'require', 'unreasonable', 'number', 'step', 'amount', 'compute', 'power', 'oneway', 'function', 'exist', 'cryptographer', 'whackamole', 'process', 'find', 'flaw', 'develop', 'ever', 'strong', 'scheme', 'block', 'clever', 'hacker', 'persist', 'indefinitely', 'question', 'oneway', 'function', 'exist', 'really', 'important', 'problem', 'say', 'pass', 'theoretical', 'computer', 'scientist', 'conundrum', 'date', '1970', 'dawn', 'research', 'area', 'know', 'computational', 'complexity', 'theory', 'decade', 'theorist', 'cryptographer', 'look', 'way', 'establish', 'function', 'exist', 'perhaps', 'problem', 'hope', 'suspect', 'oneway', 'easy', 'breakable', 'one', 'disguise', 'pass', 'explore', 'oneway', 'function', 'connect', 'raft', 'open', 'problem', 'promising', 'line', 'research', 'draw', 'theorist', 'quest', 'time', 'people', 'focus', 'practical', 'side', 'cryptography', 'plow', 'ahead', 'hunt', 'new', 'scheme', 'provably', 'hard', 'seemingly', 'strong', 'enough', 'hold', 'quantum', 'computer', 'computer', 'scientist', 'find', 'curious', 'crossroad', 'unsure', 'postquantum', 'algorithm', 'truly', 'unassailable', 'believe', 'last', 'year', 'job', 'find', 'good', 'candidate', 'spearhead', 'standard', 'technology', 'nist', 'government', 'body', 'charge', 'collect', 'testing', 'standardize', 'cryptographic', 'algorithm', 'public', 'use', 'nist', 'run', 'dozen', 'potential', 'post\xadquantum', 'algorithm', 'gauntlet', 'test', 'make', 'available', 'outside', 'testing', 'process', 'winnow', 'field', 'finalist', 'nist', 'announce', 'call', 'crystalskyber', 'take', 'approach', 'believe', 'robust', 'enough', 'counter', 'quantum', 'attack', 'first', 'officially', 'recommend', 'public', 'use', 'company', 'government', 'adopt', 'encrypt', 'datum', 'hold', 'answer', 'help', 'determine', 'trajectory', 'cybersecurity', 'near', 'term', 'far', 'settle', 'history', 'suggest', 'faith', 'unbreakability', 'often', 'misplace', 'year', 'seemingly', 'impenetrable', 'encryption', 'candidate', 'fall', 'surprisingly', 'simple', 'attack', 'computer', 'scientist', 'find', 'curious', 'crossroad', 'unsure', 'postquantum', 'algorithm', 'truly', 'unassailable', 'believe', 'distinction', 'heart', 'modern', 'encryption', 'security', 'secure', 'secret', 'message', 'always', 'tie', 'difficult', 'math', 'problem', 'recently', 'cryptography', 'barely', 'mathematical', 'ancient', 'greece', 'military', 'leader', 'encode', 'message', 'use', 'scytale', 'cylindrical', 'device', 'reveal', 'hidden', 'message', 'strip', 'seemingly', 'jumbled', 'text', 'wind', 'century', 'later', 'roman', 'historian', 'describe', 'code', 'often', 'attribute', 'involve', 'shift', 'letter', 'message', 'spot', 'alphabet', 'example', 'write', 'company', 'move', 'away', 'set', 'qubit', 'record', 'favor', 'practical', 'hardware', 'longterm', 'goal', 'history', 'modern', 'world', 'secret', 'code', 'frequently', 'break', '16th', 'century', 'decade', 'spend', 'imprison', 'cousin', 'queen', 'scot', 'use', 'elaborate', 'symbolbase', 'cipher', 'encode', 'hundred', 'letter', 'aim', 'secure', 'freedom', 'regain', 'throne', 'prevail', 'team', 'spy', 'codebreaker', 'decode', 'copy', 'letter', 'one', 'seal', 'fate', 'mary', 'approve', 'plan', 'assassinate', 'word', 'sett', 'gentleman', 'woork', 'response', 'eventually', 'order', 'cousin', 'behead', 'codebreaker', 'crack', 'code', 'early', 'enigma', 'machine', 'invent', 'end', 'world', 'war', 'later', 'share', 'intel', 'british', 'codebreaker', 'crack', 'advanced', 'version', 'enigma', 'pass', 'theoretical', 'computer', 'scientist', 'halfjokingly', 'refer', 'time', '1970', 'dark', 'age', 'cryptography', 'cryptography', 'really', 'scientific', 'field', 'say', 'artist', 'attacker', 'need', 'artistic', 'skill', 'invent', 'encryption', 'scheme', 'deploy', 'clever', 'person', 'figure', 'break', 'go', 'change', 'pass', 'say', 'cryptographer', 'diffie', 'describe', 'novel', 'way', 'people', 'devise', 'key', 'know', 'use', 'pass', 'secret', 'message', 'crucially', 'meet', 'groundbreaking', 'notion', 'previously', 'sender', 'receiver', 'physically', 'possess', 'key', 'encoding', 'decode', 'message', 'encode', 'enigma', 'machine', 'example', 'recipient', 'need', 'key', 'sheet', 'reveal', 'initial', 'encryption', 'setting', 'secret', 'diffiehellman', 'strategy', 'people', 'build', 'key', 'use', 'straightforward', 'mathematical', 'problem', 'easy', 'compute', 'direction', 'laborious', 'work', 'people', 'want', 'communicate', 'secretly', 'usually', 'designate', 'setup', 'pick', 'secret', 'number', 'together', 'agree', 'pair', 'number', 'share', 'publicly', 'big', 'prime', 'call', 'base', 'next', 'carry', 'series', 'mathematical', 'operation', 'combine', 'private', 'number', 'prime', 'base', 'exchange', 'result', 'carry', 'series', 'mathematical', 'operation', 'new', 'number', 'end', 'operation', 'number', 'order', 'arrive', 'answer', 'digit', 'answer', 'become', 'encryption', 'eavesdropper', 'intercept', 'transmission', 'often', 'nickname', 'eve', 'win', 'able', 'easily', 'unravel', 'mathematical', 'jumble', 'know', 'least', 'private', 'number', 'start', 'test', 'number', 'bruteforce', 'approach', 'require', 'unreasonable', 'amount', 'calculation', 'complicated', 'problem', 'solve', 'call', 'find', 'discrete', 'logarithm', 'diffiehellman', 'approach', 'still', 'use', 'today', 'secure', 'vpns', 'example', 'integral', 'postquantum', 'scheme', 'paper', 'diffie', 'hellman', 'note', 'exist', 'algorithm', 'capable', 'solve', 'discrete', 'log', 'problem', 'reasonable', 'amount', 'time', 'still', 'go', 'introduce', 'first', 'time', 'notion', 'oneway', 'function', 'basis', 'secure', 'cryptography', 'today', 'secure', 'online', 'interaction', 'involve', 'authentication', 'digital', 'signature', 'example', 'base', 'general', 'idea', 'mathematical', 'proof', 'problem', 'rely', 'oneway', 'function', 'possibility', 'remain', 'discover', 'efficient', 'scheme', 'crack', 'today', 'online', 'transaction', 'begin', 'kind', 'digital', 'handshake', 'security', 'handshake', 'often', 'guarantee', 'math', 'problem', 'presume', 'difficult', 'popular', 'encryption', 'scheme', 'use', 'today', 'introduce', 'trio', 'young', 'computer', 'scientist', 'energize', 'diffie', 'hellman', 'paper', 'call', 'approach', 'rsa', 'last', 'name', 'scientist', 'adi', 'shamir', 'rsa', 'base', 'difficulty', 'find', 'prime', 'factor', 'relative', 'ease', 'multiply', 'together', 'bit', 'different', 'diffiehellman', 'approach', 'diffiehellman', 'share', 'secret', 'allow', 'user', 'devise', 'key', 'insecure', 'channel', 'internet', 'key', 'use', 'disguise', 'message', 'use', 'key', 'base', 'big', 'prime', 'number', 'encrypt', 'message', 'unlock', 'rsa', 'secure', 'datum', 'send', 'person', 'quickly', 'become', 'popular', 'publickey', 'encryption', 'method', 'easy', 'use', 'adapt', 'time', 'new', 'algorithm', 'emerge', 'factor', 'fast', 'computer', 'become', 'powerful', 'nist', 'recommend', 'use', 'large', 'large', 'number', 'security', 'number', 'represent', 'binary', 'form', '1', 'binary', 'digit', 'well', 'know', 'bit', 'number', 'example', 'write', 'binary', 'bit', 'nist', 'currently', 'recommend', 'use', 'key', 'represent', 'least', 'bit', 'correspond', 'number', 'digit', 'date', 'large', 'number', 'factor', 'prime', 'make', 'digit', 'process', 'take', 'nearly', 'hour', 'compute', 'time', 'strength', 'rsa', 'even', 'uncrackable', 'easy', 'keep', 'ante', 'make', 'computationally', 'impractical', 'break', 'however', 'threat', 'different', 'type', 'emerge', 'devise', 'quantum', 'computer', 'solve', 'factoring', 'problem', 'reasonable', 'amount', 'time', 'double', 'threat', 'approach', 'also', 'conquer', 'discrete', 'log', 'problem', 'diffiehellman', 'approach', 'paper', 'ignite', 'excitement', 'anxiety', 'want', 'build', 'quantum', 'computer', 'recognize', 'threat', 'pose', 'cybersecurity', 'fortunately', 'cryptographer', 'quantum', 'computer', 'last', 'decade', 'cybersecurity', 'play', 'increasingly', 'intricate', 'game', 'researcher', 'perpetually', 'build', 'breaking', 'attempt', 'break', 'new', 'candidate', 'year', 'researcher', 'technology', 'sweden', 'estimate', 'take', 'quantum', 'computer', 'compose', 'quantum', 'bit', 'qubit', 'hour', 'break', 'today', '2048bit', 'rsa', 'security', 'current', 'stateoftheart', 'machine', 'nowhere', 'close', 'size', 'large', 'quantum', 'computer', 'date', 'build', 'debut', 'last', 'year', 'qubit', 'rsa', 'consider', 'immediate', 'risk', 'quantum', 'attack', 'depend', 'largely', 'ask', 'say', 'computer', 'scientist', 'te', 'short', 'cofounde', 'cybersecurity', 'company', 'keyfactor', 'see', 'cultural', 'divide', 'theorist', 'study', 'mathematic', 'encryption', 'cryptographer', 'work', 'implementation', 'end', 'seem', 'nigh', 'talk', 'theoretical', 'computer', 'scientist', 'rsa', 'imagine', 'shorter', 'say', 'add', 'existence', 'shor', 'point', 'end', 'encryption', 'know', 'many', 'cryptographer', 'implement', 'realworld', 'security', 'system', 'less', 'concerned', 'quantum', 'future', 'today', 'clever', 'hacker', 'people', 'try', 'factor', 'efficiently', 'thousand', 'year', 'know', 'method', 'require', 'computer', 'exist', 'decru', 'cryptographer', 'belgium', 'say', 'quantum', 'threat', 'take', 'seriously', 'hard', 'know', 'rsa', 'fall', 'quantum', 'computer', 'year', 'long', 'never', 'long', 'quantum', 'computer', 'exist', 'say', 'speculative', 'way', 'say', 'pass', 'certain', 'threat', 'safe', 'say', 'existence', 'quantum', 'mean', 'crack', 'problem', 'right', 'ready', 'say', 'lily', 'mathematician', 'manage', 'nist', 'cryptographic', 'technology', 'group', 'work', 'ongoing', 'effort', 'produce', 'postquantum', 'encryption', 'standard', 'arrive', 'year', 'quantum', 'computer', 'loom', 'horizon', 'rsa', 'diffiehellman', 'encryption', 'scheme', 'leave', 'vulnerable', 'find', 'quantumresistant', 'cryptographic', 'scheme', 'easy', 'mathematical', 'problem', 'computationally', 'hard', 'last', 'decade', 'cybersecurity', 'play', 'increasingly', 'intricate', 'game', 'researcher', 'perpetually', 'build', 'breaking', 'attempt', 'break', 'new', 'candidate', 'push', 'pull', 'already', 'emerge', 'nist', 'postquantum', 'program', 'cryptographer', 'find', 'fatal', 'flaw', 'survive', 'round', 'nist', 'analysis', 'month', 'later', 'nist', 'list', 'winnow', 'decru', 'ku', 'announce', 'break', 'finalist', 'call', 'sike', 'sike', 'develop', 'year', 'ago', 'brainchild', 'collaboration', 'researcher', 'engineer', 'versaille', 'elsewhere', 'base', 'special', 'mathematical', 'map', 'call', 'isogeny', 'make', 'connection', 'elliptic', 'curve', 'map', 'turn', 'encryption', 'communication', 'outsider', 'eavesdrop', 'know', 'map', 'quantum', 'computer', 'become', 'powerful', 'enough', 'theoretically', 'crack', 'encryption', 'algorithm', 'keep', 'safe', 'race', 'find', 'new', 'one', 'leuven', 'decru', 'castryck', 'devise', 'way', 'use', 'socalled', 'isogenie', 'build', 'new', 'fast', 'encryption', 'approach', 'break', 'difficult', 'version', 'sike', 'hour', 'compute', 'time', 'use', 'ordinary', 'desktop', 'computer', 'group', 'find', 'way', 'even', 'fast', 'decru', 'almost', 'accidentally', 'week', 'declare', 'alternate', 'nist', 'finalist', 'try', 'break', 'insist', 'decru', 'try', 'generalize', 'say', 'case', 'sike', 'rainbow', 'illustrate', 'realworld', 'tension', 'drive', 'effort', 'find', 'quantumproof', 'algorithm', 'hand', 'say', 'find', 'problem', 'hard', 'quantum', 'computer', 'classical', 'computer', 'implementation', 'transform', 'hard', 'problem', 'use', 'realworld', 'cryptographic', 'system', 'even', 'today', 'welldefine', 'problem', 'shorter', 'say', 'difficult', 'predict', 'prevent', 'loophole', 'operate', 'system', 'device', 'market', 'today', 'interoperability', 'testing', 'certification', 'test', 'say', 'make', 'sure', 'implement', 'correctly', 'also', 'securely', 'mathematical', 'problem', 'sike', 'base', 'seem', 'computationally', 'hard', 'many', 'different', 'map', 'construct', 'curve', 'even', 'oneway', 'problem', 'therefore', 'quantumproof', 'flaw', 'design', 'reveal', 'much', 'transmit', 'information', 'decru', 'castryck', 'crack', 'inadvertently', 'find', 'way', 'expose', 'enough', 'connect', 'point', 'give', 'entire', 'thing', 'scheme', 'fare', 'well', 'first', 'postquantum', 'encryption', 'standardize', 'crystalskyber', 'deliver', 'security', 'approach', 'involve', 'problem', 'lattice', 'mathematical', 'model', 'array', 'point', 'main', 'family', 'postquantum', 'cryptographic', 'method', 'lattice', 'approach', 'general', 'encryption', 'scheme', 'rsa', 'use', 'task', 'secure', 'online', 'communication', 'approve', 'algorithm', 'design', 'authenticate', 'digital', 'signature', 'ensure', 'digital', 'document', 'fraudulently', 'sign', 'nist', 'plan', 'standardize', 'spring', 'sike', 'break', 'also', 'standardize', 'next', 'year', 'long', 'survive', 'round', 'scrutiny', 'mathematician', 'prove', 'oneway', 'function', 'say', 'pass', 'pattern', 'always', 'characterize', 'cryptography', 'continue', 'back', 'catandmouse', 'game', 'game', 'designer', 'propose', 'new', 'candidate', 'construction', 'designer', 'try', 'break', 'say', 'course', 'field', 'come', 'implementable', 'provably', 'oneway', 'function', 'settle', 'matter', 'encryption', 'forever', 'time', 'cryptographer', 'remain', 'messy', 'limbo', 'convincingly', 'robust', 'encryption', 'scheme', 'trust', 'perfect', 'math', 'problem', 'take', 'limbo', 'sticky', 'mess', 'cook', 'armchair', 'algebraist', 'long', 'weekend', 'strike', 'balance', 'math', 'cryptography', 'computational', 'hardness', 'side', 'easy', 'implementation', 'stray', 'far', 'property', 'become', 'vulnerable', 'future', 'hang', 'balance', 'past', 'present', 'future', 'security', 'datum', 'everywhere', 'pressure', 'orne', 'science', 'writer', 'base']","<p>Cryptographers want encryption schemes that are impossible for tomorrow’s quantum computers to crack. There’s only one catch: they might not exist.</p>
"
"Using data, AI, and cloud to transform real estate",https://www.technologyreview.com/2023/10/16/1081609/using-data-ai-and-cloud-to-transform-real-estate/,2023-10-16,"In partnership withInfosys Cobalt Many industries have reached an inflection point with hybrid and remote work, emerging advanced technologies like AI and cloud computing, and increased demands for sustainable frameworks to mitigate emissions. According to Sandeep Davé, chief digital and technology officer at global firm CBRE, the commercial real estate industry is no stranger to these changes and challenges. Delivering the best outcomes and optimizing operations means forging clear digital strategies for business transformation that are focused on the root of client and business problems. “Through our ethos, we don't chase the shiny object, whether it's AI or any other technology,” says Davé. “We aren't saying, well, what can I do for the purposes of doing AI, but what is the business problem that I'm trying to solve?” While developing a foundational strategy for transformation that is based on enabling the core business is key, advanced technologies like AI and machine learning are powerful tools that can unlock efficiencies across the entire real estate lifecycle, Davé says. AI/ML are incredibly powerful tools to become data-driven from analytics tools that can predict asset failures and market movements to infusing efficiencies across operations. “There are new and different ways in which real estate is viewed, transacted, managed, and all of that gets enabled through data and technology,” says Davé. Beyond operational improvements, advanced and smart technologies can also help reduce emissions. According to a 2019 International Energy Agency global status report, the real estate industry contributed 39% of global carbon emissions attributed to both construction and the life cycle of the asset. As a result, sustainability initiatives have become a priority for a firm of the scale of CBRE, says Davé. “At the time of managing the building, there are many solutions that offer instant gratification, stick sensors up, light up a building, and they all work well if all you need to do is to light up a building. But in order to meet the scale and the global net-zero targets that our clients have set, our solutions need to be at portfolio scale and need to be multidimensional.” Becoming data-driven remains imperative for any organization looking to keep up with the varied and changing needs of clients adapting to changes in the market and technology landscape. “The industry finds itself in the midst of two of the most dominant trends of our time, from return to work to sustainability,” says Davé. “We've seen a step change in technology in terms of what cloud and AI gives us, and all of that, I think, is going to also drive tremendous change. And we'll continue to push the bounds of technology in the service of our clients’ real-world challenges.” This episode of Business Lab is produced in partnership with Infosys Cobalt. Laurel Ruma: From MIT Technology Review, I'm Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic is digital transformation and how clear strategy and emerging technologies such as cloud computing and AI can help transform industries including commercial real estate and propel adoption of sustainability goals.Two words for you: inspiring transformation.My guest today is Sandeep Davé, the chief digital and technology officer at CBRE.This podcast is produced in partnership with Infosys Cobalt.Welcome, Sandeep. Sandeep Davé: Thank you. Thank you for having me on the show. Laurel: Could you talk a bit more about your role at CBRE and what it means to specialize in digital strategy and business transformation at the world's largest commercial real estate company? Sandeep: Sure. And for your audience, maybe perhaps I can give a little bit of a context around CBRE and what we do. So as you said, CBRE is the world's largest commercial real estate services, brokerage, and investment firm. And essentially in that capacity, we help our clients across the entire lifecycle of real estate from investing in an asset to leasing space, to designing it, to building it out, and managing it on an ongoing basis. We are a global company, Fortune 130, operating in over 100-plus countries. We process over 450 billion in transaction volume. We've managed over 7 billion square feet, so it's a sprawling operation. And in my role as chief digital and technology officer, I oversee all aspects of technology for the company, from digital strategy, tech enablement for each one of our business lines, our venture investments and partnerships, and our traditional technology infrastructure as well. Now, with respect to your question around what does it mean to drive digital strategy or business transformation in commercial real estate. Like every industry, technology is impacting commercial real estate. It's a moment of tremendous change. Real estate decisions are getting microsegmented. There are new and different ways in which real estate is viewed, transacted, managed, and all of that gets enabled through data and technology. And I work with the leaders in the company to navigate these changes. Laurel: So what does technological transformation then look like for a global firm like CBRE? Sandeep: Sure. CBRE is a 100-plus year old company and in an industry that was traditionally slow to adopt new technologies. And so we took the transformation in two parts. And in fact, even before I discuss those two parts, I'd like to just step back and set some context. I've spent time in many industries, commercial real estate, financial services, and I have seen two models of digital transformation emerge. There's no model that's right or wrong. But in one model, a company goes out there and declares themselves to be a technology company and the decisions that they take are aligned with them becoming a technology company. They set up software P&Ls, they set up their own venture investment capabilities, and the pursuit is towards those goals. Whereas we are very clear about who we are, which is that we are the largest and best commercial real estate services company, investment brokerage company in the world. But we also realize that data, insights, technology is going to be critical to deliver the best outcomes for our clients. So in that context, our transformation is based on enabling the core business. So for that purpose, we had to do two things. The first was to build a foundation. First few years we were focused on building a global talent pool that I'm very proud of. We've migrated 100% to the cloud, agile, and over the years we've built an enterprise data platform that is meaningful to our transformation. With that foundation in place, we focused on the second part of our transformation, which is to have a clear digital strategy for each line of businesses and therefore focusing on the most pressing problems for our clients. Finally, given the breadth and scale of our operations, we sit on tremendous amounts of data and we've made meaningful strides in creating a data advantage for the company. Laurel: Enterprises are always looking for an advantage, especially with data. Could you outline what that does mean for CBRE, like opportunities and benefits working with all of this data? Sandeep: Sure. Very interesting space and I spent a lot of time in consumer financial services in other consumer industries, and the one difference that was stark to me as I started working in commercial real estate is that this industry was not facing a big data problem, at least not then when each of the buildings were not centralized, but it was more of a siloed data problem. So by no means am I trivializing the data challenges that I faced in financial services. There were many, but the one thing that I had a benefit of was transaction after transaction of structured data that allowed me to slice and dice and understand my customer's behaviors. Whereas, what happens in the commercial real estate industry is that data is very siloed, but yet anchored around a property. And even the definition of the property is different depending on who you ask. Whether you are assessing or valuing the entire building versus I'm trying to lease one floor in that building, what I call property changes. So what we had to do first and foremost was to break down those barriers and we created an enterprise data platform that, against a standard taxonomy, ingests data from 300-plus different data sources and now manages billions of data points. Having sat on that foundation, we now have the ability to generate tremendous insights for our clients. So we are able to, our 350-plus clients, provide them a range of insights at a portfolio scale to a single building level, operational insights, financial insights, occupancy, energy, health and safety risks, so on and so forth. And that same foundation now allows us to unlock efficiencies at a larger scale and apply ML models to generate deeper insights and drive greater transformation. Laurel: So from the automotive industry to healthcare to corporate banking, AI has emerged as a powerful tool across supply chains and industries, and it's changing how enterprises operate and what services they can offer to clients. So in real estate and asset management, what does AI look like and what decisions, especially in the C-suite, are made using this technology to drive better business outcomes? Sandeep: Your question is spot on, which is that how do we use the technology to drive data business outcomes? And through our ethos, we don't chase the shiny object, whether it's AI or any other technology. We aren't saying, well, what can I do for the purposes of doing AI, but what is the business problem that I'm trying to solve? And we have actually been focused on AI/ML, even before the public awareness soared post the release of ChatGPT. But we look at this in the spectrum of unlocking efficiencies to enable differentiation and across the entire lifecycle of real estate, there is a play in both of those–unlocking efficiencies and enabling differentiation. The way the decisions around how property investments are made, how market movements are tracked, are very data-driven now. The decisions around how space is managed is very data-driven now.   Laurel: So could you give an example of how AI and cloud computing could be used to build smart buildings, structures, and technology capabilities? Sandeep: Sure. Using an example is great because this is such a wide field, both commercial real estate and the application of AI/ML in commercial real estate. In the area of smart buildings, we are focused on enabling three outcomes for our clients: energy, efficiency, and experience; which is how do they manage their energy usage, how do they get more efficient in everything that they do with respect to managing a property? And then what is the workplace experience for the employees in a building? And let me just take an example of efficiency. There was a certain way in which buildings were managed previously. And with the application of cloud native global technology solutions, that we have that are infused with AI/ML, we are now able to manage facilities in a smarter manner, what we call Smart FM. We are able to look at occupancy and dynamically clean the environment rather than having people cleaning the environment on a regular schedule, we are able to save our clients a lot of money with respect to dynamic cleaning. We are able to detect anomalies in how we manage buildings and assets, which can then further reduce the false alarms and the number of truck rolls that need to happen with respect to managing a building. So there are so many different ways in which we infuse AI/ML. Laurel: That's really interesting. So according to a 2019 International Energy Agency global status report, the real estate industry contributed 39% of global carbon emissions. Could you offer us an example of how smart technologies, like what you're talking about now, could boost operational efficiencies and then also help reduce emissions and improve sustainability? Sandeep: Yeah, absolutely. I think there are two ways in which we look at this space. As you indicated that 39% of carbon emissions are contributed by real estate, and so therefore the industry has a huge role to play. Part of those emissions are at the time of construction itself, and the remainder is for the life cycle of the asset. Right at the time of construction, we've built capabilities where we are able to design and redesign based on a certain energy emission target for a building. We are able to select our suppliers based on a certain energy emission target for the building. And then at the time of managing the building, there are many solutions that offer instant gratification, stick sensors up, light up a building, and they all work well if all you need to do is to light up a building. But in order to meet the scale and the global net-zero targets that our clients have set, our solutions need to be at portfolio scale and need to be multidimensional. And so therefore what we do is we have the ability to ingest data from various different sources, from sensors, and are able to harmonize that and land it against a standard taxonomy. And then we are able to assess that in many different ways. We are able to bring together different aspects of looking at energy and looking at occupancy and managing the building based on the occupancy in the building. Those interventions, for example, at one of our clients recently, meant we were able to stand up those interventions at 25-plus buildings. And that led to a reduction in peak usage energy for them and also reduction in reactive maintenance work orders, reducing truck rolls, and supporting their energy goals. Laurel: So you also are talking about this on a portfolio level. And CBRE's own corporate responsibility and environmental social and governance or ESG goals are as follows: scale to a low-carbon future, create opportunities for employees to thrive through diversity, equity, inclusion initiatives and to build trust through integrity. How is CBRE using emerging technologies like artificial intelligence and machine learning to then become more efficient and also meet those ESG goals? Sandeep: I think a lot of the ESG problem is a data problem. Today, if you talk to most who are trying and most are grappling with this problem right now, what they'll say is that do they have a clear line of sight of what their, for example, scope 1 and scope 2, scope 3 emissions are? Are they able to capture the data in a reliable manner, audit it in a reliable manner, and then report against it? While they report against it, can they also manage usage? Because if you are able to look at the data, then you will know where corrective actions are required. Building on the foundation of the data platform that we've built on, which is 100% cloud native, by the way, we can then, on top of that, apply these technologies where we can apply ML models to detect anomalies. We take a digital twins perspective to map our data against the buildings and manage the end-to-end lifecycle of that real estate process. Laurel: So looking into the future Sandeep, which I know is difficult, but how do you anticipate the commercial real estate industry transforming in the next five years as emerging technologies, hybrid ways of working, and calls for greater sustainability initiatives become more prevalent? Sandeep: There's always a risk of predictions to a five-year out prediction at a time when the pace of change is dizzying. However, since you've asked, maybe I'll share three points. The first is, let's take an example of financial services. The first is related to just insights. If you take the wealth management industry, information used to be an advantage. Now information is a commodity. Insights are an advantage. Insights that were an advantage yesterday are less of an advantage today, but the wealth manager's job is more important than ever. They're now closer to the client than ever before and they're giving more meaningful advice than ever before. We are seeing very similar changes in commercial real estate today, whether that's investment or leasing decisions, which are getting microsegmented and extremely data-driven. What matters to one client is not going to matter to another client. One client may be most focused on energy goals for their investments. The other client may be focused on labor analytics or gentrification metrics. So how we see the industry moving towards those microsegmented decisions that are data-driven. So that's one area of transformation that I see in the industry and how deeper data insights are going to be really relevant in terms of enabling that. Second, if we take any outsourcing industry, which is also part of what we do, whether that's in BPO [business process outsourcing] or technology outsourcing, they all started with augmentation. You need 10 people. I'm going to give you 10 people. But overtime labor arbitrage moves. You run out of steam quickly and you move to delivering outcomes. And once you have to deliver outcomes, then the model has to be that “I have the best processes, I have the best technology, I have the best data and the best insights, and therefore I am confidently telling you that I'm in a better position to deliver better outcomes for you.” But at the same time, deliver better margins for the company. That's exactly what we are doing in our outsourcing business, and that's a transformation that I see happening in the industry to move away from staff org, if you will, to outcomes based. And third, I think it's a pivotal moment from many different ways. The industry finds itself in the midst of two of the most dominant trends of our time from return to work to sustainability. We've seen a step change in technology in terms of what cloud and AI gives us and all of that I think is going to also drive tremendous change and we'll continue to push the bounds of technology in the service of our clients’ real-world challenges. Laurel: That is certainly comprehensive. Sandeep, thank you so much for joining us today on the Business Lab. Sandeep: Thank you. Thank you for having me. Laurel: That was Sandeep Davé, the chief digital technology officer at CBRE who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review overlooking the Charles River.That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of Insights, the custom publishing division at MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can also find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com. This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.","In partnership withInfosys Cobalt Many industries have reached an inflection point with hybrid and remote work , emerging advanced technologies like AI and cloud computing , and increased demands for sustainable frameworks to mitigate emissions . According to Sandeep Davé , chief digital and technology officer at global firm CBRE , the commercial real estate industry is no stranger to these changes and challenges . Delivering the best outcomes and optimizing operations means forging clear digital strategies for business transformation that are focused on the root of client and business problems . “ Through our ethos , we do n't chase the shiny object , whether it 's AI or any other technology , ” says Davé . “ We are n't saying , well , what can I do for the purposes of doing AI , but what is the business problem that I 'm trying to solve ? ” While developing a foundational strategy for transformation that is based on enabling the core business is key , advanced technologies like AI and machine learning are powerful tools that can unlock efficiencies across the entire real estate lifecycle , Davé says . AI/ML are incredibly powerful tools to become data-driven from analytics tools that can predict asset failures and market movements to infusing efficiencies across operations . “ There are new and different ways in which real estate is viewed , transacted , managed , and all of that gets enabled through data and technology , ” says Davé . Beyond operational improvements , advanced and smart technologies can also help reduce emissions . According to a 2019 International Energy Agency global status report , the real estate industry contributed 39 % of global carbon emissions attributed to both construction and the life cycle of the asset . As a result , sustainability initiatives have become a priority for a firm of the scale of CBRE , says Davé . “ At the time of managing the building , there are many solutions that offer instant gratification , stick sensors up , light up a building , and they all work well if all you need to do is to light up a building . But in order to meet the scale and the global net-zero targets that our clients have set , our solutions need to be at portfolio scale and need to be multidimensional. ” Becoming data-driven remains imperative for any organization looking to keep up with the varied and changing needs of clients adapting to changes in the market and technology landscape . “ The industry finds itself in the midst of two of the most dominant trends of our time , from return to work to sustainability , ” says Davé . “ We 've seen a step change in technology in terms of what cloud and AI gives us , and all of that , I think , is going to also drive tremendous change . And we 'll continue to push the bounds of technology in the service of our clients ’ real-world challenges. ” This episode of Business Lab is produced in partnership with Infosys Cobalt . Laurel Ruma : From MIT Technology Review , I 'm Laurel Ruma and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic is digital transformation and how clear strategy and emerging technologies such as cloud computing and AI can help transform industries including commercial real estate and propel adoption of sustainability goals.Two words for you : inspiring transformation.My guest today is Sandeep Davé , the chief digital and technology officer at CBRE.This podcast is produced in partnership with Infosys Cobalt.Welcome , Sandeep . Sandeep Davé : Thank you . Thank you for having me on the show . Laurel : Could you talk a bit more about your role at CBRE and what it means to specialize in digital strategy and business transformation at the world 's largest commercial real estate company ? Sandeep : Sure . And for your audience , maybe perhaps I can give a little bit of a context around CBRE and what we do . So as you said , CBRE is the world 's largest commercial real estate services , brokerage , and investment firm . And essentially in that capacity , we help our clients across the entire lifecycle of real estate from investing in an asset to leasing space , to designing it , to building it out , and managing it on an ongoing basis . We are a global company , Fortune 130 , operating in over 100-plus countries . We process over 450 billion in transaction volume . We 've managed over 7 billion square feet , so it 's a sprawling operation . And in my role as chief digital and technology officer , I oversee all aspects of technology for the company , from digital strategy , tech enablement for each one of our business lines , our venture investments and partnerships , and our traditional technology infrastructure as well . Now , with respect to your question around what does it mean to drive digital strategy or business transformation in commercial real estate . Like every industry , technology is impacting commercial real estate . It 's a moment of tremendous change . Real estate decisions are getting microsegmented . There are new and different ways in which real estate is viewed , transacted , managed , and all of that gets enabled through data and technology . And I work with the leaders in the company to navigate these changes . Laurel : So what does technological transformation then look like for a global firm like CBRE ? Sandeep : Sure . CBRE is a 100-plus year old company and in an industry that was traditionally slow to adopt new technologies . And so we took the transformation in two parts . And in fact , even before I discuss those two parts , I 'd like to just step back and set some context . I 've spent time in many industries , commercial real estate , financial services , and I have seen two models of digital transformation emerge . There 's no model that 's right or wrong . But in one model , a company goes out there and declares themselves to be a technology company and the decisions that they take are aligned with them becoming a technology company . They set up software P & Ls , they set up their own venture investment capabilities , and the pursuit is towards those goals . Whereas we are very clear about who we are , which is that we are the largest and best commercial real estate services company , investment brokerage company in the world . But we also realize that data , insights , technology is going to be critical to deliver the best outcomes for our clients . So in that context , our transformation is based on enabling the core business . So for that purpose , we had to do two things . The first was to build a foundation . First few years we were focused on building a global talent pool that I 'm very proud of . We 've migrated 100 % to the cloud , agile , and over the years we 've built an enterprise data platform that is meaningful to our transformation . With that foundation in place , we focused on the second part of our transformation , which is to have a clear digital strategy for each line of businesses and therefore focusing on the most pressing problems for our clients . Finally , given the breadth and scale of our operations , we sit on tremendous amounts of data and we 've made meaningful strides in creating a data advantage for the company . Laurel : Enterprises are always looking for an advantage , especially with data . Could you outline what that does mean for CBRE , like opportunities and benefits working with all of this data ? Sandeep : Sure . Very interesting space and I spent a lot of time in consumer financial services in other consumer industries , and the one difference that was stark to me as I started working in commercial real estate is that this industry was not facing a big data problem , at least not then when each of the buildings were not centralized , but it was more of a siloed data problem . So by no means am I trivializing the data challenges that I faced in financial services . There were many , but the one thing that I had a benefit of was transaction after transaction of structured data that allowed me to slice and dice and understand my customer 's behaviors . Whereas , what happens in the commercial real estate industry is that data is very siloed , but yet anchored around a property . And even the definition of the property is different depending on who you ask . Whether you are assessing or valuing the entire building versus I 'm trying to lease one floor in that building , what I call property changes . So what we had to do first and foremost was to break down those barriers and we created an enterprise data platform that , against a standard taxonomy , ingests data from 300-plus different data sources and now manages billions of data points . Having sat on that foundation , we now have the ability to generate tremendous insights for our clients . So we are able to , our 350-plus clients , provide them a range of insights at a portfolio scale to a single building level , operational insights , financial insights , occupancy , energy , health and safety risks , so on and so forth . And that same foundation now allows us to unlock efficiencies at a larger scale and apply ML models to generate deeper insights and drive greater transformation . Laurel : So from the automotive industry to healthcare to corporate banking , AI has emerged as a powerful tool across supply chains and industries , and it 's changing how enterprises operate and what services they can offer to clients . So in real estate and asset management , what does AI look like and what decisions , especially in the C-suite , are made using this technology to drive better business outcomes ? Sandeep : Your question is spot on , which is that how do we use the technology to drive data business outcomes ? And through our ethos , we do n't chase the shiny object , whether it 's AI or any other technology . We are n't saying , well , what can I do for the purposes of doing AI , but what is the business problem that I 'm trying to solve ? And we have actually been focused on AI/ML , even before the public awareness soared post the release of ChatGPT . But we look at this in the spectrum of unlocking efficiencies to enable differentiation and across the entire lifecycle of real estate , there is a play in both of those–unlocking efficiencies and enabling differentiation . The way the decisions around how property investments are made , how market movements are tracked , are very data-driven now . The decisions around how space is managed is very data-driven now . Laurel : So could you give an example of how AI and cloud computing could be used to build smart buildings , structures , and technology capabilities ? Sandeep : Sure . Using an example is great because this is such a wide field , both commercial real estate and the application of AI/ML in commercial real estate . In the area of smart buildings , we are focused on enabling three outcomes for our clients : energy , efficiency , and experience ; which is how do they manage their energy usage , how do they get more efficient in everything that they do with respect to managing a property ? And then what is the workplace experience for the employees in a building ? And let me just take an example of efficiency . There was a certain way in which buildings were managed previously . And with the application of cloud native global technology solutions , that we have that are infused with AI/ML , we are now able to manage facilities in a smarter manner , what we call Smart FM . We are able to look at occupancy and dynamically clean the environment rather than having people cleaning the environment on a regular schedule , we are able to save our clients a lot of money with respect to dynamic cleaning . We are able to detect anomalies in how we manage buildings and assets , which can then further reduce the false alarms and the number of truck rolls that need to happen with respect to managing a building . So there are so many different ways in which we infuse AI/ML . Laurel : That 's really interesting . So according to a 2019 International Energy Agency global status report , the real estate industry contributed 39 % of global carbon emissions . Could you offer us an example of how smart technologies , like what you 're talking about now , could boost operational efficiencies and then also help reduce emissions and improve sustainability ? Sandeep : Yeah , absolutely . I think there are two ways in which we look at this space . As you indicated that 39 % of carbon emissions are contributed by real estate , and so therefore the industry has a huge role to play . Part of those emissions are at the time of construction itself , and the remainder is for the life cycle of the asset . Right at the time of construction , we 've built capabilities where we are able to design and redesign based on a certain energy emission target for a building . We are able to select our suppliers based on a certain energy emission target for the building . And then at the time of managing the building , there are many solutions that offer instant gratification , stick sensors up , light up a building , and they all work well if all you need to do is to light up a building . But in order to meet the scale and the global net-zero targets that our clients have set , our solutions need to be at portfolio scale and need to be multidimensional . And so therefore what we do is we have the ability to ingest data from various different sources , from sensors , and are able to harmonize that and land it against a standard taxonomy . And then we are able to assess that in many different ways . We are able to bring together different aspects of looking at energy and looking at occupancy and managing the building based on the occupancy in the building . Those interventions , for example , at one of our clients recently , meant we were able to stand up those interventions at 25-plus buildings . And that led to a reduction in peak usage energy for them and also reduction in reactive maintenance work orders , reducing truck rolls , and supporting their energy goals . Laurel : So you also are talking about this on a portfolio level . And CBRE 's own corporate responsibility and environmental social and governance or ESG goals are as follows : scale to a low-carbon future , create opportunities for employees to thrive through diversity , equity , inclusion initiatives and to build trust through integrity . How is CBRE using emerging technologies like artificial intelligence and machine learning to then become more efficient and also meet those ESG goals ? Sandeep : I think a lot of the ESG problem is a data problem . Today , if you talk to most who are trying and most are grappling with this problem right now , what they 'll say is that do they have a clear line of sight of what their , for example , scope 1 and scope 2 , scope 3 emissions are ? Are they able to capture the data in a reliable manner , audit it in a reliable manner , and then report against it ? While they report against it , can they also manage usage ? Because if you are able to look at the data , then you will know where corrective actions are required . Building on the foundation of the data platform that we 've built on , which is 100 % cloud native , by the way , we can then , on top of that , apply these technologies where we can apply ML models to detect anomalies . We take a digital twins perspective to map our data against the buildings and manage the end-to-end lifecycle of that real estate process . Laurel : So looking into the future Sandeep , which I know is difficult , but how do you anticipate the commercial real estate industry transforming in the next five years as emerging technologies , hybrid ways of working , and calls for greater sustainability initiatives become more prevalent ? Sandeep : There 's always a risk of predictions to a five-year out prediction at a time when the pace of change is dizzying . However , since you 've asked , maybe I 'll share three points . The first is , let 's take an example of financial services . The first is related to just insights . If you take the wealth management industry , information used to be an advantage . Now information is a commodity . Insights are an advantage . Insights that were an advantage yesterday are less of an advantage today , but the wealth manager 's job is more important than ever . They 're now closer to the client than ever before and they 're giving more meaningful advice than ever before . We are seeing very similar changes in commercial real estate today , whether that 's investment or leasing decisions , which are getting microsegmented and extremely data-driven . What matters to one client is not going to matter to another client . One client may be most focused on energy goals for their investments . The other client may be focused on labor analytics or gentrification metrics . So how we see the industry moving towards those microsegmented decisions that are data-driven . So that 's one area of transformation that I see in the industry and how deeper data insights are going to be really relevant in terms of enabling that . Second , if we take any outsourcing industry , which is also part of what we do , whether that 's in BPO [ business process outsourcing ] or technology outsourcing , they all started with augmentation . You need 10 people . I 'm going to give you 10 people . But overtime labor arbitrage moves . You run out of steam quickly and you move to delivering outcomes . And once you have to deliver outcomes , then the model has to be that “ I have the best processes , I have the best technology , I have the best data and the best insights , and therefore I am confidently telling you that I 'm in a better position to deliver better outcomes for you. ” But at the same time , deliver better margins for the company . That 's exactly what we are doing in our outsourcing business , and that 's a transformation that I see happening in the industry to move away from staff org , if you will , to outcomes based . And third , I think it 's a pivotal moment from many different ways . The industry finds itself in the midst of two of the most dominant trends of our time from return to work to sustainability . We 've seen a step change in technology in terms of what cloud and AI gives us and all of that I think is going to also drive tremendous change and we 'll continue to push the bounds of technology in the service of our clients ’ real-world challenges . Laurel : That is certainly comprehensive . Sandeep , thank you so much for joining us today on the Business Lab . Sandeep : Thank you . Thank you for having me . Laurel : That was Sandeep Davé , the chief digital technology officer at CBRE who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review overlooking the Charles River.That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the director of Insights , the custom publishing division at MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can also find us in print , on the web , and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com . This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'cobalt', 'many', 'industry', 'reach', 'inflection', 'point', 'hybrid', 'remote', 'work', 'emerge', 'advanced', 'technology', 'ai', 'computing', 'increase', 'demand', 'sustainable', 'framework', 'mitigate', 'emission', 'accord', 'sandeep', 'davé', 'chief', 'digital', 'technology', 'officer', 'global', 'firm', 'cbre', 'commercial', 'real', 'estate', 'industry', 'stranger', 'change', 'challenge', 'deliver', 'good', 'outcome', 'optimize', 'operation', 'mean', 'forge', 'clear', 'digital', 'strategy', 'business', 'transformation', 'focus', 'root', 'client', 'business', 'problem', 'ethos', 'chase', 'shiny', 'object', 'ai', 'technology', 'say', 'davé', 'say', 'well', 'purpose', 'ai', 'business', 'problem', 'try', 'solve', 'develop', 'foundational', 'strategy', 'transformation', 'base', 'enable', 'core', 'business', 'key', 'advanced', 'technology', 'ai', 'machine', 'learning', 'powerful', 'tool', 'unlock', 'efficiency', 'entire', 'real', 'estate', 'lifecycle', 'davé', 'say', 'aiml', 'incredibly', 'powerful', 'tool', 'become', 'datadriven', 'analytic', 'tool', 'predict', 'asset', 'failure', 'market', 'movement', 'infuse', 'efficiency', 'operation', 'new', 'different', 'way', 'real', 'estate', 'view', 'transact', 'manage', 'enable', 'datum', 'technology', 'say', 'davé', 'operational', 'improvement', 'advanced', 'smart', 'technology', 'also', 'help', 'reduce', 'emission', 'accord', 'international', 'energy', 'agency', 'report', 'real', 'estate', 'industry', 'contribute', 'global', 'carbon', 'emission', 'attribute', 'construction', 'life', 'cycle', 'asset', 'result', 'sustainability', 'initiative', 'become', 'priority', 'firm', 'scale', 'cbre', 'say', 'davé', 'time', 'manage', 'building', 'many', 'solution', 'offer', 'instant', 'gratification', 'stick', 'sensor', 'light', 'building', 'work', 'well', 'need', 'light', 'building', 'order', 'meet', 'scale', 'global', 'target', 'client', 'set', 'solution', 'need', 'portfolio', 'scale', 'need', 'multidimensional', 'become', 'datadriven', 'remain', 'imperative', 'organization', 'look', 'keep', 'varied', 'change', 'need', 'client', 'adapt', 'change', 'market', 'technology', 'landscape', 'industry', 'find', 'midst', 'dominant', 'trend', 'time', 'return', 'work', 'sustainability', 'say', 'davé', 'see', 'step', 'change', 'technology', 'term', 'cloud', 'give', 'think', 'go', 'also', 'drive', 'tremendous', 'change', 'continue', 'push', 'bound', 'technology', 'service', 'client', 'realworld', 'challenge', 'episode', 'business', 'lab', 'produce', 'partnership', 'infosy', 'cobalt', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'digital', 'transformation', 'clear', 'strategy', 'emerge', 'technology', 'cloud', 'computing', 'help', 'transform', 'industry', 'include', 'commercial', 'real', 'estate', 'propel', 'adoption', 'sustainability', 'goalstwo', 'word', 'inspire', 'transformationmy', 'guest', 'today', 'sandeep', 'davé', 'chief', 'digital', 'technology', 'officer', 'podcast', 'produce', 'partnership', 'infosy', 'cobaltwelcome', 'sandeep', 'thank', 'thank', 'show', 'laurel', 'talk', 'bit', 'role', 'cbre', 'mean', 'specialize', 'digital', 'strategy', 'business', 'transformation', 'world', 'large', 'commercial', 'real', 'estate', 'company', 'sandeep', 'sure', 'audience', 'maybe', 'perhaps', 'give', 'little', 'bit', 'context', 'cbre', 'say', 'cbre', 'world', 'large', 'commercial', 'real', 'estate', 'service', 'brokerage', 'investment', 'firm', 'essentially', 'capacity', 'help', 'client', 'entire', 'lifecycle', 'real', 'estate', 'invest', 'asset', 'leasing', 'space', 'design', 'build', 'manage', 'ongoing', 'basis', 'global', 'company', 'operate', 'country', 'process', 'transaction', 'volume', 'manage', 'square', 'foot', 'sprawl', 'operation', 'role', 'chief', 'digital', 'technology', 'officer', 'oversee', 'aspect', 'technology', 'company', 'digital', 'strategy', 'tech', 'enablement', 'business', 'line', 'venture', 'investment', 'partnership', 'traditional', 'technology', 'infrastructure', 'well', 'respect', 'question', 'mean', 'drive', 'digital', 'strategy', 'business', 'transformation', 'commercial', 'real', 'estate', 'industry', 'technology', 'impact', 'commercial', 'real', 'estate', 'moment', 'tremendous', 'change', 'real', 'estate', 'decision', 'microsegmente', 'new', 'different', 'way', 'real', 'estate', 'view', 'transact', 'manage', 'enable', 'datum', 'technology', 'work', 'leader', 'company', 'navigate', 'change', 'laurel', 'technological', 'transformation', 'look', 'global', 'firm', 'cbre', 'sandeep', 'sure', 'cbre', 'year', 'old', 'company', 'industry', 'traditionally', 'slow', 'adopt', 'new', 'technology', 'take', 'transformation', 'part', 'fact', 'even', 'discuss', 'part', 'like', 'step', 'back', 'set', 'context', 'spend', 'time', 'many', 'industry', 'commercial', 'real', 'estate', 'financial', 'service', 'see', 'model', 'digital', 'transformation', 'emerge', 'model', 'right', 'wrong', 'model', 'company', 'go', 'declare', 'technology', 'company', 'decision', 'take', 'align', 'become', 'technology', 'company', 'set', 'software', 'p', 'set', 'venture', 'investment', 'capability', 'pursuit', 'goal', 'clear', 'large', 'good', 'commercial', 'real', 'estate', 'service', 'company', 'investment', 'brokerage', 'company', 'world', 'also', 'realize', 'datum', 'insight', 'technology', 'go', 'critical', 'deliver', 'good', 'outcome', 'client', 'context', 'transformation', 'base', 'enable', 'core', 'business', 'purpose', 'thing', 'first', 'build', 'foundation', 'first', 'year', 'focus', 'build', 'global', 'talent', 'pool', 'proud', 'migrate', 'cloud', 'agile', 'year', 'build', 'enterprise', 'datum', 'platform', 'meaningful', 'transformation', 'foundation', 'place', 'focus', 'second', 'part', 'transformation', 'clear', 'digital', 'strategy', 'line', 'business', 'therefore', 'focus', 'pressing', 'problem', 'client', 'finally', 'give', 'breadth', 'scale', 'operation', 'sit', 'tremendous', 'amount', 'datum', 'make', 'meaningful', 'stride', 'create', 'data', 'advantage', 'company', 'laurel', 'enterprise', 'always', 'look', 'advantage', 'especially', 'datum', 'outline', 'mean', 'cbre', 'opportunity', 'benefit', 'work', 'datum', 'sandeep', 'sure', 'interesting', 'space', 'spend', 'lot', 'time', 'consumer', 'financial', 'service', 'consumer', 'industry', 'difference', 'stark', 'start', 'work', 'commercial', 'real', 'estate', 'industry', 'face', 'big', 'data', 'problem', 'least', 'building', 'centralize', 'siloe', 'data', 'problem', 'means', 'trivialize', 'datum', 'challenge', 'face', 'financial', 'service', 'many', 'thing', 'benefit', 'transaction', 'transaction', 'structured', 'datum', 'allow', 'slice', 'dice', 'understand', 'customer', 'behavior', 'happen', 'commercial', 'real', 'estate', 'industry', 'datum', 'siloe', 'yet', 'anchor', 'property', 'even', 'definition', 'property', 'different', 'depend', 'ask', 'assess', 'value', 'entire', 'building', 'try', 'lease', 'floor', 'building', 'call', 'property', 'change', 'first', 'foremost', 'break', 'barrier', 'create', 'enterprise', 'datum', 'platform', 'standard', 'taxonomy', 'ingest', 'datum', 'different', 'datum', 'source', 'manage', 'billion', 'data', 'point', 'sit', 'foundation', 'ability', 'generate', 'tremendous', 'insight', 'client', 'able', 'client', 'provide', 'range', 'insight', 'portfolio', 'scale', 'single', 'building', 'level', 'operational', 'insight', 'financial', 'insight', 'occupancy', 'energy', 'health', 'safety', 'risk', 'forth', 'foundation', 'allow', 'unlock', 'efficiency', 'large', 'scale', 'apply', 'ml', 'model', 'generate', 'deep', 'insight', 'drive', 'great', 'transformation', 'laurel', 'automotive', 'industry', 'healthcare', 'corporate', 'banking', 'ai', 'emerge', 'powerful', 'tool', 'supply', 'chain', 'industry', 'change', 'enterprise', 'operate', 'service', 'offer', 'client', 'real', 'estate', 'asset', 'management', 'look', 'decision', 'especially', 'csuite', 'make', 'use', 'technology', 'drive', 'well', 'business', 'outcome', 'sandeep', 'question', 'spot', 'use', 'technology', 'drive', 'datum', 'business', 'outcome', 'ethos', 'chase', 'shiny', 'object', 'ai', 'technology', 'say', 'well', 'purpose', 'ai', 'business', 'problem', 'try', 'solve', 'actually', 'focus', 'aiml', 'even', 'public', 'awareness', 'soar', 'post', 'release', 'chatgpt', 'look', 'spectrum', 'unlock', 'efficiency', 'enable', 'differentiation', 'entire', 'lifecycle', 'real', 'estate', 'play', 'unlocking', 'efficiency', 'enable', 'differentiation', 'way', 'decision', 'property', 'investment', 'make', 'market', 'movement', 'track', 'datadriven', 'decision', 'space', 'manage', 'datadriven', 'give', 'example', 'ai', 'cloud', 'computing', 'use', 'build', 'smart', 'building', 'structure', 'technology', 'capability', 'sandeep', 'sure', 'use', 'example', 'great', 'wide', 'field', 'commercial', 'real', 'estate', 'application', 'aiml', 'commercial', 'real', 'estate', 'area', 'smart', 'building', 'focus', 'enable', 'outcome', 'client', 'energy', 'efficiency', 'experience', 'manage', 'energy', 'usage', 'get', 'efficient', 'respect', 'manage', 'property', 'workplace', 'experience', 'employee', 'building', 'let', 'take', 'example', 'efficiency', 'certain', 'way', 'building', 'manage', 'previously', 'application', 'cloud', 'native', 'global', 'technology', 'solution', 'infuse', 'aiml', 'able', 'manage', 'facility', 'smart', 'manner', 'call', 'smart', 'able', 'look', 'occupancy', 'dynamically', 'clean', 'environment', 'rather', 'people', 'clean', 'environment', 'regular', 'schedule', 'able', 'save', 'client', 'lot', 'money', 'respect', 'dynamic', 'cleaning', 'able', 'detect', 'anomaly', 'manage', 'building', 'asset', 'far', 'reduce', 'false', 'alarm', 'number', 'truck', 'roll', 'need', 'happen', 'respect', 'manage', 'building', 'many', 'different', 'way', 'infuse', 'aiml', 'laurel', 'really', 'interesting', 'accord', 'international', 'energy', 'agency', 'report', 'real', 'estate', 'industry', 'contribute', 'global', 'carbon', 'emission', 'offer', 'example', 'smart', 'technology', 'talk', 'boost', 'operational', 'efficiency', 'also', 'help', 'reduce', 'emission', 'improve', 'sustainability', 'sandeep', 'absolutely', 'think', 'way', 'look', 'space', 'indicate', 'carbon', 'emission', 'contribute', 'real', 'estate', 'therefore', 'industry', 'huge', 'role', 'play', 'part', 'emission', 'time', 'construction', 'remainder', 'life', 'cycle', 'asset', 'right', 'time', 'construction', 'build', 'capability', 'able', 'design', 'redesign', 'base', 'certain', 'energy', 'emission', 'target', 'building', 'able', 'select', 'supplier', 'base', 'certain', 'energy', 'emission', 'target', 'building', 'time', 'manage', 'building', 'many', 'solution', 'offer', 'instant', 'gratification', 'stick', 'sensor', 'light', 'building', 'work', 'well', 'need', 'light', 'building', 'order', 'meet', 'scale', 'global', 'target', 'client', 'set', 'solution', 'need', 'portfolio', 'scale', 'need', 'multidimensional', 'therefore', 'ability', 'ingest', 'datum', 'various', 'different', 'source', 'sensor', 'able', 'harmonize', 'land', 'standard', 'taxonomy', 'able', 'assess', 'many', 'different', 'way', 'able', 'bring', 'together', 'different', 'aspect', 'look', 'energy', 'look', 'occupancy', 'manage', 'building', 'base', 'occupancy', 'building', 'intervention', 'example', 'client', 'recently', 'mean', 'able', 'stand', 'intervention', 'building', 'lead', 'reduction', 'peak', 'usage', 'energy', 'also', 'reduction', 'reactive', 'maintenance', 'work', 'order', 'reduce', 'truck', 'roll', 'support', 'energy', 'goal', 'also', 'talk', 'portfolio', 'level', 'corporate', 'responsibility', 'environmental', 'social', 'governance', 'esg', 'goal', 'follow', 'scale', 'lowcarbon', 'future', 'create', 'opportunity', 'employee', 'thrive', 'diversity', 'equity', 'inclusion', 'initiative', 'build', 'trust', 'integrity', 'cbre', 'use', 'emerge', 'technology', 'artificial', 'intelligence', 'machine', 'learn', 'become', 'efficient', 'also', 'meet', 'esg', 'goal', 'sandeep', 'think', 'lot', 'esg', 'problem', 'data', 'problem', 'today', 'talk', 'try', 'grapple', 'problem', 'right', 'say', 'clear', 'line', 'sight', 'example', 'scope', 'scope', 'scope', 'emission', 'able', 'capture', 'datum', 'reliable', 'manner', 'audit', 'reliable', 'manner', 'report', 'report', 'also', 'manage', 'usage', 'able', 'look', 'data', 'know', 'corrective', 'action', 'require', 'building', 'foundation', 'datum', 'platform', 'build', 'cloud', 'native', 'way', 'top', 'apply', 'technology', 'apply', 'ml', 'model', 'detect', 'anomaly', 'take', 'digital', 'twin', 'perspective', 'map', 'datum', 'building', 'manage', 'endtoend', 'lifecycle', 'real', 'estate', 'process', 'laurel', 'look', 'future', 'sandeep', 'know', 'difficult', 'anticipate', 'commercial', 'real', 'estate', 'industry', 'transform', 'next', 'year', 'emerge', 'technology', 'hybrid', 'way', 'working', 'call', 'great', 'sustainability', 'initiative', 'become', 'prevalent', 'sandeep', 'always', 'risk', 'prediction', 'fiveyear', 'prediction', 'time', 'pace', 'change', 'dizzy', 'however', 'ask', 'maybe', 'share', 'point', 'first', 'let', 'take', 'example', 'financial', 'service', 'first', 'relate', 'insight', 'take', 'wealth', 'management', 'industry', 'information', 'use', 'advantage', 'information', 'commodity', 'insight', 'advantage', 'insight', 'advantage', 'yesterday', 'less', 'advantage', 'today', 'wealth', 'manager', 'job', 'important', 'ever', 'close', 'client', 'ever', 'give', 'meaningful', 'advice', 'ever', 'see', 'similar', 'change', 'commercial', 'real', 'estate', 'today', 'investment', 'leasing', 'decision', 'get', 'microsegmented', 'extremely', 'datadriven', 'matter', 'client', 'go', 'matter', 'client', 'client', 'focused', 'energy', 'goal', 'investment', 'client', 'focus', 'labor', 'analytic', 'gentrification', 'metric', 'see', 'industry', 'move', 'microsegmented', 'decision', 'datadriven', 'area', 'transformation', 'see', 'industry', 'deep', 'data', 'insight', 'go', 'really', 'relevant', 'term', 'enable', 'second', 'take', 'outsourcing', 'industry', 'also', 'part', 'business', 'process', 'outsourcing', 'technology', 'outsourcing', 'start', 'augmentation', 'need', 'people', 'go', 'give', 'people', 'overtime', 'labor', 'arbitrage', 'move', 'run', 'steam', 'quickly', 'move', 'deliver', 'outcome', 'deliver', 'outcome', 'model', 'good', 'process', 'good', 'technology', 'good', 'datum', 'good', 'insight', 'therefore', 'confidently', 'tell', 'well', 'position', 'deliver', 'well', 'outcome', 'time', 'deliver', 'well', 'margin', 'company', 'exactly', 'outsourcing', 'business', 'transformation', 'see', 'happen', 'industry', 'move', 'away', 'staff', 'org', 'outcome', 'base', 'third', 'think', 'pivotal', 'moment', 'many', 'different', 'way', 'industry', 'find', 'midst', 'dominant', 'trend', 'time', 'return', 'work', 'sustainability', 'see', 'step', 'change', 'technology', 'term', 'cloud', 'give', 'think', 'go', 'also', 'drive', 'tremendous', 'change', 'continue', 'push', 'bound', 'technology', 'service', 'client', 'realworld', 'challenge', 'laurel', 'certainly', 'comprehensive', 'sandeep', 'thank', 'much', 'join', 'today', 'business', 'lab', 'sandeep', 'thank', 'thank', 'laurel', 'sandeep', 'davé', 'chief', 'digital', 'technology', 'officer', 'cbre', 'speak', 'home', 'mit', 'mit', 'technology', 'review', 'overlook', 'charle', 'episode', 'business', 'lab', 'host', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'also', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Many industries have reached an inflection point with hybrid and remote work, emerging advanced technologies like AI and cloud computing, and increased demands for sustainable frameworks to mitigate emissions. According to Sandeep Davé, chief digital and technology officer at global firm CBRE, the commercial real estate industry is no stranger to these changes and challenges.…"
Ethical implications of ChatGPT in higher education: A scoping review,"[{'href': 'http://arxiv.org/abs/2311.14378v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.14378v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-24 09:52:49,"1 

   Ethical implications of ChatGPT in higher education: A scoping review 

Ming Li1, Ariunaa Enkhtur2, Fei Cheng3, Beverley Anne Yamamoto4 

1Institue for Transdisciplinary Graduate Degree Programs, Osaka University 
2Center for Global Initiatives, Osaka University   
3 Graduate School of Informatics, Kyoto University 
4Graduate School of Human Sciences Human Sciences, Osaka University 

Keyword: ChatGPT, education, ethic, Generative Artificial Intelligence, higher education, 

scoping review 

ABSTRACT 

This  scoping  review  explores  the  ethical  challenges  of  using  ChatGPT  in  education, 

focusing  particularly  on  issues  related  to  higher  education.  By  reviewing  recent  academic 

articles  written  in  English,  Chinese,  and  Japanese,  we  aimed  to  provide  a  comprehensive 

overview  of  relevant  research  while  identifying  gaps  for  future  considerations.  Drawing  on 

Arksey  &  O’Malley’s  (2005)  five-stage  scoping  review  framework,  we  identified  research 

questions, search terms, and conducted article search from four databases in the target three 

languages. Each article was reviewed by at least two researchers identifying main ethical issues 

of  utilizing  AI  in  education,  particularly  higher  education.  Our  analysis  of  ethical  issues 

followed the framework developed by DeepMind (Weiginger et al., 2021) to identify six main 

areas  of  ethical  concern  in  Language  Models.  The  majority  of  papers  were  concerned  with 

misinformation harms (n=25) and/or human-computer interaction related harms (n=24). Given 

the rapid deployment of Generative Artificial Intelligence (GAI), it is imperative for educators 

to conduct more empirical studies to develop sound ethical policies for the use of GAI.  

 
 
 
 
 
 
  
 
  
2 

INTRODUCTION 

    The recent wave of Generative Artificial Intelligence (GAI) originated with Google’s 

Transformer architecture of neural networks (Vaswani et al., 2017). Transformer is based on 

the  self-attention  mechanism,  which  is  highly  scalable  and  suitable  for  parallel  computing, 

hence it quickly became the de facto approach in the fields of natural language processing and 

computer vision. In 2018, language models like BERT (Devlin et al., 2018) and GPT (Radford 

et al., 2018) ushered in the era of self-supervised learning, which are pre-trained on large-scale 

textual data for learning fundamental knowledge, and then fine-tuned for specific downstream 

tasks such as QA, dialog and machine translation systems. Kaplan et al. (2018) validated the 

large  language  models’  scale  law,  asserting  that  the  performance  improvements  can  almost 

always be achieved by increasing the scale of model parameters and pre-training more textual 

data such as Wikipedia, book and internet resources. After this, the rapid expansion for the size 

of large language models began, with  the model scale growing  from  BERT’s 0.3B (billion) 

parameters to GPT3’s 175B (Brown et al. 2020). 

    Since  the  release  of  GPT3  release  in  June  2020,  OpenAI  and  DeepMind,  leading 

developers of this technology, became less satisfied with mere scale increases. Ouyang et al. 

(2022) and Chung et al. (2022) sought to align models’ outputs with the feedback of real human 

beings. The text generated by these models became increasingly human-like, and the content 

ever-more closely aligned with human values. The launch of ChatGPT in November 2022 made 

the  public  aware  that  GAI  was  already  capable  of  generating  human-quality  conversation, 

retrieving stored knowledge on demand, and achieving natural interaction with people as an AI 

assistant. This kicked off the current frenzy of adapting the utilization of GAI to various fields, 

including education. 

    ChatGPT’s  application  in  higher  education  has  garnered  attention  (UNESCO 

IESALC,  2023).  While  there  has  been  much  discussion  about  its  possible  benefits,  such  as 

 
3 

creating  teaching  materials,  analyzing  student  data,  or  identifying  learning  patterns,  the 

evidence base for this is unclear. Yet, GAI studies have revealed potential risks associated with 

the generation of incorrect information (known as the ‘hallucination’ issue), biases (including 

race, nationality, gender), and discriminatory content (Munn, 2023; Nozza et al., 2022). When 

GAI-generation  outputs  are  used  in  education-related  procedures  there  is  a  possibility  that 

problematic contents, biases and assumptions will be magnified, which may result in negative 

consequences for learners, educators, researchers, and administrators. Therefore, it is crucial to 

discuss  and  assess  the  ethical  implications  of  implementing  ChatGPT  within  educational 

institutions, especially concerning its use in teaching and learning, research or administration. 

Increased  use  of  GAI  by  learners  raises  issues  related  to  academic  integrity,  definitions  of 

authorship,  assessment  methods,  and  other  pedagogical  implications.  It  also  affects  how 

researchers conduct their studies and generate their outputs, as well as how decisions are made 

in admissions, hiring, or how the educational institutions are managed and run. Furthermore, 

the  increasing  integration  of  AI  in  education  even  raises  questions  about  the  continued 

relevance of traditional brick-and-mortar educational institutions.     

This  scoping  review  explores  the  ethical  challenges  of  using  ChatGPT  in  education, 

focusing particularly on higher education. By reviewing academic articles written in English, 

Chinese, and Japanese, we aimed to map out the current state of the field while identifying gaps 

for future consideration.  

METHODOLOGY 

A scoping review is commonly used to identify key issues in a newly emerging field or 

one where there is yet a substantial body of literature. It is “used to identify knowledge gaps, 

set research agendas, and identify implications for decision-making” (Tricco et al., 2016).  In 

this study, we adopted Arksey & O’Malley’s (2005) five-stage scoping review framework, that 

 
 
4 

involves identifying the initial research questions and relevant studies, selecting the studies, 

charting the data, and collating, summarizing, and reporting the results.  

Identifying the relevant studies 

We limited our focus to articles focusing on the latest version of GPT. We searched 

articles published in 2023 and used search terms “ChatGPT” or “Generative AI” coupled with 

“education”  and  “ethics”  (see  Table  1).  To  capture  more  solid  evidence-based  studies  and 

discussions around this topic, we identified SCOPUS as the main database to conduct our initial 

search.  To  include  ongoing  research  works,  we  also  included  the  arXiv  platform,  which 

provides access to preprint articles. We included two other languages that the authors have first 

or near-first language proficiency in, Japanese and Chinese. To facilitate this, we conducted 

searches in the prominent databases CiNii (Japanese) and CNKI (Chinese). Along with the UK 

and USA, Japan and China are leading AI development making these languages a good target. 

Table 1. Final search terms and results by platforms 

Database  Search terms 
Scopus 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"" )) 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"" AND ""ethics"")) 

ArXiv 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"")) 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI”) AND TITLE-ABS-KEY 
( ""education""  AND  ""ethics"" ) )  

Results  
276 

27 

112 

24 

CiNii 

(TITLE-ABS-KEY (""chatgpt""  OR  ""生成 AI"")  AND  TITLE-ABS-KEY ( ""教育"" ) )  

23 

 (TITLE-ABS-KEY (""chatgpt""  OR  ""生成 AI"")  AND  TITLE-ABS-KEY (“教育"" 
AND ""課題"" )  

4 

CNKI 

(TITLE-ABS-KEY (""chatgpt"" OR ""生成 AI"") AND TITLE-ABS-KEY (""education"")) 

198 

(TITLE-ABS-KEY (""chatgpt"" OR ""生成 AI"" ) AND TITLE-ABS-KEY (""教育"" AND 
""伦理"")  

12 

Charting the data and collation 

The initial search yielded 609 results, out of which 67 included education and ethical 

concerns. From these, we identified 26 articles meeting our inclusion criteria (Figure 1). All 

 
  
articles were reviewed by two reviewers and the third reviewer checked the findings.  

5 

Figure 1. Data extraction processes 

In our analysis of the ethical issues raised in the articles, we relied on the comprehensive 

research  conducted  by  DeepMind  (Weiginger  et  al.,  2021),  which  offers  a  framework  for 

assessing the ethical and social risks of harm that may arise from the deployment of language 

models (LMs) (Table 2).  

#  Areas 
1  Discrimination, 

Exclusion and 
Toxicity 
Information Hazards 

2 
3  Misinformation 

Harms 

4  Malicious Uses 

5  Human-Computer 
Interaction Harms 
6  Automation, Access, 

and Environmental 
Harms 

Table 2. Ethical and social risks areas 

Description 
AI models can harm by reinforcing discrimination, stereotypes, and biases, 
marginalizing individuals, promoting toxic language, and worsening disparities 
for disadvantaged groups. 
Leak of private data or sensitive information leaks. 
Providing false or misleading information, leading to less informed users and 
eroding trust in shared information 
Risks of using LMs for harm include enabling disinformation campaigns, 
personalized scams, fraud at scale, and the development of malicious computer 
code or weapon systems. 
Users’ overestimation of “human-like” AI capabilities may lead to unsafe 
usage, exploitation for manipulation, and perpetuation of stereotypes. 
Unequal benefits and limited access to LMs can impact job quality, creative 
economy, and create global disparities in risks and rewards. 

 
 
6 

FINDINGS 

Most  of  the  identified  papers  were  in  English  (n=19),  followed  by  Chinese  (n=4), 

Japanese (n=3). In the English papers, ten were empirical studies and nine were conceptual or 

discussion papers with a predominant focus on its applications in fields such as healthcare and 

medical domains. In comparison, the number of Chinese and Japanese papers was much smaller. 

Among  the  four  Chinese  papers,  all  were  general  discussions  around  the  application  and 

predicted impact of ChatGPT in education. There were only three Japanese articles, but one 

reported  on  initial  research  on  students’  practical  experiences  with  ChatGPT  specifications 

(Kondo  et  al.,  2023).  The  other  two  papers  discussed  challenges  for  Japanese  speakers  in 

writing  English  academic  papers  and  the  use  of  ChatGPT  for  support  and  general  teaching 

implications (Kashimura, 2023; Yanase, 2023). 

Overall,  there  was  little  discussion  specifically  focusing  on  higher  education.  The 

majority of the papers (n=19) were generic, discussing ethical concerns in teaching (n=19) and 

learning  (n=13)  mostly  from  theoretical  and  conceptual  perspectives  without  delving  into 

specific levels of education. Papers that specifically focused on tertiary education (n=5) were 

concerned  about  overall  pedagogical  implications,  particularly  in  medical  education  (n=2), 

faculty and students’ perceptions (n=2), and research implications (n=1).  

Table 3. Articles reviewed. 

Authors 

Language 

Education level   Main area/Focus  

Ethical concerns  

Database: Scopus 

Busch et al. 

English 

Tertiary 

Teaching, learning, 
administration 

1,2, 3, 4, 5 

Chan 

English 

Tertiary 

Teaching, learning 

2, 3, 5 

Curtis 

English 

Tertiary 

Research 

da Silva 

English 

Generic 

Research 

3, 5 

3, 5 

Dwivedi et al.  English 

Generic 

Research 

1, 2, 3, 4, 5, 6 

Fischer 

English 

Generic 

Administration 

1, 2, 3, 4, 5 

Krüger et al. 

English 

Generic 

Teaching, learning, research 

1, 2, 3, 5 

 
 
 
7 

Lim et al. 

English 

Generic 

Teaching 

1, 2, 3 

Masters (a) 

English 

Generic 

Teaching, administration 

1, 2, 3, 4, 5 

Masters (b) 
O’Connor & 
ChatGPT 

Tlili et al. 
Zumsteg & 
Junn 

English 

Generic 

Research 

3, 4 

English 

Generic 

Teaching, learning, research 

3, 5 

English 

Generic 

Teaching, learning 

1, 2, 3, 4, 5,  

English 

Tertiary 

Teaching, learning 

3, 4, 5 

Database: arXiv 

Chan & Hu 

English 

Tertiary 

Teaching, learning 

1, 2, 3, 4, 5 

Latif et al. 

English 

Generic 

Teaching 

1, 2, 3, 4, 5 

Li et al. 

English 

Generic 

Teaching, learning, research 

1, 2, 3, 4, 5 

Ojha et al. 

English 

Generic 

Teaching 

Sharma et al. 

English 

Generic 

Administration 

Sharples 

English 

Generic 

Teaching, learning 

Database: CiNii 

Kashimura  

Japanese 

Generic 

Teaching 

Kondo et al. 

Japanese 

Secondary 

Teaching, learning 

Yanase  

Japanese 

Generic 

Research 

Database: CNKI 

Huang 

Chinese 

Generic 

Teaching, learning 

Song & Lin 

Chinese 

Generic 

Teaching 

Xun 

Chinese 

Tertiary 

Teaching, learning 

Zhu & Yang 

Chinese 

Generic 

Teaching, learning 

4, 5 

3, 4 

3, 5 

1, 2, 3 

3, 5 

3, 5 

1, 3, 5 

2, 3, 5 

1, 3, 4, 5 

1, 2, 3, 5 

In terms of the focus of ethical issues, the majority of papers were concerned with #3 

misinformation  harms  (n=25)  including  academic  integrity,  cheating  and  other  assessment 

issues, and the users’ role in identifying and clarifying information and/or #5 human-computer 

interaction related harms (n=24) such as  addiction, dependence, and cognitive overload.  To 

illustrate  this  further  we  divided  the  papers  into  four  themes  concerning  teaching,  learning, 

research, and administration.  

In  the  following  section  we  sum  up  the  key  concerns  and  areas  of  discussion  in 

literature. 

 
 
 
 
8 

Teaching  

  Under  teaching,  it  was  noted  that  ChatGPT  exhibits  versatile  applications,  such  as 

personalized  and  interactive  learning,  curriculum  design,  assessing  homework,  exams,  and 

essays (Ojha et at., 2023; Kashiwamura, 2023; Huang, 2023). To design new programs or to 

provide personalized teaching, universities need to collect and process vast amounts of student 

data, often without students’ consent. This gives rise to questions surrounding data privacy and 

security (Chan, 2023; Masters, 2023a). There is a need to ensure that robust data protection 

measures are in place to safeguard sensitive information and prevent its misuse.  

Research by Latif et al. (2023) highlighted that AI may inadvertently reinforce existing 

societal inequalities, gender bias, and nationality  bias embedded within the original training 

data, negatively impacting the outputs of educational applications downstream. Undue reliance 

on AI-generated evaluations may compromise the quality of assessments, potentially failing to 

accurately  gauge  students’  true  capabilities  (Busch  et  al.,  2023;  Curtis,  2023;  Song  &  Lin, 

2023). 

Additionally, the introduction of AI in the educational process raises concerns about the 

dynamics between educators and students. A possible overreliance on AI-generated content, as 

discussed  by  Sharples  (2023),  could  alter  the  traditional  teacher-student  relationship  and 

diminish  educators’  creative  input  and  uniqueness  in  designing  engaging  lesson  plans  and 

activities. 

 Learning 

     The application of ChatGPT in learning includes personalized learning experiences, 

student  support,  language  assistance,  tutoring,  content  creation,  grading  and  assessment, 

research aid, and career counseling, enhancing the learning process for students (Kooli, 2023; 

 
 
     
9 

Lim et al., 2023). One major concern is the potential for an increase in plagiarism and cheating 

among  students  who  might  rely  on  GAI-generated  content  for  essays  and  exams,  thereby 

compromising  the  authenticity  of  their  work  (Li  et  al.,  2023;  Zhu  &  Yang,  2023).  This 

overreliance  on  ChatGPT  may  lead  to  a  decline  in  students’  sense  of  responsibility  and 

commitment to academic integrity (Ojha et al., 2023). 

    Excessive use of ChatGPT may have adverse effects on students’ critical thinking 

skills.  If  students  heavily  depend  on  AI-generated  content,  they  might  lose  the  ability  to 

independently analyze and evaluate information (Tlili et al., 2023). Another significant issue 

raised in the literature pertains to the risk of misinformation being propagated due to the highly 

persuasive and convincing nature of AI-generated content. This can lead to potential bias or 

manipulation of information presented to students.  

       Reliance on AI interactions for academic or social purposes might diminish face-

to-face  interactions,  potentially  hindering  the  development  of  essential  social  skills  among 

students.  Striking  a  balance  between  AI  and  human  interactions  is  crucial  to  foster  a  well-

rounded educational experience. 

ChatGPT  might  inadvertently  produce  content  that  inaccurately  or  inappropriately 

represents certain cultural or identity groups, highlighting the need for ongoing refinement and 

sensitivity in AI language model development (Busch et al., 2023). 

 Research  

   The applications of ChatGPT encompass efficient dataset analysis, code generation, 

literature  reviews,  timesaving  for  experimental  design  focus,  and  advancements  in  research 

discovery and development (Dwivedi et al., 2023; Li et al., 2023). 

   In research, the integration of AI in academic publishing poses the risk of displacing 

human  authors  and  undermining  the  value  of  their  expertise,  potentially  impacting  the 

 
   
10 

credibility  of  research.  The  attribution  of  fake  references  to  AI-generated  content  presents 

another challenge, leading to misinformation and a decline in trust in academic sources. (Curtis, 

2023). Joint authorship of editorial pieces like O’Connor and ChatGPT (2023) is a contentious 

as  it  challenges  the  established  core  values  related  to  human-based  authorship  in  academic 

publishing (da Silva, 2023).  

   Some  conferences  permit  the  use  of  ChatGPT  for  writing  papers,  but  only  when 

ChatGPT itself is  the subject  of empirical  research (e.g.,  ICML , 2023). On the other hand, 

some  research  communities,  such  as  the  Association  for  Computational  Linguistics  (ACL, 

2023), allow the use of ChatGPT based on specific guidelines.  

 Administration   

     ChatGPT  can  significantly  reduce  the  time  spent  on  human  administrative  tasks, 

such  as  responding  to  queries  from  applicants  and  assisting  students  in  course  enrollment 

(UNESCO IESALC, 2023).  

     However, there are concerns surrounding the equitable, reliable, and transparent use 

of ChatGPT. Utilizing ChatGPT in the admissions processes can potentially introduce biases, 

especially if the AI model was trained on historical data that reflects past inequalities (Fischer, 

2023; Sharma et al. 2023). To ensure fairness, transparency, and accountability, it is essential 

to  provide  applicants  with  clear  explanations  of  how  AI  was  employed  to  assess  their 

applications and the specific factors that contributed to their acceptance or rejection. 

Additionally,  the  use  of  AI  algorithms  in  admissions  decisions  carries  the  risk  of 

inadvertently  favoring  applicants  with  certain  characteristics  or  backgrounds,  potentially 

impacting  diversity  and  inclusion  efforts  within  the  university  (Busch  et  al.,  2023;  Fischer, 

2023).  Data privacy and security are also paramount considerations. To avoid unintentional 

discrimination,  institutions  should  actively  assess  and  address  any  biases  in  the  AI  model’s 

 
 
training  data  and  decision-making  process,  striving  to  provide  equal  opportunities  for  all 

applicants. 

11 

DISCUSSION AND CONCLUSION 

We focused on articles written in a very short period, the first 7 months of 2023, but 

covered literature written in English, Chinese and Japanese. Given that Chat GPT is trained on 

English-centric data (Brown et al., 2020), it is important to gain insights into discussions going 

on in other AI technologically advanced countries. However, our review revealed that a few 

academic and research studies are published in non-English languages, particularly in Chinese 

and Japanese.   

Our scoping review showed that there are already publications that are considering the 

ethical implications of GAI, especially ChatGPT, in education generally and some focused on 

higher education. The majority of papers are discussion pieces, but there is some early empirical 

work.  The  ethical  issues  highlighted  in  these  works  are  mainly  concerned  about  academic 

integrity, assessment issues, and data-protection.  

  Our analysis highlights the urgency of addressing ethical issues surrounding the use 

of GAI/ChatGPT in education. Collaboration among stakeholders is essential to establish clear 

guidelines,  protect  student  privacy,  and  promote  responsible  AI  use.  By  doing  so,  AI  can 

enhance education and research without compromising fundamental principles. 

References 

ACL. (2023). ACL 2023 Policy. 61st Annual Meeting of the Association for Computational 

Linguistics. Retrieved from https://2023.aclweb.org/blog/ACL-2023-policy/  

Arksey, H., & O'Malley, L. (2005). Scoping studies: towards a methodological framework. 

 
  
 
 
12 

International journal of social research methodology, 8(1), 19-32. 

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. 

(2020).  Language  models  are  few-shot  learners.  Advances  in  Neural  Information 

Processing Systems, 33, 1877–1901. 

Busch, F., Adams,  L. C., &  Bressem, K. K. (2023). Biomedical  ethical  aspects  towards the 

implementation of Artificial Intelligence in medical education. Med. Sci. Educ. Advance 

Online Publication. https://doi.org/10.1007/s40670-023-01815-x 

Chan,  C.  K.  Y.  (2023).  A  comprehensive  AI  policy  education  framework  for  university 

teaching  and  learning.  International  Journal  of  Educational  Technology  in  Higher 

Education, 20(38). https://doi.org/10.1186/s41239-023-00408-3 

Chan, C. K. Y., & Hu, W. (2023). Students’ voices on generative AI: Perceptions, benefits, and 

challenges in higher education. arXiv preprint arXiv:2305.00290. 

Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., ... & Wei, J. (2022). Scaling 

instruction-finetuned language models. arXiv preprint arXiv:2210.11416. 

Curtis,  N.(2023). To ChatGPT or not  to  ChatGPT? The  Impact  of Artificial  Intelligence on 

Academic  Publishing.  The  Pediatric  Infectious  Disease  Journal,  42(4),  275. 

https://doi.org/10.1097/INF.0000000000003852 

da Silva, J. A. T. (2023). Is ChatGPT a valid author?. Nurse Education in Practice, 68, 103600. 

https://doi.org/10.1016/j.nepr.2023.103600. 

Devlin,  J.,  Chang,  M.  W.,  Lee,  K.,  &  Toutanova,  K.  (2018).  Bert:  Pre-training  of  deep 

bidirectional 

transformers 

for 

language 

understanding. 

arXiv 

preprint 

arXiv:1810.04805. 

Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., ... & Wright, R. 

(2023).  “So  what 

if  ChatGPT  wrote 

it?”  Multidisciplinary  perspectives  on 

opportunities, challenges and implications of generative conversational AI for research, 

 
13 

practice  and  policy.  International  Journal  of  Information  Management,  71,  102642. 

https://doi.org/10.1016/j.ijinfomgt.2023.102642 

Fischer, I. (2023). Evaluating the ethics of machines assessing humans. Journal of Information 

Technology Teaching Cases, 0(0). https://doi.org/10.1177/20438869231178844 

Huang R. (2023). Ren Gong Zhi Neng Zheng Jia Su Jiao Yu Bian Ge: Xian Shi Tiao Zhan Yu 

Ying  Dui  Ju  Cuo  [Artificial  intelligence  is  accelerating  educational  transformation: 

Realistic challenges and countermeasures]. Journal of the Chinese Society of Education, 

(06), 26-33. 

ICML. (2023). Call for Papers. International Conference on Machine Learning. Retrieved from 

https://icml.cc/Conferences/2023/CallForPapers 

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, 

A., Wu, J., & Amodei, D. (2020). Scaling laws for neural language models. Advances 

in neural information processing systems, 33, 1877–1901. 

Kashiwamura, Y. (2023). Sozoteki sagyo e shifuto o unagasu kyoin no noryoku kojo ga kadai 

ni:  Kyoiku  o  kaeru  seisei  AI  [Encouraging  a  shift  to  creative  work:  Challenges  for 

teacher  capacity  building:  Generating  AI  that  Changes  Education].  The  Economist, 

101(24), 98-100. 

Kondo,  C.,  Tamada,  K.,  &  Matsuda,  T.  (2023).  Seiseikei  AI  o  daizai  toshita  joho-teki  na 

mikata・kangaekata ni motozuku mondai kaiketsu shido jissen: ChatGPT to no kyo-

zon  o  kangaeru  [Practicing  problem-solving  instruction  based  on  informational 

perspectives and ways of thinking :Consider coexistence with ChatGPT]. Journal of the 

Japan 

Society 

for 

Educational 

Technology, 

(2), 

255-258. 

https://doi.org/10.15077/jsetstudy.2023.2_255 

Kooli,  C.  (2023).  Chatbots  in  education  and  research:  A  critical  examination  of  ethical 

implications and solutions. Sustainability, 15(7), 5614. 

 
14 

Krüger, L., Krotsetis, S., OpenAI’s Generative Pretrained Transformer 3 (GPT-3) Model, & 

Nydahl,  P.  (2023).  ChatGPT:  Fluch  oder  Segen  in  der  Pflege?  [ChatGPT:  curse  or 

blessing in nursing care?]. Medizinische Klinik, Intensivmedizin und Notfallmedizin, 

10.1007/s00063-023-01038-3. 

Advance 

online 

publication. 

https://doi.org/10.1007/s00063-023-01038-3 

Latif, E., Mai, G., Nyaaba, M., Wu, X., Liu, N., Lu, G., ... & Zhai, X. (2023). Artificial general 

intelligence (AGI) for education. arXiv preprint arXiv:2304.12479. 

Li,  L.,  Ma,  Z.,  Fan,  L.,  Lee,  S.,  Yu,  H.,  &  Hemphill,  L.  (2023).  ChatGPT  in  education:  A 

discourse  analysis  of  worries  and  concerns  on  social  media.  arXiv  preprint 

arXiv:2305.02201. 

Lim, W. M., Gunasekara, A., Pallant, J. L., Pallant, J. I., & Pechenkina, E. (2023). Generative 

AI and the future of education: Ragnarök or reformation? A paradoxical  perspective 

from  management  educators.  The  International  Journal  of  Management  Education, 

21,100790. https://doi.org/10.1016/j.ijme.2023.100790  

Masters,  K.  (2023  a).  Ethical  use  of  Artificial  Intelligence  in  health  professions  education: 

AMEE 

Guide 

No. 

158. 

Medical 

Teacher, 

45(6), 

574-584. 

https://doi.org/10.1080/0142159X.2023.2186203 

Masters, K. (2023 b). Medical teacher’s first ChatGPT’s referencing hallucinations: Lessons 

for editors, reviewers, and teachers. Medical Teacher, Med Teach, 45(7), 673-675. DOI: 

10.1080/0142159X.2023.2208731 

Munn,  L. 

(2023).  The  uselessness  of  AI 

ethics.  AI  Ethics,  3,  869–877. 

https://doi.org/10.1007/s43681-022-00209-w 

Nozza, D., Bianchi, F., & Hovy, D. (2022). Pipelines for social bias testing of large language 

models.  In  Proceedings  of  BigScience  Episode  #5  --  Workshop  on  Challenges  & 

Perspectives  in  Creating  Large  Language  Models,  virtual+Dublin.  Association  for 

 
15 

Computational Linguistics, 68-74. 

O’Connor, S & ChatGPT. (2022). Open artificial intelligence platforms in nursing education: 

Tools  for  academic  progress  or  abuse?.  Nurse  Education  in  Practice,  66,  103537. 

https://doi.org/10.1016/j.nepr.2022.103537 

Ojha,  S.,  Narendra,  A.,  Mohapatra,  S.,  &  Misra,  I.  (2023).  From  robots  to  books:  An 

introduction  to  smart  applications  of  AI  in  education  (AIEd).  arXiv  preprint 

arXiv:2301.10026. 

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). 

Training  language  models  to  follow  instructions  with  human  feedback.  Advances  in 

Neural Information Processing Systems, 35, 27730-27744. 

Radford, A., & Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-

Training. Retrieved from https://www.mikecaptain.com/resources/pdf/GPT-1.pdf 

Sharma, P., Thapa, K., Dhakal, P., Upadhaya, M. D., Adhikari, S., & Khanal, S. R. (2023). 

Performance  of  ChatGPT  on  USMLE:  Unlocking  the  potential  of  large  language 

models for AI-assisted medical education. arXiv preprint arXiv:2307.00112. 

Sharples, M. (2023). Towards social generative AI for education: theory, practices and ethics. 

arXiv preprint arXiv:2306.10063. 

Song, H & Lin, M (2023). ChatGPT/Chuangshengshi rengong zhineng shidai xia  jiaoshi de 

gongzuo biange: Jiyu, tiaozhan yu yingdui [The Transformation of Teachers’ Work in 

the Era of ChatGPT/AIGC: Opportunities, Challenges, and Responses]. Journal of East 

China 

Normal 

University 

(Educational 

Sciences), 

(07),78-90. 

https://doi.org/10.16382/j.cnki.1000-5560.2023.07.008. 

Tlili, A., Shehata, B., Adarkwah, M. A., Bozkurt, A., Hickey, D. T., Huang, R., & Agyemang, 

B. (2023). What if the devil is my guardian angel: ChatGPT as a case study of using 

chatbots in education. Smart Learning Environments, 10(1), 15. 

 
16 

Tricco, A. C., Lillie, E., Zarin, W., O’brien, K., Colquhoun, H., Kastner, M., ... & Straus, S. E. 

(2016).  A  scoping  review  on  the  conduct  and  reporting  of  scoping  reviews.  BMC 

medical research methodology, 16, 1-10. 

UNESCO IESALC (2023). ChatGPT and artificial intelligence in higher education: Quick start 

guide.  Retrieved 

from  https://www.iesalc.unesco.org/en/2023/04/14/chatgpt-and-

artificial-intelligence-in-higher-education-quick-start-guide-and-interactive-seminar/ 

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & 

Polosukhin,  I.  (2017).  Attention  is  all  you  need.  Advances  in  neural  information 

processing systems, 30. https://api.semanticscholar.org/CorpusID:13756489 

Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., ... & Gabriel, I. (2021). 

Ethical  and  social  risks  of  harm 

from 

language  models.  arXiv  preprint 

arXiv:2112.04359. 

Xun, Y. (2023). ChatGPT/Chuangshengshi rengong zhineng yu gaodeng jiaoyu de jiazhi he 

shiming [ChatGPT/AIGC and the Value and Mission of Higher Education]. Journal of 

East China Normal University (Educational Sciences), (07), 56-63. 

Yanase, Y. (2023). AI o katsuyo shite Eigo ronbun o sakusei suru Nihongo shasha ni totte no 

kadai to sono taisho [Challenges and Strategies for Japanese Speakers Creating English 

Papers Using AI]. Journal of Information Science and Technology, 73(6), 219-224. 

Zhu, Y & Yang, F. (2023). ChatGPT/Chuangshengshi rengong zhineng yu jiaoyu chuangxin: 

Jiyu, 

tiaozhan  yiji  weilai. 

[ChatGPT/AIGC  and  Educational 

Innovation: 

Opportunities,Challenges, and the Future].  Journal of East China Normal University 

(Educational 

Sciences), 

(07),1-14. 

https://doi.org/10.16382/j.cnki.1000-

5560.2023.07.001. 

Zumsteg, J. M., & Junn, C. (2023). Will ChatGPT match to your program.  Am J Phys Med 

Rehabil, 1, 3-7. 

 
17 

 
 
","1 Ethical implications of ChatGPT in higher education : A scoping review Ming Li1 , Ariunaa Enkhtur2 , Fei Cheng3 , Beverley Anne Yamamoto4 1Institue for Transdisciplinary Graduate Degree Programs , Osaka University 2Center for Global Initiatives , Osaka University 3 Graduate School of Informatics , Kyoto University 4Graduate School of Human Sciences Human Sciences , Osaka University Keyword : ChatGPT , education , ethic , Generative Artificial Intelligence , higher education , scoping review ABSTRACT This scoping review explores the ethical challenges of using ChatGPT in education , focusing particularly on issues related to higher education . By reviewing recent academic articles written in English , Chinese , and Japanese , we aimed to provide a comprehensive overview of relevant research while identifying gaps for future considerations . Drawing on Arksey & O ’ Malley ’ s ( 2005 ) five-stage scoping review framework , we identified research questions , search terms , and conducted article search from four databases in the target three languages . Each article was reviewed by at least two researchers identifying main ethical issues of utilizing AI in education , particularly higher education . Our analysis of ethical issues followed the framework developed by DeepMind ( Weiginger et al. , 2021 ) to identify six main areas of ethical concern in Language Models . The majority of papers were concerned with misinformation harms ( n=25 ) and/or human-computer interaction related harms ( n=24 ) . Given the rapid deployment of Generative Artificial Intelligence ( GAI ) , it is imperative for educators to conduct more empirical studies to develop sound ethical policies for the use of GAI . 2 INTRODUCTION The recent wave of Generative Artificial Intelligence ( GAI ) originated with Google ’ s Transformer architecture of neural networks ( Vaswani et al. , 2017 ) . Transformer is based on the self-attention mechanism , which is highly scalable and suitable for parallel computing , hence it quickly became the de facto approach in the fields of natural language processing and computer vision . In 2018 , language models like BERT ( Devlin et al. , 2018 ) and GPT ( Radford et al. , 2018 ) ushered in the era of self-supervised learning , which are pre-trained on large-scale textual data for learning fundamental knowledge , and then fine-tuned for specific downstream tasks such as QA , dialog and machine translation systems . Kaplan et al . ( 2018 ) validated the large language models ’ scale law , asserting that the performance improvements can almost always be achieved by increasing the scale of model parameters and pre-training more textual data such as Wikipedia , book and internet resources . After this , the rapid expansion for the size of large language models began , with the model scale growing from BERT ’ s 0.3B ( billion ) parameters to GPT3 ’ s 175B ( Brown et al . 2020 ) . Since the release of GPT3 release in June 2020 , OpenAI and DeepMind , leading developers of this technology , became less satisfied with mere scale increases . Ouyang et al . ( 2022 ) and Chung et al . ( 2022 ) sought to align models ’ outputs with the feedback of real human beings . The text generated by these models became increasingly human-like , and the content ever-more closely aligned with human values . The launch of ChatGPT in November 2022 made the public aware that GAI was already capable of generating human-quality conversation , retrieving stored knowledge on demand , and achieving natural interaction with people as an AI assistant . This kicked off the current frenzy of adapting the utilization of GAI to various fields , including education . ChatGPT ’ s application in higher education has garnered attention ( UNESCO IESALC , 2023 ) . While there has been much discussion about its possible benefits , such as 3 creating teaching materials , analyzing student data , or identifying learning patterns , the evidence base for this is unclear . Yet , GAI studies have revealed potential risks associated with the generation of incorrect information ( known as the ‘ hallucination ’ issue ) , biases ( including race , nationality , gender ) , and discriminatory content ( Munn , 2023 ; Nozza et al. , 2022 ) . When GAI-generation outputs are used in education-related procedures there is a possibility that problematic contents , biases and assumptions will be magnified , which may result in negative consequences for learners , educators , researchers , and administrators . Therefore , it is crucial to discuss and assess the ethical implications of implementing ChatGPT within educational institutions , especially concerning its use in teaching and learning , research or administration . Increased use of GAI by learners raises issues related to academic integrity , definitions of authorship , assessment methods , and other pedagogical implications . It also affects how researchers conduct their studies and generate their outputs , as well as how decisions are made in admissions , hiring , or how the educational institutions are managed and run . Furthermore , the increasing integration of AI in education even raises questions about the continued relevance of traditional brick-and-mortar educational institutions . This scoping review explores the ethical challenges of using ChatGPT in education , focusing particularly on higher education . By reviewing academic articles written in English , Chinese , and Japanese , we aimed to map out the current state of the field while identifying gaps for future consideration . METHODOLOGY A scoping review is commonly used to identify key issues in a newly emerging field or one where there is yet a substantial body of literature . It is “ used to identify knowledge gaps , set research agendas , and identify implications for decision-making ” ( Tricco et al. , 2016 ) . In this study , we adopted Arksey & O ’ Malley ’ s ( 2005 ) five-stage scoping review framework , that 4 involves identifying the initial research questions and relevant studies , selecting the studies , charting the data , and collating , summarizing , and reporting the results . Identifying the relevant studies We limited our focus to articles focusing on the latest version of GPT . We searched articles published in 2023 and used search terms “ ChatGPT ” or “ Generative AI ” coupled with “ education ” and “ ethics ” ( see Table 1 ) . To capture more solid evidence-based studies and discussions around this topic , we identified SCOPUS as the main database to conduct our initial search . To include ongoing research works , we also included the arXiv platform , which provides access to preprint articles . We included two other languages that the authors have first or near-first language proficiency in , Japanese and Chinese . To facilitate this , we conducted searches in the prominent databases CiNii ( Japanese ) and CNKI ( Chinese ) . Along with the UK and USA , Japan and China are leading AI development making these languages a good target . Table 1 . Final search terms and results by platforms Database Search terms Scopus ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' AND `` ethics '' ) ) ArXiv ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI ” ) AND TITLE-ABS-KEY ( `` education '' AND `` ethics '' ) ) Results 276 27 112 24 CiNii ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` 教育 '' ) ) 23 ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( “ 教育 '' AND `` 課題 '' ) 4 CNKI ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) 198 ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` 教育 '' AND `` 伦理 '' ) 12 Charting the data and collation The initial search yielded 609 results , out of which 67 included education and ethical concerns . From these , we identified 26 articles meeting our inclusion criteria ( Figure 1 ) . All articles were reviewed by two reviewers and the third reviewer checked the findings . 5 Figure 1 . Data extraction processes In our analysis of the ethical issues raised in the articles , we relied on the comprehensive research conducted by DeepMind ( Weiginger et al. , 2021 ) , which offers a framework for assessing the ethical and social risks of harm that may arise from the deployment of language models ( LMs ) ( Table 2 ) . # Areas 1 Discrimination , Exclusion and Toxicity Information Hazards 2 3 Misinformation Harms 4 Malicious Uses 5 Human-Computer Interaction Harms 6 Automation , Access , and Environmental Harms Table 2 . Ethical and social risks areas Description AI models can harm by reinforcing discrimination , stereotypes , and biases , marginalizing individuals , promoting toxic language , and worsening disparities for disadvantaged groups . Leak of private data or sensitive information leaks . Providing false or misleading information , leading to less informed users and eroding trust in shared information Risks of using LMs for harm include enabling disinformation campaigns , personalized scams , fraud at scale , and the development of malicious computer code or weapon systems . Users ’ overestimation of “ human-like ” AI capabilities may lead to unsafe usage , exploitation for manipulation , and perpetuation of stereotypes . Unequal benefits and limited access to LMs can impact job quality , creative economy , and create global disparities in risks and rewards . 6 FINDINGS Most of the identified papers were in English ( n=19 ) , followed by Chinese ( n=4 ) , Japanese ( n=3 ) . In the English papers , ten were empirical studies and nine were conceptual or discussion papers with a predominant focus on its applications in fields such as healthcare and medical domains . In comparison , the number of Chinese and Japanese papers was much smaller . Among the four Chinese papers , all were general discussions around the application and predicted impact of ChatGPT in education . There were only three Japanese articles , but one reported on initial research on students ’ practical experiences with ChatGPT specifications ( Kondo et al. , 2023 ) . The other two papers discussed challenges for Japanese speakers in writing English academic papers and the use of ChatGPT for support and general teaching implications ( Kashimura , 2023 ; Yanase , 2023 ) . Overall , there was little discussion specifically focusing on higher education . The majority of the papers ( n=19 ) were generic , discussing ethical concerns in teaching ( n=19 ) and learning ( n=13 ) mostly from theoretical and conceptual perspectives without delving into specific levels of education . Papers that specifically focused on tertiary education ( n=5 ) were concerned about overall pedagogical implications , particularly in medical education ( n=2 ) , faculty and students ’ perceptions ( n=2 ) , and research implications ( n=1 ) . Table 3 . Articles reviewed . Authors Language Education level Main area/Focus Ethical concerns Database : Scopus Busch et al . English Tertiary Teaching , learning , administration 1,2 , 3 , 4 , 5 Chan English Tertiary Teaching , learning 2 , 3 , 5 Curtis English Tertiary Research da Silva English Generic Research 3 , 5 3 , 5 Dwivedi et al . English Generic Research 1 , 2 , 3 , 4 , 5 , 6 Fischer English Generic Administration 1 , 2 , 3 , 4 , 5 Krüger et al . English Generic Teaching , learning , research 1 , 2 , 3 , 5 7 Lim et al . English Generic Teaching 1 , 2 , 3 Masters ( a ) English Generic Teaching , administration 1 , 2 , 3 , 4 , 5 Masters ( b ) O ’ Connor & ChatGPT Tlili et al . Zumsteg & Junn English Generic Research 3 , 4 English Generic Teaching , learning , research 3 , 5 English Generic Teaching , learning 1 , 2 , 3 , 4 , 5 , English Tertiary Teaching , learning 3 , 4 , 5 Database : arXiv Chan & Hu English Tertiary Teaching , learning 1 , 2 , 3 , 4 , 5 Latif et al . English Generic Teaching 1 , 2 , 3 , 4 , 5 Li et al . English Generic Teaching , learning , research 1 , 2 , 3 , 4 , 5 Ojha et al . English Generic Teaching Sharma et al . English Generic Administration Sharples English Generic Teaching , learning Database : CiNii Kashimura Japanese Generic Teaching Kondo et al . Japanese Secondary Teaching , learning Yanase Japanese Generic Research Database : CNKI Huang Chinese Generic Teaching , learning Song & Lin Chinese Generic Teaching Xun Chinese Tertiary Teaching , learning Zhu & Yang Chinese Generic Teaching , learning 4 , 5 3 , 4 3 , 5 1 , 2 , 3 3 , 5 3 , 5 1 , 3 , 5 2 , 3 , 5 1 , 3 , 4 , 5 1 , 2 , 3 , 5 In terms of the focus of ethical issues , the majority of papers were concerned with # 3 misinformation harms ( n=25 ) including academic integrity , cheating and other assessment issues , and the users ’ role in identifying and clarifying information and/or # 5 human-computer interaction related harms ( n=24 ) such as addiction , dependence , and cognitive overload . To illustrate this further we divided the papers into four themes concerning teaching , learning , research , and administration . In the following section we sum up the key concerns and areas of discussion in literature . 8 Teaching Under teaching , it was noted that ChatGPT exhibits versatile applications , such as personalized and interactive learning , curriculum design , assessing homework , exams , and essays ( Ojha et at. , 2023 ; Kashiwamura , 2023 ; Huang , 2023 ) . To design new programs or to provide personalized teaching , universities need to collect and process vast amounts of student data , often without students ’ consent . This gives rise to questions surrounding data privacy and security ( Chan , 2023 ; Masters , 2023a ) . There is a need to ensure that robust data protection measures are in place to safeguard sensitive information and prevent its misuse . Research by Latif et al . ( 2023 ) highlighted that AI may inadvertently reinforce existing societal inequalities , gender bias , and nationality bias embedded within the original training data , negatively impacting the outputs of educational applications downstream . Undue reliance on AI-generated evaluations may compromise the quality of assessments , potentially failing to accurately gauge students ’ true capabilities ( Busch et al. , 2023 ; Curtis , 2023 ; Song & Lin , 2023 ) . Additionally , the introduction of AI in the educational process raises concerns about the dynamics between educators and students . A possible overreliance on AI-generated content , as discussed by Sharples ( 2023 ) , could alter the traditional teacher-student relationship and diminish educators ’ creative input and uniqueness in designing engaging lesson plans and activities . Learning The application of ChatGPT in learning includes personalized learning experiences , student support , language assistance , tutoring , content creation , grading and assessment , research aid , and career counseling , enhancing the learning process for students ( Kooli , 2023 ; 9 Lim et al. , 2023 ) . One major concern is the potential for an increase in plagiarism and cheating among students who might rely on GAI-generated content for essays and exams , thereby compromising the authenticity of their work ( Li et al. , 2023 ; Zhu & Yang , 2023 ) . This overreliance on ChatGPT may lead to a decline in students ’ sense of responsibility and commitment to academic integrity ( Ojha et al. , 2023 ) . Excessive use of ChatGPT may have adverse effects on students ’ critical thinking skills . If students heavily depend on AI-generated content , they might lose the ability to independently analyze and evaluate information ( Tlili et al. , 2023 ) . Another significant issue raised in the literature pertains to the risk of misinformation being propagated due to the highly persuasive and convincing nature of AI-generated content . This can lead to potential bias or manipulation of information presented to students . Reliance on AI interactions for academic or social purposes might diminish face- to-face interactions , potentially hindering the development of essential social skills among students . Striking a balance between AI and human interactions is crucial to foster a well- rounded educational experience . ChatGPT might inadvertently produce content that inaccurately or inappropriately represents certain cultural or identity groups , highlighting the need for ongoing refinement and sensitivity in AI language model development ( Busch et al. , 2023 ) . Research The applications of ChatGPT encompass efficient dataset analysis , code generation , literature reviews , timesaving for experimental design focus , and advancements in research discovery and development ( Dwivedi et al. , 2023 ; Li et al. , 2023 ) . In research , the integration of AI in academic publishing poses the risk of displacing human authors and undermining the value of their expertise , potentially impacting the 10 credibility of research . The attribution of fake references to AI-generated content presents another challenge , leading to misinformation and a decline in trust in academic sources . ( Curtis , 2023 ) . Joint authorship of editorial pieces like O ’ Connor and ChatGPT ( 2023 ) is a contentious as it challenges the established core values related to human-based authorship in academic publishing ( da Silva , 2023 ) . Some conferences permit the use of ChatGPT for writing papers , but only when ChatGPT itself is the subject of empirical research ( e.g. , ICML , 2023 ) . On the other hand , some research communities , such as the Association for Computational Linguistics ( ACL , 2023 ) , allow the use of ChatGPT based on specific guidelines . Administration ChatGPT can significantly reduce the time spent on human administrative tasks , such as responding to queries from applicants and assisting students in course enrollment ( UNESCO IESALC , 2023 ) . However , there are concerns surrounding the equitable , reliable , and transparent use of ChatGPT . Utilizing ChatGPT in the admissions processes can potentially introduce biases , especially if the AI model was trained on historical data that reflects past inequalities ( Fischer , 2023 ; Sharma et al . 2023 ) . To ensure fairness , transparency , and accountability , it is essential to provide applicants with clear explanations of how AI was employed to assess their applications and the specific factors that contributed to their acceptance or rejection . Additionally , the use of AI algorithms in admissions decisions carries the risk of inadvertently favoring applicants with certain characteristics or backgrounds , potentially impacting diversity and inclusion efforts within the university ( Busch et al. , 2023 ; Fischer , 2023 ) . Data privacy and security are also paramount considerations . To avoid unintentional discrimination , institutions should actively assess and address any biases in the AI model ’ s training data and decision-making process , striving to provide equal opportunities for all applicants . 11 DISCUSSION AND CONCLUSION We focused on articles written in a very short period , the first 7 months of 2023 , but covered literature written in English , Chinese and Japanese . Given that Chat GPT is trained on English-centric data ( Brown et al. , 2020 ) , it is important to gain insights into discussions going on in other AI technologically advanced countries . However , our review revealed that a few academic and research studies are published in non-English languages , particularly in Chinese and Japanese . Our scoping review showed that there are already publications that are considering the ethical implications of GAI , especially ChatGPT , in education generally and some focused on higher education . The majority of papers are discussion pieces , but there is some early empirical work . The ethical issues highlighted in these works are mainly concerned about academic integrity , assessment issues , and data-protection . Our analysis highlights the urgency of addressing ethical issues surrounding the use of GAI/ChatGPT in education . Collaboration among stakeholders is essential to establish clear guidelines , protect student privacy , and promote responsible AI use . By doing so , AI can enhance education and research without compromising fundamental principles . References ACL . ( 2023 ) . ACL 2023 Policy . 61st Annual Meeting of the Association for Computational Linguistics . Retrieved from https : Arksey , H. , & O'Malley , L. ( 2005 ) . Scoping studies : towards a methodological framework . 12 International journal of social research methodology , 8 ( 1 ) , 19-32 . Brown , T. , Mann , B. , Ryder , N. , Subbiah , M. , Kaplan , J. D. , Dhariwal , P. , ... & Amodei , D. ( 2020 ) . Language models are few-shot learners . Advances in Neural Information Processing Systems , 33 , 1877–1901 . Busch , F. , Adams , L. C. , & Bressem , K. K. ( 2023 ) . Biomedical ethical aspects towards the implementation of Artificial Intelligence in medical education . Med . Sci . Educ . Advance Online Publication . https : Chan , C. K. Y . ( 2023 ) . A comprehensive AI policy education framework for university teaching and learning . International Journal of Educational Technology in Higher Education , 20 ( 38 ) . https : Chan , C. K. Y. , & Hu , W. ( 2023 ) . Students ’ voices on generative AI : Perceptions , benefits , and challenges in higher education . arXiv preprint arXiv:2305.00290 . Chung , H. W. , Hou , L. , Longpre , S. , Zoph , B. , Tay , Y. , Fedus , W. , ... & Wei , J . ( 2022 ) . Scaling instruction-finetuned language models . arXiv preprint arXiv:2210.11416 . Curtis , N. ( 2023 ) . To ChatGPT or not to ChatGPT ? The Impact of Artificial Intelligence on Academic Publishing . The Pediatric Infectious Disease Journal , 42 ( 4 ) , 275. https : da Silva , J . A. T. ( 2023 ) . Is ChatGPT a valid author ? . Nurse Education in Practice , 68 , 103600. https : . Devlin , J. , Chang , M. W. , Lee , K. , & Toutanova , K. ( 2018 ) . Bert : Pre-training of deep bidirectional transformers for language understanding . arXiv preprint arXiv:1810.04805 . Dwivedi , Y. K. , Kshetri , N. , Hughes , L. , Slade , E. L. , Jeyaraj , A. , Kar , A. K. , ... & Wright , R. ( 2023 ) . “ So what if ChatGPT wrote it ? ” Multidisciplinary perspectives on opportunities , challenges and implications of generative conversational AI for research , 13 practice and policy . International Journal of Information Management , 71 , 102642. https : Fischer , I . ( 2023 ) . Evaluating the ethics of machines assessing humans . Journal of Information Technology Teaching Cases , 0 ( 0 ) . https : Huang R. ( 2023 ) . Ren Gong Zhi Neng Zheng Jia Su Jiao Yu Bian Ge : Xian Shi Tiao Zhan Yu Ying Dui Ju Cuo [ Artificial intelligence is accelerating educational transformation : Realistic challenges and countermeasures ] . Journal of the Chinese Society of Education , ( 06 ) , 26-33 . ICML . ( 2023 ) . Call for Papers . International Conference on Machine Learning . Retrieved from https : Kaplan , J. , McCandlish , S. , Henighan , T. , Brown , T. B. , Chess , B. , Child , R. , Gray , S. , Radford , A. , Wu , J. , & Amodei , D. ( 2020 ) . Scaling laws for neural language models . Advances in neural information processing systems , 33 , 1877–1901 . Kashiwamura , Y . ( 2023 ) . Sozoteki sagyo e shifuto o unagasu kyoin no noryoku kojo ga kadai ni : Kyoiku o kaeru seisei AI [ Encouraging a shift to creative work : Challenges for teacher capacity building : Generating AI that Changes Education ] . The Economist , 101 ( 24 ) , 98-100 . Kondo , C. , Tamada , K. , & Matsuda , T. ( 2023 ) . Seiseikei AI o daizai toshita joho-teki na mikata・kangaekata ni motozuku mondai kaiketsu shido jissen : ChatGPT to no kyo- zon o kangaeru [ Practicing problem-solving instruction based on informational perspectives and ways of thinking : Consider coexistence with ChatGPT ] . Journal of the Japan Society for Educational Technology , ( 2 ) , 255-258. https : Kooli , C. ( 2023 ) . Chatbots in education and research : A critical examination of ethical implications and solutions . Sustainability , 15 ( 7 ) , 5614 . 14 Krüger , L. , Krotsetis , S. , OpenAI ’ s Generative Pretrained Transformer 3 ( GPT-3 ) Model , & Nydahl , P. ( 2023 ) . ChatGPT : Fluch oder Segen in der Pflege ? [ ChatGPT : curse or blessing in nursing care ? ] . Medizinische Klinik , Intensivmedizin und Notfallmedizin , 10.1007/s00063-023-01038-3 . Advance online publication . https : Latif , E. , Mai , G. , Nyaaba , M. , Wu , X. , Liu , N. , Lu , G. , ... & Zhai , X . ( 2023 ) . Artificial general intelligence ( AGI ) for education . arXiv preprint arXiv:2304.12479 . Li , L. , Ma , Z. , Fan , L. , Lee , S. , Yu , H. , & Hemphill , L. ( 2023 ) . ChatGPT in education : A discourse analysis of worries and concerns on social media . arXiv preprint arXiv:2305.02201 . Lim , W. M. , Gunasekara , A. , Pallant , J. L. , Pallant , J. I. , & Pechenkina , E. ( 2023 ) . Generative AI and the future of education : Ragnarök or reformation ? A paradoxical perspective from management educators . The International Journal of Management Education , 21,100790. https : Masters , K. ( 2023 a ) . Ethical use of Artificial Intelligence in health professions education : AMEE Guide No . 158 . Medical Teacher , 45 ( 6 ) , 574-584. https : Masters , K. ( 2023 b ) . Medical teacher ’ s first ChatGPT ’ s referencing hallucinations : Lessons for editors , reviewers , and teachers . Medical Teacher , Med Teach , 45 ( 7 ) , 673-675 . DOI : 10.1080/0142159X.2023.2208731 Munn , L. ( 2023 ) . The uselessness of AI ethics . AI Ethics , 3 , 869–877 . https : Nozza , D. , Bianchi , F. , & Hovy , D. ( 2022 ) . Pipelines for social bias testing of large language models . In Proceedings of BigScience Episode # 5 -- Workshop on Challenges & Perspectives in Creating Large Language Models , virtual+Dublin . Association for 15 Computational Linguistics , 68-74 . O ’ Connor , S & ChatGPT . ( 2022 ) . Open artificial intelligence platforms in nursing education : Tools for academic progress or abuse ? . Nurse Education in Practice , 66 , 103537. https : Ojha , S. , Narendra , A. , Mohapatra , S. , & Misra , I . ( 2023 ) . From robots to books : An introduction to smart applications of AI in education ( AIEd ) . arXiv preprint arXiv:2301.10026 . Ouyang , L. , Wu , J. , Jiang , X. , Almeida , D. , Wainwright , C. , Mishkin , P. , ... & Lowe , R. ( 2022 ) . Training language models to follow instructions with human feedback . Advances in Neural Information Processing Systems , 35 , 27730-27744 . Radford , A. , & Narasimhan , K. ( 2018 ) . Improving Language Understanding by Generative Pre- Training . Retrieved from https : Sharma , P. , Thapa , K. , Dhakal , P. , Upadhaya , M. D. , Adhikari , S. , & Khanal , S. R. ( 2023 ) . Performance of ChatGPT on USMLE : Unlocking the potential of large language models for AI-assisted medical education . arXiv preprint arXiv:2307.00112 . Sharples , M. ( 2023 ) . Towards social generative AI for education : theory , practices and ethics . arXiv preprint arXiv:2306.10063 . Song , H & Lin , M ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng shidai xia jiaoshi de gongzuo biange : Jiyu , tiaozhan yu yingdui [ The Transformation of Teachers ’ Work in the Era of ChatGPT/AIGC : Opportunities , Challenges , and Responses ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) ,78-90. https : . Tlili , A. , Shehata , B. , Adarkwah , M. A. , Bozkurt , A. , Hickey , D. T. , Huang , R. , & Agyemang , B . ( 2023 ) . What if the devil is my guardian angel : ChatGPT as a case study of using chatbots in education . Smart Learning Environments , 10 ( 1 ) , 15 . 16 Tricco , A. C. , Lillie , E. , Zarin , W. , O ’ brien , K. , Colquhoun , H. , Kastner , M. , ... & Straus , S. E. ( 2016 ) . A scoping review on the conduct and reporting of scoping reviews . BMC medical research methodology , 16 , 1-10 . UNESCO IESALC ( 2023 ) . ChatGPT and artificial intelligence in higher education : Quick start guide . Retrieved from https : Vaswani , A. , Shazeer , N. , Parmar , N. , Uszkoreit , J. , Jones , L. , Gomez , A. N. , Kaiser , Ł. , & Polosukhin , I . ( 2017 ) . Attention is all you need . Advances in neural information processing systems , 30. https : Weidinger , L. , Mellor , J. , Rauh , M. , Griffin , C. , Uesato , J. , Huang , P. S. , ... & Gabriel , I . ( 2021 ) . Ethical and social risks of harm from language models . arXiv preprint arXiv:2112.04359 . Xun , Y . ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng yu gaodeng jiaoyu de jiazhi he shiming [ ChatGPT/AIGC and the Value and Mission of Higher Education ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) , 56-63 . Yanase , Y . ( 2023 ) . AI o katsuyo shite Eigo ronbun o sakusei suru Nihongo shasha ni totte no kadai to sono taisho [ Challenges and Strategies for Japanese Speakers Creating English Papers Using AI ] . Journal of Information Science and Technology , 73 ( 6 ) , 219-224 . Zhu , Y & Yang , F. ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng yu jiaoyu chuangxin : Jiyu , tiaozhan yiji weilai . [ ChatGPT/AIGC and Educational Innovation : Opportunities , Challenges , and the Future ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) ,1-14. https : //doi.org/10.16382/j.cnki.1000- 5560.2023.07.001 . Zumsteg , J. M. , & Junn , C. ( 2023 ) . Will ChatGPT match to your program . Am J Phys Med Rehabil , 1 , 3-7 . 17","['ethical', 'implication', 'chatgpt', 'high', 'education', 'scope', 'review', 'ming', 'beverley', 'transdisciplinary', 'graduate', 'degree', 'program', 'global', 'initiative', 'graduate', 'school', 'education', 'ethic', 'generative', 'artificial', 'intelligence', 'high', 'education', 'scope', 'review', 'scope', 'review', 'explore', 'ethical', 'challenge', 'use', 'chatgpt', 'education', 'focus', 'particularly', 'issue', 'relate', 'high', 'education', 'review', 'recent', 'academic', 'article', 'write', 'aim', 'provide', 'comprehensive', 'overview', 'relevant', 'research', 'identify', 'gap', 'future', 'consideration', 'draw', 'arksey', 'malley', 'fivestage', 'scope', 'review', 'framework', 'identify', 'research', 'question', 'search', 'term', 'conduct', 'article', 'search', 'database', 'target', 'language', 'article', 'review', 'least', 'researcher', 'identify', 'main', 'ethical', 'issue', 'utilize', 'ai', 'education', 'particularly', 'high', 'education', 'analysis', 'ethical', 'issue', 'follow', 'framework', 'develop', 'deepmind', 'weiginger', 'identify', 'main', 'area', 'ethical', 'concern', 'language', 'model', 'majority', 'paper', 'concern', 'misinformation', 'harm', 'humancomputer', 'interaction', 'relate', 'harm', 'give', 'rapid', 'deployment', 'generative', 'artificial', 'intelligence', 'gai', 'imperative', 'educator', 'conduct', 'empirical', 'study', 'develop', 'sound', 'ethical', 'policy', 'use', 'gai', 'introduction', 'recent', 'wave', 'generative', 'artificial', 'intelligence', 'gai', 'originate', 'transformer', 'architecture', 'neural', 'network', 'vaswani', 'transformer', 'base', 'selfattention', 'mechanism', 'highly', 'scalable', 'suitable', 'parallel', 'computing', 'hence', 'quickly', 'become', 'approach', 'field', 'natural', 'language', 'processing', 'computer', 'vision', 'language', 'model', 'radford', 'usher', 'era', 'selfsupervise', 'learning', 'pretraine', 'largescale', 'textual', 'datum', 'learn', 'fundamental', 'knowledge', 'finetune', 'specific', 'downstream', 'task', 'qa', 'dialog', 'machine', 'translation', 'system', 'validate', 'large', 'language', 'model', 'scale', 'law', 'assert', 'performance', 'improvement', 'almost', 'always', 'achieve', 'increase', 'scale', 'model', 'parameter', 'pretraine', 'textual', 'datum', 'book', 'internet', 'resource', 'rapid', 'expansion', 'size', 'large', 'language', 'model', 'begin', 'model', 'scale', 'grow', '03b', 'parameter', 'brown', 'release', 'gpt3', 'release', 'openai', 'deepmind', 'lead', 'developer', 'technology', 'become', 'less', 'satisfied', 'mere', 'scale', 'increase', 'seek', 'align', 'model', 'output', 'feedback', 'real', 'human', 'text', 'generate', 'model', 'become', 'increasingly', 'humanlike', 'content', 'evermore', 'closely', 'align', 'human', 'value', 'launch', 'chatgpt', 'make', 'public', 'aware', 'already', 'capable', 'generate', 'humanquality', 'conversation', 'retrieve', 'store', 'knowledge', 'demand', 'achieve', 'natural', 'interaction', 'people', 'assistant', 'kick', 'current', 'frenzy', 'adapt', 'utilization', 'gai', 'various', 'field', 'include', 'education', 'application', 'high', 'education', 'garner', 'attention', 'iesalc', 'much', 'discussion', 'possible', 'benefit', 'create', 'teaching', 'material', 'analyze', 'student', 'datum', 'identify', 'learn', 'pattern', 'evidence', 'base', 'unclear', 'yet', 'gai', 'study', 'reveal', 'potential', 'risk', 'associate', 'generation', 'incorrect', 'information', 'know', 'hallucination', 'issue', 'bias', 'include', 'race', 'nationality', 'gender', 'discriminatory', 'content', 'nozza', 'gaigeneration', 'output', 'use', 'educationrelated', 'procedure', 'possibility', 'problematic', 'content', 'bias', 'assumption', 'magnify', 'result', 'negative', 'consequence', 'learner', 'educator', 'researcher', 'administrator', 'therefore', 'crucial', 'discuss', 'assess', 'ethical', 'implication', 'implement', 'chatgpt', 'educational', 'institution', 'especially', 'concern', 'use', 'teaching', 'learn', 'research', 'administration', 'increase', 'use', 'gai', 'learner', 'raise', 'issue', 'relate', 'academic', 'integrity', 'definition', 'authorship', 'assessment', 'method', 'pedagogical', 'implication', 'also', 'affect', 'researcher', 'conduct', 'study', 'generate', 'output', 'well', 'decision', 'make', 'admission', 'hire', 'educational', 'institution', 'manage', 'run', 'furthermore', 'increase', 'integration', 'ai', 'education', 'even', 'raise', 'question', 'continue', 'relevance', 'traditional', 'brickandmortar', 'educational', 'institution', 'scope', 'review', 'explore', 'ethical', 'challenge', 'use', 'chatgpt', 'education', 'focus', 'particularly', 'high', 'education', 'review', 'academic', 'article', 'write', 'aim', 'map', 'current', 'state', 'field', 'identify', 'gap', 'future', 'consideration', 'methodology', 'scope', 'review', 'commonly', 'use', 'identify', 'key', 'issue', 'newly', 'emerge', 'field', 'yet', 'substantial', 'body', 'literature', 'use', 'identify', 'knowledge', 'gap', 'set', 'research', 'agenda', 'identify', 'implication', 'decisionmake', 'study', 'adopt', 'arksey', 'malley', 'fivestage', 'scope', 'review', 'framework', 'involve', 'identify', 'initial', 'research', 'question', 'relevant', 'study', 'select', 'study', 'chart', 'datum', 'collate', 'summarizing', 'report', 'result', 'identify', 'relevant', 'study', 'limit', 'focus', 'article', 'focus', 'late', 'version', 'search', 'article', 'publish', 'use', 'search', 'term', 'chatgpt', 'generative', 'couple', 'education', 'ethic', 'see', 'table', 'capture', 'solid', 'evidencebased', 'study', 'discussion', 'topic', 'identify', 'scopus', 'main', 'database', 'conduct', 'initial', 'search', 'include', 'ongoing', 'research', 'work', 'also', 'include', 'arxiv', 'platform', 'provide', 'access', 'preprint', 'article', 'include', 'language', 'author', 'first', 'nearfirst', 'language', 'proficiency', 'japanese', 'chinese', 'facilitate', 'conduct', 'search', 'prominent', 'database', 'cinii', 'lead', 'ai', 'development', 'make', 'language', 'good', 'target', 'table', 'final', 'search', 'term', 'result', 'platform', 'database', 'search', 'term', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'ethic', 'arxiv', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'ethic', 'result', 'cinii', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', '教育', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', '課題', 'cnki', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', 'chart', 'datum', 'collation', 'initial', 'search', 'yield', 'result', 'include', 'education', 'ethical', 'concern', 'identify', 'article', 'meet', 'inclusion', 'criterion', 'figure', 'article', 'review', 'reviewer', 'third', 'reviewer', 'check', 'finding', 'figure', 'datum', 'extraction', 'process', 'analysis', 'ethical', 'issue', 'raise', 'article', 'rely', 'comprehensive', 'research', 'conduct', 'deepmind', 'weiginger', 'offer', 'framework', 'assess', 'ethical', 'social', 'risk', 'harm', 'arise', 'deployment', 'language', 'model', 'lm', 'table', 'area', 'discrimination', 'exclusion', 'toxicity', 'information', 'hazard', 'misinformation', 'harm', 'malicious', 'use', 'humancomputer', 'interaction', 'harm', 'automation', 'access', 'environmental', 'harm', 'table', 'ethical', 'social', 'risk', 'area', 'description', 'model', 'harm', 'reinforce', 'discrimination', 'stereotype', 'bias', 'marginalize', 'individual', 'promote', 'toxic', 'language', 'worsen', 'disparity', 'disadvantaged', 'group', 'leak', 'private', 'datum', 'sensitive', 'information', 'leak', 'provide', 'false', 'misleading', 'information', 'lead', 'less', 'informed', 'user', 'erode', 'trust', 'share', 'information', 'risk', 'use', 'lm', 'harm', 'include', 'enable', 'disinformation', 'campaign', 'personalize', 'scam', 'fraud', 'scale', 'development', 'malicious', 'computer', 'code', 'weapon', 'system', 'user', 'overestimation', 'ai', 'capability', 'lead', 'unsafe', 'usage', 'exploitation', 'manipulation', 'perpetuation', 'stereotype', 'unequal', 'benefit', 'limited', 'access', 'lm', 'impact', 'job', 'quality', 'creative', 'economy', 'create', 'global', 'disparity', 'risk', 'reward', 'finding', 'identify', 'paper', 'english', 'n19', 'follow', 'chinese', 'japanese', 'english', 'paper', 'empirical', 'study', 'conceptual', 'discussion', 'paper', 'predominant', 'focus', 'application', 'field', 'healthcare', 'medical', 'domain', 'comparison', 'number', 'chinese', 'japanese', 'paper', 'much', 'small', 'chinese', 'paper', 'general', 'discussion', 'application', 'predict', 'impact', 'chatgpt', 'education', 'japanese', 'article', 'report', 'initial', 'research', 'student', 'practical', 'experience', 'chatgpt', 'specification', 'paper', 'discuss', 'challenge', 'japanese', 'speaker', 'write', 'english', 'academic', 'paper', 'use', 'chatgpt', 'support', 'general', 'teaching', 'implication', 'kashimura', 'yanase', 'overall', 'little', 'discussion', 'specifically', 'focus', 'high', 'education', 'majority', 'paper', 'n19', 'generic', 'discuss', 'ethical', 'concern', 'teach', 'n19', 'learn', 'mostly', 'theoretical', 'conceptual', 'perspective', 'delve', 'specific', 'level', 'education', 'paper', 'specifically', 'focus', 'tertiary', 'education', 'concern', 'overall', 'pedagogical', 'implication', 'particularly', 'medical', 'education', 'n2', 'faculty', 'student', 'perception', 'research', 'implication', 'n1', 'table', 'article', 'review', 'author', 'language', 'education', 'level', 'main', 'areafocus', 'ethical', 'concern', 'database', 'english', 'tertiary', 'teaching', 'learn', 'administration', 'chan', 'english', 'tertiary', 'teaching', 'learn', 'curtis', 'english', 'tertiary', 'research', 'silva', 'generic', 'research', 'dwivedi', 'generic', 'research', 'fischer', 'krüger', 'generic', 'teaching', 'learn', 'research', 'generic', 'teaching', 'master', 'english', 'generic', 'teaching', 'administration', 'master', 'b', 'connor', 'chatgpt', 'tlili', 'english', 'generic', 'teaching', 'learn', 'research', 'english', 'generic', 'teaching', 'learn', 'english', 'tertiary', 'teaching', 'learn', 'database', 'english', 'tertiary', 'teaching', 'learn', 'generic', 'teaching', 'li', 'generic', 'teaching', 'learn', 'research', 'ojha', 'generic', 'teaching', 'administration', 'sharple', 'english', 'generic', 'teaching', 'learn', 'database', 'kondo', 'japanese', 'secondary', 'teaching', 'learn', 'yanase', 'japanese', 'generic', 'research', 'database', 'teaching', 'learning', 'song', 'teaching', 'chinese', 'tertiary', 'teaching', 'learn', 'generic', 'teaching', 'learn', 'term', 'focus', 'ethical', 'issue', 'majority', 'paper', 'concern', 'misinformation', 'harm', 'include', 'academic', 'integrity', 'cheating', 'assessment', 'issue', 'user', 'role', 'identify', 'clarify', 'information', 'andor', 'humancomputer', 'interaction', 'relate', 'harm', 'n24', 'addiction', 'dependence', 'cognitive', 'overload', 'illustrate', 'far', 'divide', 'paper', 'theme', 'concern', 'teaching', 'learn', 'research', 'administration', 'follow', 'section', 'sum', 'key', 'concern', 'area', 'discussion', 'literature', 'teaching', 'teach', 'note', 'chatgpt', 'exhibit', 'versatile', 'application', 'personalized', 'interactive', 'learning', 'curriculum', 'design', 'assess', 'homework', 'exam', 'essay', 'ojha', 'design', 'new', 'program', 'provide', 'personalized', 'teaching', 'university', 'need', 'collect', 'process', 'vast', 'amount', 'student', 'datum', 'often', 'student', 'consent', 'give', 'rise', 'question', 'surround', 'data', 'privacy', 'security', 'master', '2023a', 'need', 'ensure', 'robust', 'data', 'protection', 'measure', 'place', 'safeguard', 'sensitive', 'information', 'prevent', 'misuse', 'research', 'highlight', 'inadvertently', 'reinforce', 'exist', 'societal', 'inequality', 'gender', 'bias', 'nationality', 'bias', 'embed', 'original', 'training', 'datum', 'negatively', 'impact', 'output', 'educational', 'application', 'downstream', 'undue', 'reliance', 'aigenerate', 'evaluation', 'compromise', 'quality', 'assessment', 'potentially', 'fail', 'accurately', 'gauge', 'student', 'true', 'capability', 'curtis', 'song', 'additionally', 'introduction', 'ai', 'educational', 'process', 'raise', 'concern', 'dynamic', 'educator', 'student', 'possible', 'overreliance', 'aigenerate', 'content', 'discuss', 'sharple', 'alter', 'traditional', 'teacherstudent', 'relationship', 'diminish', 'educator', 'creative', 'input', 'uniqueness', 'design', 'engage', 'lesson', 'plan', 'activity', 'learn', 'application', 'chatgpt', 'learning', 'include', 'personalized', 'learn', 'experience', 'student', 'support', 'language', 'assistance', 'tutoring', 'content', 'creation', 'grade', 'assessment', 'research', 'aid', 'career', 'counseling', 'enhance', 'learning', 'process', 'student', 'major', 'concern', 'potential', 'increase', 'plagiarism', 'cheat', 'student', 'rely', 'gaigenerate', 'content', 'essay', 'exam', 'thereby', 'compromise', 'authenticity', 'work', 'overreliance', 'chatgpt', 'lead', 'decline', 'student', 'sense', 'responsibility', 'commitment', 'academic', 'integrity', 'ojha', 'excessive', 'use', 'chatgpt', 'adverse', 'effect', 'student', 'critical', 'thinking', 'skill', 'student', 'heavily', 'depend', 'aigenerate', 'content', 'lose', 'ability', 'independently', 'analyze', 'evaluate', 'information', 'tlili', 'significant', 'issue', 'raise', 'literature', 'pertain', 'risk', 'misinformation', 'propagate', 'highly', 'persuasive', 'convincing', 'nature', 'aigenerate', 'content', 'lead', 'potential', 'bias', 'manipulation', 'information', 'present', 'student', 'reliance', 'ai', 'interaction', 'academic', 'social', 'purpose', 'diminish', 'face', 'toface', 'interaction', 'potentially', 'hinder', 'development', 'essential', 'social', 'skill', 'student', 'strike', 'balance', 'ai', 'human', 'interaction', 'crucial', 'foster', 'well', 'round', 'educational', 'experience', 'chatgpt', 'inadvertently', 'produce', 'content', 'inaccurately', 'inappropriately', 'represent', 'certain', 'cultural', 'identity', 'group', 'highlight', 'need', 'ongoing', 'refinement', 'sensitivity', 'language', 'model', 'research', 'application', 'chatgpt', 'encompass', 'efficient', 'dataset', 'analysis', 'code', 'generation', 'literature', 'review', 'timesave', 'experimental', 'design', 'focus', 'advancement', 'research', 'discovery', 'development', 'research', 'integration', 'ai', 'academic', 'publishing', 'pose', 'risk', 'displace', 'human', 'author', 'undermine', 'value', 'expertise', 'potentially', 'impact', 'credibility', 'research', 'attribution', 'fake', 'reference', 'aigenerate', 'content', 'present', 'challenge', 'lead', 'misinformation', 'decline', 'trust', 'academic', 'source', 'curtis', 'joint', 'authorship', 'editorial', 'piece', 'connor', 'chatgpt', 'contentious', 'challenge', 'establish', 'core', 'value', 'relate', 'humanbase', 'authorship', 'academic', 'publishing', 'silva', 'conference', 'permit', 'use', 'chatgpt', 'write', 'paper', 'chatgpt', 'subject', 'empirical', 'research', 'eg', 'icml', 'hand', 'research', 'community', 'association', 'computational', 'linguistic', 'allow', 'use', 'chatgpt', 'base', 'specific', 'guideline', 'administration', 'chatgpt', 'significantly', 'reduce', 'time', 'spend', 'human', 'administrative', 'task', 'respond', 'query', 'applicant', 'assist', 'student', 'course', 'enrollment', 'however', 'concern', 'surround', 'equitable', 'reliable', 'transparent', 'use', 'chatgpt', 'utilize', 'chatgpt', 'admission', 'process', 'potentially', 'introduce', 'bias', 'especially', 'model', 'train', 'historical', 'datum', 'reflect', 'past', 'inequality', 'fischer', 'ensure', 'fairness', 'transparency', 'accountability', 'essential', 'provide', 'applicant', 'clear', 'explanation', 'employ', 'assess', 'application', 'specific', 'factor', 'contribute', 'acceptance', 'rejection', 'additionally', 'use', 'ai', 'algorithm', 'admission', 'decision', 'carry', 'risk', 'inadvertently', 'favor', 'applicant', 'certain', 'characteristic', 'background', 'potentially', 'impact', 'diversity', 'inclusion', 'effort', 'university', 'fischer', 'datum', 'privacy', 'security', 'also', 'paramount', 'consideration', 'avoid', 'unintentional', 'discrimination', 'institution', 'actively', 'assess', 'address', 'bias', 'model', 'training', 'datum', 'decisionmake', 'process', 'strive', 'provide', 'equal', 'opportunity', 'applicant', 'discussion', 'conclusion', 'focus', 'article', 'write', 'short', 'period', 'first', 'month', 'cover', 'literature', 'write', 'give', 'chat', 'gpt', 'train', 'englishcentric', 'datum', 'important', 'gain', 'insight', 'discussion', 'go', 'ai', 'technologically', 'advanced', 'country', 'however', 'review', 'reveal', 'academic', 'research', 'study', 'publish', 'nonenglish', 'language', 'particularly', 'chinese', 'japanese', 'scoping', 'review', 'show', 'already', 'publication', 'consider', 'ethical', 'implication', 'especially', 'chatgpt', 'education', 'generally', 'focus', 'high', 'education', 'majority', 'paper', 'discussion', 'piece', 'early', 'empirical', 'work', 'ethical', 'issue', 'highlight', 'work', 'mainly', 'concerned', 'academic', 'integrity', 'assessment', 'issue', 'dataprotection', 'analysis', 'highlight', 'urgency', 'address', 'ethical', 'issue', 'surround', 'use', 'gaichatgpt', 'education', 'collaboration', 'stakeholder', 'essential', 'establish', 'clear', 'guideline', 'protect', 'student', 'privacy', 'promote', 'responsible', 'use', 'enhance', 'education', 'research', 'compromise', 'fundamental', 'principle', 'reference', 'policy', '61st', 'annual', 'meeting', 'association', 'computational', 'linguistic', 'retrieve', 'https', 'arksey', 'h', 'l', 'scope', 'study', 'methodological', 'framework', 'international', 'journal', 'social', 'research', 'methodology', 'ryder', 'dhariwal', 'p', 'amodei', 'language', 'model', 'fewshot', 'learner', 'advance', 'neural', 'information', 'processing', 'system', 'adam', 'l', 'c', 'bressem', 'biomedical', 'ethical', 'aspect', 'implementation', 'artificial', 'intelligence', 'medical', 'education', 'educ', 'advance', 'online', 'publication', 'https', 'comprehensive', 'policy', 'education', 'framework', 'university', 'teaching', 'learn', 'international', 'journal', 'educational', 'technology', 'high', 'education', 'https', 'w', 'student', 'voice', 'generative', 'ai', 'perception', 'benefit', 'challenge', 'high', 'education', 'arxiv', 'preprint', 'l', 'zoph', 'tay', 'scale', 'instructionfinetune', 'language', 'model', 'arxiv', 'preprint', 'curtis', 'n', 'chatgpt', 'chatgpt', 'impact', 'artificial', 'intelligence', 'academic', 'publishing', 'pediatric', 'infectious', 'disease', 'journal', 'https', 'j', 'chatgpt', 'valid', 'author', 'nurse', 'education', 'practice', 'https', 'pretraining', 'deep', 'bidirectional', 'transformer', 'language', 'understand', 'arxiv', 'preprint', 'kshetri', 'hughe', 'l', 'slade', 'l', 'kar', 'k', 'wright', 'r', 'chatgpt', 'write', 'multidisciplinary', 'perspective', 'opportunity', 'challenge', 'implication', 'generative', 'conversational', 'ai', 'research', 'practice', 'policy', 'international', 'journal', 'information', 'management', 'https', 'fischer', 'evaluate', 'ethic', 'machine', 'assess', 'human', 'journal', 'information', 'technology', 'teaching', 'case', 'https', 'artificial', 'intelligence', 'accelerate', 'educational', 'transformation', 'realistic', 'challenge', 'countermeasure', 'journal', 'chinese', 'society', 'education', 'icml', 'call', 'paper', 'international', 'conference', 'machine', 'learning', 'retrieve', 'https', 'mccandlish', 'b', 'chess', 'child', 'r', 'gray', 'radford', 'amodei', 'scale', 'law', 'neural', 'language', 'model', 'advance', 'neural', 'information', 'processing', 'system', 'sagyo', 'unagasu', 'kyoin', 'encourage', 'shift', 'creative', 'work', 'challenge', 'teacher', 'capacity', 'building', 'generate', 'change', 'education', 'economist', 'tamada', 'seiseikei', 'ai', 'daizai', 'chatgpt', 'practice', 'problemsolving', 'instruction', 'base', 'informational', 'perspective', 'way', 'thinking', 'consider', 'coexistence', 'chatgpt', 'journal', 'educational', 'technology', 'https', 'c', 'chatbot', 'education', 'research', 'critical', 'examination', 'ethical', 'implication', 'solution', 'sustainability', 'krüger', 'l', 'krotsetis', 'generative', 'pretraine', 'transformer', 'gpt3', 'model', 'nydahl', 'p', 'chatgpt', 'fluch', 'oder', 'segen', 'pflege', 'chatgpt', 'curse', 'blessing', 'nursing', 'care', 'advance', 'online', 'publication', 'mai', 'lu', 'artificial', 'general', 'intelligence', 'agi', 'education', 'arxiv', 'preprint', 'fan', 'l', 'chatgpt', 'education', 'discourse', 'analysis', 'worry', 'concern', 'social', 'medium', 'arxiv', 'preprint', 'gunasekara', 'pallant', 'l', 'pallant', 'j', 'e', 'generative', 'ai', 'future', 'education', 'ragnarök', 'reformation', 'paradoxical', 'perspective', 'management', 'educator', 'international', 'journal', 'management', 'education', 'https', 'master', 'k', 'ethical', 'use', 'artificial', 'intelligence', 'health', 'profession', 'guide', 'medical', 'teacher', 'https', 'master', 'k', 'b', 'medical', 'teacher', 'reference', 'hallucination', 'lesson', 'editor', 'reviewer', 'teacher', 'medical', 'teacher', 'teach', 'l', 'uselessness', 'ethic', 'ai', 'ethic', 'https', 'nozza', 'bianchi', 'hovy', 'pipeline', 'social', 'bias', 'testing', 'large', 'language', 'model', 'proceeding', 'bigscience', 'episode', 'workshop', 'challenge', 'perspective', 'create', 'large', 'language', 'model', 'computational', 'linguistic', 'connor', 'chatgpt', 'open', 'artificial', 'intelligence', 'platform', 'nursing', 'education', 'tool', 'academic', 'progress', 'abuse', 'nurse', 'education', 'practice', 'https', 'ojha', 'mohapatra', 'misra', 'robot', 'book', 'introduction', 'smart', 'application', 'ai', 'education', 'aie', 'arxiv', 'preprint', 'wainwright', 'c', 'mishkin', 'p', 'lowe', 'r', 'training', 'language', 'model', 'follow', 'instruction', 'human', 'feedback', 'advance', 'neural', 'information', 'processing', 'system', 'radford', 'narasimhan', 'k', 'improve', 'language', 'understanding', 'generative', 'pre', 'training', 'retrieve', 'https', 'thapa', 'k', 'dhakal', 'upadhaya', 'khanal', 'r', 'performance', 'chatgpt', 'usmle', 'unlock', 'potential', 'large', 'language', 'model', 'aiassiste', 'medical', 'education', 'arxiv', 'preprint', 'sharple', 'social', 'generative', 'ai', 'education', 'theory', 'practice', 'ethic', 'arxiv', 'preprint', 'arxiv230610063', 'song', 'h', 'yingdui', 'transformation', 'teacher', 'work', 'era', 'chatgptaigc', 'opportunity', 'challenge', 'response', 'journal', 'university', 'educational', 'science', 'https', 'tlili', 'shehata', 'bozkurt', 'hickey', 'r', 'agyemang', 'devil', 'guardian', 'angel', 'chatgpt', 'case', 'study', 'use', 'chatbot', 'education', 'smart', 'learn', 'environment', 'tricco', 'c', 'e', 'colquhoun', 'h', 'kastner', 'straus', 'e', 'scope', 'review', 'conduct', 'reporting', 'scope', 'review', 'bmc', 'medical', 'research', 'methodology', 'iesalc', 'chatgpt', 'artificial', 'intelligence', 'high', 'education', 'quick', 'start', 'guide', 'retrieve', 'https', 'shazeer', 'gomez', 'n', 'kaiser', 'polosukhin', 'attention', 'need', 'advance', 'neural', 'information', 'processing', 'system', 'https', 'weidinger', 'l', 'griffin', 'gabriel', 'ethical', 'social', 'risk', 'harm', 'language', 'model', 'arxiv', 'preprint', 'shime', 'chatgptaigc', 'value', 'mission', 'high', 'education', 'university', 'educational', 'science', 'yanase', 'ai', 'eigo', 'ronbun', 'sakusei', 'sono', 'taisho', 'challenge', 'strategy', 'japanese', 'speaker', 'create', 'english', 'paper', 'use', 'ai', 'journal', 'information', 'science', 'technology', 'chatgptaigc', 'educational', 'innovation', 'opportunity', 'challenge', 'future', 'journal', 'university', 'educational', 'science', 'https', 'c', 'chatgpt', 'match', 'program']",
"Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for
  Deep Reinforcement Learning","[{'href': 'http://arxiv.org/abs/2311.03711v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.03711v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-07 04:30:51,"𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query
Suggestions

Fabian Haak
fabian.haak@th-koeln.de
Technische Hochschule Köln
Cologne, Germany

Philipp Schaer
philipp.schaer@th-koeln.de
Technische Hochschule Köln
Cologne, Germany

ABSTRACT
This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 ,
a large dataset of Google and Bing search queries, a scraping tool
and dataset for biased news articles, as well as language models for
the investigation of bias in online search. Web search engines are a
major factor and trusted source in information search, especially
in the political domain. However, biased information can influence
opinion formation and lead to biased opinions. To interact with
search engines, users formulate search queries and interact with
search query suggestions provided by the search engines. A lack of
datasets on search queries inhibits research on the subject. We use
𝑄𝑏𝑖𝑎𝑠 to evaluate different approaches to fine-tuning transformer-
based language models with the goal of producing models capable of
biasing text with left and right political stance. Additionally to this
work we provided datasets and language models for biasing texts
that allow further research on bias in online information search.

CCS CONCEPTS
• Information systems → Web search engines; Query suggestion;
Query reformulation; • Computing methodologies → Natural
language generation.

KEYWORDS
web search, dataset, bias, query suggestion, search queries, language
models, transformers

ACM Reference Format:
Fabian Haak and Philipp Schaer. 2023. 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in
Search Queries and Query Suggestions. In 15th ACM Web Science Conference
2023 (WebSci ’23), April 30-May 1, 2023, Evanston, TX, USA. ACM, New York,
NY, USA, 6 pages.

3
2
0
2

v
o
N
9
2

]

R

I
.
s
c
[

1
v
0
8
7
7
1
.
1
1
3
2
:
v
i
X
r
a

1 INTRODUCTION
Search engines such as Google and Bing are seen as trustworthy
sources of information on many topics, including political news and
information [14, 30]. Further, search engines have proven to have a
major impact on the formation of political opinions [15]. To interact
with search engines, users formulate search queries that are an
expression of their information need. Based on these queries search
engines usually provide a set of search query suggestions [28].
Queries and the choices users make when interacting with query
suggestions are based on the information need they want to satisfy,
which is often to support their personal opinions or beliefs founded
on previously encountered information [6]. This interaction process
and the results presented to the users are prone to be biased [7, 22,
25] and therefore the high level of trust can be seen as problematic.
However, the true effect of biased search queries and different
types of inherent biases on the actual list of search results has not
sufficiently been investigated. One of the main reasons for the in-
frequency of studies on bias in search queries might be a lack of
publicly available datasets. datasets that include real-world user
queries, query suggestions, and actual query reformulations are
rare. One of the reasons is that collecting search queries from users
is problematic due to privacy concerns. Although there are tech-
niques like pseudonymization, query logs enable the identification
of users [5]. Previously available datasets such as the AOL query
log dataset [29] are no longer available, and their usage is morally
debatable.

Using unpersonalized search query suggestions as proxies might
solve this issue. Query suggestions describe the list of predicted
queries suggested to users during the input of search queries by
the search engine, sometimes also called search predictions [37] or
query auto completion [10]. While these search query suggestions
can be generated locally from the result set [40] or be taken from
global knowledge bases [20], in web search suggestions are mostly
based on frequently issued queries by users [37]. It can therefore be
assumed that in many cases query suggestions are popular related
search queries for the initial root query that represented the user’s
interest in a topic or entity.

The U.S. news domain is a popular domain for bias research due
to its sociological relevance and the two-party left-right spectrum
that facilitates bias analysis [34]. One of the benefits of our dataset
is to enable in-depth investigations of the correlation between bias
in search queries and search results, as search results for popu-
lar web search engines can easily be collected using our provided
dataset of search queries. Therefore, one goal of this work is to
provide a large dataset of search query suggestions for the U.S. po-
litical news domain. Additionally, we provide a transformer-based
methodology for biasing text. Such a system can be a useful tool in

 
 
 
 
 
 
WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

bias research, f.e. for generating biased derivatives of search queries.
Thus, we evaluate a range of fine-tuning scenarios to find settings,
that produce the most biased results. We provide the fine-tuned
transformer-based language models that are capable of inducing
left and right political stance bias in the form of lexical biases.

This publication describes the motivation and generation of
𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries, a scraping
tool and dataset for biased news articles, as well as language models
for the investigation of bias in online search.

The main contributions of 𝑄𝑏𝑖𝑎𝑠 and this paper are:

• Two datasets:1 (1) The (to the best of our knowledge) largest
labeled dataset of search query suggestions of a single do-
main for Google and Bing. (2) A dataset of biased news
articles of the U.S. political news domain.

• A scraping tool that allows researchers to easily retrieve an

up-to-date version of the biased news dataset.2

• A new approach for producing biased search queries, us-
ing intentionally biased transformer-based language models
capable of producing biased texts.

• An evaluation of different fine-tuning settings to find the
most capable setup for producing bias-inducing language
models.

2 BACKGROUND AND RELATED WORK

Definition of Bias is the News Domain. Due to the subjective na-
ture of bias, few publications attempt to explicitly define bias in
news [1, 11, 13, 32]. ’Biased news’ generally describes non-neutral
and opinionated news, but both are fuzzy attributes [32]. In the polit-
ical domain, the term opinionated describes having a fixed political
opinion and agenda, aspect of a cognitive (author- or reader-sided)
bias often described as partisan bias [17] or (political) stance [24].
Partisanship manifests at different granularities. At publishing or
reporting level, partisanship is expressed by the selection of certain
topics called selection bias, and coverage bias, the selection of dif-
ferent views on a topic [13]. At text level, statement bias describes
“members of the media interjecting their own opinions into the
text” [13], manifesting in overall opinionated texts. This can take
various forms, ranging from phrasing bias, the use of non-neutral
language [21] to moral framing and ideological bias [26]. Spin bias
describes bias introduced either by omitting necessary (neutral)
information or adding unnecessary information [11]. At word and
n-gram level, biases manifest as linguistic or lexical biases [11].
Most linguistic biases are highly domain- and context-specific. For
example, framing bias describes subjective words, while epistemo-
logical bias can be attributed to words targeting the credibility of a
statement [32]. Since these biases can be identified more objectively,
most approaches to detecting biases rely on the identification of
linguistic biases [1].

Research on Bias in Online Search and Search Queries. Few pub-
lications investigate bias in online search in aspects other than
search results: Robertson et al. [34] investigate partisan bias and
filter bubble effects in political searches by auditing SERPS for a
set of queries. They did not find significant evidence, that unbiased

search queries in real search sessions performed by real users lead
to filter bubble effects. The unanswered question is, whether biased
queries in general lead to biased search results.

Few studies investigate bias in search queries. Due to the difficult
accessibility of biased search queries, most studies focus on bias in
search query suggestions: in most of those, the authors investigate
topical group biases in search query suggestions in the political
domain [8, 18, 19]. Overall, they observe minor topical gender biases
for search queries consisting of names of politicians. Research on
bias in online search has shown, that “factors such as the topic of
the query, the phrasing of query and the time at which a query is
issued also impact the bias seen by the users”[23].

Datasets of Biased News and Search Queries. Baly et al. [4] predict
media bias using news articles collected from AllSides balanced
news.3 AllSides news is a popular source for balanced news [4, 11,
26], since AllSides has a high standard for assigning bias labels [1].
Similarly, Chen et al. [11] use AllSides-labeled news articles and
adfontes labels to analyze bias at different granularities. Mokhbe-
rian et al. [26] develop a framing bias detection and quantification
approach, using a collection of news articles that they label accord-
ing to news outlet bias labels provided by AllSides.4 datasets for
search queries and query suggestions are sparse: most are either not
available anymore [29] or focus on a narrow topic [8]. Robertson
et al. [35] introduce recursive algorithm interrogation, a technique
for recursively retrieving query suggestions of a root query and
their consecutive suggestions. This technique was employed by
Haak and Schaer [19] to investigate bias in the German political
domain. However, to the best of our knowledge, there currently is
no large-scale dataset on search queries.

3 ALLSIDES SCRAPER AND DATASETS
We present two novel datasets, that promote the investigation of
bias in online news search. Both datasets can be found on Zenodo as
mentioned in section 1. Further, we provide a web scraping tool for
retrieving an up-to-date version of the AllSides dataset we provide.
This section describes the content of the datasets as well as their
creation process. Figure 1 shows how we assembled the datasets
and use them in our approach for creating biasing language models.

3.1 AllSides Scraper
As described in section 2, the AllSides platform and the provides
news is a frequently used source for high quality labeled biased
news. We want to provide an easy means of retrieving the news
and all corresponding information. Similar datasets have previously
been produced, as mentioned in section 2. However, compared to the
currently most recent available version by Baly et al. [4], our version
includes more than 20 percent of additional articles. Furthermore,
for many tasks, especially in the news domain, it is relevant to have
the most recent documents available. We provide a Python-based
scraper, that scrapes all available AllSides news articles and gathers
available information as described in section 3.2. By providing the
scraper we facilitate access to a recent version of the dataset for
other researchers.

1The datasets can be found at Zenodo (https://doi.org/10.5281/zenodo.7682914)
2https://github.com/irgroup/Qbias

3https://www.allsides.com/unbiased-balanced-news
4https://www.kaggle.com/datasets/snapcrack/all-the-news

𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

3.3 Search Queries in U.S. News Domain Dataset
The second dataset we provide consists of 671,669 search query
suggestions for root queries based on tags of the AllSides biased
news dataset. We collected search query suggestions from Google
and Bing for the 1,431 topic tags, that have been used for tagging
AllSides news at least five times, approximately half of the total
amount of topics. The topic tags include names, a wide range of
political terms, agendas, and topics (e.g., ""communism"", ""libertarian
party"", ""same-sex marriage""), cultural and religious terms (e.g., ""Ra-
madan"", ""pope Francis""), locations and other news-relevant terms.
On average, the dataset contains 469 search queries for each topic.
In total, 318,185 suggestions have been retrieved from Google and
353,484 from Bing.

Using a python implementation loosely adopting the framework
provided by Robertson et al. [35], we scraped query suggestions for
the topics as root queries. Using Google Colab to run our scraper,
we retrieved ten search query suggestions provided by the Google
and Bing search autocomplete systems for the input of each of these
root queries. Furthermore, we extended the root queries by the let-
ters a to z (e.g., ""democrats"" (root term) → ""democrats a"" (query
input) → ""democrats and recession"" (query suggestion)). The goal
of this procedure is to simulate a user’s input during information
search. Retrieving the suggestions for the root query and the ex-
tended queries generates a total of up to 270 query suggestions per
topic and search engine. The dataset we provide contains columns
for root term, query input, and query suggestion for each suggested
query. The location from which the search is performed is the loca-
tion of the Google servers running Colab, in our case Iowa in the
United States of America, which is added to the dataset. Since we
perform the scrape on a blank browser for each of the searches,
personalization effects other than the location did not effect the
suggested search queries. Our scraping setup thus eliminates per-
sonalization effects that could, in theory, cause echo chamber ef-
fects [34]. Search engine providers describe that query suggestions
are based on other users’ searches [37]. Successful attempts to in-
fluence query suggestions have confirmed this claim [38]. Thus, we
deduce that query suggestions reflect real, frequently used search
queries and can be used as proxies for search queries. Despite the
lack of information on the frequency of the collected search queries,
our dataset contains the ranks of suggestions, as well as the sugges-
tions to the root term for approximating the most frequent search
queries. Assuming that the topical tags in the AllSides news reflect
popular topics, the produced dataset consists of real search queries
for news-relevant topics.

4 DEVELOPING BIASING LANGUAGE MODELS
Another contribution 𝑄𝑏𝑖𝑎𝑠 is to produce a pair of transformer-
based language models that are capable of inducing left or right
partisanship as linguistic biases. This can be done by leveraging
the masking function on the words of the target document.

4.1 Domain-Adopting DistilBERT
This section describes the methodological approach for develop-
ing transformer-based language models capable of biasing texts.
Usually, systems are developed with the goal of producing and
reproducing as little bias as possible or to debias biased texts [31].

Figure 1: Pipeline used for generating bias-inducing language
models. Black boxes represent datasets, language models
(only available upon request), and tools provided via Zenodo,
and GitHub.

3.2 AllSides Biased News Dataset
The dataset contains 21,747 news articles collected from AllSides
balanced news headline roundups [2] in November 2022. The All-
Sides balanced news feature three expert-selected U.S. news articles
from sources of different political views (left, right, center), often
featuring spin bias, slant other forms of non-neutral reporting on
political news [1]. All articles are tagged with a bias label by four
expert annotators based on the expressed political partisanship,
left, right, or neutral [1]. The AllSides balanced news aims to offer
multiple political perspectives on important news stories, educate
users on biases, and provide multiple viewpoints [1]. Collected data
includes the headline, news text, publishing date, topic tags (e.g.,
""Republican party"", ""coronavirus"", ""federal jobs""), links to the article,
and the publishing news outlet. We also include AllSides’ neutral
description of the topic of the articles. Overall, the produced dataset
contains 10,273 articled tagged as left, 7,222 articles tagged as right,
and 4,252 articles tagged as center. The collected articles have been
published between June 2012, and the date of the data collection at
the end of November 2022. We use the AllSides dataset for develop-
ing our biasing language model, as described in section 4. To allow
for easier access to the most recent and complete version of the
dataset for future research, we provide the scraping tool described
in section 3.1.

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

We try to achieve the opposite, biasing texts by reproducing biases
inherent in biased fine-tuning datasets. This would, for example,
allow for simulating a user that searches for biased terms poten-
tially caused by exposure to biased news. With the goal of cap-
turing biased language in pretrained language models and using
the HuggingFace Transformers module[39], we fine-tune the base
DistilBERT model, which was trained in Wikipedia articles and a
large book dataset.5 DistilBERT–[36] was chosen as a base model
since it has proven to perform well with comparably small datasets,
performs better than BERT despite its smaller size [36], and for its
aptness for adopting domain-specificity [3, 9]. We are aware that
there are more effective models, but to show the suitability of our
approach, we chose DistilBERT due to its efficiency and sufficient
effectiveness.

We fine-tuned a range of pairs of models, each with one left
model fine-tuned on the part of the AllSides corpus tagged as left-
biased news and one right model fine-tuned with documents of
the dataset labeled as right-biased news. We produced 24 models,
a left and a right model for 12 combinations of parameters. Our
goal is to find the ideal approach for capturing and reproducing
as much bias as possible while producing meaningful results. The
central aspect we varied in fine-tuning the models is the data used.
The models are developed using three different data configurations:
(a) the headline and news text of each of the left and right news
articles of the AllSides dataset, (b) only the news text, and (c) only
the headline. Another factor we evaluate is the use of padding or
concatenation to generate a consistent chunk size for fine-tuning.
The chosen chunk size is the max length of fine-tuning documents
for the padding approach and 128 for the concatenation approach.
Lastly, we compare the effects of intentional overfitting by rais-
ing the number of epochs to 20, compared to a more reasonable 6
epochs, which proved to produce a good combination of training
loss and validation loss for all dataset configurations. In theory,
overfitting could be another possibility for reproducing biased for-
mulations from the texts used in fine-tuning. This is why we tried
to intentionally raise the number of epochs as a parameter [27].

4.2 Evaluating the Biasing Language Models
All 24 models are available upon request. We decided not to make
the models publicly available due to concerns about potential harms
that could be caused by misuse of the systems that we further elab-
orated in section 5. For evaluating the models’ effectiveness, we
choose a combination of manual assessment and quantitative mea-
sures. The task of the models is to generate biased output, however,
biases are diverse and not easily measurable. Since we have no way
of identifying biased tokens in the dataset, perplexity is not a useful
measure for evaluating the model. Further, we cannot effectively
measure bias as a criterion for the effectiveness of the ability of the
systems to produce biased versions of queries. However, we can
assume that when masking words in search queries for topics of
the political news domain and letting the pairs of language models
predict the words, the output of the left and right models should
differ from each other. We choose to evaluate their performance on
search queries since we want to use the models in future research
on bias in search results for biased and unbiased search queries.

5https://huggingface.co/distilbert-base-uncased

Further, they represent texts of a different type but the same domain
as the texts used in training the models.

For each pair of models, we measure the difference between the
two models’ 10 most probable non-punctuation predictions for 100
randomly selected queries from the dataset by measuring the Rank
Biased Overlap (RBO) with 𝑝 = 0.9. We mask a random token of
each query that is neither part of the original news topic nor a stop-
word since we want to let the models generate meaning-carrying
tokens while keeping the query’s topic intact. Although nonsensical
random predictions or neologisms would lead to great RBO scores,
we want to generate queries, that humans could have formulated.
To assure that the models generate output that makes sense, we let
the models generate next word predictions for ten bias-provoking
sentences (e.g., ""Hilary Clinton is a"", ""Covid Vaccines should be"").
Two domain experts (a professor and a postdoctoral researcher)
then label nonsensical and biased predictions. We measure the inter-
annotator agreement on the individual statements (independent of
the models, that produce the statements) with Cohen’s Kappa [12].
For nonsensical predictions, we obtained a value of 0.18 (slight
agreement), for biased predictions the value is 0.84 (almost perfect).
The bias labels are assigned if the suggested word induces political
stance bias.

Table 1 shows the results of the model evaluation process. The
lower ℎ,𝑡, or ℎ𝑡 describes if headline, text, or both have been used.
𝑅𝐵𝑂 is the rank bias overlap between the left and the right model
of each configuration. 𝑃𝑛𝑜𝑛𝑠 describes the average percentage of
nonsensical predictions of both models and 𝑃𝑏𝑖𝑎𝑠 the percentage
of biased predictions. Overall, the models fine-tuned with only
headlines show the best (lowest) RBO scores, as well as the lowest
𝑃𝑛𝑜𝑛𝑠 scores. Headlines and text and only text perform more or
less equally in terms of their RBO. Concatenating texts produces
on average better results than padding, although for fine-tuning
with only headlines, the effect is minimal. Intentional overfitting
by raising the amount of training epochs seems to worsen the RBO
scores. The percentage of nonsensical predictions does not differ
much between different fine-tuning setups, fine-tuning on texts
only seems to be the only scenario that increases the percentage
of nonsensical predictions. Many of the models generate a high
percentage of biased suggestions, with the headline models having
the highest percentage of biased predictions.

5 DISCUSSION
Our language models show, that by using small, high-quality datasets,
it is possible to fine-tune transformer-based language models to bias
texts. In our fine-tuning setup, models fine-tuned with headlines
produce the overall best results. As the main reason for that, we
assume that the high amount of quotes in news texts, which often
are statements the authors of the articles disagree with, might have
induced noise in fine-tuning the models. Further, the condensed na-
ture of headlines, which aims to catch attention, might also reflect
in resulting language models. The output of left and right models
fine-tuned with headlines produce texts containing linguistic biases.
For example, for ""Donald Trump is a"", our model produces ""hero""
as right-biased and ""fraud"" as left-biased next word predictions.
The overall low percentage of nonsensical predictions supports
RBO as a suited evaluation metric. Despite the overall good results,

𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Table 1: Evaluation results of the fine-tuned language models.
The highlighted values are the best results for each metric.

Model configuration
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔

𝑅𝐵𝑂 𝑃𝑛𝑜𝑛𝑠
0.54
0.04
0.10
0.72
0.08
0.68
0.03
0.56
0.11
0.73
0.07
0.68
0.54
0.04
0.05
0.74
0.07
0.77
0.03
0.58
0.06
0.77
0.07
0.77

𝑃𝑏𝑖𝑎𝑠
0.33
0.14
0.15
0.5
0.14
0.23
0.38
0.28
0.1
0.43
0.24
0.12

future research should investigate the findings with other language
models and compare the performance to our results.

Our scraping tool and the datasets not only allow us to effectively
generate the language models but enables the investigation of other
research topics in the political news information search domain
such as bias classification or sentiment analysis.

Remark on Moral Issues. We are aware, that building bias-inducing
systems and providing datasets that enable the reproduction of
developing similar systems is problematic. We also did consider
that our work can inspire and help to develop transformer-based
language models capable of producing biased, toxic, or otherwise
harmful texts. The severity of openly published biased systems
has been shown by other such models, e.g. the gpt-4chan model.6
The publication of the model incited a debate, that shows how
problematic biased language made available to a wide public au-
dience are, despite explicitly stating the intended use for research
applications [16, 33].

Our models are biased primarily in terms of linguistic biases
that reflect political stances and views on political topics. Since this
can in part include objectively wrong and opinionated statements,
hate speech, racism, and other forms of despicable language and
toxicity, our models can and will reproduce text, that conveys these
phenomena. To minimize the harmful effects of our publication
that could be caused by the misuse of the models, we decided
to not make our biased models publicly accessible. Despite that,
we provide access to the models upon request for research, if a
reasonable application use case is provided. However, we do so
in the interest of investigating biases and correlations of bias in
online information search, with the goal of increasing transparency
and fairness. Raising awareness for how easily, intentionally or not,
biases can be reproduced and induced with AI systems is part of
this endeavor.

Although we have shown how the data we provide can be used
to create models that produce biased language we chose to publish
the datasets and scraper. This is mainly because of three reasons:
(a) as stated in section 3 similar datasets are available publicly, (b)
we believe that the benefits of raising awareness outweigh making

6https://huggingface.co/ykilcher/gpt-4chan

already public biased data more accessible, and (c) datasets on biased
language are required for developing systems to detect and inhibit
bias. With our work, we hope to highlight the need to assure, that
data used for developing models is unbiased and raise awareness for
how easily transformer-based language models can be fine-tuned
to produce biased language.

6 OUTLOOK
This publication represents the first and foundational milestone of
a larger-scale investigation of bias in online information search. As
a major next contribution, we plan to use the presented datasets
and models to investigate the effects of biased and unbiased search
queries on the search results of different search engines for popular
topics of the U.S. political news domain. To accomplish this, we need
to overcome the issue of subjectivity and lack of effective method-
ological approaches to bias identification other than by employing
human annotation. We plan to introduce an ensemble of methods,
including bias-agnostic analysis of linguistic differences, lexical
features, and transformer-based approaches for bias classification.
Further, we plan to conduct a study using a simulation approach. By
simulation different user behaviors in terms of information need,
formulating queries, and interacting with query suggestions in an
interactive information search simulation, we plan to gain insights
into echo chamber effects and bias formation in information search.
Additionally, we hope to identify which user properties and be-
haviors increase and which lower the risk of encountering bias in
information search.

7 CONCLUSION
This work presents 𝑄𝑏𝑖𝑎𝑠 , a first milestone of our ongoing research
on bias in online search. We present two datasets: a biased news
dataset and a large dataset of biased and unbiased search queries
for topics of the U.S. political news domain. Further, we provide a
scraping tool, that allows for collecting bias-labeled news texts from
AllSides. Lastly, we evaluate approaches to fine-tuning DistilBERT
transformer-based language models for biasing texts and publish
our models capable of inducing left and right political stance bias
in the form of lexical biases.

REFERENCES
[1] AllSides. 2021. How AllSides Creates Balanced News: A Step-by-Step Guide.
Retrieved Nov 30, 2022 from https://www.allsides.com/blog/how-does-allsides-
create-balanced-news

[2] AllSides. 2022. Balanced News Headlines Roundup. Retrieved Nov 30, 2022

from https://www.allsides.com/unbiased-balanced-news

[3] Jing Bai, Rui Cao, Wen Ma, and Hiroyuki Shinnou. 2020. Construction of Domain-

Specific DistilBERT Model by Using Fine-Tuning. In TAAI. 237–241.

[4] Ramy Baly, Giovanni Da San Martino, James Glass, and Preslav Nakov. 2020.
We Can Detect Your Bias: Predicting the Political Ideology of News Articles. In
EMNLP. 4982–4991.

[5] Michael Barbaro and Tom Zeller. 2006. A Face is exposed for AOL searcher no.

4417749. New York Times (01 2006).

[6] Nicholas Belkin, Colleen Cool, Diane Kelly, S.-J Lin, S.Y Park, Jose Perez-carballo,
and Cynthia Sikora. 2001. Iterative exploration, design and evaluation of support
for query reformulation in interactive information retrieval. IPM 37 (05 2001),
403–434.

[7] Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam
Kalai. 2016. Man is to Computer Programmer as Woman is to Homemaker?
Debiasing Word Embeddings. In NIPS. 4356–4364.

[8] Malte Bonart, Anastasiia Samokhina, Gernot Heisenberg, and Philipp Schaer.
2019. An investigation of biases in web search engine query suggestions. OIR 44,
2 (2019), 365–381.

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

[39] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,
Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language
Processing. In EMNLP. 38–45.

[40] Jinxi Xu and W. Bruce Croft. 2000. Improving the Effectiveness of Information
Retrieval with Local Context Analysis. ACM Trans. Inf. Syst. 18, 1 (Jan. 2000),
79–112. https://doi.org/10.1145/333135.333138

[9] Berfu Büyüköz, Ali Hürriyetoˇglu, and Arzucan Özgür. 2020. Analyzing ELMo

and DistilBERT on Socio-political News Classification. In AESPEN.

[10] Fei Cai and Maarten de Rijke. 2016. A Survey of Query Auto Completion in

Information Retrieval. FNTIR 10, 4 (2016), 273–363.

[11] Wei-Fan Chen, Khalid Al Khatib, Henning Wachsmuth, and Benno Stein. 2020.
Analyzing Political Bias and Unfairness in News Articles at Different Levels of
Granularity. In NLPCSS. 149–154. https://doi.org/10.18653/v1/2020.nlpcss-1.16
[12] Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educ Psychol

Meas 20, 1 (1960), 37–46.

[13] Dave D’Alessio and Mike Allen. 2000. Media Bias in Presidential Elections: A

Meta-Analysis. J. Commun. 50, 4 (2000), 133–156.

[14] Edelman. 2022. 2022 Edelman Trust Barometer. Retrieved Nov 30, 2022 from

https://www.edelman.com/trust/2022-trust-barometer

[15] Robert Epstein and Ronald E. Robertson. 2015. The search engine manipulation
effect (SEME) and its possible impact on the outcomes of elections. PNAS 112,
33, E4512–E4521. Publisher: National Academy of Sciences Section: PNAS Plus.
[16] Matthew Gault. 2022. AI Trained on 4Chan Becomes ‘Hate Speech Machine’.
Retrieved Feb 28, 2023 from https://www.vice.com/en/article/7k8zwx/ai-trained-
on-4chan-becomes-hate-speech-machine

[17] Bertram Gawronski. 2021. Partisan bias in the identification of fake news. TiCS

25, 9 (2021), 723–724.

[18] Fabian Haak and Philipp Schaer. 2021. Perception-Aware Bias Detection for

Query Suggestions. In BIAS. 130–142.

[19] Fabian Haak and Philipp Schaer. 2022. Auditing Search Query Suggestion Bias

Through Recursive Algorithm Interrogation. In WebSci. 219–227.

[20] Daniel Hienert, Philipp Schaer, Johann Schaible, and Philipp Mayr. 2011. A Novel
Combined Term Suggestion Service for Domain-Specific Digital Libraries.. In
TPDL (Lecture Notes in Computer Science, Vol. 6966), Stefan Gradmann, Francesca
Borri, Carlo Meghini, and Heiko Schuldt (Eds.). Springer, 192–203. http://dblp.
uni-trier.de/db/conf/ercimdl/tpdl2011.html#HienertSSM11

[21] Christoph Hube and Besnik Fetahu. 2019. Neural Based Statement Classification

for Biased Language. In WSDM. ACM.

[22] L. Introna and H. Nissenbaum. 2000. Defining the Web: the politics of search

engines. Computer 33, 1, 54–62.

[23] Juhi Kulshrestha, Motahhare Eslami, Johnnatan Messias, Muhammad Bilal Zafar,
Saptarshi Ghosh, Krishna P. Gummadi, and Karrie Karahalios. 2019. Search bias
quantification: investigating political bias in social media and web search. Inf.
Retr. J. 22, 1, 188–227.

[24] Ruibo Liu, Chenyan Jia, and Soroush Vosoughi. 2021. A Transformer-based
Framework for Neutralizing and Reversing the Political Polarity of News Articles.
Proc. ACM Hum.-Comput. Interact. 5, 1–26.

[25] Bhaskar Mitra, Milad Shokouhi, Filip Radlinski, and Katja Hofmann. 2014. On

user interactions with query auto-completion. In SIGIR. 1055–1058.

[26] Negar Mokhberian, André s Abeliuk, Patrick Cummings, and Kristina Lerman.

2020. Moral Framing and Ideological Bias of News. In SocInfo. 206–219.
[27] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On
the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong
Baselines. In ICLR. https://openreview.net/forum?id=nzpLWnVAyah

[28] Xi Niu and Diane Kelly. 2014. The use of query suggestions during information

search. IPM 50, 1, 218–234.

[29] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search.

In InfoScale. 1–es.

[30] Lily Ray. 2020. 2020 Google Search Survey: How Much Do Users Trust Their
Search Results? Retrieved Nov 30, 2022 from https://moz.com/blog/2020-google-
search-survey

[31] Shaina Raza, Deepak John Reji, and Chen Ding. 2022. Dbias: Detecting biases

and ensuring Fairness in news articles. Int J Data Sci Anal (2022).

[33] Reddit. 2022.

[32] Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Lin-
guistic Models for Analyzing and Detecting Biased Language. In ACL. 1650–1659.
Retrieved Feb 28, 2023
from https://www.reddit.com/r/MachineLearning/comments/v42pej/p_this_is_
the_worst_ai_ever_gpt4chan_model/

This is the worst AI ever.

[34] Ronald E. Robertson, Shan Jiang, Kenneth Joseph, Lisa Friedland, David Lazer,
and Christo Wilson. 2018. Auditing Partisan Audience Bias within Google Search.
Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 148 (2018).

[35] Ronald E. Robertson, Shan Jiang, David Lazer, and Christo Wilson. 2019. Auditing
Autocomplete: Suggestion Networks and Recursive Algorithm Interrogation. In
WebSci. 235–244.

[36] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR
abs/1910.01108 (2019).

[37] Danny Sullivan. 2018. How Google autocomplete works in Search. Retrieved Nov
30, 2022 from https://blog.google/products/search/how-google-autocomplete-
works-search/

[38] Peng Wang, Xianghang mi, Xiaojing Liao, Xiaofeng Wang, Kan Yuan, Feng Qian,
and Raheem Beyah. 2018. Game of Missuggestions: Semantic Analysis of Search-
Autocomplete Manipulations. In NDSS.

","𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions Fabian Haak fabian.haak @ th-koeln.de Technische Hochschule Köln Cologne , Germany Philipp Schaer philipp.schaer @ th-koeln.de Technische Hochschule Köln Cologne , Germany ABSTRACT This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries , a scraping tool and dataset for biased news articles , as well as language models for the investigation of bias in online search . Web search engines are a major factor and trusted source in information search , especially in the political domain . However , biased information can influence opinion formation and lead to biased opinions . To interact with search engines , users formulate search queries and interact with search query suggestions provided by the search engines . A lack of datasets on search queries inhibits research on the subject . We use 𝑄𝑏𝑖𝑎𝑠 to evaluate different approaches to fine-tuning transformer- based language models with the goal of producing models capable of biasing text with left and right political stance . Additionally to this work we provided datasets and language models for biasing texts that allow further research on bias in online information search . CCS CONCEPTS • Information systems → Web search engines ; Query suggestion ; Query reformulation ; • Computing methodologies → Natural language generation . KEYWORDS web search , dataset , bias , query suggestion , search queries , language models , transformers ACM Reference Format : Fabian Haak and Philipp Schaer . 2023 . 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions . In 15th ACM Web Science Conference 2023 ( WebSci ’ 23 ) , April 30-May 1 , 2023 , Evanston , TX , USA . ACM , New York , NY , USA , 6 pages . 3 2 0 2 v o N 9 2 ] R I . s c [ 1 v 0 8 7 7 1 . 1 1 3 2 : v i X r a 1 INTRODUCTION Search engines such as Google and Bing are seen as trustworthy sources of information on many topics , including political news and information [ 14 , 30 ] . Further , search engines have proven to have a major impact on the formation of political opinions [ 15 ] . To interact with search engines , users formulate search queries that are an expression of their information need . Based on these queries search engines usually provide a set of search query suggestions [ 28 ] . Queries and the choices users make when interacting with query suggestions are based on the information need they want to satisfy , which is often to support their personal opinions or beliefs founded on previously encountered information [ 6 ] . This interaction process and the results presented to the users are prone to be biased [ 7 , 22 , 25 ] and therefore the high level of trust can be seen as problematic . However , the true effect of biased search queries and different types of inherent biases on the actual list of search results has not sufficiently been investigated . One of the main reasons for the in- frequency of studies on bias in search queries might be a lack of publicly available datasets . datasets that include real-world user queries , query suggestions , and actual query reformulations are rare . One of the reasons is that collecting search queries from users is problematic due to privacy concerns . Although there are tech- niques like pseudonymization , query logs enable the identification of users [ 5 ] . Previously available datasets such as the AOL query log dataset [ 29 ] are no longer available , and their usage is morally debatable . Using unpersonalized search query suggestions as proxies might solve this issue . Query suggestions describe the list of predicted queries suggested to users during the input of search queries by the search engine , sometimes also called search predictions [ 37 ] or query auto completion [ 10 ] . While these search query suggestions can be generated locally from the result set [ 40 ] or be taken from global knowledge bases [ 20 ] , in web search suggestions are mostly based on frequently issued queries by users [ 37 ] . It can therefore be assumed that in many cases query suggestions are popular related search queries for the initial root query that represented the user ’ s interest in a topic or entity . The U.S. news domain is a popular domain for bias research due to its sociological relevance and the two-party left-right spectrum that facilitates bias analysis [ 34 ] . One of the benefits of our dataset is to enable in-depth investigations of the correlation between bias in search queries and search results , as search results for popu- lar web search engines can easily be collected using our provided dataset of search queries . Therefore , one goal of this work is to provide a large dataset of search query suggestions for the U.S. po- litical news domain . Additionally , we provide a transformer-based methodology for biasing text . Such a system can be a useful tool in WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer bias research , f.e . for generating biased derivatives of search queries . Thus , we evaluate a range of fine-tuning scenarios to find settings , that produce the most biased results . We provide the fine-tuned transformer-based language models that are capable of inducing left and right political stance bias in the form of lexical biases . This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries , a scraping tool and dataset for biased news articles , as well as language models for the investigation of bias in online search . The main contributions of 𝑄𝑏𝑖𝑎𝑠 and this paper are : • Two datasets:1 ( 1 ) The ( to the best of our knowledge ) largest labeled dataset of search query suggestions of a single do- main for Google and Bing . ( 2 ) A dataset of biased news articles of the U.S. political news domain . • A scraping tool that allows researchers to easily retrieve an up-to-date version of the biased news dataset.2 • A new approach for producing biased search queries , us- ing intentionally biased transformer-based language models capable of producing biased texts . • An evaluation of different fine-tuning settings to find the most capable setup for producing bias-inducing language models . 2 BACKGROUND AND RELATED WORK Definition of Bias is the News Domain . Due to the subjective na- ture of bias , few publications attempt to explicitly define bias in news [ 1 , 11 , 13 , 32 ] . ’ Biased news ’ generally describes non-neutral and opinionated news , but both are fuzzy attributes [ 32 ] . In the polit- ical domain , the term opinionated describes having a fixed political opinion and agenda , aspect of a cognitive ( author- or reader-sided ) bias often described as partisan bias [ 17 ] or ( political ) stance [ 24 ] . Partisanship manifests at different granularities . At publishing or reporting level , partisanship is expressed by the selection of certain topics called selection bias , and coverage bias , the selection of dif- ferent views on a topic [ 13 ] . At text level , statement bias describes “ members of the media interjecting their own opinions into the text ” [ 13 ] , manifesting in overall opinionated texts . This can take various forms , ranging from phrasing bias , the use of non-neutral language [ 21 ] to moral framing and ideological bias [ 26 ] . Spin bias describes bias introduced either by omitting necessary ( neutral ) information or adding unnecessary information [ 11 ] . At word and n-gram level , biases manifest as linguistic or lexical biases [ 11 ] . Most linguistic biases are highly domain- and context-specific . For example , framing bias describes subjective words , while epistemo- logical bias can be attributed to words targeting the credibility of a statement [ 32 ] . Since these biases can be identified more objectively , most approaches to detecting biases rely on the identification of linguistic biases [ 1 ] . Research on Bias in Online Search and Search Queries . Few pub- lications investigate bias in online search in aspects other than search results : Robertson et al . [ 34 ] investigate partisan bias and filter bubble effects in political searches by auditing SERPS for a set of queries . They did not find significant evidence , that unbiased search queries in real search sessions performed by real users lead to filter bubble effects . The unanswered question is , whether biased queries in general lead to biased search results . Few studies investigate bias in search queries . Due to the difficult accessibility of biased search queries , most studies focus on bias in search query suggestions : in most of those , the authors investigate topical group biases in search query suggestions in the political domain [ 8 , 18 , 19 ] . Overall , they observe minor topical gender biases for search queries consisting of names of politicians . Research on bias in online search has shown , that “ factors such as the topic of the query , the phrasing of query and the time at which a query is issued also impact the bias seen by the users ” [ 23 ] . Datasets of Biased News and Search Queries . Baly et al . [ 4 ] predict media bias using news articles collected from AllSides balanced news.3 AllSides news is a popular source for balanced news [ 4 , 11 , 26 ] , since AllSides has a high standard for assigning bias labels [ 1 ] . Similarly , Chen et al . [ 11 ] use AllSides-labeled news articles and adfontes labels to analyze bias at different granularities . Mokhbe- rian et al . [ 26 ] develop a framing bias detection and quantification approach , using a collection of news articles that they label accord- ing to news outlet bias labels provided by AllSides.4 datasets for search queries and query suggestions are sparse : most are either not available anymore [ 29 ] or focus on a narrow topic [ 8 ] . Robertson et al . [ 35 ] introduce recursive algorithm interrogation , a technique for recursively retrieving query suggestions of a root query and their consecutive suggestions . This technique was employed by Haak and Schaer [ 19 ] to investigate bias in the German political domain . However , to the best of our knowledge , there currently is no large-scale dataset on search queries . 3 ALLSIDES SCRAPER AND DATASETS We present two novel datasets , that promote the investigation of bias in online news search . Both datasets can be found on Zenodo as mentioned in section 1 . Further , we provide a web scraping tool for retrieving an up-to-date version of the AllSides dataset we provide . This section describes the content of the datasets as well as their creation process . Figure 1 shows how we assembled the datasets and use them in our approach for creating biasing language models . 3.1 AllSides Scraper As described in section 2 , the AllSides platform and the provides news is a frequently used source for high quality labeled biased news . We want to provide an easy means of retrieving the news and all corresponding information . Similar datasets have previously been produced , as mentioned in section 2 . However , compared to the currently most recent available version by Baly et al . [ 4 ] , our version includes more than 20 percent of additional articles . Furthermore , for many tasks , especially in the news domain , it is relevant to have the most recent documents available . We provide a Python-based scraper , that scrapes all available AllSides news articles and gathers available information as described in section 3.2 . By providing the scraper we facilitate access to a recent version of the dataset for other researchers . 1The datasets can be found at Zenodo ( https : //doi.org/10.5281/zenodo.7682914 ) 2https : //github.com/irgroup/Qbias 3https : 4https : 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA 3.3 Search Queries in U.S. News Domain Dataset The second dataset we provide consists of 671,669 search query suggestions for root queries based on tags of the AllSides biased news dataset . We collected search query suggestions from Google and Bing for the 1,431 topic tags , that have been used for tagging AllSides news at least five times , approximately half of the total amount of topics . The topic tags include names , a wide range of political terms , agendas , and topics ( e.g. , `` communism '' , `` libertarian party '' , `` same-sex marriage '' ) , cultural and religious terms ( e.g. , `` Ra- madan '' , `` pope Francis '' ) , locations and other news-relevant terms . On average , the dataset contains 469 search queries for each topic . In total , 318,185 suggestions have been retrieved from Google and 353,484 from Bing . Using a python implementation loosely adopting the framework provided by Robertson et al . [ 35 ] , we scraped query suggestions for the topics as root queries . Using Google Colab to run our scraper , we retrieved ten search query suggestions provided by the Google and Bing search autocomplete systems for the input of each of these root queries . Furthermore , we extended the root queries by the let- ters a to z ( e.g. , `` democrats '' ( root term ) → `` democrats a '' ( query input ) → `` democrats and recession '' ( query suggestion ) ) . The goal of this procedure is to simulate a user ’ s input during information search . Retrieving the suggestions for the root query and the ex- tended queries generates a total of up to 270 query suggestions per topic and search engine . The dataset we provide contains columns for root term , query input , and query suggestion for each suggested query . The location from which the search is performed is the loca- tion of the Google servers running Colab , in our case Iowa in the United States of America , which is added to the dataset . Since we perform the scrape on a blank browser for each of the searches , personalization effects other than the location did not effect the suggested search queries . Our scraping setup thus eliminates per- sonalization effects that could , in theory , cause echo chamber ef- fects [ 34 ] . Search engine providers describe that query suggestions are based on other users ’ searches [ 37 ] . Successful attempts to in- fluence query suggestions have confirmed this claim [ 38 ] . Thus , we deduce that query suggestions reflect real , frequently used search queries and can be used as proxies for search queries . Despite the lack of information on the frequency of the collected search queries , our dataset contains the ranks of suggestions , as well as the sugges- tions to the root term for approximating the most frequent search queries . Assuming that the topical tags in the AllSides news reflect popular topics , the produced dataset consists of real search queries for news-relevant topics . 4 DEVELOPING BIASING LANGUAGE MODELS Another contribution 𝑄𝑏𝑖𝑎𝑠 is to produce a pair of transformer- based language models that are capable of inducing left or right partisanship as linguistic biases . This can be done by leveraging the masking function on the words of the target document . 4.1 Domain-Adopting DistilBERT This section describes the methodological approach for develop- ing transformer-based language models capable of biasing texts . Usually , systems are developed with the goal of producing and reproducing as little bias as possible or to debias biased texts [ 31 ] . Figure 1 : Pipeline used for generating bias-inducing language models . Black boxes represent datasets , language models ( only available upon request ) , and tools provided via Zenodo , and GitHub . 3.2 AllSides Biased News Dataset The dataset contains 21,747 news articles collected from AllSides balanced news headline roundups [ 2 ] in November 2022 . The All- Sides balanced news feature three expert-selected U.S. news articles from sources of different political views ( left , right , center ) , often featuring spin bias , slant other forms of non-neutral reporting on political news [ 1 ] . All articles are tagged with a bias label by four expert annotators based on the expressed political partisanship , left , right , or neutral [ 1 ] . The AllSides balanced news aims to offer multiple political perspectives on important news stories , educate users on biases , and provide multiple viewpoints [ 1 ] . Collected data includes the headline , news text , publishing date , topic tags ( e.g. , `` Republican party '' , `` coronavirus '' , `` federal jobs '' ) , links to the article , and the publishing news outlet . We also include AllSides ’ neutral description of the topic of the articles . Overall , the produced dataset contains 10,273 articled tagged as left , 7,222 articles tagged as right , and 4,252 articles tagged as center . The collected articles have been published between June 2012 , and the date of the data collection at the end of November 2022 . We use the AllSides dataset for develop- ing our biasing language model , as described in section 4 . To allow for easier access to the most recent and complete version of the dataset for future research , we provide the scraping tool described in section 3.1 . WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer We try to achieve the opposite , biasing texts by reproducing biases inherent in biased fine-tuning datasets . This would , for example , allow for simulating a user that searches for biased terms poten- tially caused by exposure to biased news . With the goal of cap- turing biased language in pretrained language models and using the HuggingFace Transformers module [ 39 ] , we fine-tune the base DistilBERT model , which was trained in Wikipedia articles and a large book dataset.5 DistilBERT– [ 36 ] was chosen as a base model since it has proven to perform well with comparably small datasets , performs better than BERT despite its smaller size [ 36 ] , and for its aptness for adopting domain-specificity [ 3 , 9 ] . We are aware that there are more effective models , but to show the suitability of our approach , we chose DistilBERT due to its efficiency and sufficient effectiveness . We fine-tuned a range of pairs of models , each with one left model fine-tuned on the part of the AllSides corpus tagged as left- biased news and one right model fine-tuned with documents of the dataset labeled as right-biased news . We produced 24 models , a left and a right model for 12 combinations of parameters . Our goal is to find the ideal approach for capturing and reproducing as much bias as possible while producing meaningful results . The central aspect we varied in fine-tuning the models is the data used . The models are developed using three different data configurations : ( a ) the headline and news text of each of the left and right news articles of the AllSides dataset , ( b ) only the news text , and ( c ) only the headline . Another factor we evaluate is the use of padding or concatenation to generate a consistent chunk size for fine-tuning . The chosen chunk size is the max length of fine-tuning documents for the padding approach and 128 for the concatenation approach . Lastly , we compare the effects of intentional overfitting by rais- ing the number of epochs to 20 , compared to a more reasonable 6 epochs , which proved to produce a good combination of training loss and validation loss for all dataset configurations . In theory , overfitting could be another possibility for reproducing biased for- mulations from the texts used in fine-tuning . This is why we tried to intentionally raise the number of epochs as a parameter [ 27 ] . 4.2 Evaluating the Biasing Language Models All 24 models are available upon request . We decided not to make the models publicly available due to concerns about potential harms that could be caused by misuse of the systems that we further elab- orated in section 5 . For evaluating the models ’ effectiveness , we choose a combination of manual assessment and quantitative mea- sures . The task of the models is to generate biased output , however , biases are diverse and not easily measurable . Since we have no way of identifying biased tokens in the dataset , perplexity is not a useful measure for evaluating the model . Further , we can not effectively measure bias as a criterion for the effectiveness of the ability of the systems to produce biased versions of queries . However , we can assume that when masking words in search queries for topics of the political news domain and letting the pairs of language models predict the words , the output of the left and right models should differ from each other . We choose to evaluate their performance on search queries since we want to use the models in future research on bias in search results for biased and unbiased search queries . 5https : Further , they represent texts of a different type but the same domain as the texts used in training the models . For each pair of models , we measure the difference between the two models ’ 10 most probable non-punctuation predictions for 100 randomly selected queries from the dataset by measuring the Rank Biased Overlap ( RBO ) with 𝑝 = 0.9 . We mask a random token of each query that is neither part of the original news topic nor a stop- word since we want to let the models generate meaning-carrying tokens while keeping the query ’ s topic intact . Although nonsensical random predictions or neologisms would lead to great RBO scores , we want to generate queries , that humans could have formulated . To assure that the models generate output that makes sense , we let the models generate next word predictions for ten bias-provoking sentences ( e.g. , `` Hilary Clinton is a '' , `` Covid Vaccines should be '' ) . Two domain experts ( a professor and a postdoctoral researcher ) then label nonsensical and biased predictions . We measure the inter- annotator agreement on the individual statements ( independent of the models , that produce the statements ) with Cohen ’ s Kappa [ 12 ] . For nonsensical predictions , we obtained a value of 0.18 ( slight agreement ) , for biased predictions the value is 0.84 ( almost perfect ) . The bias labels are assigned if the suggested word induces political stance bias . Table 1 shows the results of the model evaluation process . The lower ℎ , 𝑡 , or ℎ𝑡 describes if headline , text , or both have been used . 𝑅𝐵𝑂 is the rank bias overlap between the left and the right model of each configuration . 𝑃𝑛𝑜𝑛𝑠 describes the average percentage of nonsensical predictions of both models and 𝑃𝑏𝑖𝑎𝑠 the percentage of biased predictions . Overall , the models fine-tuned with only headlines show the best ( lowest ) RBO scores , as well as the lowest 𝑃𝑛𝑜𝑛𝑠 scores . Headlines and text and only text perform more or less equally in terms of their RBO . Concatenating texts produces on average better results than padding , although for fine-tuning with only headlines , the effect is minimal . Intentional overfitting by raising the amount of training epochs seems to worsen the RBO scores . The percentage of nonsensical predictions does not differ much between different fine-tuning setups , fine-tuning on texts only seems to be the only scenario that increases the percentage of nonsensical predictions . Many of the models generate a high percentage of biased suggestions , with the headline models having the highest percentage of biased predictions . 5 DISCUSSION Our language models show , that by using small , high-quality datasets , it is possible to fine-tune transformer-based language models to bias texts . In our fine-tuning setup , models fine-tuned with headlines produce the overall best results . As the main reason for that , we assume that the high amount of quotes in news texts , which often are statements the authors of the articles disagree with , might have induced noise in fine-tuning the models . Further , the condensed na- ture of headlines , which aims to catch attention , might also reflect in resulting language models . The output of left and right models fine-tuned with headlines produce texts containing linguistic biases . For example , for `` Donald Trump is a '' , our model produces `` hero '' as right-biased and `` fraud '' as left-biased next word predictions . The overall low percentage of nonsensical predictions supports RBO as a suited evaluation metric . Despite the overall good results , 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Table 1 : Evaluation results of the fine-tuned language models . The highlighted values are the best results for each metric . Model configuration 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝑅𝐵𝑂 𝑃𝑛𝑜𝑛𝑠 0.54 0.04 0.10 0.72 0.08 0.68 0.03 0.56 0.11 0.73 0.07 0.68 0.54 0.04 0.05 0.74 0.07 0.77 0.03 0.58 0.06 0.77 0.07 0.77 𝑃𝑏𝑖𝑎𝑠 0.33 0.14 0.15 0.5 0.14 0.23 0.38 0.28 0.1 0.43 0.24 0.12 future research should investigate the findings with other language models and compare the performance to our results . Our scraping tool and the datasets not only allow us to effectively generate the language models but enables the investigation of other research topics in the political news information search domain such as bias classification or sentiment analysis . Remark on Moral Issues . We are aware , that building bias-inducing systems and providing datasets that enable the reproduction of developing similar systems is problematic . We also did consider that our work can inspire and help to develop transformer-based language models capable of producing biased , toxic , or otherwise harmful texts . The severity of openly published biased systems has been shown by other such models , e.g . the gpt-4chan model.6 The publication of the model incited a debate , that shows how problematic biased language made available to a wide public au- dience are , despite explicitly stating the intended use for research applications [ 16 , 33 ] . Our models are biased primarily in terms of linguistic biases that reflect political stances and views on political topics . Since this can in part include objectively wrong and opinionated statements , hate speech , racism , and other forms of despicable language and toxicity , our models can and will reproduce text , that conveys these phenomena . To minimize the harmful effects of our publication that could be caused by the misuse of the models , we decided to not make our biased models publicly accessible . Despite that , we provide access to the models upon request for research , if a reasonable application use case is provided . However , we do so in the interest of investigating biases and correlations of bias in online information search , with the goal of increasing transparency and fairness . Raising awareness for how easily , intentionally or not , biases can be reproduced and induced with AI systems is part of this endeavor . Although we have shown how the data we provide can be used to create models that produce biased language we chose to publish the datasets and scraper . This is mainly because of three reasons : ( a ) as stated in section 3 similar datasets are available publicly , ( b ) we believe that the benefits of raising awareness outweigh making 6https : already public biased data more accessible , and ( c ) datasets on biased language are required for developing systems to detect and inhibit bias . With our work , we hope to highlight the need to assure , that data used for developing models is unbiased and raise awareness for how easily transformer-based language models can be fine-tuned to produce biased language . 6 OUTLOOK This publication represents the first and foundational milestone of a larger-scale investigation of bias in online information search . As a major next contribution , we plan to use the presented datasets and models to investigate the effects of biased and unbiased search queries on the search results of different search engines for popular topics of the U.S. political news domain . To accomplish this , we need to overcome the issue of subjectivity and lack of effective method- ological approaches to bias identification other than by employing human annotation . We plan to introduce an ensemble of methods , including bias-agnostic analysis of linguistic differences , lexical features , and transformer-based approaches for bias classification . Further , we plan to conduct a study using a simulation approach . By simulation different user behaviors in terms of information need , formulating queries , and interacting with query suggestions in an interactive information search simulation , we plan to gain insights into echo chamber effects and bias formation in information search . Additionally , we hope to identify which user properties and be- haviors increase and which lower the risk of encountering bias in information search . 7 CONCLUSION This work presents 𝑄𝑏𝑖𝑎𝑠 , a first milestone of our ongoing research on bias in online search . We present two datasets : a biased news dataset and a large dataset of biased and unbiased search queries for topics of the U.S. political news domain . Further , we provide a scraping tool , that allows for collecting bias-labeled news texts from AllSides . Lastly , we evaluate approaches to fine-tuning DistilBERT transformer-based language models for biasing texts and publish our models capable of inducing left and right political stance bias in the form of lexical biases . REFERENCES [ 1 ] AllSides . 2021 . How AllSides Creates Balanced News : A Step-by-Step Guide . Retrieved Nov 30 , 2022 from https : create-balanced-news [ 2 ] AllSides . 2022 . Balanced News Headlines Roundup . Retrieved Nov 30 , 2022 from https : [ 3 ] Jing Bai , Rui Cao , Wen Ma , and Hiroyuki Shinnou . 2020 . Construction of Domain- Specific DistilBERT Model by Using Fine-Tuning . In TAAI . 237–241 . [ 4 ] Ramy Baly , Giovanni Da San Martino , James Glass , and Preslav Nakov . 2020 . We Can Detect Your Bias : Predicting the Political Ideology of News Articles . In EMNLP . 4982–4991 . [ 5 ] Michael Barbaro and Tom Zeller . 2006 . A Face is exposed for AOL searcher no . 4417749 . New York Times ( 01 2006 ) . [ 6 ] Nicholas Belkin , Colleen Cool , Diane Kelly , S.-J Lin , S.Y Park , Jose Perez-carballo , and Cynthia Sikora . 2001 . Iterative exploration , design and evaluation of support for query reformulation in interactive information retrieval . IPM 37 ( 05 2001 ) , 403–434 . [ 7 ] Tolga Bolukbasi , Kai-Wei Chang , James Zou , Venkatesh Saligrama , and Adam Kalai . 2016 . Man is to Computer Programmer as Woman is to Homemaker ? Debiasing Word Embeddings . In NIPS . 4356–4364 . [ 8 ] Malte Bonart , Anastasiia Samokhina , Gernot Heisenberg , and Philipp Schaer . 2019 . An investigation of biases in web search engine query suggestions . OIR 44 , 2 ( 2019 ) , 365–381 . WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer [ 39 ] Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 . Transformers : State-of-the-Art Natural Language Processing . In EMNLP . 38–45 . [ 40 ] Jinxi Xu and W. Bruce Croft . 2000 . Improving the Effectiveness of Information Retrieval with Local Context Analysis . ACM Trans . Inf . Syst . 18 , 1 ( Jan. 2000 ) , 79–112 . https : //doi.org/10.1145/333135.333138 [ 9 ] Berfu Büyüköz , Ali Hürriyetoˇglu , and Arzucan Özgür . 2020 . Analyzing ELMo and DistilBERT on Socio-political News Classification . In AESPEN . [ 10 ] Fei Cai and Maarten de Rijke . 2016 . A Survey of Query Auto Completion in Information Retrieval . FNTIR 10 , 4 ( 2016 ) , 273–363 . [ 11 ] Wei-Fan Chen , Khalid Al Khatib , Henning Wachsmuth , and Benno Stein . 2020 . Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity . In NLPCSS . 149–154 . https : [ 12 ] Jacob Cohen . 1960 . A Coefficient of Agreement for Nominal Scales . Educ Psychol Meas 20 , 1 ( 1960 ) , 37–46 . [ 13 ] Dave D ’ Alessio and Mike Allen . 2000 . Media Bias in Presidential Elections : A Meta-Analysis . J. Commun . 50 , 4 ( 2000 ) , 133–156 . [ 14 ] Edelman . 2022 . 2022 Edelman Trust Barometer . Retrieved Nov 30 , 2022 from https : [ 15 ] Robert Epstein and Ronald E. Robertson . 2015 . The search engine manipulation effect ( SEME ) and its possible impact on the outcomes of elections . PNAS 112 , 33 , E4512–E4521 . Publisher : National Academy of Sciences Section : PNAS Plus . [ 16 ] Matthew Gault . 2022 . AI Trained on 4Chan Becomes ‘ Hate Speech Machine ’ . Retrieved Feb 28 , 2023 from https : [ 17 ] Bertram Gawronski . 2021 . Partisan bias in the identification of fake news . TiCS 25 , 9 ( 2021 ) , 723–724 . [ 18 ] Fabian Haak and Philipp Schaer . 2021 . Perception-Aware Bias Detection for Query Suggestions . In BIAS . 130–142 . [ 19 ] Fabian Haak and Philipp Schaer . 2022 . Auditing Search Query Suggestion Bias Through Recursive Algorithm Interrogation . In WebSci . 219–227 . [ 20 ] Daniel Hienert , Philipp Schaer , Johann Schaible , and Philipp Mayr . 2011 . A Novel Combined Term Suggestion Service for Domain-Specific Digital Libraries .. In TPDL ( Lecture Notes in Computer Science , Vol . 6966 ) , Stefan Gradmann , Francesca Borri , Carlo Meghini , and Heiko Schuldt ( Eds. ) . Springer , 192–203 . http : //dblp . # HienertSSM11 [ 21 ] Christoph Hube and Besnik Fetahu . 2019 . Neural Based Statement Classification for Biased Language . In WSDM . ACM . [ 22 ] L. Introna and H. Nissenbaum . 2000 . Defining the Web : the politics of search engines . Computer 33 , 1 , 54–62 . [ 23 ] Juhi Kulshrestha , Motahhare Eslami , Johnnatan Messias , Muhammad Bilal Zafar , Saptarshi Ghosh , Krishna P. Gummadi , and Karrie Karahalios . 2019 . Search bias quantification : investigating political bias in social media and web search . Inf . Retr . J . 22 , 1 , 188–227 . [ 24 ] Ruibo Liu , Chenyan Jia , and Soroush Vosoughi . 2021 . A Transformer-based Framework for Neutralizing and Reversing the Political Polarity of News Articles . Proc . ACM Hum.-Comput . Interact . 5 , 1–26 . [ 25 ] Bhaskar Mitra , Milad Shokouhi , Filip Radlinski , and Katja Hofmann . 2014 . On user interactions with query auto-completion . In SIGIR . 1055–1058 . [ 26 ] Negar Mokhberian , André s Abeliuk , Patrick Cummings , and Kristina Lerman . 2020 . Moral Framing and Ideological Bias of News . In SocInfo . 206–219 . [ 27 ] Marius Mosbach , Maksym Andriushchenko , and Dietrich Klakow . 2021 . On the Stability of Fine-tuning BERT : Misconceptions , Explanations , and Strong Baselines . In ICLR . https : //openreview.net/forum ? id=nzpLWnVAyah [ 28 ] Xi Niu and Diane Kelly . 2014 . The use of query suggestions during information search . IPM 50 , 1 , 218–234 . [ 29 ] Greg Pass , Abdur Chowdhury , and Cayley Torgeson . 2006 . A Picture of Search . In InfoScale . 1–es . [ 30 ] Lily Ray . 2020 . 2020 Google Search Survey : How Much Do Users Trust Their Search Results ? Retrieved Nov 30 , 2022 from https : //moz.com/blog/2020-google- search-survey [ 31 ] Shaina Raza , Deepak John Reji , and Chen Ding . 2022 . Dbias : Detecting biases and ensuring Fairness in news articles . Int J Data Sci Anal ( 2022 ) . [ 33 ] Reddit . 2022 . [ 32 ] Marta Recasens , Cristian Danescu-Niculescu-Mizil , and Dan Jurafsky . 2013 . Lin- guistic Models for Analyzing and Detecting Biased Language . In ACL . 1650–1659 . Retrieved Feb 28 , 2023 from https : the_worst_ai_ever_gpt4chan_model/ This is the worst AI ever . [ 34 ] Ronald E. Robertson , Shan Jiang , Kenneth Joseph , Lisa Friedland , David Lazer , and Christo Wilson . 2018 . Auditing Partisan Audience Bias within Google Search . Proc . ACM Hum.-Comput . Interact . 2 , CSCW , Article 148 ( 2018 ) . [ 35 ] Ronald E. Robertson , Shan Jiang , David Lazer , and Christo Wilson . 2019 . Auditing Autocomplete : Suggestion Networks and Recursive Algorithm Interrogation . In WebSci . 235–244 . [ 36 ] Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf . 2019 . Dis- tilBERT , a distilled version of BERT : smaller , faster , cheaper and lighter . CoRR abs/1910.01108 ( 2019 ) . [ 37 ] Danny Sullivan . 2018 . How Google autocomplete works in Search . Retrieved Nov 30 , 2022 from https : works-search/ [ 38 ] Peng Wang , Xianghang mi , Xiaojing Liao , Xiaofeng Wang , Kan Yuan , Feng Qian , and Raheem Beyah . 2018 . Game of Missuggestions : Semantic Analysis of Search- Autocomplete Manipulations . In NDSS .","['dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', 'fabian', 'haak', 'fabianhaak', 'thkoelnde', 'technische', 'hochschule', 'köln', 'cologne', 'thkoelnde', 'technische', 'hochschule', 'köln', 'cologne', 'publication', 'describe', 'motivation', 'generation', 'large', 'dataset', 'bing', 'search', 'query', 'scrape', 'tool', 'dataset', 'biased', 'news', 'article', 'well', 'language', 'model', 'investigation', 'bias', 'online', 'search', 'web', 'search', 'engine', 'major', 'factor', 'trust', 'source', 'information', 'search', 'especially', 'political', 'domain', 'however', 'bias', 'information', 'influence', 'opinion', 'formation', 'lead', 'biased', 'opinion', 'interact', 'search', 'engine', 'user', 'formulate', 'search', 'query', 'interact', 'search', 'query', 'suggestion', 'provide', 'search', 'engine', 'lack', 'dataset', 'search', 'query', 'inhibit', 'research', 'subject', 'use', 'evaluate', 'different', 'approach', 'finetune', 'transformer', 'base', 'language', 'model', 'goal', 'produce', 'model', 'capable', 'bias', 'text', 'left', 'right', 'political', 'stance', 'additionally', 'work', 'provide', 'dataset', 'language', 'model', 'bias', 'text', 'allow', 'research', 'bias', 'online', 'information', 'search', 'ccs', 'concept', 'information', 'system', 'web', 'search', 'engine', 'query', 'suggestion', 'query', 'reformulation', 'computing', 'methodology', 'natural', 'language', 'generation', 'keyword', 'web', 'search', 'dataset', 'bias', 'query', 'suggestion', 'search', 'query', 'language', 'model', 'transformer', 'acm', 'reference', 'format', 'fabian', 'haak', 'schaer', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '15th', 'acm', 'web', 'science', 'conference', 'page', 'n', 'r', 'c', 'v', 'r', 'introduction', 'search', 'engine', 'bing', 'see', 'trustworthy', 'source', 'information', 'many', 'topic', 'include', 'political', 'news', 'information', 'search', 'engine', 'prove', 'major', 'impact', 'formation', 'political', 'opinion', 'interact', 'search', 'engine', 'user', 'formulate', 'search', 'query', 'expression', 'information', 'need', 'base', 'query', 'search', 'engine', 'usually', 'provide', 'set', 'search', 'query', 'suggestion', 'query', 'choice', 'user', 'make', 'interact', 'query', 'suggestion', 'base', 'information', 'need', 'want', 'satisfy', 'often', 'support', 'personal', 'opinion', 'belief', 'found', 'previously', 'encounter', 'information', 'interaction', 'process', 'result', 'present', 'user', 'prone', 'bias', 'therefore', 'high', 'level', 'trust', 'see', 'problematic', 'however', 'true', 'effect', 'biased', 'search', 'query', 'different', 'type', 'inherent', 'bias', 'actual', 'list', 'search', 'result', 'sufficiently', 'investigate', 'main', 'reason', 'frequency', 'study', 'bias', 'search', 'query', 'lack', 'publicly', 'available', 'dataset', 'dataset', 'include', 'query', 'query', 'suggestion', 'actual', 'query', 'reformulation', 'rare', 'reason', 'collect', 'search', 'query', 'user', 'problematic', 'privacy', 'concern', 'tech', 'nique', 'pseudonymization', 'query', 'log', 'enable', 'identification', 'user', 'previously', 'available', 'dataset', 'long', 'available', 'usage', 'morally', 'debatable', 'use', 'unpersonalized', 'search', 'query', 'suggestion', 'proxy', 'solve', 'issue', 'query', 'suggestion', 'describe', 'list', 'predict', 'query', 'suggest', 'user', 'input', 'search', 'query', 'search', 'engine', 'sometimes', 'also', 'call', 'search', 'prediction', 'query', 'auto', 'completion', 'search', 'query', 'suggestion', 'generate', 'locally', 'result', 'set', 'take', 'global', 'knowledge', 'basis', 'web', 'search', 'suggestion', 'mostly', 'base', 'frequently', 'issue', 'query', 'user', 'therefore', 'assume', 'many', 'case', 'query', 'suggestion', 'popular', 'related', 'search', 'query', 'initial', 'root', 'query', 'represent', 'user', 'interest', 'topic', 'entity', 'news', 'domain', 'popular', 'domain', 'bias', 'research', 'sociological', 'relevance', 'twoparty', 'leftright', 'spectrum', 'facilitate', 'bias', 'analysis', 'benefit', 'dataset', 'enable', 'indepth', 'investigation', 'correlation', 'bias', 'search', 'query', 'search', 'result', 'search', 'result', 'popu', 'lar', 'web', 'search', 'engine', 'easily', 'collect', 'use', 'provide', 'dataset', 'search', 'query', 'therefore', 'goal', 'work', 'provide', 'large', 'dataset', 'search', 'query', 'suggestion', 'litical', 'news', 'domain', 'additionally', 'provide', 'transformerbased', 'methodology', 'bias', 'text', 'system', 'useful', 'tool', '30may', 'generate', 'biased', 'derivative', 'search', 'query', 'thus', 'evaluate', 'range', 'finetune', 'scenario', 'find', 'setting', 'produce', 'biased', 'result', 'provide', 'finetune', 'transformerbase', 'language', 'model', 'capable', 'induce', 'left', 'right', 'political', 'stance', 'bias', 'form', 'lexical', 'bias', 'publication', 'describe', 'motivation', 'generation', 'large', 'dataset', 'bing', 'search', 'query', 'scrape', 'tool', 'dataset', 'biased', 'news', 'article', 'well', 'language', 'model', 'investigation', 'bias', 'online', 'search', 'main', 'contribution', 'paper', 'datasets1', 'good', 'knowledge', 'large', 'label', 'dataset', 'search', 'query', 'suggestion', 'single', 'main', 'google', 'bing', 'dataset', 'biased', 'news', 'article', 'political', 'news', 'domain', 'scrape', 'tool', 'allow', 'researcher', 'easily', 'retrieve', 'uptodate', 'version', 'biased', 'news', 'dataset2', 'new', 'approach', 'produce', 'biased', 'search', 'query', 'intentionally', 'bias', 'transformerbase', 'language', 'model', 'capable', 'produce', 'biased', 'text', 'evaluation', 'different', 'finetuning', 'setting', 'find', 'capable', 'setup', 'produce', 'biasinduce', 'language', 'model', 'background', 'related', 'work', 'definition', 'bias', 'news', 'domain', 'subjective', 'ture', 'bias', 'publication', 'attempt', 'explicitly', 'define', 'bias', 'news', 'bias', 'news', 'generally', 'describe', 'nonneutral', 'opinionated', 'news', 'fuzzy', 'attribute', 'polit', 'ical', 'domain', 'term', 'opinionate', 'describe', 'fix', 'political', 'opinion', 'agenda', 'aspect', 'cognitive', 'author', 'readerside', 'bias', 'often', 'describe', 'partisan', 'bias', 'political', 'stance', 'partisanship', 'manifest', 'different', 'granularity', 'publishing', 'reporting', 'level', 'partisanship', 'express', 'selection', 'certain', 'topic', 'call', 'selection', 'bias', 'coverage', 'bias', 'selection', 'dif', 'ferent', 'view', 'topic', 'text', 'level', 'statement', 'bias', 'describe', 'member', 'medium', 'interject', 'opinion', 'text', 'manifest', 'overall', 'opinionate', 'text', 'take', 'various', 'form', 'range', 'phrase', 'bias', 'use', 'nonneutral', 'language', 'moral', 'framing', 'ideological', 'bias', 'spin', 'bias', 'describe', 'bias', 'introduce', 'omit', 'necessary', 'neutral', 'information', 'add', 'unnecessary', 'information', 'word', 'ngram', 'level', 'bias', 'manifest', 'linguistic', 'lexical', 'bias', 'linguistic', 'bias', 'highly', 'domain', 'contextspecific', 'example', 'frame', 'bias', 'describe', 'subjective', 'word', 'epistemo', 'logical', 'bias', 'attribute', 'word', 'target', 'credibility', 'statement', 'bias', 'identify', 'objectively', 'approach', 'detect', 'bias', 'rely', 'identification', 'linguistic', 'bias', 'research', 'bias', 'online', 'search', 'search', 'query', 'pub', 'lication', 'investigate', 'bias', 'online', 'search', 'aspect', 'search', 'result', 'investigate', 'partisan', 'bias', 'filter', 'bubble', 'effect', 'political', 'search', 'audit', 'serps', 'set', 'query', 'find', 'significant', 'evidence', 'unbiased', 'search', 'query', 'real', 'search', 'session', 'perform', 'real', 'user', 'lead', 'filter', 'bubble', 'effect', 'unanswered', 'question', 'biased', 'query', 'general', 'lead', 'biased', 'search', 'result', 'study', 'investigate', 'bias', 'search', 'query', 'difficult', 'accessibility', 'biased', 'search', 'query', 'study', 'focus', 'bias', 'search', 'query', 'suggestion', 'author', 'investigate', 'topical', 'group', 'bias', 'search', 'query', 'suggestion', 'political', 'domain', 'overall', 'observe', 'minor', 'topical', 'gender', 'bias', 'search', 'query', 'consist', 'name', 'politician', 'research', 'bias', 'online', 'search', 'show', 'factor', 'topic', 'query', 'phrasing', 'query', 'time', 'query', 'issue', 'also', 'impact', 'bias', 'see', 'user', 'dataset', 'biased', 'news', 'search', 'query', 'predict', 'medium', 'bias', 'use', 'news', 'article', 'collect', 'allside', 'balanced', 'news3', 'allside', 'news', 'popular', 'source', 'balanced', 'news', 'allside', 'high', 'standard', 'assign', 'bias', 'label', 'similarly', 'use', 'allsideslabele', 'news', 'article', 'adfonte', 'label', 'analyze', 'bias', 'different', 'granularity', 'mokhbe', 'rian', 'develop', 'frame', 'bias', 'detection', 'quantification', 'approach', 'use', 'collection', 'news', 'article', 'label', 'accord', 'label', 'provide', 'allsides4', 'dataset', 'search', 'query', 'query', 'suggestion', 'sparse', 'available', 'anymore', 'focus', 'narrow', 'topic', 'robertson', 'introduce', 'recursive', 'technique', 'recursively', 'retrieve', 'query', 'suggestion', 'root', 'query', 'consecutive', 'suggestion', 'technique', 'employ', 'haak', 'schaer', 'investigate', 'bias', 'german', 'political', 'domain', 'however', 'good', 'knowledge', 'currently', 'largescale', 'dataset', 'search', 'query', 'allside', 'scraper', 'dataset', 'present', 'novel', 'dataset', 'promote', 'investigation', 'bias', 'online', 'news', 'search', 'dataset', 'find', 'zenodo', 'mention', 'section', 'far', 'provide', 'web', 'scraping', 'tool', 'retrieve', 'uptodate', 'version', 'allside', 'dataset', 'provide', 'section', 'describe', 'content', 'dataset', 'well', 'creation', 'process', 'figure', 'show', 'assemble', 'dataset', 'use', 'approach', 'create', 'bias', 'language', 'model', 'allside', 'scraper', 'describe', 'section', 'allside', 'platform', 'provide', 'news', 'frequently', 'use', 'source', 'high', 'quality', 'label', 'biased', 'news', 'want', 'provide', 'easy', 'mean', 'retrieve', 'news', 'corresponding', 'information', 'similar', 'dataset', 'previously', 'produce', 'mention', 'section', 'however', 'compare', 'currently', 'recent', 'available', 'version', 'version', 'include', 'percent', 'additional', 'article', 'furthermore', 'many', 'task', 'especially', 'news', 'domain', 'relevant', 'recent', 'document', 'available', 'provide', 'pythonbased', 'scraper', 'scrape', 'available', 'allside', 'news', 'article', 'gather', 'available', 'information', 'describe', 'section', 'provide', 'scraper', 'facilitate', 'access', 'recent', 'version', 'dataset', 'researcher', 'dataset', 'find', 'zenodo', 'https', 'doiorg105281zenodo7682914', 'githubcomirgroupqbia', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '30may', 'search', 'query', 'news', 'domain', 'dataset', 'second', 'dataset', 'provide', 'consist', 'search', 'query', 'suggestion', 'root', 'query', 'base', 'tag', 'allside', 'bias', 'news', 'dataset', 'collect', 'search', 'query', 'suggestion', 'bing', 'topic', 'tag', 'use', 'tagging', 'allside', 'news', 'least', 'time', 'approximately', 'half', 'total', 'amount', 'topic', 'topic', 'tag', 'include', 'name', 'wide', 'range', 'political', 'term', 'agenda', 'topic', 'communism', 'party', 'samesex', 'marriage', 'cultural', 'religious', 'term', 'francis', 'location', 'newsrelevant', 'term', 'average', 'dataset', 'contain', 'search', 'query', 'topic', 'total', 'suggestion', 'retrieve', 'bing', 'use', 'python', 'implementation', 'loosely', 'adopt', 'framework', 'provide', 'scrape', 'query', 'suggestion', 'topic', 'root', 'query', 'use', 'run', 'scraper', 'retrieve', 'search', 'query', 'suggestion', 'provide', 'bing', 'search', 'autocomplete', 'system', 'input', 'root', 'query', 'furthermore', 'extend', 'root', 'query', 'let', 'ter', 'root', 'term', 'query', 'input', 'democrat', 'recession', 'query', 'suggestion', 'goal', 'procedure', 'simulate', 'user', 'input', 'information', 'search', 'retrieve', 'suggestion', 'root', 'query', 'ex', 'tend', 'query', 'generate', 'total', 'query', 'suggestion', 'topic', 'search', 'engine', 'dataset', 'provide', 'contain', 'column', 'root', 'term', 'query', 'input', 'query', 'suggestion', 'suggest', 'query', 'location', 'search', 'perform', 'loca', 'tion', 'server', 'run', 'colab', 'case', 'iowa', 'add', 'dataset', 'perform', 'scrape', 'blank', 'browser', 'search', 'personalization', 'effect', 'location', 'effect', 'suggest', 'search', 'query', 'scrape', 'setup', 'thus', 'eliminate', 'sonalization', 'effect', 'theory', 'cause', 'fect', 'search', 'engine', 'provider', 'describe', 'query', 'suggestion', 'base', 'user', 'search', 'successful', 'attempt', 'fluence', 'query', 'suggestion', 'confirm', 'claim', 'thus', 'deduce', 'query', 'suggestion', 'reflect', 'real', 'frequently', 'use', 'search', 'query', 'use', 'proxy', 'search', 'query', 'lack', 'information', 'frequency', 'collect', 'search', 'query', 'dataset', 'contain', 'rank', 'suggestion', 'well', 'sugge', 'tion', 'root', 'term', 'approximate', 'frequent', 'search', 'query', 'assume', 'topical', 'tag', 'allside', 'news', 'reflect', 'popular', 'topic', 'produce', 'dataset', 'consist', 'real', 'search', 'query', 'newsrelevant', 'topic', 'develop', 'bias', 'language', 'model', 'contribution', 'produce', 'pair', 'transformer', 'base', 'language', 'model', 'capable', 'induce', 'left', 'right', 'partisanship', 'linguistic', 'bias', 'leverage', 'masking', 'function', 'word', 'target', 'document', 'domainadopte', 'distilbert', 'section', 'describe', 'methodological', 'approach', 'develop', 'transformerbase', 'language', 'model', 'capable', 'bias', 'text', 'usually', 'system', 'develop', 'goal', 'produce', 'reproduce', 'little', 'bias', 'possible', 'bias', 'text', 'figure', 'pipeline', 'use', 'generate', 'biasinduce', 'language', 'model', 'black', 'box', 'represent', 'dataset', 'language', 'model', 'available', 'request', 'tool', 'provide', 'zenodo', 'allside', 'biased', 'news', 'dataset', 'dataset', 'contain', 'news', 'article', 'collect', 'allside', 'balanced', 'news', 'headline', 'roundup', 'side', 'balance', 'news', 'feature', 'expertselecte', 'news', 'article', 'source', 'different', 'political', 'view', 'leave', 'right', 'center', 'often', 'feature', 'spin', 'bias', 'slant', 'form', 'nonneutral', 'reporting', 'political', 'news', 'article', 'tag', 'bias', 'label', 'expert', 'annotator', 'base', 'express', 'political', 'partisanship', 'leave', 'right', 'neutral', 'allside', 'balance', 'news', 'aim', 'offer', 'multiple', 'political', 'perspective', 'important', 'news', 'story', 'educate', 'user', 'bias', 'provide', 'multiple', 'viewpoint', 'collect', 'datum', 'include', 'headline', 'news', 'text', 'publishing', 'date', 'topic', 'tag', 'coronavirus', 'federal', 'job', 'link', 'article', 'publishing', 'outlet', 'also', 'include', 'allside', 'neutral', 'description', 'topic', 'article', 'overall', 'produce', 'dataset', 'contain', 'article', 'tag', 'leave', 'article', 'tag', 'right', 'article', 'tag', 'center', 'collect', 'article', 'publish', 'date', 'datum', 'collection', 'end', 'use', 'allside', 'dataset', 'develop', 'bias', 'language', 'model', 'describe', 'section', 'allow', 'easy', 'access', 'recent', 'complete', 'version', 'dataset', 'future', 'research', 'provide', 'scrape', 'tool', 'describe', 'section', '30may', 'haak', 'schaer', 'try', 'achieve', 'opposite', 'bias', 'text', 'reproduce', 'bias', 'inherent', 'biased', 'finetune', 'dataset', 'example', 'allow', 'simulate', 'user', 'search', 'biased', 'term', 'poten', 'tially', 'cause', 'exposure', 'biased', 'news', 'goal', 'cap', 'ture', 'biased', 'language', 'pretraine', 'language', 'model', 'use', 'huggingface', 'transformer', 'module', 'finetune', 'base', 'distilbert', 'model', 'train', 'wikipedia', 'article', 'large', 'book', 'dataset5', 'distilbert', 'choose', 'base', 'model', 'prove', 'perform', 'well', 'comparably', 'small', 'dataset', 'perform', 'well', 'small', 'size', 'aptness', 'adopt', 'domainspecificity', 'aware', 'effective', 'model', 'show', 'suitability', 'approach', 'choose', 'distilbert', 'efficiency', 'sufficient', 'effectiveness', 'finetune', 'range', 'pair', 'model', 'left', 'model', 'finetune', 'part', 'allside', 'tag', 'leave', 'biased', 'news', 'right', 'model', 'finetune', 'document', 'dataset', 'label', 'rightbiase', 'news', 'produce', 'model', 'left', 'right', 'model', 'combination', 'parameter', 'goal', 'find', 'ideal', 'approach', 'capture', 'reproduce', 'much', 'bias', 'possible', 'produce', 'meaningful', 'result', 'central', 'aspect', 'vary', 'finetune', 'model', 'datum', 'use', 'model', 'develop', 'use', 'different', 'datum', 'configuration', 'headline', 'news', 'text', 'left', 'right', 'news', 'article', 'allside', 'dataset', 'b', 'news', 'text', 'c', 'headline', 'factor', 'evaluate', 'use', 'padding', 'concatenation', 'generate', 'consistent', 'chunk', 'size', 'finetune', 'choose', 'chunk', 'size', 'length', 'finetune', 'document', 'padding', 'approach', 'concatenation', 'approach', 'lastly', 'compare', 'effect', 'intentional', 'overfitting', 'number', 'epoch', 'compare', 'reasonable', 'epoch', 'prove', 'produce', 'good', 'combination', 'training', 'loss', 'validation', 'loss', 'dataset', 'configuration', 'theory', 'overfitting', 'possibility', 'reproduce', 'bias', 'mulation', 'text', 'use', 'finetune', 'try', 'intentionally', 'raise', 'number', 'epoch', 'parameter', 'evaluate', 'bias', 'language', 'model', 'model', 'available', 'request', 'decide', 'make', 'model', 'publicly', 'available', 'concern', 'potential', 'harm', 'cause', 'misuse', 'system', 'far', 'orate', 'section', 'evaluate', 'model', 'effectiveness', 'choose', 'combination', 'manual', 'assessment', 'quantitative', 'mea', 'sure', 'task', 'model', 'generate', 'biased', 'output', 'however', 'bias', 'diverse', 'easily', 'measurable', 'way', 'identify', 'biased', 'token', 'dataset', 'perplexity', 'useful', 'measure', 'evaluate', 'model', 'far', 'effectively', 'measure', 'bias', 'criterion', 'effectiveness', 'ability', 'system', 'produce', 'biased', 'version', 'query', 'however', 'assume', 'mask', 'word', 'search', 'query', 'topic', 'political', 'news', 'domain', 'let', 'pair', 'language', 'model', 'predict', 'word', 'output', 'left', 'right', 'model', 'differ', 'choose', 'evaluate', 'performance', 'search', 'query', 'want', 'use', 'model', 'future', 'research', 'bias', 'search', 'result', 'biased', 'unbiased', 'search', 'query', 'far', 'represent', 'text', 'different', 'type', 'domain', 'text', 'use', 'train', 'model', 'pair', 'model', 'measure', 'difference', 'model', 'probable', 'nonpunctuation', 'prediction', 'randomly', 'select', 'query', 'dataset', 'measure', 'rank', 'bias', 'overlap', 'rbo', '𝑝', 'mask', 'random', 'token', 'query', 'part', 'original', 'news', 'topic', 'stop', 'word', 'want', 'let', 'model', 'generate', 'meaningcarrye', 'token', 'keep', 'query', 'topic', 'intact', 'nonsensical', 'random', 'prediction', 'neologism', 'lead', 'great', 'rbo', 'score', 'want', 'generate', 'query', 'human', 'formulate', 'assure', 'model', 'generate', 'output', 'make', 'sense', 'let', 'model', 'generate', 'next', 'word', 'prediction', 'biasprovoke', 'sentence', 'hilary', 'covid', 'vaccine', 'domain', 'expert', 'professor', 'postdoctoral', 'researcher', 'label', 'nonsensical', 'biased', 'prediction', 'measure', 'annotator', 'agreement', 'individual', 'statement', 'independent', 'model', 'produce', 'statement', 'nonsensical', 'prediction', 'obtain', 'value', 'slight', 'agreement', 'biased', 'prediction', 'value', 'almost', 'perfect', 'bias', 'label', 'assign', 'suggest', 'word', 'induce', 'political', 'stance', 'bias', 'table', 'show', 'result', 'model', 'evaluation', 'process', 'low', 'ℎ𝑡', 'describe', 'headline', 'text', 'use', 'rank', 'bias', 'overlap', 'left', 'right', 'model', 'configuration', 'describe', 'average', 'percentage', 'nonsensical', 'prediction', 'model', '𝑃𝑏𝑖𝑎𝑠', 'percentage', 'biased', 'prediction', 'overall', 'model', 'finetune', 'headline', 'show', 'good', 'low', 'rbo', 'score', 'well', 'low', 'score', 'headline', 'text', 'text', 'perform', 'less', 'equally', 'term', 'rbo', 'concatenate', 'text', 'produce', 'average', 'well', 'result', 'pad', 'finetune', 'headline', 'effect', 'minimal', 'intentional', 'overfitting', 'raise', 'amount', 'training', 'epoch', 'seem', 'worsen', 'rbo', 'score', 'percentage', 'nonsensical', 'prediction', 'differ', 'much', 'different', 'finetuning', 'setup', 'finetune', 'text', 'seem', 'scenario', 'increase', 'percentage', 'nonsensical', 'prediction', 'many', 'model', 'generate', 'high', 'percentage', 'biased', 'suggestion', 'headline', 'model', 'high', 'percentage', 'biased', 'prediction', 'discussion', 'language', 'model', 'show', 'use', 'small', 'highquality', 'dataset', 'possible', 'finetune', 'transformerbase', 'language', 'model', 'bias', 'text', 'finetune', 'setup', 'model', 'finetune', 'headline', 'produce', 'overall', 'good', 'result', 'main', 'reason', 'assume', 'high', 'amount', 'quote', 'news', 'text', 'often', 'statement', 'author', 'article', 'disagree', 'induce', 'noise', 'finetune', 'model', 'far', 'condense', 'ture', 'headline', 'aim', 'catch', 'attention', 'also', 'reflect', 'result', 'language', 'model', 'output', 'left', 'right', 'model', 'finetune', 'headline', 'produce', 'text', 'contain', 'linguistic', 'bias', 'example', 'trump', 'model', 'produce', 'hero', 'rightbiase', 'fraud', 'leftbiase', 'next', 'word', 'prediction', 'overall', 'low', 'percentage', 'nonsensical', 'prediction', 'support', 'rbo', 'suited', 'evaluation', 'metric', 'overall', 'good', 'result', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '30may', 'table', 'evaluation', 'result', 'finetune', 'language', 'model', 'highlight', 'value', 'good', 'result', 'metric', 'model', 'configuration', '𝑜𝑣𝑒𝑟', '𝑖𝑡𝑡𝑖𝑛𝑔', '𝑜𝑣𝑒𝑟', '𝑜𝑣𝑒𝑟', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ', '𝑜𝑣𝑒𝑟', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡', '𝑜𝑣𝑒𝑟', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡', '𝑜𝑣𝑒𝑟', 'future', 'research', 'investigate', 'finding', 'language', 'model', 'compare', 'performance', 'result', 'scrape', 'tool', 'dataset', 'allow', 'effectively', 'generate', 'language', 'model', 'enable', 'investigation', 'research', 'topic', 'political', 'news', 'information', 'search', 'domain', 'bias', 'classification', 'sentiment', 'analysis', 'remark', 'moral', 'issue', 'aware', 'build', 'biasinducing', 'system', 'provide', 'dataset', 'enable', 'reproduction', 'develop', 'similar', 'system', 'problematic', 'also', 'consider', 'work', 'inspire', 'help', 'develop', 'transformerbase', 'language', 'model', 'capable', 'produce', 'biased', 'toxic', 'otherwise', 'harmful', 'text', 'severity', 'openly', 'publish', 'biased', 'system', 'show', 'model', 'model6', 'publication', 'model', 'incite', 'debate', 'show', 'problematic', 'biased', 'language', 'make', 'available', 'wide', 'public', 'au', 'dience', 'explicitly', 'state', 'intend', 'use', 'research', 'application', 'model', 'bias', 'primarily', 'term', 'linguistic', 'bias', 'reflect', 'political', 'stance', 'view', 'political', 'topic', 'part', 'include', 'objectively', 'wrong', 'opinionated', 'statement', 'hate', 'speech', 'racism', 'form', 'despicable', 'language', 'toxicity', 'model', 'reproduce', 'text', 'convey', 'phenomenon', 'minimize', 'harmful', 'effect', 'publication', 'cause', 'misuse', 'model', 'decide', 'make', 'biased', 'model', 'publicly', 'accessible', 'provide', 'access', 'model', 'request', 'research', 'reasonable', 'application', 'use', 'case', 'provide', 'however', 'interest', 'investigate', 'bias', 'correlation', 'bias', 'online', 'information', 'search', 'goal', 'increase', 'transparency', 'fairness', 'raise', 'awareness', 'easily', 'intentionally', 'bias', 'reproduce', 'induce', 'system', 'part', 'endeavor', 'show', 'datum', 'provide', 'use', 'create', 'model', 'produce', 'biased', 'language', 'choose', 'publish', 'dataset', 'scraper', 'mainly', 'reason', 'state', 'section', 'similar', 'dataset', 'available', 'publicly', 'b', 'believe', 'benefit', 'raise', 'awareness', 'outweigh', 'make', '6https', 'already', 'public', 'biased', 'datum', 'accessible', 'c', 'dataset', 'biased', 'language', 'require', 'develop', 'system', 'detect', 'inhibit', 'bias', 'work', 'hope', 'highlight', 'need', 'assure', 'datum', 'use', 'develop', 'model', 'unbiased', 'raise', 'awareness', 'easily', 'transformerbase', 'language', 'model', 'finetune', 'produce', 'biased', 'language', 'outlook', 'publication', 'represent', 'first', 'foundational', 'milestone', 'largerscale', 'investigation', 'bias', 'online', 'information', 'search', 'major', 'next', 'contribution', 'plan', 'use', 'present', 'dataset', 'model', 'investigate', 'effect', 'biased', 'unbiased', 'search', 'query', 'search', 'result', 'different', 'search', 'engine', 'popular', 'topic', 'political', 'news', 'domain', 'accomplish', 'need', 'overcome', 'issue', 'subjectivity', 'lack', 'effective', 'method', 'ological', 'approach', 'bias', 'identification', 'employ', 'human', 'annotation', 'plan', 'introduce', 'ensemble', 'method', 'include', 'biasagnostic', 'analysis', 'linguistic', 'difference', 'lexical', 'feature', 'transformerbase', 'approach', 'bias', 'classification', 'far', 'plan', 'conduct', 'study', 'use', 'simulation', 'approach', 'simulation', 'different', 'user', 'behavior', 'term', 'information', 'need', 'formulating', 'query', 'interact', 'query', 'suggestion', 'interactive', 'information', 'search', 'simulation', 'plan', 'gain', 'insight', 'effect', 'bias', 'formation', 'information', 'search', 'additionally', 'hope', 'identify', 'user', 'property', 'havior', 'increase', 'lower', 'risk', 'encounter', 'bias', 'information', 'search', 'conclusion', 'work', 'present', 'first', 'milestone', 'ongoing', 'research', 'bias', 'online', 'search', 'present', 'dataset', 'biased', 'news', 'dataset', 'large', 'dataset', 'biased', 'unbiased', 'search', 'query', 'topic', 'political', 'news', 'domain', 'far', 'provide', 'scrape', 'tool', 'allow', 'collect', 'biaslabele', 'news', 'text', 'allside', 'lastly', 'evaluate', 'approach', 'finetune', 'distilbert', 'transformerbase', 'language', 'model', 'bias', 'text', 'publish', 'model', 'capable', 'induce', 'left', 'right', 'political', 'stance', 'bias', 'form', 'lexical', 'bias', 'reference', 'allside', 'allside', 'create', 'balanced', 'news', 'stepbystep', 'guide', 'retrieve', 'https', 'allside', 'balanced', 'news', 'headline', 'roundup', 'retrieve', 'https', 'jing', 'bai', 'shinnou', 'construction', 'domain', 'specific', 'distilbert', 'model', 'use', 'finetune', 'taai', 'preslav', 'nakov', 'detect', 'bias', 'predict', 'political', 'ideology', 'news', 'article', 'emnlp', 'zeller', 'face', 'expose', 'searcher', 'cool', 'diane', 'park', 'iterative', 'exploration', 'design', 'evaluation', 'support', 'query', 'reformulation', 'interactive', 'information', 'retrieval', 'tolga', 'bolukbasi', 'zou', 'venkatesh', 'saligrama', 'man', 'computer', 'programmer', 'woman', 'homemaker', 'debiase', 'word', 'embedding', 'nip', 'schaer', 'investigation', 'bias', 'web', 'search', 'engine', 'query', 'suggestion', '30may', 'haak', 'schaer', 'lysandre', 'debut', 'victor', 'sanh', 'clement', 'delangue', 'pierric', 'cistac', 'clara', 'canwen', 'sylvain', 'gugger', 'quentin', 'lhoest', 'rush', 'transformer', 'stateoftheart', 'natural', 'language', 'processing', 'emnlp', 'improve', 'effectiveness', 'information', 'retrieval', 'local', 'context', 'analysis', 'acm', 'tran', 'inf', 'syst', 'https', 'arzucan', 'özgür', 'analyze', 'elmo', 'distilbert', 'sociopolitical', 'news', 'classification', 'aespen', 'maarten', 'de', 'rijke', 'survey', 'query', 'auto', 'completion', 'information', 'retrieval', 'fntir', 'weifan', 'wachsmuth', 'analyze', 'political', 'bias', 'unfairness', 'news', 'article', 'different', 'level', 'granularity', 'https', 'coefficient', 'agreement', 'nominal', 'scale', 'psychol', 'alessio', 'medium', 'bias', 'presidential', 'election', 'metaanalysis', 'j', 'commun', 'edelman', 'edelman', 'trust', 'barometer', 'retrieve', 'https', 'search', 'engine', 'manipulation', 'effect', 'seme', 'possible', 'impact', 'outcome', 'election', 'pna', 'e4512', 'publisher', 'section', 'pna', 'train', 'become', 'hate', 'speech', 'machine', 'retrieve', 'https', 'bertram', 'gawronski', 'partisan', 'bias', 'identification', 'fake', 'news', 'tic', 'fabian', 'haak', 'schaer', 'perceptionaware', 'bias', 'detection', 'query', 'suggestion', 'bias', 'fabian', 'haak', 'schaer', 'auditing', 'search', 'query', 'suggestion', 'bias', 'schaer', 'schaible', 'novel', 'combine', 'term', 'suggestion', 'service', 'domainspecific', 'digital', 'library', 'tpdl', 'lecture', 'note', 'computer', 'science', 'vol', 'heiko', 'schuldt', 'ed', 'springer', 'http', 'hienertssm11', 'besnik', 'neural', 'base', 'statement', 'classification', 'biased', 'language', 'wsdm', 'acm', 'l', 'introna', 'define', 'web', 'politic', 'search', 'engine', 'computer', 'juhi', 'messia', 'krishna', 'p', 'gummadi', 'search', 'bias', 'quantification', 'investigate', 'political', 'bias', 'social', 'medium', 'web', 'search', 'inf', 'retr', 'j', 'ruibo', 'transformerbase', 'framework', 'neutralize', 'reverse', 'political', 'polarity', 'news', 'article', 'proc', 'acm', 'humcomput', 'interact', 'bhaskar', 'milad', 'shokouhi', 'filip', 'radlinski', 'user', 'interaction', 'query', 'autocompletion', 'sigir', 'negar', 'cumming', 'moral', 'framing', 'ideological', 'bias', 'news', 'socinfo', 'marius', 'andriushchenko', 'stability', 'finetune', 'bert', 'misconception', 'explanation', 'strong', 'baseline', 'iclr', 'https', 'openreviewnetforum', 'idnzplwnvayah', 'niu', 'diane', 'use', 'query', 'suggestion', 'information', 'search', 'ipm', 'pass', 'abdur', 'chowdhury', 'picture', 'search', 'infoscale', 'lily', 'ray', 'search', 'survey', 'much', 'user', 'trust', 'search', 'result', 'retrieve', 'mozcomblog2020google', 'shaina', 'deepak', 'de', 'dbia', 'detect', 'bias', 'ensure', 'fairness', 'news', 'article', 'int', 'anal', 'reddit', 'recasen', 'danescuniculescumizil', 'guistic', 'model', 'analyze', 'detect', 'biased', 'language', '1650–1659', 'retrieve', 'bad', 'ai', 'ever', 'auditing', 'partisan', 'audience', 'bias', 'search', 'proc', 'acm', 'humcomput', 'interact', 'cscw', 'article', 'auditing', 'autocomplete', 'suggestion', 'network', 'recursive', 'victor', 'sanh', 'lysandre', 'tilbert', 'distilled', 'version', 'small', 'fast', 'cheap', 'light', 'danny', 'autocomplete', 'work', 'search', 'retrieve', 'workssearch', 'raheem', 'beyah', 'game', 'missuggestion', 'semantic', 'analysis', 'search', 'autocomplete', 'manipulation']",
"DreamSmooth: Improving Model-based Reinforcement Learning via Reward
  Smoothing","[{'href': 'http://arxiv.org/abs/2311.01450v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.01450v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-02 17:57:38,"3
2
0
2

v
o
N
8
2

]
L
C
.
s
c
[

1
v
4
6
2
7
1
.
1
1
3
2
:
v
i
X
r
a

Preprint

RETSIM: RESILIENT AND EFFICIENT TEXT
SIMILARITY

Marina Zhang1, Owen Vallis1, Aysegul Bumin*2, Tanay Vakharia1, Elie Bursztein1
Google1 University of Florida2

ABSTRACT

This paper introduces RETSim (Resilient and Efficient Text Similarity), a
lightweight, multilingual deep learning model trained to produce robust metric
embeddings for near-duplicate text retrieval, clustering, and dataset deduplication
tasks. We demonstrate that RETSim is significantly more robust and accurate
than MinHash and neural text embeddings, achieving new state-of-the-art perfor-
mance on dataset deduplication, adversarial text retrieval benchmarks, and spam
clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dver-
sarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval
capabilities under adversarial settings. RETSim and the W4NT3D benchmark are
open-sourced under the MIT License at https://github.com/google/unisim.

1

INTRODUCTION

Robust near-duplicate text detection is an essential component of many tasks, including retriev-
ing documents, detecting plagiarism (Sun et al., 2013) and blocking adversarial spam cam-
paigns (Ahmed et al., 2022). Users have come to expect that systems can return accurate results
despite their queries exhibiting a 20% to 30% typo rate (Hagen et al., 2017). Furthermore, effi-
ciently deduplicating text datasets is critical to training state-of-the-art large language models (Lee
et al., 2022; Kandpal et al., 2022).

For more than two decades, MinHash-based (Broder et al., 1998) locality-sensitive hashing (LSH)
has been the most prevalent algorithm used for near-duplicate detection due to its simplicity, robust-
ness, and speed. For example, the vast majority of dataset deduplication efforts still rely on MinHash
(Lee et al., 2022; Kocetkov et al., 2022). However, like all LSH-based techniques, MinHash is not
without downsides; chief among them being that it is very parameter-sensitive and requires heavy
tuning. Additionally, MinHash lacks resilience to typos due to its reliance on n-grams, leading to
poor performance on noisy data and a vulnerability to hash-busting attacks (Issac et al., 2014).

On the other hand, deep learning models are the dominant way to perform vector-based semantic text
retrieval (Muennighoff et al., 2022), but so far, no neural embedding has been able to consistently
outperform MinHash for robust near-duplicate detection (Silcock et al., 2022). This is mostly due to
the focus on improving semantic capabilities, which leads models to be too large to run extremely
quickly and the use of sub-word level tokenization, which is not resilient to typos and adversarial
attacks (Morris et al., 2020; Bursztein et al., 2023).

To fill this gap, we introduce RETSim (Resilient and Efficient Text Similarity), a lightweight, mul-
tilingual deep learning model trained specifically to produce robust neural embeddings specialized
for near-duplicate detection. By combining the state-of-the-art RETVec text vectorizer, a modern
transformer block (Hua et al., 2022), a large typo-augmented training corpus, and a metric learn-
ing training regime, RETSim is able to achieve new state-of-the-art performance on near-duplicate
detection benchmarks (Section 4.2), dataset deduplication tasks (Sections 4.3 and 5.1), and spam
clustering applications (Section 5.2).

Furthermore, while datasets and benchmarks exist for corpus deduplication and near-duplicate text
retrieval, none of these have focused on systematically evaluating near-duplicate retrieval perfor-
mance under the presence of typos, word manipulations, and sentence or paragraph-level modifica-

*This work was done during the author’s internship at Google.

1

 
 
 
 
 
 
Preprint

tions. To address this need, we additionally introduce the W4NT3D benchmark (Wiki-40B 4dver-
sarial Near-T3xt Dataset) which enables the evaluation of algorithms on adversarial near-duplicate
text retrieval in a multilingual setting. We report the performance of RETSim, MinHash, and pop-
ular neural embeddings such as Universal Sentence Encoder (Cer et al., 2018) and LaBSE (Feng
et al., 2022) on this new benchmark in Section 4.2, highlighting uneven performance across lan-
guages and types of adversarial manipulations. The RETSim model and the W4NT3D benchmark
are open-sourced at https://github.com/google/unisim under the MIT License.

2 RELATED WORK

Near-Duplicate Detection Identifying noisy near-duplicate documents in a large corpus is a fun-
damental task with a wide range of applications, such as detecting plagiarism, finding reproduced
content in literature or news articles (Gyawali et al., 2020; Silcock et al., 2022), and deduplicat-
ing training datasets for language models. Previous research has shown that duplicates in training
datasets lead to inefficient training (Lee et al., 2022) and privacy concerns for large language models
(LLMs), where models memorize and regenerate duplicated training sequences at a much higher
frequency (Kandpal et al., 2022).

Unlike semantic text similarity, the task of identifying textual near-duplicates has been predominated
by non-neural, n-gram-based algorithms such as MinHash (Broder et al., 1998), which is the most
widely used technique for deduplicating large training corpuses (Kocetkov et al., 2022; Lee et al.,
2022). MinHash is a technique for estimating the Jaccard similarity between two sets. Algorithms
such as MinHash or SimHash (Charikar, 2002) can be combined with locality-sensitive hashing
(LSH) techniques for fast, approximate nearest neighbor search and data clustering. This allows
them to scale and deduplicate corpuses containing terabytes of data such as C4 (Lee et al., 2022)
and The Stack (Kocetkov et al., 2022). However, n-gram or shingling-based techniques typically
require texts to be parsed into a standardized form (e.g. by lower-casing or stripping punctuation),
which makes them susceptible to typos and adversarial attacks and pose a challenge when attempt-
ing to differentiate between dissimilar documents and near-duplicate documents with adversarial
augmentations.

Semantic Text Similarity The task of computing semantic similarity between text is closely re-
lated to near-duplicate detection. Semantic text similarity refers to the assessment of the semantic
relatedness of two pieces of text based on their meaning rather than their syntactic structure, as in
the case of near-duplicate detection. Recently, transformer-based language models such as Universal
Sentence Encoder (Yang et al., 2019), LaBSE (Feng et al., 2022) and LLM-based embeddings (Anil
et al., 2023) which embed text into high-dimensional embedding vectors have been successfully
used to retrieve semantically-related documents using cosine similarity. Modern text retrieval sys-
tems combine these embeddings with an approximate nearest neighbor (ANN) search algorithm to
efficiently retrieve documents matching user queries.

However, language models have been shown to be vulnerable to adversarial attacks and naturally-
occurring typos (Alzantot et al., 2018; Gao et al., 2018; Morris et al., 2020). Furthermore, language
models are typically very large and costly to run even with hardware acceleration, which makes
them unsuited for large-scale dataset deduplication or identifying near-duplicates in the presence of
typos or adversarial text manipulations.

Metric Learning Metric learning aims to learn an embedding space where similar items have
a small distance between their embeddings and dissimilar items are further away. Many state-of-
the-art embeddings use metric learning for unsupervised training or fine-tuning including Sentence-
BERT (Reimers & Gurevych, 2019) and E5 (Wang et al., 2022).

RETVec is a resilient, multilingual embedding and text vectorizer trained to be robust against various
forms of character-level typos and adversarial attacks. We extend the RETVec training regime to
full text documents for RETSim. We use Multi-Similarity Loss (Wang et al., 2019) for pair-based
metric learning, where typo-laden and near-duplicate versions of texts are trained to be closer in
the embedding space, while other texts are pushed further away. Multi-Similarity Loss is based
on a general weighting framework for pair-based losses and achieves state-of-the-art performance,
outperforming alternatives such as Triplet Loss (Schroff et al., 2015).

2

Preprint

Figure 1: RETSim model architecture diagram. RETSim works on arbitrary length text by split-
ting texts into chunks of 512 characters during its vectorization phase and encodes them using the
RETVec character vectorizer. The RETSim model then embeds each chunk of text into 256-dim
partial embeddings and combines them to produce the global embedding.

3 RETSIM

3.1 ARCHITECTURE

The RETSim model is composed of three main components (as depicted in Figure 1):

The character-level vectorizer
splits the input text into chunks of 512 characters, then uses the
RETVec chararcter encoder (Bursztein et al., 2023) to encode each chunk, resulting in a batch of
(512, 24) dense inputs. The RETVec character vectorizer encodes each Unicode character as a
compact 24-bit binary representation based on its integer codepoint value. This allows the vectorizer
to encode all valid Unicode characters and support all languages. Furthermore, the character-level
vectorizer has been shown to be more resilient against typos and adversarial attacks.

A small transformer model
is used to compute 256-dimension embeddings for each chunk of the
input text. RETSimPartial-Dup uses these embeddings directly to finding documents that have matching
chunks of text. Architecturally, the model consists of two Gated Attention Unit (GAU) blocks (Hua
(Radenovi´c et al., 2018), a dense
et al., 2022), followed by a Generalized-Mean pooling layer
projection layer which projects the embedding into 256 dimensions, and an L2 normalization layer.
The model has only 536k parameters, which is more than two orders of magnitude smaller than other
neural embeddings (Table 1). L2-normalization allows the embeddings to be compared using cosine
similarity. We discuss the impact of key architecture design choices in Section 6. Hyperparameter
details are provided in Appendix A.1.1, and additional ablations results in Appendix A.5.

An embedding averaging module
is then used to combine partial text embeddings into a full-text
embedding which is used for global near-duplicate matching (RETSimNear-Dup). Averaging chunked
embeddings to produce a global embedding is a standard technique used by many models (Cer
et al., 2018) to support infinite length inputs in a cost-efficient manner. We experimented with other
aggregation techniques to produce more accurate global embeddings, including training a deep-
averaging network (Iyyer et al., 2015), but this did not improve performance and resulted in higher
computation cost. RETSimNear-Dup and RETSimPartial-Dup are computed in a single forward pass
which makes it computationally efficient. We output both types of embeddings as they have different
applications: RETSimNear-Dup is better-suited for full-text matching and retrieval (Section 4.2), while
RETSimPartial-Dup is used to find partial text matches where the near-duplicate content appears only
in part of the document (Section 4.3).

3.2 MODEL TRAINING

Dataset We use the multilingual C4 dataset (mC4) for raw text data and following (Xue et al.,
2020), we use a language sampling exponent of α = 0.3 to balance sampling between low and
high-resource languages. We only use text containing at least 16 characters, and we randomly select
between 1 and 8 sentences (roughly 512 characters) for each text chunk. For each example in the

3

Input text

Chunk vector

Chunk vector

....

Chunk vector

Chunked and 
vectorized text
(num_chunks, 512, 24)

RETSim model

RetSim Partial-Dup 
(num_chunks, 256)

RetSim Near-Dup  
(256)

r

e

d

o

c

n

E

r

a

h

C

c

e

v

T

E

R

U

A

G

U

A

G

g

n

i

l

o

o

P

2

L

+

e

s

n

e

D

t

x

e

t

l

a

i

t

r

a

P

s

g

n

i

d

d

e

b

m

e

2

L

+

g

n

i

g

r

a

r

e

v

A

g

n

i

d

d

e

b

m

e

t

x

e

t

l

l

u

F

 
 
 
 
 
 
 
 
 
Preprint

training dataset, we generate 5 pairs of augmented examples. We apply three levels of augmentation
to each example text chunk (in this order): sentence-level, word-level, and character-level. For each
level, we randomly select the augmentation to be applied from the following categories: insertion,
deletion, substitution, and transposition. We randomly apply between 0 − 25% sentence-level aug-
mentation and up to 30% combined character and word-level augmentation. Empirically, we found
that increasing the percentage of augmentation beyond this point causes RETSim’s performance to
degrade. The full list of augmentations used can be found in Appendix A.2.

Training Procedure We train RETSim using Multi-Similarity Loss (Wang et al., 2019) with α =
4, β = 40, λ = 0.5, and ϵ = 0.1. We hypertuned these parameters and the results are shown in
Appendix A.5. We train for 1 million steps with batch size = 1024. The similarity loss trains the
model to embed augmented versions of the same text closer in the embedding space, while dissimilar
texts are pushed further apart. We use the LAMB optimizer (You et al., 2019) with a max learning
rate of 0.001 and cosine decay. Detailed training hyperparameters are reported in Appendix A.1.2.

4 EVALUATION

Model/Algorithm

Type

Embed./Hash Size

# Model Parameters

LaBSE
Multilingual USE
Multilingual E5-Base
PaLM 2 (Gecko)

SimHash
MinHash

RETSim

Neural
Neural
Neural
Neural

Hashing
Hashing

Neural

768
512
768
768

b bits
n hashes

256

471M
69M
278M
?

N/A
N/A

536k

Table 1: Embedding models and hashing algorithms benchmarked in the paper.

4.1 MODELS AND ALGORITHMS EVALUATED

We benchmark RETSim against four multilingual semantic text embeddings as well as popular n-
gram based algorithms primarily used in near-duplicate text detection (Table 1). Our baseline text
embeddings include Multilingual Universal Sentence Encoder
(Yang et al., 2019), LaBSE (Feng
et al., 2022), Multilingual E5 (Wang et al., 2022), and PaLM 2 Gecko Embeddings (Anil et al.,
2023). All text embeddings are L2-normalized and compared using cosine similarity. We use exact
search to index and retrieve nearest neighbors from our vector index for the experiments in Section 4.

For non-neural near-duplicate detection and clustering algorithms, we selected the two most popular
algorithms: MinHash (Broder et al., 1998) and SimHash (Charikar, 2002). For MinHash, we use
Datasketch’s MinHashLSH library. Following the most common practices in the literature (Silcock
et al., 2022), we use 10 hash functions for MinHash unless otherwise specified. We use word-level
n-grams where we select the best value out of n = {2, 3, 4, ..., 10}. For SimHash, we use 64-
bit SimHash and conduct shingling at the character level, where the shingle size is selected from
n = {2, 3, 4, ..., 10}. For the near-duplicate detection benchmarks (NEWS-COPY and CORE Near-
Duplicates datasets), we tune the optimal deduplication threshold (e.g. based on cosine similarity
for neural-based embeddings and Jaccard similarity for MinHash). Detailed hyperparameter settings
for RETSim and baseline algorithms used in the evaluation can be found in Appendix A.3.

4.2 W4NT3D: WIKI-40B 4DVERSARIAL NEAR-T3XT DATASET EVALUATION

Dataset Description The vast majority of text retrieval benchmarks are focused on evaluating
semantic performance. To the best of our knowledge, there is no multilingual benchmark for sys-
tematically measuring adversarial robustness for near-duplicate text retrieval. In an attempt to fill in
the gap, we create and publish the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset),
which contains around 400k pairs of syntactically similar texts to evaluate near-duplicate text re-
trieval in the presence of various forms of text manipulations and typos.

W4NT3D is based on the Wiki-40B dataset (Guo et al., 2020). The dataset is split into query exam-
ples and target examples, where query examples are synthetically-modified near-duplicate versions

4

Preprint

of a target example (e.g. with typos). For each of the 41 language splits in Wiki-40B, we randomly
select 10,000 texts. The length of the target string is uniformly selected from between 16 and 8192
characters, in order to test performance on short and long text. To construct the query text corre-
sponding to a target text, we randomly apply up to 25% word and character augmentations, and up to
25% sentence and paragraph augmentations. For each augmentation, we uniformly select from the
[insert, delete, substitute, and swap] operations. We use Recall@k with k = 1 as the main metric,
following the setup commonly found in semantic text retrieval benchmarks.

Model/Algorithm

Arabic Chinese English German French

Spanish

Japanese Korean Russian Thai

Avg (41 Langs)

LaBSE
Multilingual USE
Multilingual E5-Base
PaLM 2 (Gecko)

SimHash
MinHash

RETSimPartial-Dup
RETSimNear-Dup

0.915
0.915
0.936
0.497

0.558
0.633

0.928
0.971

0.917
0.986
0.980
0.623

0.276
0.172

0.946
0.971

0.944
0.958
0.959
0.961

0.591
0.591

0.954
0.987

0.931
0.942
0.944
0.932

0.561
0.558

0.949
0.978

0.930
0.938
0.948
0.934

0.519
0.556

0.947
0.983

0.888
0.903
0.896
0.911

0.513
0.575

0.938
0.976

0.931
0.990
0.979
0.578

0.465
0.223

0.963
0.986

0.949
0.984
0.986
0.701

0.593
0.814

0.971
0.991

0.918
0.910
0.911
0.851

0.554
0.523

0.946
0.970

0.882
0.888
0.921
0.571

0.669
0.416

0.941
0.946

0.921
0.912
0.937
0.823

0.550
0.538

0.949
0.977

Table 2: Per-language retrieval performance for various embedding models and algorithms on the
W4NT3D benchmark. Results on selected languages are reported alongside the average Recall@1
for all 41 languages. Full results for all languages are reported in Appendix A.4.

Multilingual Performance Overall, RETSimNear-Dup achieves an average Recall@1 of 0.977
across all 41 languages on the W4NT3D benchmark (Table 2). RETSimPartial-Dup is second best
with a Recall@1 of 0.949 and Multilingual E5, the best-performing baseline, is third with an aver-
age Recall@1 of 0.932. We expect that RETSimNear-Dup outperforms RETSimPartial-Dup because the
W4NT3D benchmark requires an algorithm to not just find near-duplicates, but to find the most simi-
lar text. RETSimPartial-Dup optimizes for finding the most similar chunk of text in the corpus, which is
not always the most similar text overall. Similarly, we hypothesize that MinHash and SimHash per-
form poorly on the W4NT3D benchmark due to their lack of ability to distinguish which is the most
similar text among the near-duplicates detected, and embedding-based models and cosine similarity
offer a more fine-grained measure of similarity.

RETSimNear-Dup outperforms baseline algorithms on all languages except for Chinese and Japanese.
For these languages, we theorize that semantic embeddings may have the slight edge in performance
because their significantly larger model sizes (more than 100x larger than RETSim, as shown in Ta-
ble 1) allow them to have a better representation on languages with large character sets. Furthermore,
the sub-word level tokenizers used in the baseline embeddings often treat each character in Chinese
or Japanese as individual tokens, which could offer higher resilience to typos.

Figure 2: Recall@1 performance on the W4NT3D benchmark, broken down by augmentation type.
Results are averaged across all 41 language splits in W4NT3D.

Adversarial Resilience Delving deeper into the impact of various types of text manipulation re-
veals that RETSimNear-Dup and RETSimPartial-Dup perform almost equally well regardless of the type
of augmentation applied (Figure 2). Semantic text embeddings perform well on paragraph, sentence
and word-level manipulations, but as expected, exhibit significantly weaker performance towards
character-level typos. MinHash and SimHash struggle more with word-level augmentations than
deep-learning based embeddings and collapse when character-level typos are introduced. We at-

5

1.00

0.75

0.50

0.25

0.00

l
l

1
@
a
c
e
R

Paragraph

Sentence

Word

Character

Augmentation Level

LaBSE

Multilingual USE

Multilingual E5-Base

PaLM 2 (Gecko)

SimHash

MinHash

RETSim (Partial-Dup)

RETSim (Near-Dup)

Preprint

tribute RETSim’s resilience to adversarial manipulations to the RETVec character encoder as well
as using deep metric learning to train robust embeddings.

Figure 4 reports the Recall@1 performance of the algorithms as the amount of augmentation in-
creases. All algorithms perform perfectly when no augmentation is applied (exact matching), but as
the percentage of augmentation increases, n-gram based approaches exhibit a steep drop in perfor-
mance. Semantic text embeddings are able to sustain a larger degree of augmentation before their
retrieval capabilities start to degrade (over 20%). RETSimNear-Dup is the most robust algorithm, with
a noticeable drop in performance only after around 40% augmentation. This makes RETSim the
most effective approach at clustering and deduplicating text under adversarial settings.

Figure 3: Recall@1 performances on the
W4NT3D benchmark (English only) for
varying max target lengths.

Figure 4: Recall@1 performances on the W4NT3D
benchmark (English only) as the amount of augmenta-
tion applied to the query text increases.

Text Length Impact on Performance Figure 3 reports the Recall@1 performance of RETSim
and baseline algorithms as the length of the query and target text varies. We see that RETSimNear-Dup
and RETSimPartial-Dup outperforms all other methods on short texts with fewer than 128 characters.
As the text length increases beyond 512 characters, RETSimNear-Dup remains close to perfect while
RETSimPartial-Dup’s performance degrades since it splits the text into multiple embeddings and finds
the nearest matching chunk of text. MinHash and SimHash also perform poorly on short text lengths
and start to degrade on longer texts. For neural-based embeddings, we observe a slight drop in
performance on longer texts for all models except RETSimNear-Dup and Multilingual USE, the only
two embeddings that can handle arbitrary length inputs.

4.3 REAL-WORLD NEAR-DUPLICATE DETECTION EVALUATION

Setup We benchmark RETSim’s ability to identify near-duplicate content on real-world datasets
from the literature. The NEWS-COPY Deduplication dataset (Silcock et al., 2022) contains 27,210
historical news articles with 122,876 positive duplicate pairs. The dataset consists of noisy near-
duplicates due to factors like OCR errors, plagiarism, and news aggregation. We also evaluate the
algorithms on the CORE Near-Duplicates dataset (Gyawali et al., 2020), which consists of 100k
scholarly articles with 25k exact duplicates, 25k near-duplicates, and 50k non-duplicates. Near-
duplicates in this dataset arise from article revisions, versioning and metadata differences, and hu-
man typos. A key difference between these two benchmarks and the W4NT3D benchmark is that
these two benchmarks are focused on detecting and clustering near-duplicate text, rather than robust
text retrieval based on syntactic similarity. For both benchmarks, we follow the experimental setup
provided in the papers and report Adjusted Rand Index (ARI) for the NEWS-COPY dataset and
report precision/recall/F1 scores on the CORE Near-Duplicates dataset.

Results On the NEWS-COPY dataset, RETSimPartial-Dup outperforms all other approaches by a
significant margin (4.8% ARI compared to our best MinHash result), as reported in Table 3. In the
dataset, there are many near-duplicate pairs where one text is significantly longer than the other, so it
is expected that RETSimPartial-Dup, which can find matching text chunks in documents, is more suited
for the task and outperforms RETSimNear-Dup. Bucketing the near-duplicate detection rate of each
algorithm by the length ratio between positive pairs (Figure 5), we observe that RETSimPartial-Dup
outperforms MinHash regardless of the length ratio, but MinHash surpasses RETSimNear-Dup per-
formance when one text is above roughly 1.5x the length of the other text in a near-duplicate pair.

6

1.0

0.8

0.6

0.4

l
l

1
@
a
c
e
R

16

32

64

128

256

512

1024

2048

4096

8192

Max Target Text Length

l
l

1
@
a
c
e
R

1.0

0.8

0.6

0.4

0%

10%

20%

30%

40%

50%

Text Augmentation Amount (%)

LaBSE

Multilingual USE

Multilingual E5-Base

PaLM 2 (Gecko)

SimHash

MinHash

RETSim (Partial-Dup)

RETSim (Near-Dup)

Preprint

Model/Algorithm ARI

Multilingual USE
Multilingual E5-Base
S-BERT*

SimHash
MinHash*
MinHash (Ours)

RETSimPartial-Dup
RETSimNear-Dup

0.730
0.742
0.700

0.695
0.737
0.783

0.831
0.704

Table 3: Performance comparison on the
NEWS-COPY dataset. Adjusted Rand Index
(ARI) values are reported. * denotes results
from Silcock et al. (2022).

Figure 5: Near-duplicate detection rate of RET-
Sim vs MinHash for different length ratios of pos-
itive pairs. X-axis is the length of longer divided
by shorter text, rounded to the nearest integer.

Additionally, we noticed that the labels in the dataset were occasionally noisy, as a substantial por-
tion of the RETSim false positives appear to be near-duplicates upon inspection (Appendix A.6).

On the CORE Near-Duplicates dataset (Table 4), where documents (article title + abstract) are
roughly the same size, RETSimPartial-Dup and RETSimNear-Dup performance is roughly equivalent.
Both methods outperform the baselines in terms of macro F1 score and accuracy. We use MinHash
+ LSH with 256 hash functions for computational efficiency, as recommended by the datasketch
library1 for better accuracy than the default setting. Deduplication thresholds and detailed hyperpa-
rameter settings for the algorithms on both near-duplication datasets can be found in Appendix A.3.

Model / Algorithm

Exact Title Matching*

LaBSE
Multilingual USE
Multilingual E5-Base

MinHash + LSH

RETSimPartial-Dup
RETSimNear-Dup

Precision
Duplicates

Recall
Duplicates

Precision
Non-Duplicates

Recall
Non-Duplicates

Macro F1 Accuracy

0.830

0.937
0.917
0.931

0.929

0.945
0.928

0.500

0.923
0.907
0.908

0.902

0.941
0.937

0.709

0.930
0.918
0.919

0.915

0.945
0.942

0.992

0.943
0.927
0.939

0.938

0.949
0.934

0.757

0.933
0.917
0.924

0.921

0.945
0.935

0.746

0.919
0.909
0.920

0.918

0.928
0.926

Table 4: Evaluation results on the CORE Near-Duplicates dataset. Precision/recall/macro F1 and
accuracy numbers are reported. * denotes results from Gyawali et al. (2020).

5 APPLICATIONS

5.1 TRAINING DATASET DEDUPLICATION

Model/Algorithm

% train examples
with dup in train

% valid examples
with dup in train

MinHash + LSH
Exact Substring*
RETSimNear-Dup
RETSimPartial-Dup

0.47%
2.76%
3.17%
12.77%

0.46%
0.52%
0.59%
2.66%

Table 5: Deduplication rate on Wiki-40B (English). * denotes results from Lee et al. (2022).

Setup We evaluate RETSim’s ability to deduplicate text training datasets by deduplicating the
English split of Wiki-40B (Guo et al., 2020). We conservatively set the cosine similarity deduplica-
tion threshold to 0.1 for RETSimNear-Dup and 0.15 for RETSimPartial-Dup to limit the amount of false
positives, based on the optimal thresholds found in the evaluation (Appendix A.3). We use USe-
arch’s default vector index for approximate nearest neighbor search (Vardanian, 2023). We compare

1datasketch: Big Data Looks Small. https://github.com/ekzhu/datasketch.

7

t

e
a
R
n
o

t

i
t
c
e
e
D
e
a
c

t

i
l

p
u
D

-
r
a
e
N

1.0

0.8

0.5

0.3

0.0

1

2

3

4

5

6

7

8

9

10

Length Ratio between Near-Duplicate Text Pair

RETSim (Partial-Dup)

RETSim (Near-Dup)

MinHash

 
 
Preprint

Model/Algorithm

Accelerator

Batch Size

Embedding / Hashing
time (sec)

examples/sec

MinHash + LSH
RETSim
RETSim
RETSim

CPU AMD 7950 32 cores
Onnx CPU AMD 7950 32 cores
TensorFlow GPU RTX 4090
TensorFlow GPU NVIDIA H100

-
256
4096
16384

234
10839
720
363

12544
270
4062
8069

Table 6: Embedding/hashing speed of RETSim vs MinHash + LSH on the Wiki-40B dataset.

against MinHash + LSH, where we set the number of hash functions to be 256 following Kocetkov
et al. (2022) and use a Jaccard similarity threshold of 0.8 for deduplication (Lee et al., 2022).

Results Overall, as reported in Table 5, RETSimNear-Dup finds slightly more duplicates in the Wiki-
40B training and validation splits. This is in-line with our deduplication results (Section 4.3) where
RETSimNear-Dup outperforms other algorithms. On the other hand, RETSimPartial-Dup finds signifi-
cantly more matches than the exact substring matching algorithm used in the previous study (Lee
et al., 2022), showcasing the usefulness of performing both near-duplicate and partial-duplicate
matching at once. This larger-than-expected number of partial matches indicate that machine learn-
ing practitioners should take extra care to deduplicate Wikipedia at the chunk level to avoid feeding
duplicate text to their models.

In terms of embedding speed (Table 6), RETSim is significantly slower than MinHash + LSH on
CPU (46x slower), competitive when using a desktop GPU such as the RTX 4090 (3x slower) and
almost on-par when using a high-end GPU like the NVIDIA H100 (1.5x slower). Our current code
is written in Python and not fully optimized, so we expect this performance gap to significantly
shrink as we optimize our implementation. Although RETSim is slower than MinHash, RETSim
is significantly smaller and faster than other text embedding models, and closes the performance
gap between neural and non-neural based methods for near-duplicate text detection and dataset
deduplication. Both RETSimNear-Dup and RETSimPartial-Dup are returned at the same time so they
have the same embedding speed. Indexing and retrieval times will depend on the vector index and
search algorithm used. For longer documents, RETSimPartial-Dup will produce more embeddings
than RETSimNear-Dup, so RETSimPartial-Dup offers a tradeoff between finer-grained matching versus
indexing/retrieval speed, which will depend on the specific vector search algorithm and dataset used.

5.2

IN THE WILD: SPAM EMAIL CLUSTERING

In this section, we showcase RETSim’s real-world performance on clustering near-duplicate text
which has been heavily manipulated by adversarial attacks by performing an evaluation on spam
campaigns. Spam constitutes a strong proving ground for near-duplicate clustering algorithms as
spammers employ adversarial augmentation techniques in an attempt to evade detection. Such aug-
mentations typically include appending or prepending unrelated text, interleaving random words and
different languages, intentionally introducing typos, abusing extended character sets such as emojis
and homoglyphs, and more. These techniques are collectively referred to as hash-busting.

Setup The dataset consists of 5,252 spam emails from 196 spam campaigns, donated by Gmail
users who flagged them when they reached their inboxes. Each example contains the email subject
concatenated with the message content. The emails were misclassified by a spam classifier due to
their effective adversarial text manipulation techniques, which makes them a challenging test set for
clustering evaluations. Some examples of hash-busting attacks and adversarial manipulations we
observe include the use of homoglpyphs, uncommon Unicode character sets, invisible characters,
and padding with random words from different languages. To get the ground truth campaign clusters,
emails were manually reviewed and assigned to a specific spam campaign based on similarity by
human reviewers. We use agglomerative clustering to cluster spam emails, and report homogeneity,
completeness, V-Measure, and Adjusted Rand Index (ARI) metrics.

Results Overall, we observed that RETSim is significantly better at clustering near-duplicates
with adversarial manipulations, outperforming both SimHash and USE across all metrics considered
(Table 7). In particular, we observed that RETSim outperforms USE by 4.6% on the V-Measure
score which is our main metric. The results reported in this section are in-line with what we observe
since we deployed RETSim as our main near-duplicate detection algorithm in December 2022.

8

Preprint

Model / Algorithm Homogeneity Completeness V-Measure ARI

USE
SimHash + LSH
RETSimNear-Dup

0.856
0.867
0.937

0.955
0.876
0.963

0.903
0.871
0.949

0.6
0.571
0.747

Table 7: Performance on clustering adversarial spam campaigns in practice.

6 ABLATION STUDIES

Setup In this section, we summarize the key ablation studies we performed when designing RET-
Sim. All the models used in this section are trained using the setup detailed in Appendix A.1.2, ex-
cept we only train them for 100k steps to reduce computational costs. We evaluate RETSimNear-Dup’s
performance for each model on a subset of the W4NT3D benchmark, where we randomly select
1000 examples from each of the 41 language splits and use Recall@1 as reported metric.

Block Type Recall@1

Chunk Size Recall@1

Embed. Dim Recall@1

RETVec MLP
ConvNeXt
BERT
T5
*GAU

0.975
0.978
0.973
0.980
0.986

128
256
*512
1024
2048

0.979
0.984
0.986
0.983
0.978

64
128
*256
512
768

0.969
0.980
0.986
0.986
0.986

Table 8: RETSim ablation study results on architecture block type (left), text chunk size (middle),
and embedding dimension (right). *Bold denotes the value selected for the final RETSim model.

Results Table 8 contains RETSim ablation study results on max text chunk size, architecture block
type, and embedding size. The most important architectural decision was to decide the optimal text
chunk size and finding the right balance between having the smallest size possible to maximize
RETSimPartial-Dup efficiency while ensuring RETSimNear-Dup full-text embeddings can work effec-
tively on full documents. We find that chunks of 512 characters offer the best performance.

We also tested various model architectures and transformer blocks to find the best balance between
efficiency and performance. We find that the more modern GAU block (Hua et al., 2022) outper-
forms the vanilla BERT transformer block (Devlin et al., 2019) and the T5 block (Xue et al., 2020).
We also tried modern CNN architectures such as ConvNeXt (Liu et al., 2022) and the MLP architec-
ture proposed in RETVec (Bursztein et al., 2023), but both were worse than GAU block performance.
Last but not least, we found that increasing the embedding size past 256 dimensions does not yield
any meaningful improvements for RETSimNear-Dup. Accordingly, we opted to use a 256-dimension
embedding for space-efficiency and to maximize indexing and query speed. Additional ablation
studies for other hyperparameters can be found in Appendix A.5.

7 FUTURE WORK

RETSim’s novel training regime, which combines metric learning and data augmentation, has many
other potential applications that we plan to explore in future work. For example, it could be adapted
or extended to train robust semantic embeddings or image similarity embeddings. Additionally,
we expect that as general models become bigger and more expensive to run in the future, smaller,
specialized models such as RETSim will emerge as an efficient alternative for a wide range of tasks.

8 CONCLUSION

In this paper, we introduced RETSim, a novel, multilingual text embedding which achieves state-
of-the-art performance on near-duplicate text detection, dataset deduplication, and syntactic text
similarity benchmarks. RETSim is significantly faster than previous neural-based text embeddings
and more robust than n-gram based algorithms, which makes it suitable for large-scale text retrieval
and dataset deduplication, especially in adversarial settings such as spam detection. Furthermore,
we introduced the W4NT3D benchmark, the first multilingual dataset designed to measure the ad-
versarial robustness of near-duplicate text detection algorithms. We open-source both RETSim and
the W4NT3D benchmark under the MIT License.

9

Preprint

REFERENCES

Naeem Ahmed, Rashid Amin, Hamza Aldabbas, Deepika Koundal, Bader Alouffi, and Tariq Shah.
Machine Learning Techniques for Spam Detection in Email and IoT Platforms: Analysis and
Research Challenges. Security and Communication Networks, 2022:1–19, February 2022. ISSN
1939-0122, 1939-0114. doi: 10.1155/2022/1862888. URL https://www.hindawi.com/
journals/scn/2022/1862888/.

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei
Chang. Generating Natural Language Adversarial Examples, September 2018. URL http:
//arxiv.org/abs/1804.07998. arXiv:1804.07998 [cs].

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark
Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Brad-
bury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christo-
pher A. Choquette-Choo, Aakanksha Chowdhery, Cl´ement Crepy, Shachi Dave, Mostafa De-
hghani, Sunipa Dev, Jacob Devlin, Mark D´ıaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy
Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,
Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,
Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-
cello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary
Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex
Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros,
Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov,
David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yun-
han Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang
Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 Technical Report, September 2023.
URL http://arxiv.org/abs/2305.10403. arXiv:2305.10403 [cs].

Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael Mitzenmacher. Min-wise inde-
In Proceedings of the thirtieth annual ACM symposium on Theory of

pendent permutations.
computing, pp. 327–336, 1998.

Eli Bursztein, Marina Zhang, Owen vallis, Xinyu Jia, and Alexey Kurakin. RetVec: Resilient and

Efficient Text Vectorizer. 2023.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, and Chris Tar. Universal sentence encoder. arXiv
preprint arXiv:1803.11175, 2018.

Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of

the thiry-fourth annual ACM symposium on Theory of computing, pp. 380–388, 2002.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding, May 2019. URL http://arxiv.
org/abs/1810.04805. arXiv:1810.04805 [cs].

Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-
agnostic BERT Sentence Embedding, March 2022. URL http://arxiv.org/abs/2007.
01852. arXiv:2007.01852 [cs].

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box Generation of Adversarial Text
Sequences to Evade Deep Learning Classifiers, May 2018. URL http://arxiv.org/abs/
1801.04354. arXiv:1801.04354 [cs].

10

Preprint

Mandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami Al-Rfou. Wiki-40b: Multilingual language
model dataset. In Proceedings of the 12th Language Resources and Evaluation Conference, pp.
2440–2452, 2020.

Bikash Gyawali, Lucas Anastasiou, and Petr Knoth. Deduplication of Scholarly Documents us-
ing Locality Sensitive Hashing and Word Embeddings. In Proceedings of the Twelfth Language
Resources and Evaluation Conference, pp. 901–910, Marseille, France, May 2020. European Lan-
guage Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.
org/2020.lrec-1.113.

Matthias Hagen, Martin Potthast, Marcel Gohsen, Anja Rathgeber, and Benno Stein. A large-
In Proceedings of the 40th International ACM SIGIR

scale query spelling correction corpus.
Conference on Research and Development in Information Retrieval, pp. 1261–1264, 2017.

Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time.

In

International Conference on Machine Learning, pp. 9099–9117. PMLR, 2022.

B. Issac, R. Chiong, and S. M. Jacob. Analysis of phishing attacks and countermeasures, 2014.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum´e Iii. Deep Unordered
Composition Rivals Syntactic Methods for Text Classification.
In Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1681–1691, Bei-
jing, China, 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1162. URL
http://aclweb.org/anthology/P15-1162.

Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating Training Data Mitigates Pri-
vacy Risks in Language Models.
In Proceedings of the 39th International Conference on
Machine Learning, pp. 10697–10707. PMLR, June 2022. URL https://proceedings.
mlr.press/v162/kandpal22a.html. ISSN: 2640-3498.

Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu˜noz Ferrandis,
Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von
Werra, and Harm de Vries. The Stack: 3 TB of permissively licensed source code, November
2022. URL http://arxiv.org/abs/2211.15533. arXiv:2211.15533 [cs].

Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-
Burch, and Nicholas Carlini. Deduplicating Training Data Makes Language Models Better, March
2022. URL http://arxiv.org/abs/2107.06499. arXiv:2107.06499 [cs].

Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 11976–11986, 2022.

John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A
Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP, Oc-
tober 2020. URL http://arxiv.org/abs/2005.05909. arXiv:2005.05909 [cs].

Niklas Muennighoff, Nouamane Tazi, Lo¨ıc Magne, and Nils Reimers. MTEB: Massive Text Em-
bedding Benchmark, October 2022. URL https://arxiv.org/abs/2210.07316v1.

Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. Fine-tuning CNN image retrieval with no
human annotation. IEEE transactions on pattern analysis and machine intelligence, 41(7):1655–
1668, 2018. Publisher: IEEE.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-

networks. arXiv preprint arXiv:1908.10084, 2019.

Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Embedding for
Face Recognition and Clustering.
In 2015 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 815–823, June 2015. doi: 10.1109/CVPR.2015.7298682. URL http:
//arxiv.org/abs/1503.03832. arXiv:1503.03832 [cs].

11

Preprint

Emily Silcock, Luca D’Amico-Wong, Jinglin Yang, and Melissa Dell. Noise-Robust De-Duplication
at Scale, October 2022. URL http://arxiv.org/abs/2210.04261. arXiv:2210.04261
[cs].

Yifang Sun, Jianbin Qin, and Wei Wang. Near Duplicate Text Detection Using Frequency-Biased
Signatures. In David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mat-
tern, John C. Mitchell, Moni Naor, Oscar Nierstrasz, C. Pandu Rangan, Bernhard Steffen, Madhu
Sudan, Demetri Terzopoulos, Doug Tygar, Moshe Y. Vardi, Gerhard Weikum, Xuemin Lin, Yan-
nis Manolopoulos, Divesh Srivastava, and Guangyan Huang (eds.), Web Information Systems
Engineering – WISE 2013, volume 8180, pp. 277–291. Springer Berlin Heidelberg, Berlin, Hei-
delberg, 2013. ISBN 978-3-642-41229-5 978-3-642-41230-1. doi: 10.1007/978-3-642-41230-1
24. URL http://link.springer.com/10.1007/978-3-642-41230-1_24. Series
Title: Lecture Notes in Computer Science.

Ash Vardanian. USearch by Unum Cloud, October 2023. URL https://github.com/

unum-cloud/usearch.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-
jumder, and Furu Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training, De-
cember 2022. URL http://arxiv.org/abs/2212.03533. arXiv:2212.03533 [cs].

Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss
with general pair weighting for deep metric learning. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pp. 5022–5030, 2019.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer.
arXiv preprint arXiv:2010.11934, 2020.

Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez
Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Multilingual
Universal Sentence Encoder for Semantic Retrieval, July 2019. URL http://arxiv.org/
abs/1907.04307. arXiv:1907.04307 [cs].

Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.

12

Preprint

A APPENDIX

A.1 RETSIM DETAILS

A.1.1 RETSIM MODEL HYPERPARAMETERS

The full list of RETSim model hyperparameters can be found in Table 9.

Hyperparameter

Max input length (per chunk)
Block type
# blocks
Hidden dim
Expansion rate
Activation function
Attention activation function
Absolute positional encoding
Relative positional encoding
Norm type
Pooling type
Dropout rate
Embedding dim
# Parameters

Value

512
GAU
2
256
1
Swish
relu2
ScaledSin
RoPE
ScaleNorm
GeM (p = 3)
0
256
536k

Table 9: Detailed RETSim model hyperparameters.

A.1.2 RETSIM TRAINING HYPERPARAMETERS

Table 10 details the hyperparameters settings for training configuration, loss, and optimizer used to
train the RETSim model.

Hyperparameter

Value

Batch size
Train steps
LAMB ϵ
LAMB β1
LAMB β2
Max learning rate
End learning rate
Learning rate decay
Weight decay

1024
1 million
1e-6
0.9
0.999
0.001
0
Cosine
0

Table 10: RETSim detailed training hyperparameters.

A.2 TRAINING DATASET DETAILS

Below, we provide the full list of augmentations used to generate augmented text for the RETSim
training dataset, as described in Section 3.2.

SENTENCE-LEVEL AUGMENTATIONS

• Deletion:

– Random sentence deletion
– Random sentence truncation

• Insertion:

13

Preprint

– Random prefix sentence
– Random suffix sentence
– Random sentence insertion
– Repeat sentence

• Substitution:

– Lowercase/uppercase sentence
– Random sentence substitution

• Transposition:

– Neighboring Swap

WORD-LEVEL AUGMENTATIONS

• Deletion:

– Random word deletion

• Insertion:

– Random word insertion
– Random word insertion per language

• Substitution:

– 3-gram frequency based word substitution
– Random word substitution
– Random word substitution per language
– Repeat word

• Transposition:

– Neighboring Swap

CHARACTER-LEVEL AUGMENTATIONS

• Deletion:

– Random character deletion

• Substitution:

– Case substitution
– n-gram based substitution for n = 3, 4, 5
– QWERTY keyboard typo substitution
– Homoglyphs substitution
– Random ASCII substitution
– Random character from language alphabet substitution
– Random punctuation substitution
– Random Unicode character substitution

• Insertion:

– Character repetition
– n-grams based insertion for n = 3, 4, 5
– Random character from language alphabet insertion
– Random punctuation insertion
– Random Unicode character insertion

• Transposition:

– Neighboring swap

A.3 DETAILED EVALUATION HYPERPARAMETERS

Figures 6 and 7 contain information on deduplication thresholds values and hyperparameter settings
for each algorithm benchmarked on the NEWS-COPY and CORE deduplication datasets.

14

Preprint

Model / Algorithm

Threshold Type

Threshold Value Hyperparameters

Multilingual USE
Cosine Similarity
Multilingual E5-Base Cosine Similarity
SimHash
MinHash (Ours)
RETSimNear-Dup
RETSimPartial-Dup

Hamming Distance
Jaccard Similarity
Cosine Similarity
Cosine Similarity

0.96
0.88
10
0.6
0.89
0.84

-
-
64 bits, 5-grams (character-level)
10 hash functions, 2-grams (word-level)
-
-

Figure 6: Hyperparameter settings for NEWS-COPY dataset evaluation in Section 4.3.

Model / Algorithm

Threshold Type

Threshold Value Hyperparameters

Cosine Similarity
LaBSE
Multilingual USE
Cosine Similarity
Multilingual E5-Base Cosine Similarity
SimHash + LSH
MinHash + LSH
RETSimNear-Dup
RETSimPartial-Dup

Hamming Distance
Jaccard Similarity
Cosine Similarity
Cosine Similarity

0.88
0.97
0.87
6
0.5
0.86
0.82

-
-
-
64 bits, 3-grams (character-level)
256 hash functions, 3-grams (word-level)
-
-

Figure 7: Hyperparameter settings for CORE Near-Duplicates dataset evaluation in Section 4.3.

A.3.1 DEDUPLICATION THRESHOLD IMPACT

Figure 8: Precision/Recall/F1 scores for different cosine distance deduplication thresholds for
RETSimNear-Dup (left) and RETSimPartial-Dup (right) on the NEWS-COPY dataset.

A.4 DETAILED W4NT3D BENCHMARK RESULTS

Tables 11 and 12 show detailed performance results for RETSim and all baseline algorithms for
every language split in the W4NT3D benchmark.

15

Preprint

9
4
9
.
0

4
8
9
.
0

6
8
9
.
0

1
0
7
.
0

3
9
5
.
0

4
1
8
.
0

1
7
9
.
0

1
9
9
.
0

o
k

1
3
9
.
0

0
9
9
.
0

9
7
9
.
0

8
7
5
.
0

5
6
4
.
0

3
2
2
.
0

3
6
9
.
0

6
8
9
.
0

a
j

9
2
9
.
0

7
3
9
.
0

3
3
9
.
0

7
3
9
.
0

3
1
5
.
0

5
7
5
.
0

4
4
9
.
0

2
8
9
.
0

5
1
9
.
0

8
2
9
.
0

2
4
9
.
0

4
2
9
.
0

3
3
5
.
0

3
6
5
.
0

0
5
9
.
0

6
7
9
.
0

8
9
8
.
0

5
8
8
.
0

9
8
8
.
0

6
7
8
.
0

0
3
5
.
0

2
1
5
.
0

0
5
9
.
0

2
6
9
.
0

2
1
9
.
0

9
8
8
.
0

1
0
9
.
0

3
0
9
.
0

7
5
5
.
0

6
5
5
.
0

4
3
9
.
0

2
6
9
.
0

7
2
9
.
0

8
4
6
.
0

4
6
9
.
0

5
3
4
.
0

3
6
5
.
0

6
0
6
.
0

1
4
9
.
0

5
7
9
.
0

7
3
9
.
0

1
4
8
.
0

8
5
9
.
0

9
8
5
.
0

1
5
6
.
0

7
9
6
.
0

5
4
9
.
0

9
8
9
.
0

0
3
9
.
0

8
3
9
.
0

8
4
9
.
0

4
3
9
.
0

9
1
5
.
0

6
5
5
.
0

7
4
9
.
0

3
8
9
.
0

6
2
9
.
0

3
1
9
.
0

3
4
9
.
0

4
1
9
.
0

2
3
6
.
0

8
6
5
.
0

6
5
9
.
0

1
8
9
.
0

2
1
9
.
0

0
7
8
.
0

9
2
9
.
0

6
5
3
.
0

8
6
5
.
0

5
9
5
.
0

6
4
9
.
0

1
7
9
.
0

3
2
9
.
0

0
3
9
.
0

1
5
9
.
0

6
2
9
.
0

5
6
6
.
0

5
8
5
.
0

3
5
9
.
0

5
8
9
.
0

8
8
8
.
0

3
0
9
.
0

6
9
8
.
0

1
1
9
.
0

6
9
4
.
0

4
0
5
.
0

8
3
9
.
0

6
7
9
.
0

4
4
9
.
0

8
5
9
.
0

9
5
9
.
0

1
6
9
.
0

1
9
5
.
0

1
9
5
.
0

4
5
9
.
0

7
8
9
.
0

8
1
9
.
0

4
7
8
.
0

4
3
9
.
0

9
8
5
.
0

7
5
5
.
0

4
7
5
.
0

5
3
9
.
0

4
7
9
.
0

1
3
9
.
0

2
4
9
.
0

4
4
9
.
0

2
3
9
.
0

1
6
5
.
0

8
5
5
.
0

9
4
9
.
0

8
7
9
.
0

8
3
9
.
0

7
2
9
.
0

6
5
9
.
0

3
4
9
.
0

2
9
5
.
0

8
9
5
.
0

3
5
9
.
0

8
8
9
.
0

2
1
9
.
0

0
0
9
.
0

7
2
9
.
0

1
1
9
.
0

9
7
5
.
0

1
8
5
.
0

0
5
9
.
0

3
7
9
.
0

7
9
8
.
0

3
8
8
.
0

0
9
8
.
0

2
0
9
.
0

5
8
4
.
0

8
0
5
.
0

4
2
9
.
0

8
6
9
.
0

5
1
9
.
0

9
0
9
.
0

9
9
8
.
0

1
5
8
.
0

0
1
5
.
0

6
0
5
.
0

2
4
9
.
0

6
7
9
.
0

5
1
9
.
0

5
1
9
.
0

6
3
9
.
0

7
9
4
.
0

8
5
5
.
0

3
3
6
.
0

8
2
9
.
0

1
7
9
.
0

e
s
a
B
-
5
E

l
a
u
g
n
i
l
i
t
l
u
M

E
S
U

l
a
u
g
n
i
l
i
t
l
u
M

)
o
k
c
e
G

(

2
M
L
a
P

E
S
B
a
L

p
u
D

-
l
a
i
t
r
a
P
m
S
T
E
R

i

p
u
D

i

-
r
a
e
N
m
S
T
E
R

h
s
a
H
m
S

i

h
s
a
H
n
i
M

t
i

d
i

u
h

r
h

i
h

e
h

r
f

fi

a
f

t
e

s
e

n
e

l
e

e
d

a
d

s
c

a
c

g
b

r
a

m
h
t
i
r
o
g
l
A

/

l
e
d
o
M

.
)
1

t
r
a
p
(
k
r
a
m
h
c
n
e
b
D
3
T
N
4
W

e
h
t

n
o

s

m
h
t
i
r
o
g
l
a

d
n
a

s
l
e
d
o
m
g
n
i
d
d
e
b
m
e

s
u
o
i
r
a
v

r
o
f

e
c
n
a
m
r
o
f
r
e
p

1
@

l
l
a
c
e
R
e
g
a
u
g
n
a
l
-
r
e
p
l
l
u
F

:
1
1
e
l
b
a
T

w

t
-
h
z

n
c
-
h
z

i
v

k
u

r
t

l
t

h
t

v
s

r
s

l
s

k
s

u
r

o
r

t
p

l
p

o
n

l
n

s

m

v
l

t
l

m
h
t
i
r
o
g
l
A

/

l
e
d
o
M

16

8
1
9
.
0

5
8
9
.
0

8
7
9
.
0

9
0
6
.
0

5
1
3
.
0

0
0
2
.
0

7
5
9
.
0

8
6
9
.
0

7
1
9
.
0

6
8
9
.
0

0
8
9
.
0

3
2
6
.
0

6
7
2
.
0

2
7
1
.
0

6
4
9
.
0

1
7
9
.
0

2
3
9
.
0

0
1
9
.
0

5
4
9
.
0

3
6
8
.
0

9
0
6
.
0

1
8
5
.
0

1
5
9
.
0

5
8
9
.
0

9
9
8
.
0

3
9
8
.
0

2
7
8
.
0

2
2
8
.
0

7
1
5
.
0

0
2
5
.
0

1
4
9
.
0

7
5
9
.
0

0
3
9
.
0

0
4
9
.
0

1
5
9
.
0

2
0
9
.
0

6
0
6
.
0

3
8
5
.
0

4
5
9
.
0

8
7
9
.
0

7
4
9
.
0

9
4
9
.
0

9
6
9
.
0

4
4
9
.
0

7
0
5
.
0

0
7
5
.
0

1
6
9
.
0

9
8
9
.
0

2
8
8
.
0

8
8
8
.
0

1
2
9
.
0

1
7
5
.
0

9
6
6
.
0

6
1
4
.
0

1
4
9
.
0

6
4
9
.
0

6
0
9
.
0

9
9
8
.
0

5
2
9
.
0

9
1
9
.
0

2
5
5
.
0

0
2
5
.
0

3
5
9
.
0

9
6
9
.
0

0
3
9
.
0

6
0
9
.
0

6
9
8
.
0

6
5
8
.
0

3
5
5
.
0

5
2
5
.
0

3
6
9
.
0

9
7
9
.
0

1
3
9
.
0

8
0
9
.
0

0
4
9
.
0

6
1
9
.
0

0
8
5
.
0

0
7
5
.
0

7
4
9
.
0

7
7
9
.
0

2
2
9
.
0

1
0
9
.
0

1
2
9
.
0

4
2
9
.
0

5
7
5
.
0

3
7
5
.
0

1
6
9
.
0

1
7
9
.
0

8
1
9
.
0

0
1
9
.
0

1
1
9
.
0

1
5
8
.
0

4
5
5
.
0

3
2
5
.
0

6
4
9
.
0

0
7
9
.
0

9
0
9
.
0

9
6
8
.
0

6
3
9
.
0

3
9
8
.
0

4
1
5
.
0

0
2
5
.
0

8
4
9
.
0

7
7
9
.
0

4
4
9
.
0

2
5
9
.
0

5
5
9
.
0

0
5
9
.
0

8
4
5
.
0

3
7
5
.
0

4
5
9
.
0

5
8
9
.
0

8
2
9
.
0

1
3
9
.
0

8
2
9
.
0

3
1
9
.
0

6
8
5
.
0

3
6
5
.
0

3
5
9
.
0

9
7
9
.
0

8
2
9
.
0

1
2
9
.
0

4
4
9
.
0

0
3
9
.
0

7
7
5
.
0

0
6
5
.
0

8
5
9
.
0

1
8
9
.
0

1
3
9
.
0

6
3
9
.
0

4
3
9
.
0

1
3
9
.
0

7
2
5
.
0

0
4
5
.
0

0
5
9
.
0

9
7
9
.
0

9
1
9
.
0

2
3
9
.
0

9
4
9
.
0

8
2
9
.
0

3
3
5
.
0

1
1
5
.
0

1
6
9
.
0

0
8
9
.
0

2
2
9
.
0

9
1
9
.
0

5
3
9
.
0

7
0
9
.
0

4
2
6
.
0

9
7
5
.
0

5
4
9
.
0

3
8
9
.
0

9
1
9
.
0

2
0
9
.
0

1
4
9
.
0

9
0
9
.
0

9
0
6
.
0

8
6
5
.
0

7
5
9
.
0

0
8
9
.
0

e
s
a
B
-
5
E

l
a
u
g
n
i
l
i
t
l
u
M

E
S
U

l
a
u
g
n
i
l
i
t
l
u
M

)
o
k
c
e
G

(

2
M
L
a
P

E
S
B
a
L

p
u
D

-
l
a
i
t
r
a
P
m
S
T
E
R

i

h
s
a
H
m
S

i

h
s
a
H
n
i
M

p
u
D

i

-
r
a
e
N
m
S
T
E
R

.
)
2

t
r
a
p
(
k
r
a
m
h
c
n
e
b
D
3
T
N
4
W

e
h
t

n
o

s

m
h
t
i
r
o
g
l
a

d
n
a

s
l
e
d
o
m
g
n
i
d
d
e
b
m
e

s
u
o
i
r
a
v

r
o
f

e
c
n
a
m
r
o
f
r
e
p

1
@

l
l
a
c
e
R
e
g
a
u
g
n
a
l
-
r
e
p
l
l
u
F

:
2
1
e
l
b
a
T

Preprint

A.5 ADDITIONAL ABLATION STUDIES

This section includes ablation studies on additional hyperparameters for the RETSim model, includ-
ing the loss function, pooling type, and model capacity.

α β

2
2
2
2
4
4
4
4

20
20
40
40
20
20
40
40

λ

0.5
1
0.5
1
0.5
1
0.5
1

Recall@1

0.982
0.948
0.984
0.919
0.982
0.947
0.986
0.923

Table 13: Ablation study on Multi-Similarity Loss hyperparameters for RETSim training. Bold
indicates the hyperparameter setting selected for the final model.

# Blocks Hidden Dim Recall@1

2
2
2
2
3
3
3
3
4
4
4
4

64
128
256
512
64
128
256
512
64
128
256
512

0.965
0.980
0.986
0.986
0.962
0.980
0.984
0.987
0.966
0.980
0.985
0.986

Table 14: Ablation study for RETSim model capacity and size (number of GAU blocks and hidden
dimension for the blocks). Bold indicates the hyperparameter setting selected for the final model.

Pooling Type

Recall@1

Average Pooling
Max Pooling
Generalized Mean Pooling

0.985
0.983
0.986

Table 15: Ablation study on pooling type for the RETSim model. Bold indicates the hyperparameter
setting selected for the final model.

A.6 SELECTED EXAMPLES FROM NEWS-COPY DATASET

In this section, we randomly selected a set of false positives and false negatives for RETSim on the
NEWS-COPY deduplication dataset to provide further insight into the results.

17

Preprint

Text 1
chauffeur, a policeman and a passing jour-
nalist who tried to intervene. Beaton and the
policeman were reported in serious condition.
The 23-year-old princess and her husband of
five months, Capt. Mark Phillips, were not
hurt. But police experts said the holes left by
one of the bullets fired into the car indicated it
passed between them, missing them by inch-
es. A police informant said it was believed 11
shots were fired by the assailant. Experts were
studying two revolvers found at the scene.
They said fi...
By United Press tnfernational Ay SSAST OR
BE FRE NG SG The federal government has
proposed new methods of eoustructing federal
buildings in a move to save ad- ditional en-
ergy and suggested ils elfort could be adapted
to all new buildings,

Washington, Jan. 27. —(P)—Im- mediate
removal of John F. J. Her- bert, as prohibition
administrator for Montana and Idaho, was
de- manded in the senate today by Sen- ators
Borah, Idaho, and Whe´eeler, Montana, on
the ground of charges placed before them by
department of justice investigators. Wheeler
accompanied his demand (Continued on Page
2)

By RAYMOND CLAPPEA (Dnited
Presa Stal Correspandoayy London, Jai,
38—(UP—-The Am ‘erlcnn delegation to
the navat confer ence today won ls demand
for pre- sentation: of the cnse of suxiliary
warships limitation first at tho noxt plenary
session Thuvaday, ‘Tho chlet delegates, mec-
tittg at St. James palace, also decided that tho
plenary sesslon would discuss the Main con-
ference questions in alpha betical order of ihe
countriea pro- posing. Press ta be Admitted
The American delegation woo a second vic-
tory whe...

Text 2
‘LONDON (AP) — Ian Ball, a 26-year- old
unemployed Englishman, was brought into
court today and charged with attempted mur-
der during an at- tempt to kidnap Princess
Anne from her car in the heart of London
Wed- nesday night. Ball, lean-faced and
bearded, stood stiffly in the dock at the Bow
Street Magistrate’s court, handcuffed to two
detectives. He spoke only once during his 60-
second appearance, saying iha London accent:
“I want to apply for legal aid.” The court or-
dered him held for another hearing on Ma...
Hy United Press International The federal
government has Proposed new methods of
constructing federal buildings in a move lo
save addilional energy and suggested ils effort
could be adapted to all new buildings, Arthur
F, Sampson, General Services Administration
ad- ministrater, said new features for such
construction would include the collection of
rain waler for cooling and irriga- tion, solar
energy collectors and the covering of exterior
walls with earth. “Whal we are saying is that
these design criteri...
— Washington, Jan. 27 1 AP).—Immiedl-
aie mmoval of John F. Herbert as pro- — hi-
bition administrator for Montana and ‘Idaho
was demanded m the Seuate to- ‘day by Sen-
ators Borah. idaho, and Waeeler, Montana. on
the ground of charges placed before them by
Depart- meat of Justice investigators. Wheeler
accompanied his demand nith a declaration
that prohibition en- foreemen: had brukea
down. He blamed the “politicians” and called
upon the Law Enforcement Commussion to
sum- mon members of the Republican Na-
tona...
London, Jan. 24, W.P—The Amer- jean dele-
gation fo the naval cen- ference teday won its
demand for presentation of the case of auxil-
jary warships linsitation flrst at the next ple-
trary session ‘Vhursday, The chief delegates,
meeting at Si, James Pelace, also decided that
the plenary session would discuss the main
confeyence questions in alphabetical order af
the cauntries proposing. The American del-
egation won a second victory when it was
decided to udmil certain representatives of the
press at fie plenary...

Table 16: Example false negatives for RETSim on the NEWS-COPY dataset (pairs of texts not
detected as near-duplicates by RETSim but labeled as near-duplicates in the original dataset). Ex-
amples are randomly selected and truncated at 512 characters for display.

18

Preprint

Text 1
BOZEMAN, Mont. (AP) — Chet Huntley,
whose resonant voice and rough-hewn face
be- came familiar to millions on the nightly
television news, died Wednesday in his moun-
tain resort home. He was 62. He underwent
surgery for lung cancer in January but had
remained activesuntil recent weeks. He died
at 2:20 a.m, according to his widow, Tippy
Hunt.cy. Huntiey was teamed for 14 years
with David Brinkley on NBC’s Huntley-
Brinkley Re- port. He quit in 1970 and re-
turned to his native Montana to develop the
$20-millio...
By THE ASSOCIATED PRESS Some Amer-
icans are paying up to 50 per cent more per
month for electricity this year than they did
last, an Associ- ated Press survey shows. Con-
sumers are beginning to organize to fight the
rate hikes. A spot check of monthly elec- tric
bills this year and last showed that most in-
creases have been about $1 or $2, gen- erally
about 10 per cent, with the highest reported
boost com- ing in Jacksonville, Fia., where
the average tab went from $17.90 last year to
$27.70 this year. Utility...
BOZEMAN, Mont. (AP) — Vice President
Gerald R. Ford says the world will miss the
“‘unique abilities” of former television news
anchorman Chet Huntley. Huntley, 62, died
at his home Wednesday after a long bout
with lung cancer. Family ‘spokesmen said
a memorial service would be conducted for
Huntley Sunday at the Big Sky of Montana’
resort and recreation area south of Bozeman.
Huntley was chairman of the Big Sky board
of directors. Another memorial service is
scheduled Tuesday in the New York studios
of the...
WASHINGTON (AP) — The House has
passed legislation raising the minimum wage
from $1.60 an hour to $2 this year for most
workers covered and to $2.30 for all by 1978.
The bill, approved Wednesday 375 to 37, also
would increase by 7 million to 56.5 million
the number of workers covered by the mini-
mum wage laws. The bill is a modified ver-
sion of one President Nixon vetoed last year.
However, he is expected to sign this one if it
is finally approved after ad- justment with a
similar Senate passed measure, altho...

Text 2
BOZEMAN, Mont. (AP) - Chet Huntley,
whose resonant voice and rough-hewn face
became familiar to millions on the nightly
television news, died Wednesday in his moun-
tain resort home. He was 62. He underwent
surgery for lung cancer in January but had
remained active until recent weeks. He died
at 2:20 a.m., according to his widow, Tippy
Huntley. Huntley was teamed for 14 years
with David Brinkley on NBC’s Huntley-
Brinkley Report. He quit in 1970 and returned
to his native Montana to develop the $20 mil-
lion Bi...
By Louise Cook Acenciaiod Prece Writer
Same Americans are paying up io 20 per cent
more per month far electricity this year ihan
they did last, an -Associ- Press survey shows.
onsumers are beginning to ze to fight the rate
hikes, A spot check of monthly elec- tre hills
this year and Jast showed that most increases
ve been about $1 or $2, gen- erally about 10
per cent, with the highest reported boost com-
ing in Jacksonville, Fla., where the average
tab went from $17.90 last year to $27.70 this
year...
BOZEMAN, Mont. (AP) — Vice President
Gerald R. Ford says the world will miss the
“unique abilities” of former television news
anchorman Chet Huntley. Huntley, 62, died
at his home Wednesday after a long bout with
lung cancer. Family spokesmen said a me-
morial service would be con- ducted for Hunt-
ley Sunday at the Big- Sky of Montana resort
and recreation area south of Bozeman. Hunt-
ley was chair- man of the Big Sky board of
directors. Another memorial service is sched-
uled Tuesday in the New York studios of...

WASHINGTON (AP) — The House has
passed legislation raising the minimum wage
from $1.60 an hour to $2 this year for most
workers covered and to $2.30 for all by 1978.
The bill, approved Wednes- day 375 to 37,
also would in- crease by 7 million to 56.5 mil-
lion the number of workers cov- ered by the
minimum wage laws. The bill is a modified
version of one President Nixon vetoed last
year. However, he is ex- ted to sign this one if
it is inally approved after adjust- ment with a
similar Senate- passed measu...

Table 17: Example false positives for RETSim on the NEWS-COPY dataset (pairs of texts detected
as near-duplicates by RETSim but not labeled as near-duplicates in the original dataset). Examples
are randomly selected and truncated at 512 characters for display.

19

","3 2 0 2 v o N 8 2 ] L C . s c [ 1 v 4 6 2 7 1 . 1 1 3 2 : v i X r a Preprint RETSIM : RESILIENT AND EFFICIENT TEXT SIMILARITY Marina Zhang1 , Owen Vallis1 , Aysegul Bumin * 2 , Tanay Vakharia1 , Elie Bursztein1 Google1 University of Florida2 ABSTRACT This paper introduces RETSim ( Resilient and Efficient Text Similarity ) , a lightweight , multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval , clustering , and dataset deduplication tasks . We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings , achieving new state-of-the-art perfor- mance on dataset deduplication , adversarial text retrieval benchmarks , and spam clustering tasks . We also introduce the W4NT3D benchmark ( Wiki-40B 4dver- sarial Near-T3xt Dataset ) for evaluating multilingual , near-duplicate text retrieval capabilities under adversarial settings . RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https : //github.com/google/unisim . 1 INTRODUCTION Robust near-duplicate text detection is an essential component of many tasks , including retriev- ing documents , detecting plagiarism ( Sun et al. , 2013 ) and blocking adversarial spam cam- paigns ( Ahmed et al. , 2022 ) . Users have come to expect that systems can return accurate results despite their queries exhibiting a 20 % to 30 % typo rate ( Hagen et al. , 2017 ) . Furthermore , effi- ciently deduplicating text datasets is critical to training state-of-the-art large language models ( Lee et al. , 2022 ; Kandpal et al. , 2022 ) . For more than two decades , MinHash-based ( Broder et al. , 1998 ) locality-sensitive hashing ( LSH ) has been the most prevalent algorithm used for near-duplicate detection due to its simplicity , robust- ness , and speed . For example , the vast majority of dataset deduplication efforts still rely on MinHash ( Lee et al. , 2022 ; Kocetkov et al. , 2022 ) . However , like all LSH-based techniques , MinHash is not without downsides ; chief among them being that it is very parameter-sensitive and requires heavy tuning . Additionally , MinHash lacks resilience to typos due to its reliance on n-grams , leading to poor performance on noisy data and a vulnerability to hash-busting attacks ( Issac et al. , 2014 ) . On the other hand , deep learning models are the dominant way to perform vector-based semantic text retrieval ( Muennighoff et al. , 2022 ) , but so far , no neural embedding has been able to consistently outperform MinHash for robust near-duplicate detection ( Silcock et al. , 2022 ) . This is mostly due to the focus on improving semantic capabilities , which leads models to be too large to run extremely quickly and the use of sub-word level tokenization , which is not resilient to typos and adversarial attacks ( Morris et al. , 2020 ; Bursztein et al. , 2023 ) . To fill this gap , we introduce RETSim ( Resilient and Efficient Text Similarity ) , a lightweight , mul- tilingual deep learning model trained specifically to produce robust neural embeddings specialized for near-duplicate detection . By combining the state-of-the-art RETVec text vectorizer , a modern transformer block ( Hua et al. , 2022 ) , a large typo-augmented training corpus , and a metric learn- ing training regime , RETSim is able to achieve new state-of-the-art performance on near-duplicate detection benchmarks ( Section 4.2 ) , dataset deduplication tasks ( Sections 4.3 and 5.1 ) , and spam clustering applications ( Section 5.2 ) . Furthermore , while datasets and benchmarks exist for corpus deduplication and near-duplicate text retrieval , none of these have focused on systematically evaluating near-duplicate retrieval perfor- mance under the presence of typos , word manipulations , and sentence or paragraph-level modifica- * This work was done during the author ’ s internship at Google . 1 Preprint tions . To address this need , we additionally introduce the W4NT3D benchmark ( Wiki-40B 4dver- sarial Near-T3xt Dataset ) which enables the evaluation of algorithms on adversarial near-duplicate text retrieval in a multilingual setting . We report the performance of RETSim , MinHash , and pop- ular neural embeddings such as Universal Sentence Encoder ( Cer et al. , 2018 ) and LaBSE ( Feng et al. , 2022 ) on this new benchmark in Section 4.2 , highlighting uneven performance across lan- guages and types of adversarial manipulations . The RETSim model and the W4NT3D benchmark are open-sourced at https : //github.com/google/unisim under the MIT License . 2 RELATED WORK Near-Duplicate Detection Identifying noisy near-duplicate documents in a large corpus is a fun- damental task with a wide range of applications , such as detecting plagiarism , finding reproduced content in literature or news articles ( Gyawali et al. , 2020 ; Silcock et al. , 2022 ) , and deduplicat- ing training datasets for language models . Previous research has shown that duplicates in training datasets lead to inefficient training ( Lee et al. , 2022 ) and privacy concerns for large language models ( LLMs ) , where models memorize and regenerate duplicated training sequences at a much higher frequency ( Kandpal et al. , 2022 ) . Unlike semantic text similarity , the task of identifying textual near-duplicates has been predominated by non-neural , n-gram-based algorithms such as MinHash ( Broder et al. , 1998 ) , which is the most widely used technique for deduplicating large training corpuses ( Kocetkov et al. , 2022 ; Lee et al. , 2022 ) . MinHash is a technique for estimating the Jaccard similarity between two sets . Algorithms such as MinHash or SimHash ( Charikar , 2002 ) can be combined with locality-sensitive hashing ( LSH ) techniques for fast , approximate nearest neighbor search and data clustering . This allows them to scale and deduplicate corpuses containing terabytes of data such as C4 ( Lee et al. , 2022 ) and The Stack ( Kocetkov et al. , 2022 ) . However , n-gram or shingling-based techniques typically require texts to be parsed into a standardized form ( e.g . by lower-casing or stripping punctuation ) , which makes them susceptible to typos and adversarial attacks and pose a challenge when attempt- ing to differentiate between dissimilar documents and near-duplicate documents with adversarial augmentations . Semantic Text Similarity The task of computing semantic similarity between text is closely re- lated to near-duplicate detection . Semantic text similarity refers to the assessment of the semantic relatedness of two pieces of text based on their meaning rather than their syntactic structure , as in the case of near-duplicate detection . Recently , transformer-based language models such as Universal Sentence Encoder ( Yang et al. , 2019 ) , LaBSE ( Feng et al. , 2022 ) and LLM-based embeddings ( Anil et al. , 2023 ) which embed text into high-dimensional embedding vectors have been successfully used to retrieve semantically-related documents using cosine similarity . Modern text retrieval sys- tems combine these embeddings with an approximate nearest neighbor ( ANN ) search algorithm to efficiently retrieve documents matching user queries . However , language models have been shown to be vulnerable to adversarial attacks and naturally- occurring typos ( Alzantot et al. , 2018 ; Gao et al. , 2018 ; Morris et al. , 2020 ) . Furthermore , language models are typically very large and costly to run even with hardware acceleration , which makes them unsuited for large-scale dataset deduplication or identifying near-duplicates in the presence of typos or adversarial text manipulations . Metric Learning Metric learning aims to learn an embedding space where similar items have a small distance between their embeddings and dissimilar items are further away . Many state-of- the-art embeddings use metric learning for unsupervised training or fine-tuning including Sentence- BERT ( Reimers & Gurevych , 2019 ) and E5 ( Wang et al. , 2022 ) . RETVec is a resilient , multilingual embedding and text vectorizer trained to be robust against various forms of character-level typos and adversarial attacks . We extend the RETVec training regime to full text documents for RETSim . We use Multi-Similarity Loss ( Wang et al. , 2019 ) for pair-based metric learning , where typo-laden and near-duplicate versions of texts are trained to be closer in the embedding space , while other texts are pushed further away . Multi-Similarity Loss is based on a general weighting framework for pair-based losses and achieves state-of-the-art performance , outperforming alternatives such as Triplet Loss ( Schroff et al. , 2015 ) . 2 Preprint Figure 1 : RETSim model architecture diagram . RETSim works on arbitrary length text by split- ting texts into chunks of 512 characters during its vectorization phase and encodes them using the RETVec character vectorizer . The RETSim model then embeds each chunk of text into 256-dim partial embeddings and combines them to produce the global embedding . 3 RETSIM 3.1 ARCHITECTURE The RETSim model is composed of three main components ( as depicted in Figure 1 ) : The character-level vectorizer splits the input text into chunks of 512 characters , then uses the RETVec chararcter encoder ( Bursztein et al. , 2023 ) to encode each chunk , resulting in a batch of ( 512 , 24 ) dense inputs . The RETVec character vectorizer encodes each Unicode character as a compact 24-bit binary representation based on its integer codepoint value . This allows the vectorizer to encode all valid Unicode characters and support all languages . Furthermore , the character-level vectorizer has been shown to be more resilient against typos and adversarial attacks . A small transformer model is used to compute 256-dimension embeddings for each chunk of the input text . RETSimPartial-Dup uses these embeddings directly to finding documents that have matching chunks of text . Architecturally , the model consists of two Gated Attention Unit ( GAU ) blocks ( Hua ( Radenovi´c et al. , 2018 ) , a dense et al. , 2022 ) , followed by a Generalized-Mean pooling layer projection layer which projects the embedding into 256 dimensions , and an L2 normalization layer . The model has only 536k parameters , which is more than two orders of magnitude smaller than other neural embeddings ( Table 1 ) . L2-normalization allows the embeddings to be compared using cosine similarity . We discuss the impact of key architecture design choices in Section 6 . Hyperparameter details are provided in Appendix A.1.1 , and additional ablations results in Appendix A.5 . An embedding averaging module is then used to combine partial text embeddings into a full-text embedding which is used for global near-duplicate matching ( RETSimNear-Dup ) . Averaging chunked embeddings to produce a global embedding is a standard technique used by many models ( Cer et al. , 2018 ) to support infinite length inputs in a cost-efficient manner . We experimented with other aggregation techniques to produce more accurate global embeddings , including training a deep- averaging network ( Iyyer et al. , 2015 ) , but this did not improve performance and resulted in higher computation cost . RETSimNear-Dup and RETSimPartial-Dup are computed in a single forward pass which makes it computationally efficient . We output both types of embeddings as they have different applications : RETSimNear-Dup is better-suited for full-text matching and retrieval ( Section 4.2 ) , while RETSimPartial-Dup is used to find partial text matches where the near-duplicate content appears only in part of the document ( Section 4.3 ) . 3.2 MODEL TRAINING Dataset We use the multilingual C4 dataset ( mC4 ) for raw text data and following ( Xue et al. , 2020 ) , we use a language sampling exponent of α = 0.3 to balance sampling between low and high-resource languages . We only use text containing at least 16 characters , and we randomly select between 1 and 8 sentences ( roughly 512 characters ) for each text chunk . For each example in the 3 Input text Chunk vector Chunk vector .... Chunk vector Chunked and vectorized text ( num_chunks , 512 , 24 ) RETSim model RetSim Partial-Dup ( num_chunks , 256 ) RetSim Near-Dup ( 256 ) r e d o c n E r a h C c e v T E R U A G U A G g n i l o o P 2 L + e s n e D t x e t l a i t r a P s g n i d d e b m e 2 L + g n i g r a r e v A g n i d d e b m e t x e t l l u F Preprint training dataset , we generate 5 pairs of augmented examples . We apply three levels of augmentation to each example text chunk ( in this order ) : sentence-level , word-level , and character-level . For each level , we randomly select the augmentation to be applied from the following categories : insertion , deletion , substitution , and transposition . We randomly apply between 0 − 25 % sentence-level aug- mentation and up to 30 % combined character and word-level augmentation . Empirically , we found that increasing the percentage of augmentation beyond this point causes RETSim ’ s performance to degrade . The full list of augmentations used can be found in Appendix A.2 . Training Procedure We train RETSim using Multi-Similarity Loss ( Wang et al. , 2019 ) with α = 4 , β = 40 , λ = 0.5 , and ϵ = 0.1 . We hypertuned these parameters and the results are shown in Appendix A.5 . We train for 1 million steps with batch size = 1024 . The similarity loss trains the model to embed augmented versions of the same text closer in the embedding space , while dissimilar texts are pushed further apart . We use the LAMB optimizer ( You et al. , 2019 ) with a max learning rate of 0.001 and cosine decay . Detailed training hyperparameters are reported in Appendix A.1.2 . 4 EVALUATION Model/Algorithm Type Embed./Hash Size # Model Parameters LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim Neural Neural Neural Neural Hashing Hashing Neural 768 512 768 768 b bits n hashes 256 471M 69M 278M ? N/A N/A 536k Table 1 : Embedding models and hashing algorithms benchmarked in the paper . 4.1 MODELS AND ALGORITHMS EVALUATED We benchmark RETSim against four multilingual semantic text embeddings as well as popular n- gram based algorithms primarily used in near-duplicate text detection ( Table 1 ) . Our baseline text embeddings include Multilingual Universal Sentence Encoder ( Yang et al. , 2019 ) , LaBSE ( Feng et al. , 2022 ) , Multilingual E5 ( Wang et al. , 2022 ) , and PaLM 2 Gecko Embeddings ( Anil et al. , 2023 ) . All text embeddings are L2-normalized and compared using cosine similarity . We use exact search to index and retrieve nearest neighbors from our vector index for the experiments in Section 4 . For non-neural near-duplicate detection and clustering algorithms , we selected the two most popular algorithms : MinHash ( Broder et al. , 1998 ) and SimHash ( Charikar , 2002 ) . For MinHash , we use Datasketch ’ s MinHashLSH library . Following the most common practices in the literature ( Silcock et al. , 2022 ) , we use 10 hash functions for MinHash unless otherwise specified . We use word-level n-grams where we select the best value out of n = { 2 , 3 , 4 , ... , 10 } . For SimHash , we use 64- bit SimHash and conduct shingling at the character level , where the shingle size is selected from n = { 2 , 3 , 4 , ... , 10 } . For the near-duplicate detection benchmarks ( NEWS-COPY and CORE Near- Duplicates datasets ) , we tune the optimal deduplication threshold ( e.g . based on cosine similarity for neural-based embeddings and Jaccard similarity for MinHash ) . Detailed hyperparameter settings for RETSim and baseline algorithms used in the evaluation can be found in Appendix A.3 . 4.2 W4NT3D : WIKI-40B 4DVERSARIAL NEAR-T3XT DATASET EVALUATION Dataset Description The vast majority of text retrieval benchmarks are focused on evaluating semantic performance . To the best of our knowledge , there is no multilingual benchmark for sys- tematically measuring adversarial robustness for near-duplicate text retrieval . In an attempt to fill in the gap , we create and publish the W4NT3D benchmark ( Wiki-40B 4dversarial Near-T3xt Dataset ) , which contains around 400k pairs of syntactically similar texts to evaluate near-duplicate text re- trieval in the presence of various forms of text manipulations and typos . W4NT3D is based on the Wiki-40B dataset ( Guo et al. , 2020 ) . The dataset is split into query exam- ples and target examples , where query examples are synthetically-modified near-duplicate versions 4 Preprint of a target example ( e.g . with typos ) . For each of the 41 language splits in Wiki-40B , we randomly select 10,000 texts . The length of the target string is uniformly selected from between 16 and 8192 characters , in order to test performance on short and long text . To construct the query text corre- sponding to a target text , we randomly apply up to 25 % word and character augmentations , and up to 25 % sentence and paragraph augmentations . For each augmentation , we uniformly select from the [ insert , delete , substitute , and swap ] operations . We use Recall @ k with k = 1 as the main metric , following the setup commonly found in semantic text retrieval benchmarks . Model/Algorithm Arabic Chinese English German French Spanish Japanese Korean Russian Thai Avg ( 41 Langs ) LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSimPartial-Dup RETSimNear-Dup 0.915 0.915 0.936 0.497 0.558 0.633 0.928 0.971 0.917 0.986 0.980 0.623 0.276 0.172 0.946 0.971 0.944 0.958 0.959 0.961 0.591 0.591 0.954 0.987 0.931 0.942 0.944 0.932 0.561 0.558 0.949 0.978 0.930 0.938 0.948 0.934 0.519 0.556 0.947 0.983 0.888 0.903 0.896 0.911 0.513 0.575 0.938 0.976 0.931 0.990 0.979 0.578 0.465 0.223 0.963 0.986 0.949 0.984 0.986 0.701 0.593 0.814 0.971 0.991 0.918 0.910 0.911 0.851 0.554 0.523 0.946 0.970 0.882 0.888 0.921 0.571 0.669 0.416 0.941 0.946 0.921 0.912 0.937 0.823 0.550 0.538 0.949 0.977 Table 2 : Per-language retrieval performance for various embedding models and algorithms on the W4NT3D benchmark . Results on selected languages are reported alongside the average Recall @ 1 for all 41 languages . Full results for all languages are reported in Appendix A.4 . Multilingual Performance Overall , RETSimNear-Dup achieves an average Recall @ 1 of 0.977 across all 41 languages on the W4NT3D benchmark ( Table 2 ) . RETSimPartial-Dup is second best with a Recall @ 1 of 0.949 and Multilingual E5 , the best-performing baseline , is third with an aver- age Recall @ 1 of 0.932 . We expect that RETSimNear-Dup outperforms RETSimPartial-Dup because the W4NT3D benchmark requires an algorithm to not just find near-duplicates , but to find the most simi- lar text . RETSimPartial-Dup optimizes for finding the most similar chunk of text in the corpus , which is not always the most similar text overall . Similarly , we hypothesize that MinHash and SimHash per- form poorly on the W4NT3D benchmark due to their lack of ability to distinguish which is the most similar text among the near-duplicates detected , and embedding-based models and cosine similarity offer a more fine-grained measure of similarity . RETSimNear-Dup outperforms baseline algorithms on all languages except for Chinese and Japanese . For these languages , we theorize that semantic embeddings may have the slight edge in performance because their significantly larger model sizes ( more than 100x larger than RETSim , as shown in Ta- ble 1 ) allow them to have a better representation on languages with large character sets . Furthermore , the sub-word level tokenizers used in the baseline embeddings often treat each character in Chinese or Japanese as individual tokens , which could offer higher resilience to typos . Figure 2 : Recall @ 1 performance on the W4NT3D benchmark , broken down by augmentation type . Results are averaged across all 41 language splits in W4NT3D . Adversarial Resilience Delving deeper into the impact of various types of text manipulation re- veals that RETSimNear-Dup and RETSimPartial-Dup perform almost equally well regardless of the type of augmentation applied ( Figure 2 ) . Semantic text embeddings perform well on paragraph , sentence and word-level manipulations , but as expected , exhibit significantly weaker performance towards character-level typos . MinHash and SimHash struggle more with word-level augmentations than deep-learning based embeddings and collapse when character-level typos are introduced . We at- 5 1.00 0.75 0.50 0.25 0.00 l l 1 @ a c e R Paragraph Sentence Word Character Augmentation Level LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim ( Partial-Dup ) RETSim ( Near-Dup ) Preprint tribute RETSim ’ s resilience to adversarial manipulations to the RETVec character encoder as well as using deep metric learning to train robust embeddings . Figure 4 reports the Recall @ 1 performance of the algorithms as the amount of augmentation in- creases . All algorithms perform perfectly when no augmentation is applied ( exact matching ) , but as the percentage of augmentation increases , n-gram based approaches exhibit a steep drop in perfor- mance . Semantic text embeddings are able to sustain a larger degree of augmentation before their retrieval capabilities start to degrade ( over 20 % ) . RETSimNear-Dup is the most robust algorithm , with a noticeable drop in performance only after around 40 % augmentation . This makes RETSim the most effective approach at clustering and deduplicating text under adversarial settings . Figure 3 : Recall @ 1 performances on the W4NT3D benchmark ( English only ) for varying max target lengths . Figure 4 : Recall @ 1 performances on the W4NT3D benchmark ( English only ) as the amount of augmenta- tion applied to the query text increases . Text Length Impact on Performance Figure 3 reports the Recall @ 1 performance of RETSim and baseline algorithms as the length of the query and target text varies . We see that RETSimNear-Dup and RETSimPartial-Dup outperforms all other methods on short texts with fewer than 128 characters . As the text length increases beyond 512 characters , RETSimNear-Dup remains close to perfect while RETSimPartial-Dup ’ s performance degrades since it splits the text into multiple embeddings and finds the nearest matching chunk of text . MinHash and SimHash also perform poorly on short text lengths and start to degrade on longer texts . For neural-based embeddings , we observe a slight drop in performance on longer texts for all models except RETSimNear-Dup and Multilingual USE , the only two embeddings that can handle arbitrary length inputs . 4.3 REAL-WORLD NEAR-DUPLICATE DETECTION EVALUATION Setup We benchmark RETSim ’ s ability to identify near-duplicate content on real-world datasets from the literature . The NEWS-COPY Deduplication dataset ( Silcock et al. , 2022 ) contains 27,210 historical news articles with 122,876 positive duplicate pairs . The dataset consists of noisy near- duplicates due to factors like OCR errors , plagiarism , and news aggregation . We also evaluate the algorithms on the CORE Near-Duplicates dataset ( Gyawali et al. , 2020 ) , which consists of 100k scholarly articles with 25k exact duplicates , 25k near-duplicates , and 50k non-duplicates . Near- duplicates in this dataset arise from article revisions , versioning and metadata differences , and hu- man typos . A key difference between these two benchmarks and the W4NT3D benchmark is that these two benchmarks are focused on detecting and clustering near-duplicate text , rather than robust text retrieval based on syntactic similarity . For both benchmarks , we follow the experimental setup provided in the papers and report Adjusted Rand Index ( ARI ) for the NEWS-COPY dataset and report precision/recall/F1 scores on the CORE Near-Duplicates dataset . Results On the NEWS-COPY dataset , RETSimPartial-Dup outperforms all other approaches by a significant margin ( 4.8 % ARI compared to our best MinHash result ) , as reported in Table 3 . In the dataset , there are many near-duplicate pairs where one text is significantly longer than the other , so it is expected that RETSimPartial-Dup , which can find matching text chunks in documents , is more suited for the task and outperforms RETSimNear-Dup . Bucketing the near-duplicate detection rate of each algorithm by the length ratio between positive pairs ( Figure 5 ) , we observe that RETSimPartial-Dup outperforms MinHash regardless of the length ratio , but MinHash surpasses RETSimNear-Dup per- formance when one text is above roughly 1.5x the length of the other text in a near-duplicate pair . 6 1.0 0.8 0.6 0.4 l l 1 @ a c e R 16 32 64 128 256 512 1024 2048 4096 8192 Max Target Text Length l l 1 @ a c e R 1.0 0.8 0.6 0.4 0 % 10 % 20 % 30 % 40 % 50 % Text Augmentation Amount ( % ) LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim ( Partial-Dup ) RETSim ( Near-Dup ) Preprint Model/Algorithm ARI Multilingual USE Multilingual E5-Base S-BERT * SimHash MinHash * MinHash ( Ours ) RETSimPartial-Dup RETSimNear-Dup 0.730 0.742 0.700 0.695 0.737 0.783 0.831 0.704 Table 3 : Performance comparison on the NEWS-COPY dataset . Adjusted Rand Index ( ARI ) values are reported . * denotes results from Silcock et al . ( 2022 ) . Figure 5 : Near-duplicate detection rate of RET- Sim vs MinHash for different length ratios of pos- itive pairs . X-axis is the length of longer divided by shorter text , rounded to the nearest integer . Additionally , we noticed that the labels in the dataset were occasionally noisy , as a substantial por- tion of the RETSim false positives appear to be near-duplicates upon inspection ( Appendix A.6 ) . On the CORE Near-Duplicates dataset ( Table 4 ) , where documents ( article title + abstract ) are roughly the same size , RETSimPartial-Dup and RETSimNear-Dup performance is roughly equivalent . Both methods outperform the baselines in terms of macro F1 score and accuracy . We use MinHash + LSH with 256 hash functions for computational efficiency , as recommended by the datasketch library1 for better accuracy than the default setting . Deduplication thresholds and detailed hyperpa- rameter settings for the algorithms on both near-duplication datasets can be found in Appendix A.3 . Model / Algorithm Exact Title Matching * LaBSE Multilingual USE Multilingual E5-Base MinHash + LSH RETSimPartial-Dup RETSimNear-Dup Precision Duplicates Recall Duplicates Precision Non-Duplicates Recall Non-Duplicates Macro F1 Accuracy 0.830 0.937 0.917 0.931 0.929 0.945 0.928 0.500 0.923 0.907 0.908 0.902 0.941 0.937 0.709 0.930 0.918 0.919 0.915 0.945 0.942 0.992 0.943 0.927 0.939 0.938 0.949 0.934 0.757 0.933 0.917 0.924 0.921 0.945 0.935 0.746 0.919 0.909 0.920 0.918 0.928 0.926 Table 4 : Evaluation results on the CORE Near-Duplicates dataset . Precision/recall/macro F1 and accuracy numbers are reported . * denotes results from Gyawali et al . ( 2020 ) . 5 APPLICATIONS 5.1 TRAINING DATASET DEDUPLICATION Model/Algorithm % train examples with dup in train % valid examples with dup in train MinHash + LSH Exact Substring * RETSimNear-Dup RETSimPartial-Dup 0.47 % 2.76 % 3.17 % 12.77 % 0.46 % 0.52 % 0.59 % 2.66 % Table 5 : Deduplication rate on Wiki-40B ( English ) . * denotes results from Lee et al . ( 2022 ) . Setup We evaluate RETSim ’ s ability to deduplicate text training datasets by deduplicating the English split of Wiki-40B ( Guo et al. , 2020 ) . We conservatively set the cosine similarity deduplica- tion threshold to 0.1 for RETSimNear-Dup and 0.15 for RETSimPartial-Dup to limit the amount of false positives , based on the optimal thresholds found in the evaluation ( Appendix A.3 ) . We use USe- arch ’ s default vector index for approximate nearest neighbor search ( Vardanian , 2023 ) . We compare 1datasketch : Big Data Looks Small . https : //github.com/ekzhu/datasketch . 7 t e a R n o t i t c e e D e a c t i l p u D - r a e N 1.0 0.8 0.5 0.3 0.0 1 2 3 4 5 6 7 8 9 10 Length Ratio between Near-Duplicate Text Pair RETSim ( Partial-Dup ) RETSim ( Near-Dup ) MinHash Preprint Model/Algorithm Accelerator Batch Size Embedding / Hashing time ( sec ) examples/sec MinHash + LSH RETSim RETSim RETSim CPU AMD 7950 32 cores Onnx CPU AMD 7950 32 cores TensorFlow GPU RTX 4090 TensorFlow GPU NVIDIA H100 - 256 4096 16384 234 10839 720 363 12544 270 4062 8069 Table 6 : Embedding/hashing speed of RETSim vs MinHash + LSH on the Wiki-40B dataset . against MinHash + LSH , where we set the number of hash functions to be 256 following Kocetkov et al . ( 2022 ) and use a Jaccard similarity threshold of 0.8 for deduplication ( Lee et al. , 2022 ) . Results Overall , as reported in Table 5 , RETSimNear-Dup finds slightly more duplicates in the Wiki- 40B training and validation splits . This is in-line with our deduplication results ( Section 4.3 ) where RETSimNear-Dup outperforms other algorithms . On the other hand , RETSimPartial-Dup finds signifi- cantly more matches than the exact substring matching algorithm used in the previous study ( Lee et al. , 2022 ) , showcasing the usefulness of performing both near-duplicate and partial-duplicate matching at once . This larger-than-expected number of partial matches indicate that machine learn- ing practitioners should take extra care to deduplicate Wikipedia at the chunk level to avoid feeding duplicate text to their models . In terms of embedding speed ( Table 6 ) , RETSim is significantly slower than MinHash + LSH on CPU ( 46x slower ) , competitive when using a desktop GPU such as the RTX 4090 ( 3x slower ) and almost on-par when using a high-end GPU like the NVIDIA H100 ( 1.5x slower ) . Our current code is written in Python and not fully optimized , so we expect this performance gap to significantly shrink as we optimize our implementation . Although RETSim is slower than MinHash , RETSim is significantly smaller and faster than other text embedding models , and closes the performance gap between neural and non-neural based methods for near-duplicate text detection and dataset deduplication . Both RETSimNear-Dup and RETSimPartial-Dup are returned at the same time so they have the same embedding speed . Indexing and retrieval times will depend on the vector index and search algorithm used . For longer documents , RETSimPartial-Dup will produce more embeddings than RETSimNear-Dup , so RETSimPartial-Dup offers a tradeoff between finer-grained matching versus indexing/retrieval speed , which will depend on the specific vector search algorithm and dataset used . 5.2 IN THE WILD : SPAM EMAIL CLUSTERING In this section , we showcase RETSim ’ s real-world performance on clustering near-duplicate text which has been heavily manipulated by adversarial attacks by performing an evaluation on spam campaigns . Spam constitutes a strong proving ground for near-duplicate clustering algorithms as spammers employ adversarial augmentation techniques in an attempt to evade detection . Such aug- mentations typically include appending or prepending unrelated text , interleaving random words and different languages , intentionally introducing typos , abusing extended character sets such as emojis and homoglyphs , and more . These techniques are collectively referred to as hash-busting . Setup The dataset consists of 5,252 spam emails from 196 spam campaigns , donated by Gmail users who flagged them when they reached their inboxes . Each example contains the email subject concatenated with the message content . The emails were misclassified by a spam classifier due to their effective adversarial text manipulation techniques , which makes them a challenging test set for clustering evaluations . Some examples of hash-busting attacks and adversarial manipulations we observe include the use of homoglpyphs , uncommon Unicode character sets , invisible characters , and padding with random words from different languages . To get the ground truth campaign clusters , emails were manually reviewed and assigned to a specific spam campaign based on similarity by human reviewers . We use agglomerative clustering to cluster spam emails , and report homogeneity , completeness , V-Measure , and Adjusted Rand Index ( ARI ) metrics . Results Overall , we observed that RETSim is significantly better at clustering near-duplicates with adversarial manipulations , outperforming both SimHash and USE across all metrics considered ( Table 7 ) . In particular , we observed that RETSim outperforms USE by 4.6 % on the V-Measure score which is our main metric . The results reported in this section are in-line with what we observe since we deployed RETSim as our main near-duplicate detection algorithm in December 2022 . 8 Preprint Model / Algorithm Homogeneity Completeness V-Measure ARI USE SimHash + LSH RETSimNear-Dup 0.856 0.867 0.937 0.955 0.876 0.963 0.903 0.871 0.949 0.6 0.571 0.747 Table 7 : Performance on clustering adversarial spam campaigns in practice . 6 ABLATION STUDIES Setup In this section , we summarize the key ablation studies we performed when designing RET- Sim . All the models used in this section are trained using the setup detailed in Appendix A.1.2 , ex- cept we only train them for 100k steps to reduce computational costs . We evaluate RETSimNear-Dup ’ s performance for each model on a subset of the W4NT3D benchmark , where we randomly select 1000 examples from each of the 41 language splits and use Recall @ 1 as reported metric . Block Type Recall @ 1 Chunk Size Recall @ 1 Embed . Dim Recall @ 1 RETVec MLP ConvNeXt BERT T5 * GAU 0.975 0.978 0.973 0.980 0.986 128 256 * 512 1024 2048 0.979 0.984 0.986 0.983 0.978 64 128 * 256 512 768 0.969 0.980 0.986 0.986 0.986 Table 8 : RETSim ablation study results on architecture block type ( left ) , text chunk size ( middle ) , and embedding dimension ( right ) . * Bold denotes the value selected for the final RETSim model . Results Table 8 contains RETSim ablation study results on max text chunk size , architecture block type , and embedding size . The most important architectural decision was to decide the optimal text chunk size and finding the right balance between having the smallest size possible to maximize RETSimPartial-Dup efficiency while ensuring RETSimNear-Dup full-text embeddings can work effec- tively on full documents . We find that chunks of 512 characters offer the best performance . We also tested various model architectures and transformer blocks to find the best balance between efficiency and performance . We find that the more modern GAU block ( Hua et al. , 2022 ) outper- forms the vanilla BERT transformer block ( Devlin et al. , 2019 ) and the T5 block ( Xue et al. , 2020 ) . We also tried modern CNN architectures such as ConvNeXt ( Liu et al. , 2022 ) and the MLP architec- ture proposed in RETVec ( Bursztein et al. , 2023 ) , but both were worse than GAU block performance . Last but not least , we found that increasing the embedding size past 256 dimensions does not yield any meaningful improvements for RETSimNear-Dup . Accordingly , we opted to use a 256-dimension embedding for space-efficiency and to maximize indexing and query speed . Additional ablation studies for other hyperparameters can be found in Appendix A.5 . 7 FUTURE WORK RETSim ’ s novel training regime , which combines metric learning and data augmentation , has many other potential applications that we plan to explore in future work . For example , it could be adapted or extended to train robust semantic embeddings or image similarity embeddings . Additionally , we expect that as general models become bigger and more expensive to run in the future , smaller , specialized models such as RETSim will emerge as an efficient alternative for a wide range of tasks . 8 CONCLUSION In this paper , we introduced RETSim , a novel , multilingual text embedding which achieves state- of-the-art performance on near-duplicate text detection , dataset deduplication , and syntactic text similarity benchmarks . RETSim is significantly faster than previous neural-based text embeddings and more robust than n-gram based algorithms , which makes it suitable for large-scale text retrieval and dataset deduplication , especially in adversarial settings such as spam detection . Furthermore , we introduced the W4NT3D benchmark , the first multilingual dataset designed to measure the ad- versarial robustness of near-duplicate text detection algorithms . We open-source both RETSim and the W4NT3D benchmark under the MIT License . 9 Preprint REFERENCES Naeem Ahmed , Rashid Amin , Hamza Aldabbas , Deepika Koundal , Bader Alouffi , and Tariq Shah . Machine Learning Techniques for Spam Detection in Email and IoT Platforms : Analysis and Research Challenges . Security and Communication Networks , 2022:1–19 , February 2022 . ISSN 1939-0122 , 1939-0114. doi : 10.1155/2022/1862888 . URL https : //www.hindawi.com/ journals/scn/2022/1862888/ . Moustafa Alzantot , Yash Sharma , Ahmed Elgohary , Bo-Jhang Ho , Mani Srivastava , and Kai-Wei Chang . Generating Natural Language Adversarial Examples , September 2018 . URL http : //arxiv.org/abs/1804.07998 . arXiv:1804.07998 [ cs ] . Rohan Anil , Andrew M. Dai , Orhan Firat , Melvin Johnson , Dmitry Lepikhin , Alexandre Passos , Siamak Shakeri , Emanuel Taropa , Paige Bailey , Zhifeng Chen , Eric Chu , Jonathan H. Clark , Laurent El Shafey , Yanping Huang , Kathy Meier-Hellstern , Gaurav Mishra , Erica Moreira , Mark Omernick , Kevin Robinson , Sebastian Ruder , Yi Tay , Kefan Xiao , Yuanzhong Xu , Yujing Zhang , Gustavo Hernandez Abrego , Junwhan Ahn , Jacob Austin , Paul Barham , Jan Botha , James Brad- bury , Siddhartha Brahma , Kevin Brooks , Michele Catasta , Yong Cheng , Colin Cherry , Christo- pher A. Choquette-Choo , Aakanksha Chowdhery , Cl´ement Crepy , Shachi Dave , Mostafa De- hghani , Sunipa Dev , Jacob Devlin , Mark D´ıaz , Nan Du , Ethan Dyer , Vlad Feinberg , Fangxiaoyu Feng , Vlad Fienber , Markus Freitag , Xavier Garcia , Sebastian Gehrmann , Lucas Gonzalez , Guy Gur-Ari , Steven Hand , Hadi Hashemi , Le Hou , Joshua Howland , Andrea Hu , Jeffrey Hui , Jeremy Hurwitz , Michael Isard , Abe Ittycheriah , Matthew Jagielski , Wenhao Jia , Kathleen Kenealy , Maxim Krikun , Sneha Kudugunta , Chang Lan , Katherine Lee , Benjamin Lee , Eric Li , Music Li , Wei Li , YaGuang Li , Jian Li , Hyeontaek Lim , Hanzhao Lin , Zhongtao Liu , Frederick Liu , Mar- cello Maggioni , Aroma Mahendru , Joshua Maynez , Vedant Misra , Maysam Moussalem , Zachary Nado , John Nham , Eric Ni , Andrew Nystrom , Alicia Parrish , Marie Pellat , Martin Polacek , Alex Polozov , Reiner Pope , Siyuan Qiao , Emily Reif , Bryan Richter , Parker Riley , Alex Castro Ros , Aurko Roy , Brennan Saeta , Rajkumar Samuel , Renee Shelby , Ambrose Slone , Daniel Smilkov , David R. So , Daniel Sohn , Simon Tokumine , Dasha Valter , Vijay Vasudevan , Kiran Vodrahalli , Xuezhi Wang , Pidong Wang , Zirui Wang , Tao Wang , John Wieting , Yuhuai Wu , Kelvin Xu , Yun- han Xu , Linting Xue , Pengcheng Yin , Jiahui Yu , Qiao Zhang , Steven Zheng , Ce Zheng , Weikang Zhou , Denny Zhou , Slav Petrov , and Yonghui Wu . PaLM 2 Technical Report , September 2023 . URL http : //arxiv.org/abs/2305.10403 . arXiv:2305.10403 [ cs ] . Andrei Z. Broder , Moses Charikar , Alan M. Frieze , and Michael Mitzenmacher . Min-wise inde- In Proceedings of the thirtieth annual ACM symposium on Theory of pendent permutations . computing , pp . 327–336 , 1998 . Eli Bursztein , Marina Zhang , Owen vallis , Xinyu Jia , and Alexey Kurakin . RetVec : Resilient and Efficient Text Vectorizer . 2023 . Daniel Cer , Yinfei Yang , Sheng-yi Kong , Nan Hua , Nicole Limtiaco , Rhomni St John , Noah Con- stant , Mario Guajardo-Cespedes , Steve Yuan , and Chris Tar . Universal sentence encoder . arXiv preprint arXiv:1803.11175 , 2018 . Moses S. Charikar . Similarity estimation techniques from rounding algorithms . In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing , pp . 380–388 , 2002 . Jacob Devlin , Ming-Wei Chang , Kenton Lee , and Kristina Toutanova . BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding , May 2019 . URL http : //arxiv . org/abs/1810.04805 . arXiv:1810.04805 [ cs ] . Fangxiaoyu Feng , Yinfei Yang , Daniel Cer , Naveen Arivazhagan , and Wei Wang . Language- agnostic BERT Sentence Embedding , March 2022 . URL http : //arxiv.org/abs/2007 . 01852. arXiv:2007.01852 [ cs ] . Ji Gao , Jack Lanchantin , Mary Lou Soffa , and Yanjun Qi . Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers , May 2018 . URL http : //arxiv.org/abs/ 1801.04354. arXiv:1801.04354 [ cs ] . 10 Preprint Mandy Guo , Zihang Dai , Denny Vrandeˇci´c , and Rami Al-Rfou . Wiki-40b : Multilingual language model dataset . In Proceedings of the 12th Language Resources and Evaluation Conference , pp . 2440–2452 , 2020 . Bikash Gyawali , Lucas Anastasiou , and Petr Knoth . Deduplication of Scholarly Documents us- ing Locality Sensitive Hashing and Word Embeddings . In Proceedings of the Twelfth Language Resources and Evaluation Conference , pp . 901–910 , Marseille , France , May 2020 . European Lan- guage Resources Association . ISBN 979-10-95546-34-4 . URL https : //aclanthology . org/2020.lrec-1.113 . Matthias Hagen , Martin Potthast , Marcel Gohsen , Anja Rathgeber , and Benno Stein . A large- In Proceedings of the 40th International ACM SIGIR scale query spelling correction corpus . Conference on Research and Development in Information Retrieval , pp . 1261–1264 , 2017 . Weizhe Hua , Zihang Dai , Hanxiao Liu , and Quoc Le . Transformer quality in linear time . In International Conference on Machine Learning , pp . 9099–9117 . PMLR , 2022 . B. Issac , R. Chiong , and S. M. Jacob . Analysis of phishing attacks and countermeasures , 2014 . Mohit Iyyer , Varun Manjunatha , Jordan Boyd-Graber , and Hal Daum´e Iii . Deep Unordered Composition Rivals Syntactic Methods for Text Classification . In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pp . 1681–1691 , Bei- jing , China , 2015 . Association for Computational Linguistics . doi : 10.3115/v1/P15-1162 . URL http : //aclweb.org/anthology/P15-1162 . Nikhil Kandpal , Eric Wallace , and Colin Raffel . Deduplicating Training Data Mitigates Pri- vacy Risks in Language Models . In Proceedings of the 39th International Conference on Machine Learning , pp . 10697–10707 . PMLR , June 2022 . URL https : //proceedings . mlr.press/v162/kandpal22a.html . ISSN : 2640-3498 . Denis Kocetkov , Raymond Li , Loubna Ben Allal , Jia Li , Chenghao Mou , Carlos Mu˜noz Ferrandis , Yacine Jernite , Margaret Mitchell , Sean Hughes , Thomas Wolf , Dzmitry Bahdanau , Leandro von Werra , and Harm de Vries . The Stack : 3 TB of permissively licensed source code , November 2022 . URL http : //arxiv.org/abs/2211.15533 . arXiv:2211.15533 [ cs ] . Katherine Lee , Daphne Ippolito , Andrew Nystrom , Chiyuan Zhang , Douglas Eck , Chris Callison- Burch , and Nicholas Carlini . Deduplicating Training Data Makes Language Models Better , March 2022 . URL http : //arxiv.org/abs/2107.06499 . arXiv:2107.06499 [ cs ] . Zhuang Liu , Hanzi Mao , Chao-Yuan Wu , Christoph Feichtenhofer , Trevor Darrell , and Saining Xie . A convnet for the 2020s . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp . 11976–11986 , 2022 . John X. Morris , Eli Lifland , Jin Yong Yoo , Jake Grigsby , Di Jin , and Yanjun Qi . TextAttack : A Framework for Adversarial Attacks , Data Augmentation , and Adversarial Training in NLP , Oc- tober 2020 . URL http : //arxiv.org/abs/2005.05909 . arXiv:2005.05909 [ cs ] . Niklas Muennighoff , Nouamane Tazi , Lo¨ıc Magne , and Nils Reimers . MTEB : Massive Text Em- bedding Benchmark , October 2022 . URL https : //arxiv.org/abs/2210.07316v1 . Filip Radenovi´c , Giorgos Tolias , and Ondˇrej Chum . Fine-tuning CNN image retrieval with no human annotation . IEEE transactions on pattern analysis and machine intelligence , 41 ( 7 ) :1655– 1668 , 2018 . Publisher : IEEE . Nils Reimers and Iryna Gurevych . Sentence-bert : Sentence embeddings using siamese bert- networks . arXiv preprint arXiv:1908.10084 , 2019 . Florian Schroff , Dmitry Kalenichenko , and James Philbin . FaceNet : A Unified Embedding for Face Recognition and Clustering . In 2015 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pp . 815–823 , June 2015. doi : 10.1109/CVPR.2015.7298682 . URL http : //arxiv.org/abs/1503.03832 . arXiv:1503.03832 [ cs ] . 11 Preprint Emily Silcock , Luca D ’ Amico-Wong , Jinglin Yang , and Melissa Dell . Noise-Robust De-Duplication at Scale , October 2022 . URL http : //arxiv.org/abs/2210.04261 . arXiv:2210.04261 [ cs ] . Yifang Sun , Jianbin Qin , and Wei Wang . Near Duplicate Text Detection Using Frequency-Biased Signatures . In David Hutchison , Takeo Kanade , Josef Kittler , Jon M. Kleinberg , Friedemann Mat- tern , John C. Mitchell , Moni Naor , Oscar Nierstrasz , C. Pandu Rangan , Bernhard Steffen , Madhu Sudan , Demetri Terzopoulos , Doug Tygar , Moshe Y. Vardi , Gerhard Weikum , Xuemin Lin , Yan- nis Manolopoulos , Divesh Srivastava , and Guangyan Huang ( eds . ) , Web Information Systems Engineering – WISE 2013 , volume 8180 , pp . 277–291 . Springer Berlin Heidelberg , Berlin , Hei- delberg , 2013 . ISBN 978-3-642-41229-5 978-3-642-41230-1. doi : 10.1007/978-3-642-41230-1 24 . URL http : . Series Title : Lecture Notes in Computer Science . Ash Vardanian . USearch by Unum Cloud , October 2023 . URL https : //github.com/ unum-cloud/usearch . Liang Wang , Nan Yang , Xiaolong Huang , Binxing Jiao , Linjun Yang , Daxin Jiang , Rangan Ma- jumder , and Furu Wei . Text Embeddings by Weakly-Supervised Contrastive Pre-training , De- cember 2022 . URL http : //arxiv.org/abs/2212.03533 . arXiv:2212.03533 [ cs ] . Xun Wang , Xintong Han , Weilin Huang , Dengke Dong , and Matthew R. Scott . Multi-similarity loss with general pair weighting for deep metric learning . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp . 5022–5030 , 2019 . Linting Xue , Noah Constant , Adam Roberts , Mihir Kale , Rami Al-Rfou , Aditya Siddhant , Aditya Barua , and Colin Raffel . mT5 : A massively multilingual pre-trained text-to-text transformer . arXiv preprint arXiv:2010.11934 , 2020 . Yinfei Yang , Daniel Cer , Amin Ahmad , Mandy Guo , Jax Law , Noah Constant , Gustavo Hernandez Abrego , Steve Yuan , Chris Tar , Yun-Hsuan Sung , Brian Strope , and Ray Kurzweil . Multilingual Universal Sentence Encoder for Semantic Retrieval , July 2019 . URL http : //arxiv.org/ abs/1907.04307 . arXiv:1907.04307 [ cs ] . Yang You , Jing Li , Sashank Reddi , Jonathan Hseu , Sanjiv Kumar , Srinadh Bhojanapalli , Xiaodan Song , James Demmel , Kurt Keutzer , and Cho-Jui Hsieh . Large batch optimization for deep learning : Training bert in 76 minutes . arXiv preprint arXiv:1904.00962 , 2019 . 12 Preprint A APPENDIX A.1 RETSIM DETAILS A.1.1 RETSIM MODEL HYPERPARAMETERS The full list of RETSim model hyperparameters can be found in Table 9 . Hyperparameter Max input length ( per chunk ) Block type # blocks Hidden dim Expansion rate Activation function Attention activation function Absolute positional encoding Relative positional encoding Norm type Pooling type Dropout rate Embedding dim # Parameters Value 512 GAU 2 256 1 Swish relu2 ScaledSin RoPE ScaleNorm GeM ( p = 3 ) 0 256 536k Table 9 : Detailed RETSim model hyperparameters . A.1.2 RETSIM TRAINING HYPERPARAMETERS Table 10 details the hyperparameters settings for training configuration , loss , and optimizer used to train the RETSim model . Hyperparameter Value Batch size Train steps LAMB ϵ LAMB β1 LAMB β2 Max learning rate End learning rate Learning rate decay Weight decay 1024 1 million 1e-6 0.9 0.999 0.001 0 Cosine 0 Table 10 : RETSim detailed training hyperparameters . A.2 TRAINING DATASET DETAILS Below , we provide the full list of augmentations used to generate augmented text for the RETSim training dataset , as described in Section 3.2 . SENTENCE-LEVEL AUGMENTATIONS • Deletion : – Random sentence deletion – Random sentence truncation • Insertion : 13 Preprint – Random prefix sentence – Random suffix sentence – Random sentence insertion – Repeat sentence • Substitution : – Lowercase/uppercase sentence – Random sentence substitution • Transposition : – Neighboring Swap WORD-LEVEL AUGMENTATIONS • Deletion : – Random word deletion • Insertion : – Random word insertion – Random word insertion per language • Substitution : – 3-gram frequency based word substitution – Random word substitution – Random word substitution per language – Repeat word • Transposition : – Neighboring Swap CHARACTER-LEVEL AUGMENTATIONS • Deletion : – Random character deletion • Substitution : – Case substitution – n-gram based substitution for n = 3 , 4 , 5 – QWERTY keyboard typo substitution – Homoglyphs substitution – Random ASCII substitution – Random character from language alphabet substitution – Random punctuation substitution – Random Unicode character substitution • Insertion : – Character repetition – n-grams based insertion for n = 3 , 4 , 5 – Random character from language alphabet insertion – Random punctuation insertion – Random Unicode character insertion • Transposition : – Neighboring swap A.3 DETAILED EVALUATION HYPERPARAMETERS Figures 6 and 7 contain information on deduplication thresholds values and hyperparameter settings for each algorithm benchmarked on the NEWS-COPY and CORE deduplication datasets . 14 Preprint Model / Algorithm Threshold Type Threshold Value Hyperparameters Multilingual USE Cosine Similarity Multilingual E5-Base Cosine Similarity SimHash MinHash ( Ours ) RETSimNear-Dup RETSimPartial-Dup Hamming Distance Jaccard Similarity Cosine Similarity Cosine Similarity 0.96 0.88 10 0.6 0.89 0.84 - - 64 bits , 5-grams ( character-level ) 10 hash functions , 2-grams ( word-level ) - - Figure 6 : Hyperparameter settings for NEWS-COPY dataset evaluation in Section 4.3 . Model / Algorithm Threshold Type Threshold Value Hyperparameters Cosine Similarity LaBSE Multilingual USE Cosine Similarity Multilingual E5-Base Cosine Similarity SimHash + LSH MinHash + LSH RETSimNear-Dup RETSimPartial-Dup Hamming Distance Jaccard Similarity Cosine Similarity Cosine Similarity 0.88 0.97 0.87 6 0.5 0.86 0.82 - - - 64 bits , 3-grams ( character-level ) 256 hash functions , 3-grams ( word-level ) - - Figure 7 : Hyperparameter settings for CORE Near-Duplicates dataset evaluation in Section 4.3 . A.3.1 DEDUPLICATION THRESHOLD IMPACT Figure 8 : Precision/Recall/F1 scores for different cosine distance deduplication thresholds for RETSimNear-Dup ( left ) and RETSimPartial-Dup ( right ) on the NEWS-COPY dataset . A.4 DETAILED W4NT3D BENCHMARK RESULTS Tables 11 and 12 show detailed performance results for RETSim and all baseline algorithms for every language split in the W4NT3D benchmark . 15 Preprint 9 4 9 . 0 4 8 9 . 0 6 8 9 . 0 1 0 7 . 0 3 9 5 . 0 4 1 8 . 0 1 7 9 . 0 1 9 9 . 0 o k 1 3 9 . 0 0 9 9 . 0 9 7 9 . 0 8 7 5 . 0 5 6 4 . 0 3 2 2 . 0 3 6 9 . 0 6 8 9 . 0 a j 9 2 9 . 0 7 3 9 . 0 3 3 9 . 0 7 3 9 . 0 3 1 5 . 0 5 7 5 . 0 4 4 9 . 0 2 8 9 . 0 5 1 9 . 0 8 2 9 . 0 2 4 9 . 0 4 2 9 . 0 3 3 5 . 0 3 6 5 . 0 0 5 9 . 0 6 7 9 . 0 8 9 8 . 0 5 8 8 . 0 9 8 8 . 0 6 7 8 . 0 0 3 5 . 0 2 1 5 . 0 0 5 9 . 0 2 6 9 . 0 2 1 9 . 0 9 8 8 . 0 1 0 9 . 0 3 0 9 . 0 7 5 5 . 0 6 5 5 . 0 4 3 9 . 0 2 6 9 . 0 7 2 9 . 0 8 4 6 . 0 4 6 9 . 0 5 3 4 . 0 3 6 5 . 0 6 0 6 . 0 1 4 9 . 0 5 7 9 . 0 7 3 9 . 0 1 4 8 . 0 8 5 9 . 0 9 8 5 . 0 1 5 6 . 0 7 9 6 . 0 5 4 9 . 0 9 8 9 . 0 0 3 9 . 0 8 3 9 . 0 8 4 9 . 0 4 3 9 . 0 9 1 5 . 0 6 5 5 . 0 7 4 9 . 0 3 8 9 . 0 6 2 9 . 0 3 1 9 . 0 3 4 9 . 0 4 1 9 . 0 2 3 6 . 0 8 6 5 . 0 6 5 9 . 0 1 8 9 . 0 2 1 9 . 0 0 7 8 . 0 9 2 9 . 0 6 5 3 . 0 8 6 5 . 0 5 9 5 . 0 6 4 9 . 0 1 7 9 . 0 3 2 9 . 0 0 3 9 . 0 1 5 9 . 0 6 2 9 . 0 5 6 6 . 0 5 8 5 . 0 3 5 9 . 0 5 8 9 . 0 8 8 8 . 0 3 0 9 . 0 6 9 8 . 0 1 1 9 . 0 6 9 4 . 0 4 0 5 . 0 8 3 9 . 0 6 7 9 . 0 4 4 9 . 0 8 5 9 . 0 9 5 9 . 0 1 6 9 . 0 1 9 5 . 0 1 9 5 . 0 4 5 9 . 0 7 8 9 . 0 8 1 9 . 0 4 7 8 . 0 4 3 9 . 0 9 8 5 . 0 7 5 5 . 0 4 7 5 . 0 5 3 9 . 0 4 7 9 . 0 1 3 9 . 0 2 4 9 . 0 4 4 9 . 0 2 3 9 . 0 1 6 5 . 0 8 5 5 . 0 9 4 9 . 0 8 7 9 . 0 8 3 9 . 0 7 2 9 . 0 6 5 9 . 0 3 4 9 . 0 2 9 5 . 0 8 9 5 . 0 3 5 9 . 0 8 8 9 . 0 2 1 9 . 0 0 0 9 . 0 7 2 9 . 0 1 1 9 . 0 9 7 5 . 0 1 8 5 . 0 0 5 9 . 0 3 7 9 . 0 7 9 8 . 0 3 8 8 . 0 0 9 8 . 0 2 0 9 . 0 5 8 4 . 0 8 0 5 . 0 4 2 9 . 0 8 6 9 . 0 5 1 9 . 0 9 0 9 . 0 9 9 8 . 0 1 5 8 . 0 0 1 5 . 0 6 0 5 . 0 2 4 9 . 0 6 7 9 . 0 5 1 9 . 0 5 1 9 . 0 6 3 9 . 0 7 9 4 . 0 8 5 5 . 0 3 3 6 . 0 8 2 9 . 0 1 7 9 . 0 e s a B - 5 E l a u g n i l i t l u M E S U l a u g n i l i t l u M ) o k c e G ( 2 M L a P E S B a L p u D - l a i t r a P m S T E R i p u D i - r a e N m S T E R h s a H m S i h s a H n i M t i d i u h r h i h e h r f fi a f t e s e n e l e e d a d s c a c g b r a m h t i r o g l A / l e d o M . ) 1 t r a p ( k r a m h c n e b D 3 T N 4 W e h t n o s m h t i r o g l a d n a s l e d o m g n i d d e b m e s u o i r a v r o f e c n a m r o f r e p 1 @ l l a c e R e g a u g n a l - r e p l l u F : 1 1 e l b a T w t - h z n c - h z i v k u r t l t h t v s r s l s k s u r o r t p l p o n l n s m v l t l m h t i r o g l A / l e d o M 16 8 1 9 . 0 5 8 9 . 0 8 7 9 . 0 9 0 6 . 0 5 1 3 . 0 0 0 2 . 0 7 5 9 . 0 8 6 9 . 0 7 1 9 . 0 6 8 9 . 0 0 8 9 . 0 3 2 6 . 0 6 7 2 . 0 2 7 1 . 0 6 4 9 . 0 1 7 9 . 0 2 3 9 . 0 0 1 9 . 0 5 4 9 . 0 3 6 8 . 0 9 0 6 . 0 1 8 5 . 0 1 5 9 . 0 5 8 9 . 0 9 9 8 . 0 3 9 8 . 0 2 7 8 . 0 2 2 8 . 0 7 1 5 . 0 0 2 5 . 0 1 4 9 . 0 7 5 9 . 0 0 3 9 . 0 0 4 9 . 0 1 5 9 . 0 2 0 9 . 0 6 0 6 . 0 3 8 5 . 0 4 5 9 . 0 8 7 9 . 0 7 4 9 . 0 9 4 9 . 0 9 6 9 . 0 4 4 9 . 0 7 0 5 . 0 0 7 5 . 0 1 6 9 . 0 9 8 9 . 0 2 8 8 . 0 8 8 8 . 0 1 2 9 . 0 1 7 5 . 0 9 6 6 . 0 6 1 4 . 0 1 4 9 . 0 6 4 9 . 0 6 0 9 . 0 9 9 8 . 0 5 2 9 . 0 9 1 9 . 0 2 5 5 . 0 0 2 5 . 0 3 5 9 . 0 9 6 9 . 0 0 3 9 . 0 6 0 9 . 0 6 9 8 . 0 6 5 8 . 0 3 5 5 . 0 5 2 5 . 0 3 6 9 . 0 9 7 9 . 0 1 3 9 . 0 8 0 9 . 0 0 4 9 . 0 6 1 9 . 0 0 8 5 . 0 0 7 5 . 0 7 4 9 . 0 7 7 9 . 0 2 2 9 . 0 1 0 9 . 0 1 2 9 . 0 4 2 9 . 0 5 7 5 . 0 3 7 5 . 0 1 6 9 . 0 1 7 9 . 0 8 1 9 . 0 0 1 9 . 0 1 1 9 . 0 1 5 8 . 0 4 5 5 . 0 3 2 5 . 0 6 4 9 . 0 0 7 9 . 0 9 0 9 . 0 9 6 8 . 0 6 3 9 . 0 3 9 8 . 0 4 1 5 . 0 0 2 5 . 0 8 4 9 . 0 7 7 9 . 0 4 4 9 . 0 2 5 9 . 0 5 5 9 . 0 0 5 9 . 0 8 4 5 . 0 3 7 5 . 0 4 5 9 . 0 5 8 9 . 0 8 2 9 . 0 1 3 9 . 0 8 2 9 . 0 3 1 9 . 0 6 8 5 . 0 3 6 5 . 0 3 5 9 . 0 9 7 9 . 0 8 2 9 . 0 1 2 9 . 0 4 4 9 . 0 0 3 9 . 0 7 7 5 . 0 0 6 5 . 0 8 5 9 . 0 1 8 9 . 0 1 3 9 . 0 6 3 9 . 0 4 3 9 . 0 1 3 9 . 0 7 2 5 . 0 0 4 5 . 0 0 5 9 . 0 9 7 9 . 0 9 1 9 . 0 2 3 9 . 0 9 4 9 . 0 8 2 9 . 0 3 3 5 . 0 1 1 5 . 0 1 6 9 . 0 0 8 9 . 0 2 2 9 . 0 9 1 9 . 0 5 3 9 . 0 7 0 9 . 0 4 2 6 . 0 9 7 5 . 0 5 4 9 . 0 3 8 9 . 0 9 1 9 . 0 2 0 9 . 0 1 4 9 . 0 9 0 9 . 0 9 0 6 . 0 8 6 5 . 0 7 5 9 . 0 0 8 9 . 0 e s a B - 5 E l a u g n i l i t l u M E S U l a u g n i l i t l u M ) o k c e G ( 2 M L a P E S B a L p u D - l a i t r a P m S T E R i h s a H m S i h s a H n i M p u D i - r a e N m S T E R . ) 2 t r a p ( k r a m h c n e b D 3 T N 4 W e h t n o s m h t i r o g l a d n a s l e d o m g n i d d e b m e s u o i r a v r o f e c n a m r o f r e p 1 @ l l a c e R e g a u g n a l - r e p l l u F : 2 1 e l b a T Preprint A.5 ADDITIONAL ABLATION STUDIES This section includes ablation studies on additional hyperparameters for the RETSim model , includ- ing the loss function , pooling type , and model capacity . α β 2 2 2 2 4 4 4 4 20 20 40 40 20 20 40 40 λ 0.5 1 0.5 1 0.5 1 0.5 1 Recall @ 1 0.982 0.948 0.984 0.919 0.982 0.947 0.986 0.923 Table 13 : Ablation study on Multi-Similarity Loss hyperparameters for RETSim training . Bold indicates the hyperparameter setting selected for the final model . # Blocks Hidden Dim Recall @ 1 2 2 2 2 3 3 3 3 4 4 4 4 64 128 256 512 64 128 256 512 64 128 256 512 0.965 0.980 0.986 0.986 0.962 0.980 0.984 0.987 0.966 0.980 0.985 0.986 Table 14 : Ablation study for RETSim model capacity and size ( number of GAU blocks and hidden dimension for the blocks ) . Bold indicates the hyperparameter setting selected for the final model . Pooling Type Recall @ 1 Average Pooling Max Pooling Generalized Mean Pooling 0.985 0.983 0.986 Table 15 : Ablation study on pooling type for the RETSim model . Bold indicates the hyperparameter setting selected for the final model . A.6 SELECTED EXAMPLES FROM NEWS-COPY DATASET In this section , we randomly selected a set of false positives and false negatives for RETSim on the NEWS-COPY deduplication dataset to provide further insight into the results . 17 Preprint Text 1 chauffeur , a policeman and a passing jour- nalist who tried to intervene . Beaton and the policeman were reported in serious condition . The 23-year-old princess and her husband of five months , Capt . Mark Phillips , were not hurt . But police experts said the holes left by one of the bullets fired into the car indicated it passed between them , missing them by inch- es . A police informant said it was believed 11 shots were fired by the assailant . Experts were studying two revolvers found at the scene . They said fi ... By United Press tnfernational Ay SSAST OR BE FRE NG SG The federal government has proposed new methods of eoustructing federal buildings in a move to save ad- ditional en- ergy and suggested ils elfort could be adapted to all new buildings , Washington , Jan. 27 . — ( P ) —Im- mediate removal of John F. J. Her- bert , as prohibition administrator for Montana and Idaho , was de- manded in the senate today by Sen- ators Borah , Idaho , and Whe´eeler , Montana , on the ground of charges placed before them by department of justice investigators . Wheeler accompanied his demand ( Continued on Page 2 ) By RAYMOND CLAPPEA ( Dnited Presa Stal Correspandoayy London , Jai , 38— ( UP—-The Am ‘ erlcnn delegation to the navat confer ence today won ls demand for pre- sentation : of the cnse of suxiliary warships limitation first at tho noxt plenary session Thuvaday , ‘ Tho chlet delegates , mec- tittg at St. James palace , also decided that tho plenary sesslon would discuss the Main con- ference questions in alpha betical order of ihe countriea pro- posing . Press ta be Admitted The American delegation woo a second vic- tory whe ... Text 2 ‘ LONDON ( AP ) — Ian Ball , a 26-year- old unemployed Englishman , was brought into court today and charged with attempted mur- der during an at- tempt to kidnap Princess Anne from her car in the heart of London Wed- nesday night . Ball , lean-faced and bearded , stood stiffly in the dock at the Bow Street Magistrate ’ s court , handcuffed to two detectives . He spoke only once during his 60- second appearance , saying iha London accent : “ I want to apply for legal aid. ” The court or- dered him held for another hearing on Ma ... Hy United Press International The federal government has Proposed new methods of constructing federal buildings in a move lo save addilional energy and suggested ils effort could be adapted to all new buildings , Arthur F , Sampson , General Services Administration ad- ministrater , said new features for such construction would include the collection of rain waler for cooling and irriga- tion , solar energy collectors and the covering of exterior walls with earth . “ Whal we are saying is that these design criteri ... — Washington , Jan. 27 1 AP ) .—Immiedl- aie mmoval of John F. Herbert as pro- — hi- bition administrator for Montana and ‘ Idaho was demanded m the Seuate to- ‘ day by Sen- ators Borah . idaho , and Waeeler , Montana . on the ground of charges placed before them by Depart- meat of Justice investigators . Wheeler accompanied his demand nith a declaration that prohibition en- foreemen : had brukea down . He blamed the “ politicians ” and called upon the Law Enforcement Commussion to sum- mon members of the Republican Na- tona ... London , Jan. 24 , W.P—The Amer- jean dele- gation fo the naval cen- ference teday won its demand for presentation of the case of auxil- jary warships linsitation flrst at the next ple- trary session ‘ Vhursday , The chief delegates , meeting at Si , James Pelace , also decided that the plenary session would discuss the main confeyence questions in alphabetical order af the cauntries proposing . The American del- egation won a second victory when it was decided to udmil certain representatives of the press at fie plenary ... Table 16 : Example false negatives for RETSim on the NEWS-COPY dataset ( pairs of texts not detected as near-duplicates by RETSim but labeled as near-duplicates in the original dataset ) . Ex- amples are randomly selected and truncated at 512 characters for display . 18 Preprint Text 1 BOZEMAN , Mont . ( AP ) — Chet Huntley , whose resonant voice and rough-hewn face be- came familiar to millions on the nightly television news , died Wednesday in his moun- tain resort home . He was 62 . He underwent surgery for lung cancer in January but had remained activesuntil recent weeks . He died at 2:20 a.m , according to his widow , Tippy Hunt.cy . Huntiey was teamed for 14 years with David Brinkley on NBC ’ s Huntley- Brinkley Re- port . He quit in 1970 and re- turned to his native Montana to develop the $ 20-millio ... By THE ASSOCIATED PRESS Some Amer- icans are paying up to 50 per cent more per month for electricity this year than they did last , an Associ- ated Press survey shows . Con- sumers are beginning to organize to fight the rate hikes . A spot check of monthly elec- tric bills this year and last showed that most in- creases have been about $ 1 or $ 2 , gen- erally about 10 per cent , with the highest reported boost com- ing in Jacksonville , Fia. , where the average tab went from $ 17.90 last year to $ 27.70 this year . Utility ... BOZEMAN , Mont . ( AP ) — Vice President Gerald R. Ford says the world will miss the “ ‘ unique abilities ” of former television news anchorman Chet Huntley . Huntley , 62 , died at his home Wednesday after a long bout with lung cancer . Family ‘ spokesmen said a memorial service would be conducted for Huntley Sunday at the Big Sky of Montana ’ resort and recreation area south of Bozeman . Huntley was chairman of the Big Sky board of directors . Another memorial service is scheduled Tuesday in the New York studios of the ... WASHINGTON ( AP ) — The House has passed legislation raising the minimum wage from $ 1.60 an hour to $ 2 this year for most workers covered and to $ 2.30 for all by 1978 . The bill , approved Wednesday 375 to 37 , also would increase by 7 million to 56.5 million the number of workers covered by the mini- mum wage laws . The bill is a modified ver- sion of one President Nixon vetoed last year . However , he is expected to sign this one if it is finally approved after ad- justment with a similar Senate passed measure , altho ... Text 2 BOZEMAN , Mont . ( AP ) - Chet Huntley , whose resonant voice and rough-hewn face became familiar to millions on the nightly television news , died Wednesday in his moun- tain resort home . He was 62 . He underwent surgery for lung cancer in January but had remained active until recent weeks . He died at 2:20 a.m. , according to his widow , Tippy Huntley . Huntley was teamed for 14 years with David Brinkley on NBC ’ s Huntley- Brinkley Report . He quit in 1970 and returned to his native Montana to develop the $ 20 mil- lion Bi ... By Louise Cook Acenciaiod Prece Writer Same Americans are paying up io 20 per cent more per month far electricity this year ihan they did last , an -Associ- Press survey shows . onsumers are beginning to ze to fight the rate hikes , A spot check of monthly elec- tre hills this year and Jast showed that most increases ve been about $ 1 or $ 2 , gen- erally about 10 per cent , with the highest reported boost com- ing in Jacksonville , Fla. , where the average tab went from $ 17.90 last year to $ 27.70 this year ... BOZEMAN , Mont . ( AP ) — Vice President Gerald R. Ford says the world will miss the “ unique abilities ” of former television news anchorman Chet Huntley . Huntley , 62 , died at his home Wednesday after a long bout with lung cancer . Family spokesmen said a me- morial service would be con- ducted for Hunt- ley Sunday at the Big- Sky of Montana resort and recreation area south of Bozeman . Hunt- ley was chair- man of the Big Sky board of directors . Another memorial service is sched- uled Tuesday in the New York studios of ... WASHINGTON ( AP ) — The House has passed legislation raising the minimum wage from $ 1.60 an hour to $ 2 this year for most workers covered and to $ 2.30 for all by 1978 . The bill , approved Wednes- day 375 to 37 , also would in- crease by 7 million to 56.5 mil- lion the number of workers cov- ered by the minimum wage laws . The bill is a modified version of one President Nixon vetoed last year . However , he is ex- ted to sign this one if it is inally approved after adjust- ment with a similar Senate- passed measu ... Table 17 : Example false positives for RETSim on the NEWS-COPY dataset ( pairs of texts detected as near-duplicates by RETSim but not labeled as near-duplicates in the original dataset ) . Examples are randomly selected and truncated at 512 characters for display . 19","['l', 'c', 'c', 'r', 'preprint', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'tanay', 'vakharia1', 'paper', 'introduce', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'lightweight', 'multilingual', 'deep', 'learning', 'model', 'train', 'produce', 'robust', 'metric', 'embedding', 'nearduplicate', 'text', 'retrieval', 'clustering', 'dataset', 'deduplication', 'task', 'demonstrate', 'retsim', 'significantly', 'robust', 'accurate', 'minhash', 'neural', 'text', 'embedding', 'achieve', 'new', 'stateoftheart', 'mance', 'dataset', 'deduplication', 'adversarial', 'text', 'retrieval', 'benchmark', 'spam', 'clustering', 'task', 'also', 'introduce', 'benchmark', 'wiki40b', 'sarial', 'neart3xt', 'dataset', 'evaluate', 'multilingual', 'nearduplicate', 'text', 'retrieval', 'capability', 'adversarial', 'setting', 'retsim', 'benchmark', 'opensource', 'mit', 'license', 'githubcomgoogleunisim', 'introduction', 'robust', 'nearduplicate', 'text', 'detection', 'essential', 'component', 'many', 'task', 'include', 'retriev', 'ing', 'document', 'detect', 'plagiarism', 'sun', 'block', 'adversarial', 'spam', 'cam', 'paign', 'ahme', 'user', 'come', 'expect', 'system', 'return', 'accurate', 'result', 'query', 'exhibit', 'rate', 'hagen', 'furthermore', 'ciently', 'deduplicate', 'text', 'dataset', 'critical', 'training', 'stateoftheart', 'large', 'language', 'model', 'kandpal', 'decade', 'minhashbase', 'broder', 'localitysensitive', 'hashing', 'lsh', 'prevalent', 'use', 'nearduplicate', 'detection', 'simplicity', 'robust', 'ness', 'speed', 'example', 'vast', 'majority', 'dataset', 'deduplication', 'effort', 'still', 'rely', 'kocetkov', 'however', 'lshbase', 'technique', 'minhash', 'downside', 'chief', 'parametersensitive', 'require', 'heavy', 'tuning', 'additionally', 'minhash', 'lack', 'resilience', 'typo', 'reliance', 'ngram', 'lead', 'poor', 'performance', 'noisy', 'datum', 'vulnerability', 'hashbuste', 'attack', 'issac', 'hand', 'deep', 'learning', 'model', 'dominant', 'way', 'perform', 'vectorbase', 'semantic', 'text', 'retrieval', 'far', 'neural', 'embedding', 'able', 'consistently', 'outperform', 'minhash', 'robust', 'nearduplicate', 'detection', 'silcock', 'mostly', 'due', 'focus', 'improve', 'semantic', 'capability', 'lead', 'model', 'large', 'run', 'extremely', 'quickly', 'use', 'level', 'tokenization', 'resilient', 'typo', 'adversarial', 'attack', 'fill', 'gap', 'introduce', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'lightweight', 'mul', 'tilingual', 'deep', 'learning', 'model', 'train', 'specifically', 'produce', 'robust', 'neural', 'embedding', 'specialize', 'nearduplicate', 'detection', 'combine', 'stateoftheart', 'retvec', 'text', 'vectorizer', 'modern', 'transformer', 'block', 'large', 'typoaugmente', 'training', 'corpus', 'metric', 'learn', 'ing', 'training', 'regime', 'retsim', 'able', 'achieve', 'new', 'stateoftheart', 'performance', 'nearduplicate', 'detection', 'benchmark', 'section', 'dataset', 'deduplication', 'task', 'section', 'spam', 'clustering', 'application', 'section', 'furthermore', 'dataset', 'benchmark', 'exist', 'deduplication', 'nearduplicate', 'text', 'retrieval', 'none', 'focus', 'systematically', 'evaluate', 'nearduplicate', 'retrieval', 'mance', 'presence', 'word', 'manipulation', 'sentence', 'paragraphlevel', 'work', 'author', 'internship', 'preprint', 'tion', 'address', 'need', 'additionally', 'introduce', 'benchmark', 'wiki40b', 'sarial', 'neart3xt', 'dataset', 'enable', 'evaluation', 'algorithm', 'adversarial', 'nearduplicate', 'text', 'retrieval', 'multilingual', 'setting', 'report', 'performance', 'retsim', 'minhash', 'pop', 'ular', 'neural', 'embedding', 'universal', 'sentence', 'encoder', 'cer', 'labse', 'new', 'benchmark', 'section', 'highlight', 'uneven', 'performance', 'guage', 'type', 'adversarial', 'manipulation', 'retsim', 'model', 'benchmark', 'opensource', 'https', 'githubcomgoogleunisim', 'mit', 'license', 'relate', 'work', 'nearduplicate', 'detection', 'identify', 'noisy', 'nearduplicate', 'document', 'large', 'corpus', 'fun', 'damental', 'task', 'wide', 'range', 'application', 'detect', 'plagiarism', 'finding', 'reproduce', 'content', 'literature', 'news', 'article', 'gyawali', 'silcock', 'deduplicat', 'ing', 'training', 'dataset', 'language', 'model', 'previous', 'research', 'show', 'duplicate', 'training', 'dataset', 'lead', 'inefficient', 'training', 'privacy', 'concern', 'large', 'language', 'model', 'llm', 'model', 'memorize', 'regenerate', 'duplicate', 'training', 'sequence', 'much', 'high', 'frequency', 'kandpal', 'semantic', 'text', 'similarity', 'task', 'identify', 'textual', 'nearduplicate', 'predominate', 'nonneural', 'ngrambase', 'algorithm', 'minhash', 'broder', 'widely', 'use', 'technique', 'deduplicate', 'large', 'training', 'corpus', 'kocetkov', 'minhash', 'technique', 'estimate', 'jaccard', 'similarity', 'set', 'algorithm', 'minhash', 'simhash', 'charikar', 'combine', 'localitysensitive', 'hashing', 'lsh', 'technique', 'fast', 'approximate', 'near', 'neighbor', 'search', 'datum', 'cluster', 'allow', 'scale', 'deduplicate', 'corpus', 'contain', 'terabyte', 'datum', 'c4', 'stack', 'kocetkov', 'however', 'ngram', 'shinglingbase', 'technique', 'typically', 'require', 'text', 'parse', 'standardized', 'form', 'eg', 'lowercase', 'strip', 'punctuation', 'make', 'susceptible', 'typo', 'adversarial', 'attack', 'pose', 'challenge', 'attempt', 'differentiate', 'dissimilar', 'document', 'nearduplicate', 'document', 'adversarial', 'augmentation', 'semantic', 'text', 'similarity', 'task', 'compute', 'semantic', 'similarity', 'text', 'closely', 'late', 'nearduplicate', 'detection', 'semantic', 'text', 'similarity', 'refer', 'assessment', 'semantic', 'relatedness', 'piece', 'text', 'base', 'meaning', 'rather', 'syntactic', 'structure', 'case', 'nearduplicate', 'detection', 'recently', 'transformerbase', 'language', 'model', 'universal', 'sentence', 'encoder', 'labse', 'llmbase', 'embedding', 'anil', 'embed', 'text', 'highdimensional', 'embed', 'vector', 'successfully', 'use', 'retrieve', 'semanticallyrelate', 'document', 'use', 'cosine', 'similarity', 'modern', 'text', 'retrieval', 'sys', 'tem', 'combine', 'embedding', 'approximate', 'near', 'search', 'efficiently', 'retrieve', 'document', 'match', 'user', 'query', 'however', 'language', 'model', 'show', 'vulnerable', 'adversarial', 'attack', 'naturally', 'occur', 'typo', 'gao', 'furthermore', 'language', 'model', 'typically', 'large', 'costly', 'run', 'even', 'hardware', 'acceleration', 'make', 'unsuited', 'largescale', 'dataset', 'deduplication', 'identify', 'nearduplicate', 'presence', 'typo', 'adversarial', 'text', 'manipulation', 'metric', 'learn', 'metric', 'learning', 'aim', 'learn', 'embed', 'space', 'similar', 'item', 'small', 'distance', 'embedding', 'dissimilar', 'item', 'far', 'away', 'many', 'stateof', 'theart', 'embedding', 'use', 'metric', 'learning', 'unsupervised', 'training', 'finetuning', 'include', 'sentence', 'reimer', 'gurevych', 'retvec', 'resilient', 'multilingual', 'embedding', 'text', 'vectorizer', 'train', 'robust', 'various', 'form', 'characterlevel', 'typo', 'adversarial', 'attack', 'extend', 'retvec', 'training', 'regime', 'full', 'text', 'document', 'retsim', 'use', 'multisimilarity', 'loss', 'pairbase', 'metric', 'learning', 'typoladen', 'nearduplicate', 'version', 'text', 'train', 'close', 'embed', 'space', 'text', 'push', 'far', 'away', 'multisimilarity', 'loss', 'base', 'general', 'weighting', 'framework', 'pairbase', 'loss', 'achieve', 'stateoftheart', 'performance', 'outperform', 'alternative', 'triplet', 'loss', 'schroff', 'preprint', 'figure', 'retsim', 'model', 'architecture', 'diagram', 'retsim', 'work', 'arbitrary', 'length', 'text', 'split', 'ting', 'text', 'chunk', 'character', 'vectorization', 'phase', 'encode', 'use', 'retvec', 'character', 'vectorizer', 'retsim', 'model', 'embed', 'chunk', 'text', 'partial', 'embedding', 'combine', 'produce', 'global', 'embed', 'retsim', 'architecture', 'retsim', 'model', 'compose', 'main', 'component', 'depict', 'figure', 'characterlevel', 'vectorizer', 'split', 'input', 'text', 'chunk', 'character', 'use', 'retvec', 'chararcter', 'encoder', 'encode', 'chunk', 'result', 'batch', 'dense', 'input', 'retvec', 'character', 'vectorizer', 'encode', 'unicode', 'character', 'compact', '24bit', 'binary', 'representation', 'base', 'integer', 'codepoint', 'value', 'allow', 'vectorizer', 'encode', 'valid', 'unicode', 'character', 'support', 'language', 'furthermore', 'characterlevel', 'vectorizer', 'show', 'resilient', 'typo', 'adversarial', 'attack', 'small', 'transformer', 'model', 'use', 'compute', 'embedding', 'chunk', 'input', 'text', 'retsimpartialdup', 'use', 'embedding', 'directly', 'find', 'document', 'match', 'chunk', 'text', 'architecturally', 'model', 'consist', 'gate', 'attention', 'unit', 'gau', 'block', 'dense', 'et', 'follow', 'generalizedmean', 'pooling', 'layer', 'projection', 'layer', 'project', 'embed', 'dimension', 'l2', 'normalization', 'layer', 'model', '536k', 'parameter', 'order', 'magnitude', 'small', 'neural', 'embedding', 'table', 'l2normalization', 'allow', 'embedding', 'compare', 'use', 'cosine', 'similarity', 'discuss', 'impact', 'key', 'architecture', 'design', 'choice', 'section', 'hyperparameter', 'detail', 'provide', 'a11', 'additional', 'ablation', 'result', 'appendix', 'a5', 'embed', 'averaging', 'module', 'use', 'combine', 'partial', 'text', 'embedding', 'fulltext', 'embed', 'use', 'global', 'nearduplicate', 'match', 'retsimneardup', 'average', 'chunk', 'embedding', 'produce', 'global', 'embedding', 'standard', 'technique', 'use', 'many', 'model', 'cer', 'support', 'infinite', 'length', 'input', 'costefficient', 'manner', 'experiment', 'aggregation', 'technique', 'produce', 'accurate', 'global', 'embedding', 'include', 'train', 'deep', 'averaging', 'network', 'iyyer', 'improve', 'performance', 'result', 'high', 'computation', 'cost', 'retsimneardup', 'retsimpartialdup', 'compute', 'single', 'forward', 'pass', 'make', 'computationally', 'efficient', 'output', 'type', 'embedding', 'different', 'application', 'retsimneardup', 'bettersuite', 'fulltext', 'matching', 'retrieval', 'section', 'retsimpartialdup', 'use', 'find', 'partial', 'text', 'match', 'nearduplicate', 'content', 'appear', 'part', 'document', 'section', 'model', 'training', 'dataset', 'use', 'multilingual', 'c4', 'dataset', 'mc4', 'raw', 'text', 'datum', 'follow', 'xue', 'use', 'language', 'sample', 'exponent', 'balance', 'sampling', 'low', 'highresource', 'language', 'use', 'text', 'contain', 'least', 'character', 'randomly', 'select', 'sentence', 'roughly', 'character', 'text', 'chunk', 'example', 'input', 'text', 'vector', 'chunk', 'vector', 'chunk', 'vectorized', 'text', 'numchunk', 'retsim', 'model', 'retsim', 'partialdup', 'numchunk', 'retsim', 'neardup', 'r', 'e', 'c', 'r', 'h', 'c', 'e', 'r', 'g', 'n', 'l', 'p', 'l', 'r', 'p', 'e', 'l', 'r', 'r', 'e', 'g', 'e', 'preprint', 'training', 'dataset', 'generate', 'pair', 'augment', 'example', 'apply', 'level', 'augmentation', 'example', 'text', 'chunk', 'order', 'sentencelevel', 'wordlevel', 'characterlevel', 'level', 'randomly', 'select', 'augmentation', 'apply', 'follow', 'category', 'insertion', 'deletion', 'substitution', 'transposition', 'randomly', 'apply', 'sentencelevel', 'mentation', 'combine', 'character', 'wordlevel', 'augmentation', 'empirically', 'find', 'increase', 'percentage', 'augmentation', 'point', 'cause', 'retsim', 'performance', 'degrade', 'full', 'list', 'augmentation', 'use', 'find', 'a2', 'training', 'procedure', 'train', 'retsim', 'use', 'multisimilarity', 'loss', 'β', 'ϵ', 'hypertune', 'parameter', 'result', 'show', 'appendix', 'train', 'step', 'batch', 'size', 'similarity', 'loss', 'train', 'model', 'embed', 'augment', 'version', 'text', 'close', 'embed', 'space', 'dissimilar', 'text', 'push', 'far', 'apart', 'use', 'optimizer', 'learning', 'rate', 'cosine', 'decay', 'detailed', 'training', 'hyperparameter', 'report', 'a12', 'evaluation', 'modelalgorithm', 'type', 'embedhash', 'size', 'model', 'parameter', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'neural', 'neural', 'neural', 'neural', 'hash', 'hash', 'neural', 'b', 'bit', 'hash', 'table', 'embed', 'model', 'hash', 'algorithm', 'benchmarke', 'paper', 'model', 'algorithm', 'evaluate', 'benchmark', 'retsim', 'multilingual', 'semantic', 'text', 'embedding', 'well', 'popular', 'gram', 'base', 'algorithm', 'primarily', 'use', 'nearduplicate', 'text', 'detection', 'table', 'baseline', 'text', 'embedding', 'include', 'multilingual', 'universal', 'sentence', 'encoder', 'labse', 'multilingual', 'palm', 'gecko', 'embedding', 'anil', 'text', 'embedding', 'l2normalize', 'compare', 'use', 'cosine', 'similarity', 'use', 'exact', 'search', 'index', 'retrieve', 'near', 'neighbor', 'vector', 'index', 'experiment', 'section', 'nonneural', 'nearduplicate', 'detection', 'cluster', 'algorithm', 'select', 'popular', 'algorithm', 'minhash', 'broder', 'simhash', 'charikar', 'minhash', 'use', 'datasketch', 'minhashlsh', 'library', 'follow', 'common', 'practice', 'literature', 'silcock', 'use', 'hash', 'function', 'minhash', 'otherwise', 'specify', 'use', 'wordlevel', 'ngram', 'select', 'good', 'value', 'n', 'simhash', 'use', 'bit', 'simhash', 'conduct', 'shingle', 'character', 'level', 'shingle', 'size', 'select', 'n', 'nearduplicate', 'detection', 'benchmark', 'newscopy', 'core', 'duplicate', 'dataset', 'tune', 'optimal', 'deduplication', 'threshold', 'base', 'cosine', 'similarity', 'neuralbased', 'embedding', 'jaccard', 'similarity', 'minhash', 'detailed', 'hyperparameter', 'setting', 'retsim', 'baseline', 'algorithm', 'use', 'evaluation', 'find', 'w4nt3d', 'wiki40b', 'neart3xt', 'dataset', 'evaluation', 'dataset', 'description', 'vast', 'majority', 'text', 'retrieval', 'benchmark', 'focus', 'evaluate', 'semantic', 'performance', 'good', 'knowledge', 'multilingual', 'benchmark', 'sys', 'tematically', 'measure', 'adversarial', 'robustness', 'nearduplicate', 'text', 'retrieval', 'attempt', 'fill', 'gap', 'create', 'publish', 'benchmark', 'wiki40b', 'dataset', 'contain', 'pair', 'syntactically', 'similar', 'text', 'evaluate', 'nearduplicate', 'text', 'trieval', 'presence', 'various', 'form', 'text', 'manipulation', 'typo', 'w4nt3d', 'base', 'wiki40b', 'dataset', 'guo', 'dataset', 'split', 'query', 'exam', 'ple', 'target', 'example', 'query', 'example', 'syntheticallymodifie', 'nearduplicate', 'version', 'preprint', 'target', 'example', 'eg', 'typo', 'language', 'split', 'wiki40b', 'randomly', 'select', 'text', 'length', 'target', 'string', 'uniformly', 'select', 'character', 'order', 'test', 'performance', 'short', 'long', 'text', 'construct', 'query', 'text', 'corre', 'sponde', 'target', 'text', 'randomly', 'apply', 'word', 'character', 'augmentation', 'sentence', 'paragraph', 'augmentation', 'augmentation', 'uniformly', 'select', 'insert', 'delete', 'substitute', 'swap', 'operation', 'use', 'main', 'metric', 'follow', 'setup', 'commonly', 'find', 'semantic', 'text', 'retrieval', 'benchmark', 'modelalgorithm', 'arabic', 'chinese', 'english', 'german', 'french', 'spanish', 'japanese', 'korean', 'russian', 'avg', 'lang', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsimpartialdup', 'retsimneardup', 'table', 'perlanguage', 'retrieval', 'performance', 'various', 'embed', 'model', 'algorithm', 'benchmark', 'result', 'select', 'language', 'report', 'average', 'recall', 'language', 'full', 'result', 'language', 'report', 'multilingual', 'performance', 'overall', 'retsimneardup', 'achieve', 'average', 'recall', 'language', 'benchmark', 'table', 'retsimpartialdup', 'second', 'good', 'recall', 'multilingual', 'bestperforme', 'baseline', 'third', 'aver', 'age', 'recall', 'expect', 'retsimneardup', 'outperform', 'retsimpartialdup', 'benchmark', 'require', 'find', 'nearduplicate', 'find', 'simi', 'lar', 'text', 'retsimpartialdup', 'optimize', 'find', 'similar', 'chunk', 'text', 'corpus', 'always', 'similar', 'text', 'overall', 'similarly', 'hypothesize', 'minhash', 'simhash', 'form', 'poorly', 'benchmark', 'lack', 'ability', 'distinguish', 'similar', 'text', 'nearduplicate', 'detect', 'embeddingbase', 'model', 'cosine', 'similarity', 'offer', 'finegrained', 'measure', 'similarity', 'retsimneardup', 'outperform', 'baseline', 'algorithm', 'language', 'chinese', 'language', 'theorize', 'semantic', 'embedding', 'slight', 'edge', 'performance', 'significantly', 'large', 'model', 'size', 'large', 'retsim', 'show', 'ble', 'allow', 'well', 'representation', 'language', 'large', 'character', 'set', 'furthermore', 'level', 'tokenizer', 'use', 'baseline', 'embedding', 'often', 'treat', 'character', 'chinese', 'japanese', 'individual', 'token', 'offer', 'high', 'resilience', 'figure', 'recall', 'performance', 'benchmark', 'break', 'augmentation', 'type', 'result', 'average', 'language', 'split', 'w4nt3d', 'adversarial', 'resilience', 'delve', 'deeply', 'impact', 'various', 'type', 'text', 'manipulation', 'veal', 'retsimneardup', 'retsimpartialdup', 'perform', 'almost', 'equally', 'regardless', 'type', 'augmentation', 'apply', 'figure', 'semantic', 'text', 'embedding', 'perform', 'well', 'paragraph', 'sentence', 'wordlevel', 'manipulation', 'expect', 'exhibit', 'significantly', 'weak', 'performance', 'characterlevel', 'minhash', 'simhash', 'struggle', 'wordlevel', 'augmentation', 'deeplearne', 'base', 'embedding', 'collapse', 'characterlevel', 'typo', 'introduce', 'l', 'c', 'e', 'r', 'paragraph', 'sentence', 'word', 'character', 'augmentation', 'level', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'partialdup', 'retsim', 'neardup', 'preprint', 'tribute', 'retsim', 'resilience', 'adversarial', 'manipulation', 'retvec', 'character', 'encoder', 'well', 'use', 'deep', 'metric', 'learning', 'train', 'robust', 'embedding', 'figure', 'report', 'recall', 'performance', 'algorithm', 'amount', 'augmentation', 'crease', 'algorithm', 'perform', 'perfectly', 'augmentation', 'apply', 'exact', 'matching', 'percentage', 'augmentation', 'increase', 'ngram', 'base', 'approach', 'exhibit', 'steep', 'drop', 'mance', 'semantic', 'text', 'embedding', 'able', 'sustain', 'large', 'degree', 'augmentation', 'retrieval', 'capability', 'start', 'degrade', 'retsimneardup', 'robust', 'noticeable', 'drop', 'performance', 'around', 'augmentation', 'make', 'retsim', 'effective', 'approach', 'cluster', 'deduplicate', 'text', 'adversarial', 'setting', 'figure', 'recall', 'performance', 'benchmark', 'vary', 'max', 'target', 'length', 'figure', 'recall', 'performance', 'benchmark', 'amount', 'augmenta', 'tion', 'apply', 'query', 'text', 'increase', 'text', 'length', 'impact', 'performance', 'figure', 'report', 'recall', 'performance', 'retsim', 'baseline', 'algorithm', 'length', 'query', 'target', 'text', 'vary', 'see', 'retsimneardup', 'retsimpartialdup', 'outperform', 'method', 'short', 'text', 'character', 'text', 'length', 'increase', 'character', 'retsimneardup', 'remain', 'close', 'perfect', 'retsimpartialdup', 'performance', 'degrade', 'split', 'text', 'multiple', 'embedding', 'find', 'near', 'matching', 'chunk', 'text', 'minhash', 'simhash', 'also', 'perform', 'poorly', 'short', 'text', 'length', 'start', 'degrade', 'long', 'text', 'neuralbased', 'embedding', 'observe', 'slight', 'drop', 'performance', 'long', 'text', 'model', 'retsimneardup', 'multilingual', 'use', 'embedding', 'handle', 'arbitrary', 'length', 'input', 'realworld', 'nearduplicate', 'evaluation', 'setup', 'benchmark', 'retsim', 'ability', 'identify', 'nearduplicate', 'content', 'realworld', 'dataset', 'literature', 'newscopy', 'deduplication', 'dataset', 'silcock', 'contain', 'historical', 'news', 'article', 'positive', 'duplicate', 'pair', 'dataset', 'consist', 'noisy', 'near', 'duplicate', 'due', 'factor', 'ocr', 'error', 'plagiarism', 'news', 'aggregation', 'also', 'evaluate', 'algorithm', 'core', 'nearduplicate', 'dataset', 'gyawali', 'consist', 'scholarly', 'article', '25k', 'exact', 'duplicate', '25k', 'nearduplicate', 'nonduplicate', 'duplicate', 'dataset', 'arise', 'article', 'revision', 'versioning', 'metadata', 'difference', 'man', 'key', 'difference', 'benchmark', 'benchmark', 'benchmark', 'focus', 'detect', 'cluster', 'nearduplicate', 'text', 'rather', 'robust', 'text', 'retrieval', 'base', 'syntactic', 'similarity', 'benchmark', 'follow', 'experimental', 'setup', 'provide', 'paper', 'report', 'adjust', 'index', 'ari', 'newscopy', 'dataset', 'report', 'precisionrecallf1', 'score', 'core', 'nearduplicate', 'dataset', 'result', 'newscopy', 'dataset', 'retsimpartialdup', 'outperform', 'approach', 'significant', 'margin', 'ari', 'compare', 'good', 'minhash', 'result', 'report', 'table', 'dataset', 'many', 'nearduplicate', 'pair', 'text', 'significantly', 'long', 'expect', 'retsimpartialdup', 'find', 'match', 'text', 'chunk', 'document', 'suited', 'task', 'outperform', 'retsimneardup', 'bucket', 'nearduplicate', 'detection', 'rate', 'length', 'ratio', 'positive', 'pair', 'figure', 'observe', 'retsimpartialdup', 'outperform', 'minhash', 'regardless', 'length', 'ratio', 'minhash', 'surpasse', 'retsimneardup', 'formance', 'text', 'roughly', '15x', 'length', 'text', 'nearduplicate', 'pair', 'l', 'c', 'e', 'r', 'target', 'text', 'length', 'c', 'e', 'r', 'text', 'augmentation', 'amount', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'partialdup', 'retsim', 'neardup', 'preprint', 'modelalgorithm', 'multilingual', 'use', 'multilingual', 'e5base', 'sbert', 'simhash', 'minhash', 'minhash', 'retsimpartialdup', 'retsimneardup', 'table', 'performance', 'comparison', 'newscopy', 'dataset', 'adjust', 'rand', 'index', 'value', 'report', 'denote', 'result', 'figure', 'nearduplicate', 'detection', 'rate', 'sim', 'minhash', 'different', 'length', 'ratio', 'pos', 'itive', 'pair', 'length', 'long', 'divide', 'short', 'text', 'round', 'near', 'integer', 'additionally', 'notice', 'label', 'dataset', 'occasionally', 'noisy', 'substantial', 'por', 'tion', 'retsim', 'false', 'positive', 'appear', 'nearduplicate', 'inspection', 'core', 'nearduplicate', 'dataset', 'table', 'document', 'article', 'title', 'abstract', 'roughly', 'size', 'retsimpartialdup', 'retsimneardup', 'performance', 'roughly', 'equivalent', 'method', 'outperform', 'baseline', 'term', 'macro', 'f1', 'score', 'accuracy', 'use', 'minhash', 'lsh', 'hash', 'function', 'computational', 'efficiency', 'recommend', 'datasketch', 'library1', 'well', 'accuracy', 'default', 'set', 'deduplication', 'threshold', 'detail', 'hyperpa', 'rameter', 'setting', 'algorithm', 'nearduplication', 'dataset', 'find', 'model', 'exact', 'title', 'match', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'minhash', 'lsh', 'retsimpartialdup', 'retsimneardup', 'precision', 'duplicate', 'duplicate', 'precision', 'accuracy', 'table', 'evaluation', 'result', 'core', 'nearduplicate', 'dataset', 'precisionrecallmacro', 'f1', 'accuracy', 'number', 'report', 'denote', 'result', 'application', 'training', 'dataset', 'deduplication', 'modelalgorithm', 'train', 'example', 'dup', 'train', 'valid', 'example', 'dup', 'train', 'minhash', 'lsh', 'exact', 'substre', 'retsimneardup', 'retsimpartialdup', 'table', 'deduplication', 'rate', 'wiki40b', 'english', 'denote', 'result', 'setup', 'evaluate', 'retsim', 'ability', 'deduplicate', 'text', 'training', 'dataset', 'deduplicate', 'english', 'split', 'wiki40b', 'conservatively', 'set', 'cosine', 'similarity', 'deduplica', 'tion', 'threshold', 'retsimneardup', 'retsimpartialdup', 'limit', 'amount', 'false', 'positive', 'base', 'optimal', 'threshold', 'find', 'evaluation', 'use', 'use', 'arch', 'default', 'vector', 'index', 'approximate', 'near', 'neighbor', 'search', 'vardanian', 'compare', 'big', 'datum', 'look', 'small', 'https', 'githubcomekzhudatasketch', 'e', 'r', 'c', 'e', 'c', 'l', 'r', 'e', 'length', 'ratio', 'nearduplicate', 'text', 'pair', 'retsim', 'partialdup', 'retsim', 'neardup', 'minhash', 'preprint', 'modelalgorithm', 'accelerator', 'batch', 'size', 'embed', 'hash', 'time', 'minhash', 'lsh', 'retsim', 'retsim', 'retsim', 'cpu', 'core', 'onnx', 'cpu', 'core', 'tensorflow', 'rtx', 'tensorflow', 'table', 'embeddinghashe', 'speed', 'retsim', 'minhash', 'lsh', 'wiki40b', 'dataset', 'minhash', 'lsh', 'set', 'number', 'hash', 'function', 'follow', 'kocetkov', 'use', 'jaccard', 'similarity', 'threshold', 'deduplication', 'result', 'overall', 'report', 'table', 'retsimneardup', 'find', 'slightly', 'duplicate', 'wiki', '40b', 'training', 'validation', 'split', 'inline', 'deduplication', 'result', 'section', 'retsimneardup', 'outperform', 'algorithm', 'hand', 'retsimpartialdup', 'find', 'signifi', 'cantly', 'match', 'exact', 'substring', 'match', 'use', 'previous', 'study', 'showcase', 'usefulness', 'perform', 'nearduplicate', 'partialduplicate', 'matching', 'largerthanexpected', 'number', 'partial', 'match', 'indicate', 'machine', 'learn', 'ing', 'practitioner', 'take', 'extra', 'care', 'deduplicate', 'wikipedia', 'chunk', 'level', 'avoid', 'feed', 'duplicate', 'text', 'model', 'term', 'embed', 'speed', 'table', 'retsim', 'significantly', 'slow', 'minhash', 'lsh', 'cpu', '46x', 'slow', 'competitive', 'use', 'desktop', 'gpu', 'rtx', 'slow', 'almost', 'onpar', 'use', 'highend', 'nvidia', '15x', 'slow', 'current', 'code', 'write', 'python', 'fully', 'optimize', 'expect', 'performance', 'gap', 'significantly', 'shrink', 'optimize', 'implementation', 'retsim', 'slow', 'minhash', 'retsim', 'significantly', 'small', 'fast', 'text', 'embed', 'model', 'close', 'performance', 'gap', 'neural', 'nonneural', 'base', 'method', 'nearduplicate', 'text', 'detection', 'dataset', 'deduplication', 'retsimneardup', 'retsimpartialdup', 'return', 'time', 'embed', 'speed', 'indexing', 'retrieval', 'time', 'depend', 'vector', 'index', 'search', 'use', 'long', 'document', 'retsimpartialdup', 'produce', 'embedding', 'retsimneardup', 'retsimpartialdup', 'offer', 'tradeoff', 'finergrained', 'match', 'depend', 'specific', 'vector', 'search', 'dataset', 'use', 'wild', 'spam', 'email', 'cluster', 'section', 'showcase', 'realworld', 'performance', 'cluster', 'nearduplicate', 'text', 'heavily', 'manipulate', 'adversarial', 'attack', 'perform', 'evaluation', 'spam', 'campaign', 'spam', 'constitute', 'strong', 'proving', 'ground', 'nearduplicate', 'cluster', 'algorithm', 'spammer', 'employ', 'adversarial', 'augmentation', 'technique', 'attempt', 'evade', 'detection', 'mentation', 'typically', 'include', 'append', 'prepende', 'unrelated', 'text', 'interleave', 'random', 'word', 'different', 'language', 'intentionally', 'introduce', 'typo', 'abuse', 'extended', 'character', 'set', 'emojis', 'homoglyphs', 'technique', 'collectively', 'refer', 'hashbuste', 'setup', 'dataset', 'consist', 'spam', 'email', 'spam', 'campaign', 'donate', 'gmail', 'user', 'flag', 'reach', 'inboxe', 'example', 'contain', 'email', 'subject', 'concatenate', 'message', 'content', 'email', 'misclassifie', 'spam', 'classifier', 'effective', 'adversarial', 'text', 'manipulation', 'technique', 'make', 'challenging', 'test', 'set', 'cluster', 'evaluation', 'example', 'hashbuste', 'attack', 'adversarial', 'manipulation', 'observe', 'include', 'use', 'homoglpyphs', 'uncommon', 'unicode', 'character', 'set', 'invisible', 'character', 'pad', 'random', 'word', 'different', 'language', 'get', 'ground', 'truth', 'campaign', 'cluster', 'email', 'manually', 'review', 'assign', 'specific', 'spam', 'campaign', 'base', 'similarity', 'human', 'reviewer', 'use', 'agglomerative', 'clustering', 'cluster', 'spam', 'email', 'report', 'homogeneity', 'completeness', 'vmeasure', 'adjust', 'rand', 'index', 'metric', 'result', 'overall', 'observe', 'retsim', 'significantly', 'well', 'cluster', 'nearduplicate', 'adversarial', 'manipulation', 'outperform', 'simhash', 'use', 'metric', 'consider', 'table', 'particular', 'observe', 'retsim', 'outperform', 'use', 'vmeasure', 'score', 'main', 'metric', 'result', 'report', 'section', 'inline', 'observe', 'deploy', 'retsim', 'main', 'nearduplicate', 'detection', 'preprint', 'model', 'vmeasure', 'ari', 'use', 'simhash', 'lsh', 'retsimneardup', 'table', 'performance', 'cluster', 'adversarial', 'spam', 'campaign', 'practice', 'ablation', 'study', 'setup', 'section', 'summarize', 'key', 'ablation', 'study', 'perform', 'design', 'ret', 'sim', 'model', 'use', 'section', 'train', 'use', 'setup', 'detail', 'a12', 'ex', 'cept', 'train', 'step', 'reduce', 'computational', 'cost', 'evaluate', 'retsimneardup', 'performance', 'model', 'subset', 'benchmark', 'randomly', 'select', 'example', 'language', 'split', 'use', 'recall', 'report', 'metric', 'block', 'type', 'recall', 'chunk', 'size', 'recall', 'embed', 'retvec', 'gau', 'table', 'retsim', 'ablation', 'study', 'result', 'architecture', 'block', 'type', 'leave', 'text', 'chunk', 'size', 'middle', 'embed', 'dimension', 'right', 'bold', 'denote', 'value', 'select', 'final', 'retsim', 'model', 'result', 'table', 'contain', 'retsim', 'ablation', 'study', 'result', 'text', 'chunk', 'size', 'architecture', 'block', 'type', 'embed', 'size', 'important', 'architectural', 'decision', 'decide', 'optimal', 'text', 'chunk', 'size', 'find', 'right', 'balance', 'small', 'size', 'possible', 'maximize', 'retsimpartialdup', 'efficiency', 'ensure', 'retsimneardup', 'fulltext', 'embedding', 'work', 'effec', 'tively', 'full', 'document', 'find', 'chunk', 'character', 'offer', 'good', 'performance', 'also', 'test', 'various', 'model', 'architecture', 'transformer', 'block', 'find', 'good', 'balance', 'efficiency', 'performance', 'find', 'modern', 'gau', 'block', 'outper', 'form', 'vanilla', 'block', 'block', 'xue', 'also', 'try', 'modern', 'architecture', 'architec', 'ture', 'propose', 'retvec', 'bad', 'gau', 'block', 'performance', 'last', 'least', 'find', 'increase', 'embed', 'size', 'dimension', 'yield', 'meaningful', 'improvement', 'retsimneardup', 'accordingly', 'opt', 'use', 'embed', 'spaceefficiency', 'maximize', 'indexing', 'query', 'speed', 'additional', 'ablation', 'study', 'hyperparameter', 'find', 'appendix', 'future', 'work', 'retsim', 'novel', 'training', 'regime', 'combine', 'metric', 'learning', 'datum', 'augmentation', 'many', 'potential', 'application', 'plan', 'explore', 'future', 'work', 'example', 'adapt', 'extend', 'train', 'robust', 'semantic', 'embedding', 'image', 'similarity', 'embedding', 'additionally', 'expect', 'general', 'model', 'become', 'big', 'expensive', 'run', 'future', 'small', 'specialize', 'model', 'retsim', 'emerge', 'efficient', 'alternative', 'wide', 'range', 'task', 'conclusion', 'paper', 'introduce', 'retsim', 'novel', 'multilingual', 'text', 'embed', 'achieve', 'state', 'oftheart', 'performance', 'nearduplicate', 'text', 'detection', 'dataset', 'deduplication', 'syntactic', 'text', 'similarity', 'benchmark', 'retsim', 'significantly', 'fast', 'previous', 'neuralbased', 'text', 'embedding', 'robust', 'ngram', 'base', 'algorithm', 'make', 'suitable', 'largescale', 'text', 'retrieval', 'dataset', 'deduplication', 'especially', 'adversarial', 'setting', 'spam', 'detection', 'furthermore', 'introduce', 'benchmark', 'first', 'multilingual', 'dataset', 'design', 'measure', 'ad', 'versarial', 'robustness', 'nearduplicate', 'text', 'detection', 'algorithm', 'opensource', 'retsim', 'benchmark', 'mit', 'license', 'preprint', 'reference', 'naeem', 'amin', 'hamza', 'koundal', 'bader', 'alouffi', 'shah', 'machine', 'learn', 'technique', 'spam', 'detection', 'email', 'iot', 'platform', 'analysis', 'research', 'challenge', 'security', 'communication', 'network', 'doi', 'ahme', 'elgohary', 'generate', 'natural', 'language', 'adversarial', 'example', 'lepikhin', 'alexandre', 'taropa', 'paige', 'bailey', 'yanping', 'meierhellstern', 'yuje', 'bury', 'pher', 'cl´ement', 'crepy', 'shachi', 'sunipa', 'dev', 'vlad', 'feinberg', 'vlad', 'fienber', 'freitag', 'xavi', 'sebastian', 'guy', 'hand', 'abe', 'maxim', 'sneha', 'music', 'hyeontaek', 'vedant', 'maysam', 'moussalem', 'nado', 'pellat', 'reiner', 'emily', 'reif', 'parker', 'brennan', 'slone', 'sohn', 'valter', 'linte', 'denny', 'slav', 'petrov', 'palm', 'technical', 'report', 'url', 'http', 'arxivorgabs230510403', 'arxiv230510403', 'cs', 'andrei', 'broder', 'minwise', 'inde', 'proceeding', 'thirtieth', 'annual', 'acm', 'symposium', 'theory', 'pendent', 'permutation', 'compute', 'pp', 'retvec', 'resilient', 'efficient', 'text', 'vectorizer', 'mario', 'guajardocespede', 'tar', 'universal', 'sentence', 'encoder', 'arxiv', 'preprint', 'similarity', 'estimation', 'technique', 'round', 'algorithm', 'proceeding', 'thiryfourth', 'annual', 'acm', 'symposium', 'theory', 'compute', 'pp', 'pretraining', 'deep', 'bidirectional', 'transformer', 'language', 'understanding', 'arxiv', 'fangxiaoyu', 'language', 'agnostic', 'sentence', 'embed', 'gao', 'qi', 'generation', 'adversarial', 'text', 'sequence', 'evade', 'deep', 'learning', 'classifier', 'arxivorgab', 'cs', 'preprint', 'mandy', 'guo', 'denny', 'alrfou', 'wiki40b', 'multilingual', 'language', 'model', 'dataset', 'proceeding', '12th', 'language', 'resource', 'evaluation', 'conference', 'pp', '2440–2452', 'deduplication', 'scholarly', 'document', 'ing', 'locality', 'sensitive', 'hashing', 'word', 'embedding', 'proceeding', 'twelfth', 'language', 'resource', 'evaluation', 'conference', 'pp', 'guage', 'isbn', 'aclanthology', 'hagen', 'rathgeber', 'large', 'proceeding', '40th', 'international', 'sigir', 'scale', 'query', 'spelling', 'conference', 'research', 'development', 'information', 'retrieval', 'pp', 'transformer', 'quality', 'linear', 'time', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'issac', 'r', 'chiong', 'analysis', 'phishe', 'attack', 'countermeasure', 'mohit', 'iyyer', 'varun', 'daum´e', 'deep', 'unordered', 'composition', 'rival', 'syntactic', 'method', 'text', 'classification', 'proceeding', '53rd', 'annual', 'meeting', 'association', 'computational', 'linguistic', '7th', 'international', 'joint', 'conference', 'natural', 'language', 'processing', 'volume', 'long', 'paper', 'pp', 'association', 'computational', 'linguistic', 'raffel', 'deduplicate', 'training', 'datum', 'mitigate', 'pri', 'vacy', 'risk', 'language', 'model', 'proceeding', '39th', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'https', 'proceeding', 'mlrpressv162kandpal22ahtml', 'dzmitry', 'bahdanau', 'harm', 'vrie', 'stack', 'tb', 'permissively', 'license', 'source', 'code', 'deduplicate', 'training', 'datum', 'make', 'language', 'model', 'well', 'arxiv210706499', 'feichtenhofer', 'trevor', 'darrell', 'saine', 'convnet', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'jin', 'qi', 'textattack', 'framework', 'adversarial', 'attack', 'datum', 'augmentation', 'adversarial', 'training', 'tober', 'url', 'tazi', 'lo¨ıc', 'magne', 'nil', 'reimer', 'massive', 'text', 'bed', 'benchmark', 'url', 'https', 'filip', 'radenovi´c', 'giorgo', 'tolia', 'chum', 'finetune', 'image', 'retrieval', 'human', 'annotation', 'ieee', 'transaction', 'pattern', 'analysis', 'machine', 'intelligence', 'publisher', 'ieee', 'nil', 'reimer', 'iryna', 'gurevych', 'sentencebert', 'sentence', 'embedding', 'use', 'siamese', 'network', 'arxiv', 'preprint', 'facenet', 'unified', 'embedding', 'face', 'recognition', 'cluster', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'cvpr', 'pp', 'url', 'arxivorgabs150303832', 'arxiv150303832', 'cs', 'preprint', 'emily', 'noiserobust', 'deduplication', 'scale', 'duplicate', 'text', 'detection', 'use', 'frequencybiase', 'signature', 'moni', 'naor', 'rangan', 'demetri', 'terzopoulo', 'moshe', 'weikum', 'ed', 'web', 'information', 'system', 'engineering', 'wise', 'volume', 'pp', 'springer', 'isbn', 'doi', 'url', 'series', 'title', 'lecture', 'note', 'computer', 'science', 'ash', 'vardanian', 'usearch', 'unum', 'url', 'binxe', 'daxin', 'rangan', 'wei', 'text', 'embedding', 'weaklysupervise', 'contrastive', 'pretraine', 'cember', 'r', 'multisimilarity', 'loss', 'general', 'pair', 'weighting', 'deep', 'metric', 'learning', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'linte', 'constant', 'raffel', 'mt5', 'massively', 'multilingual', 'pretraine', 'texttotext', 'transformer', 'arxiv', 'preprint', 'arxiv201011934', 'amin', 'law', 'sing', 'brian', 'multilingual', 'universal', 'sentence', 'encoder', 'semantic', 'retrieval', 'yang', 'je', 'srinadh', 'hsieh', 'large', 'batch', 'optimization', 'deep', 'learning', 'training', 'bert', 'minute', 'arxiv', 'preprint', 'preprint', 'a1', 'retsim', 'detail', 'a11', 'retsim', 'model', 'hyperparameter', 'full', 'list', 'retsim', 'model', 'hyperparameter', 'find', 'table', 'hyperparameter', 'input', 'length', 'chunk', 'block', 'type', 'block', 'hide', 'dim', 'expansion', 'rate', 'activation', 'function', 'attention', 'activation', 'function', 'absolute', 'positional', 'encoding', 'relative', 'positional', 'encoding', 'norm', 'type', 'pooling', 'type', 'dropout', 'rate', 'embed', 'dim', 'parameter', 'value', 'gau', 'swish', 'relu2', 'rope', 'p', 'table', 'detailed', 'retsim', 'model', 'hyperparameter', 'a12', 'retsim', 'training', 'hyperparameter', 'table', 'detail', 'hyperparameter', 'setting', 'train', 'configuration', 'loss', 'optimizer', 'use', 'train', 'retsim', 'model', 'hyperparameter', 'value', 'batch', 'size', 'train', 'step', 'learning', 'rate', 'end', 'learn', 'rate', 'learn', 'rate', 'decay', 'weight', 'decay', 'cosine', 'table', 'retsim', 'detailed', 'training', 'hyperparameter', 'a2', 'training', 'dataset', 'detail', 'provide', 'full', 'list', 'augmentation', 'use', 'generate', 'augmented', 'text', 'retsim', 'training', 'dataset', 'describe', 'section', 'sentencelevel', 'augmentation', 'deletion', 'random', 'sentence', 'deletion', 'random', 'sentence', 'truncation', 'insertion', 'preprint', 'random', 'prefix', 'sentence', 'random', 'suffix', 'sentence', 'random', 'sentence', 'insertion', 'repeat', 'sentence', 'substitution', 'lowercaseuppercase', 'sentence', 'random', 'sentence', 'substitution', 'transposition', 'neighboring', 'swap', 'wordlevel', 'augmentation', 'deletion', 'random', 'word', 'deletion', 'insertion', 'random', 'word', 'insertion', 'random', 'word', 'insertion', 'language', 'substitution', 'frequency', 'base', 'word', 'substitution', 'random', 'word', 'substitution', 'random', 'word', 'substitution', 'language', 'repeat', 'word', 'transposition', 'neighboring', 'swap', 'characterlevel', 'augmentation', 'deletion', 'random', 'character', 'deletion', 'substitution', 'case', 'substitution', 'base', 'substitution', 'n', 'qwerty', 'homoglyphs', 'substitution', 'random', 'ascii', 'substitution', 'random', 'character', 'language', 'alphabet', 'random', 'punctuation', 'substitution', 'random', 'unicode', 'character', 'substitution', 'insertion', 'character', 'repetition', 'ngram', 'base', 'insertion', 'n', 'random', 'character', 'language', 'alphabet', 'insertion', 'random', 'punctuation', 'insertion', 'random', 'unicode', 'character', 'insertion', 'transposition', 'neighboring', 'swap', 'a3', 'detailed', 'evaluation', 'hyperparameter', 'figure', 'contain', 'information', 'deduplication', 'threshold', 'value', 'hyperparameter', 'setting', 'benchmarke', 'newscopy', 'core', 'deduplication', 'dataset', 'preprint', 'model', 'type', 'threshold', 'value', 'hyperparameter', 'multilingual', 'use', 'cosine', 'similarity', 'multilingual', 'e5base', 'cosine', 'similarity', 'simhash', 'minhash', 'retsimneardup', 'retsimpartialdup', 'hamming', 'distance', 'similarity', 'cosine', 'similarity', 'cosine', 'similarity', 'bit', 'characterlevel', 'hash', 'function', 'wordlevel', 'figure', 'hyperparameter', 'setting', 'newscopy', 'dataset', 'evaluation', 'section', 'model', 'type', 'threshold', 'value', 'hyperparameter', 'cosine', 'similarity', 'labse', 'multilingual', 'use', 'cosine', 'similarity', 'multilingual', 'e5base', 'cosine', 'similarity', 'simhash', 'lsh', 'minhash', 'lsh', 'retsimneardup', 'retsimpartialdup', 'hamming', 'distance', 'similarity', 'cosine', 'similarity', 'cosine', 'similarity', 'bit', 'characterlevel', 'hash', 'function', 'wordlevel', 'figure', 'hyperparameter', 'setting', 'core', 'nearduplicate', 'dataset', 'evaluation', 'section', 'a31', 'deduplication', 'threshold', 'impact', 'figure', 'precisionrecallf1', 'score', 'different', 'cosine', 'distance', 'deduplication', 'threshold', 'retsimneardup', 'leave', 'retsimpartialdup', 'right', 'newscopy', 'dataset', 'detail', 'w4nt3d', 'benchmark', 'result', 'table', 'show', 'detailed', 'performance', 'result', 'retsim', 'baseline', 'algorithm', 'language', 'split', 'benchmark', 'preprint', 'k', 'j', 'b', 'e', 'l', 'l', 'l', 'k', 'l', 'p', 'e', 'l', 'l', 'r', 'p', 'e', 'r', 'r', 'e', 'e', 'r', 'h', 'h', 'h', 'h', 'u', 'h', 'r', 'h', 'h', 'e', 'r', 'f', 'fi', 'e', 'c', 'c', 'r', 'h', 'r', 'g', 'l', 'l', 'e', 'r', 'p', 'k', 'r', 'w', 'e', 'r', 'l', 'l', 'e', 'e', 'r', 'v', 'r', 'n', 'r', 'r', 'e', 'p', 'l', 'l', 'c', 'e', 'r', 'e', 'l', 'e', 'l', 'h', 'c', 'h', 'z', 'l', 'h', 'r', 'l', 'r', 'r', 'p', 'l', 'r', 'g', 'l', 'l', 'e', 'b', 'e', 'l', 'l', 'l', 'k', 'l', 'p', 'e', 'l', 'l', 'r', 'p', 'e', 'r', 'h', 'h', 'h', 'h', 'r', 'e', 'e', 'r', 'r', 'p', 'k', 'r', 'w', 'e', 'r', 'l', 'l', 'e', 'e', 'r', 'v', 'r', 'n', 'r', 'r', 'e', 'p', 'l', 'l', 'c', 'e', 'r', 'e', 'l', 'e', 'l', 'preprint', 'a5', 'additional', 'ablation', 'study', 'section', 'include', 'ablation', 'study', 'additional', 'hyperparameter', 'retsim', 'model', 'includ', 'loss', 'function', 'pooling', 'type', 'model', 'capacity', 'λ', 'recall', 'table', 'ablation', 'study', 'multisimilarity', 'loss', 'hyperparameter', 'retsim', 'training', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'block', 'hide', 'table', 'ablation', 'study', 'retsim', 'model', 'capacity', 'size', 'number', 'gau', 'block', 'hide', 'dimension', 'block', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'pool', 'type', 'recall', 'average', 'pooling', 'max', 'pooling', 'generalized', 'mean', 'pool', 'table', 'ablation', 'study', 'pool', 'type', 'retsim', 'model', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'select', 'example', 'newscopy', 'dataset', 'section', 'randomly', 'select', 'set', 'false', 'positive', 'false', 'negative', 'retsim', 'newscopy', 'deduplication', 'dataset', 'provide', 'insight', 'result', 'preprint', 'text', 'chauffeur', 'policeman', 'pass', 'jour', 'nalist', 'try', 'intervene', 'policeman', 'report', 'serious', 'condition', 'princess', 'husband', 'month', 'capt', 'mark', 'phillip', 'hurt', 'police', 'expert', 'say', 'hole', 'leave', 'bullet', 'fire', 'car', 'indicate', 'pass', 'miss', 'inch', 'police', 'informant', 'say', 'believe', 'shot', 'fire', 'assailant', 'expert', 'study', 'revolver', 'find', 'scene', 'say', 'fi', 'tnfernational', 'ay', 'ssast', 'fre', 'sg', 'federal', 'government', 'propose', 'new', 'method', 'eoustructe', 'federal', 'building', 'move', 'save', 'ad', 'ditional', 'ergy', 'suggest', 'il', 'elfort', 'adapt', 'new', 'building', 'p', 'mediate', 'removal', 'bert', 'prohibition', 'administrator', 'de', 'mande', 'today', 'ground', 'charge', 'place', 'department', 'justice', 'investigator', 'wheeler', 'accompany', 'demand', 'continue', 'page', 'dnite', 'jai', 'delegation', 'navat', 'confer', 'ence', 'today', 'win', 'ls', 'demand', 'pre', 'sentation', 'cnse', 'suxiliary', 'warship', 'limitation', 'first', 'plenary', 'session', 'thuvaday', 'chlet', 'delegate', 'mec', 'tittg', 'also', 'decide', 'plenary', 'sesslon', 'discuss', 'main', 'con', 'ference', 'question', 'alpha', 'betical', 'order', 'ihe', 'countriea', 'pro', 'posing', 'press', 'admit', 'american', 'delegation', 'woo', 'second', 'vic', 'tory', 'text', 'ian', 'ball', '26year', 'old', 'unemployed', 'englishman', 'bring', 'court', 'today', 'charge', 'attempt', 'mur', 'der', 'tempt', 'kidnap', 'anne', 'car', 'heart', 'nesday', 'night', 'ball', 'leanface', 'bearded', 'stand', 'stiffly', 'dock', 'court', 'handcuff', 'detective', 'speak', 'second', 'appearance', 'say', 'want', 'apply', 'legal', 'aid', 'court', 'dere', 'hold', 'hearing', 'federal', 'government', 'propose', 'new', 'method', 'construct', 'federal', 'building', 'move', 'save', 'addilional', 'energy', 'suggest', 'il', 'effort', 'adapt', 'new', 'building', 'sampson', 'administration', 'ad', 'ministrater', 'say', 'new', 'feature', 'construction', 'include', 'collection', 'rain', 'waler', 'cool', 'irriga', 'tion', 'solar', 'energy', 'collector', 'covering', 'exterior', 'wall', 'earth', 'whal', 'say', 'design', 'criteri', 'immiedl', 'aie', 'mmoval', 'pro', 'hi', 'bition', 'administrator', 'demand', 'seuate', 'day', 'waeel', 'ground', 'charge', 'place', 'depart', 'meat', 'justice', 'investigator', 'wheeler', 'accompany', 'demand', 'declaration', 'prohibition', 'foreemen', 'blame', 'politician', 'call', 'law', 'enforcement', 'commussion', 'sum', 'member', 'wp', 'amer', 'naval', 'cen', 'ference', 'win', 'demand', 'presentation', 'case', 'linsitation', 'flrst', 'next', 'trary', 'session', 'chief', 'delegate', 'meet', 'si', 'also', 'decide', 'plenary', 'session', 'discuss', 'main', 'confeyence', 'question', 'alphabetical', 'order', 'cauntrie', 'propose', 'win', 'second', 'victory', 'decide', 'udmil', 'certain', 'representative', 'press', 'plenary', 'table', 'example', 'false', 'negative', 'retsim', 'newscopy', 'dataset', 'pair', 'text', 'detect', 'nearduplicate', 'retsim', 'label', 'nearduplicate', 'original', 'dataset', 'ex', 'ample', 'randomly', 'select', 'truncate', 'character', 'display', 'preprint', 'text', 'bozeman', 'chet', 'huntley', 'resonant', 'voice', 'roughhewn', 'face', 'come', 'familiar', 'million', 'nightly', 'television', 'news', 'die', 'moun', 'tain', 'resort', 'home', 'undergo', 'surgery', 'lung', 'cancer', 'remain', 'recent', 'week', 'die', 'accord', 'widow', 'tippy', 'team', 'year', 'port', 'quit', 'turn', 'native', 'develop', '20millio', 'associate', 'press', 'amer', 'ican', 'pay', 'per', 'cent', 'month', 'electricity', 'year', 'last', 'ate', 'press', 'survey', 'show', 'sumer', 'begin', 'organize', 'fight', 'rate', 'hike', 'spot', 'check', 'monthly', 'elec', 'tric', 'bill', 'year', 'last', 'show', 'crease', 'erally', 'cent', 'high', 'report', 'boost', 'com', 'fia', 'average', 'tab', 'go', 'last', 'year', 'year', 'utility', 'bozeman', 'vice', 'say', 'world', 'miss', 'unique', 'ability', 'former', 'television', 'huntley', 'die', 'home', 'long', 'bout', 'lung', 'cancer', 'family', 'spokesman', 'say', 'memorial', 'service', 'conduct', 'huntley', 'big', 'sky', 'resort', 'recreation', 'area', 'south', 'huntley', 'chairman', 'big', 'sky', 'board', 'director', 'memorial', 'service', 'schedule', 'studio', 'house', 'pass', 'legislation', 'raise', 'minimum', 'wage', 'hour', 'year', 'worker', 'cover', 'bill', 'approve', 'also', 'increase', 'number', 'worker', 'cover', 'mini', 'mum', 'wage', 'law', 'bill', 'modify', 'ver', 'sion', 'veto', 'last', 'year', 'however', 'expect', 'sign', 'one', 'finally', 'approve', 'ad', 'justment', 'similar', 'pass', 'measure', 'text', 'bozeman', 'chet', 'huntley', 'resonant', 'voice', 'roughhewn', 'face', 'become', 'familiar', 'million', 'nightly', 'television', 'news', 'die', 'moun', 'tain', 'resort', 'home', 'undergo', 'surgery', 'lung', 'cancer', 'remain', 'active', 'recent', 'week', 'die', 'accord', 'widow', 'tippy', 'huntley', 'huntley', 'team', 'year', 'quit', 'return', 'native', 'develop', 'mil', 'louise', 'acenciaiod', 'prece', 'writer', 'pay', 'io', 'cent', 'month', 'far', 'electricity', 'year', 'ihan', 'last', 'press', 'survey', 'show', 'onsumer', 'begin', 'fight', 'rate', 'hike', 'spot', 'check', 'monthly', 'elec', 'tre', 'hill', 'year', 'show', 'increase', 'erally', 'cent', 'high', 'report', 'boost', 'com', 'average', 'tab', 'go', 'last', 'year', 'year', 'bozeman', 'vice', 'say', 'world', 'miss', 'unique', 'ability', 'former', 'television', 'huntley', 'die', 'home', 'long', 'bout', 'lung', 'cancer', 'family', 'spokesman', 'say', 'morial', 'service', 'ducte', 'hunt', 'big', 'sky', 'resort', 'recreation', 'area', 'south', 'hunt', 'chair', 'man', 'big', 'sky', 'board', 'director', 'memorial', 'service', 'sche', 'uled', 'studio', 'house', 'pass', 'legislation', 'raise', 'minimum', 'wage', 'hour', 'year', 'worker', 'cover', 'bill', 'approve', 'wedne', 'day', 'also', 'crease', 'mil', 'lion', 'number', 'worker', 'cov', 'ere', 'minimum', 'wage', 'law', 'bill', 'modify', 'version', 'veto', 'last', 'year', 'however', 'ex', 'te', 'sign', 'one', 'inally', 'approve', 'adjust', 'ment', 'similar', 'pass', 'table', 'example', 'false', 'positive', 'retsim', 'newscopy', 'dataset', 'pair', 'text', 'detect', 'nearduplicate', 'retsim', 'label', 'nearduplicate', 'original', 'dataset', 'example', 'randomly', 'select', 'truncate', 'character', 'display']",
"$Q_{bias}$ -- A Dataset on Media Bias in Search Queries and Query
  Suggestions","[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3578503.3583628', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2311.17780v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17780v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-29 16:26:00,"3
2
0
2

v
o
N
8
2

]
S
M

.
s
c
[

1
v
3
8
2
7
1
.
1
1
3
2
:
v
i
X
r
a

Lineax: unified linear solves and linear least-squares
in JAX and Equinox

Jason Rader
Oxford University
rader@maths.ox.ac.uk

Terry Lyons
Oxford University

Patrick Kidger
Google X
math@kidger.site

Abstract

We introduce Lineax, a library bringing linear solves and linear least-squares to
the JAX+Equinox scientific computing ecosystem. Lineax uses general linear
operators, and unifies linear solves and least-squares into a single, autodifferen-
tiable API. Solvers and operators are user-extensible, without requiring the user to
implement any custom derivative rules to get differentiability. Lineax is available
at https://github.com/google/lineax.

1

Introduction

JAX is an autodifferentiatiable Python framework popular for machine learning and scientific comput-
ing [4, 9, 12, 16]. Equinox [20] is a popular JAX library [8, 15], targeting the same use cases, that adds
additional support for parameterised functions. Solving linear systems, whether well-posed linear
solves or ill-posed linear least-squares problems, is a central sub-problem in scientific computing
[14, 27]. For example, linear solves and least-squares appear as subroutines in nonlinear optimisation
[21], finite-difference schemes [26], and signal processing [22]. As such, we introduce Lineax, a
library built in JAX and Equinox for linear solves and linear least-squares.

Lineax presents a single, differentiable interface for solving well-posed, underdetermined, and
overdetermined linear systems. It also allows users to write custom differentiable linear solvers or
least-squares solvers, and introduces a linear operator abstraction.

Overall, we intend for Lineax to integrate well with the existing JAX scientific ecosystem. This
ecosystem is growing, and includes packages for differentiable rigid-body physics simulation [11],
computational fluid dynamics [3, 6], protein structure prediction [10], ordinary and stochastic
differential equations [19], and probabilistic modeling [25]. We are beginning to see some use
of Lineax in this ecosystem already. This includes for linear subroutines in ocean dynamics [18]
and optimal transport [5]. Further, Diffrax [19] plans to adopt Lineax in the near future for linear
subroutines in differential equations solves.

1.1 Main contributions

The main contributions of Lineax are:

• A general linear operator abstraction, as implemented by dense matrices, linear functions,

Jacobians, etc.

• Stable and fast gradients through least-squares solves. This includes through user-defined

solvers, without requiring extra effort from the user.

• PyTree-valued 1 operators and vectors.

1JAX terminology for arbitrarily nested container ’node’ types (tuples/dictionaries/lists/custom types) con-
taining ’leaves’ (every other Python/JAX type.) We exclusively consider PyTrees who’s leaves are JAX arrays.

NeurIPS 2023 AI for Science Workshop.

 
 
 
 
 
 
Comparisons to existing JAX APIs

The operator abstraction introduced in Lineax offers a flexibility not found in core JAX, which
supports only dense matrices or matrix-vector product representations of operators. Lineax introduces
new solvers over core JAX, such as lineax.Tridiagonal. Lineax also offers a consistent API
between operators and solvers, which is what allows for extensibility to user-specified custom operator
and solvers.

Compilation times for most Lineax solvers are essentially identical to JAX native solvers; Lineax’s
iterative solvers (CG, GMRES, ...) compile roughly twice as fast. The ‘benchmarks’ folder on GitHub
provides a quantitative comparison.

We emphasise the stable and fast gradients to contrast with the existing JAX implementation, which
as of version 0.4.16 exhibits instability or incorrect gradients in some exceptional cases.

For these reasons, JAX is actually considering deprecating some of its own APIs in favour of Lineax
[29].

1.2 Classical linear solve example

Consider solving Ax = b for a random matrix A ∈ R10×10 against a random vector b ∈ R10. This
can be done via

1

2

3

4

5

6

7

import jax . random as jr
import lineax as lx

A_key , b_key = jr . split ( jr . PRNGKey (0))
A = lx . M atr ix Li near Op er ator ( jr . normal ( A_key , (10 , 10)))
b = jr . normal ( b_key , (10 ,))
solution = lx . linear_solve (A , b )

2 Performing linear solves and least-squares

The main entry point to linear solves and least-squares in Lineax is

lineax . linear_solve (A , b , solver )

for a linear operator A and PyTree b. This performs a linear solve Ax = b (for well-posed systems),
or returns a least-squares solution minx ∥Ax − b∥2 (for overdetermined systems), or returns a
minimum norm solution minx ∥x∥2 subject to Ax = b (for underdetermined systems). This is a lot of
operations to unify together, and it may initially seem strange to do so. The common thread – and our
justification for unifying these operations – is that mathematically, all the above operations correspond
to the pseudoinverse solution to Ax = b, ie. the solution arising from using the Moore-Penrose
pseudoinverse x = A†b [1, 23].

The user can specify which solver they’d like to use via the solver argument. This is helpful when
the user already knows which solvers should work well for a problem. Not every solver is capable of
handling every problem. For example, lineax.CG handles positive definite operators [21, section 5].
Using a solver with an incompatible problem will result in an error.

3 General linear operators

In Lineax, we represent A more generally than as an n × m matrix. Instead, we represent A as a
linear operator A : X → Y , where X and Y are spaces of PyTrees of arrays. At an implementation
level, a linear operator is an object which subclasses

lineax . A bs t ra c tL i ne a r Op e rat or .

When A is a dense matrix, A ∈ Rdim(Y )×dim(X), it can be treated as a Lineax linear operator via

lineax . Ma trix Li ne ar Op er ator ( A ).

2

Lineax operators themselves form a vector space, and are closed under addition, scalar multiplication,
and composition. Each linear operator A must implement a method to:

• Compute the matrix-vector product: Ax for x ∈ X.
• Compute the transpose of the operator: AT : Y → X.
• Materialise the operator as a matrix: A.as_matrix() ∈ Rdim(X)×dim(Y ).
• Retrieve the input/output PyTree structure, as well as the input/output dimensions. ie. the

functions domain(A) = X and codomain(A) = Y .

This increased generality comes with increased flexibility. For example: large, sparse matrices can
use data-efficient formats and utilise linear solves which use only the matrix-vector product, such
as GMRES [24] or BiCGStab [28]. For example, a linear function f : X → Y can be made into a
linear operator with

lineax . F un c ti o nL i ne a r Op e rat or (f , in_structure )

where in_structure describes the PyTree structure of the input of f (equivalently, the PyTree
structure of the elements x ∈ X.) Similarly, a nonlinear function g : X → Y can be linearised at a
point x ∈ X and use its Jacobian at x as a linear operator via

lineax . J ac o bi a nL i ne a r Op e rat or (g , x )

The lineax.AbstractLinearOperator base class is available for users to subclass and create
their own linear operator types.

3.1 Operator tags

Tags are an optional argument to most linear operators, and indicate properties of the operator A.
For example, if A ∈ Rn×n is positive semidefinite, then A can be marked as a positive semidefinite
linear operator with

lineax . Ma trix Li ne ar Op er ator (A , lineax . p o s i t i ve _ s e m i d e f i ni t e _ t a g )

This indicates to any solver which uses A that it is positive semidefinite. For example, if A is also
nonsingular, then it can be used safely with lineax.CG. Tags are also used to select the appropriate
solver in the polyalgorithm lineax.AutoLinearSolver detailed in section 6.

4 Computing gradients

In JAX, derivatives are built from Jacobian-vector products (JVPs) and vector-Jacobian products
(VJPs) for forward-mode and reverse-mode automatic differentiation respectively [12]. The JVP of a
function f : Ra → Rb maps an input-tangent pair (x, v) ∈ Ra × Ra to (f (x), ∂f (x)(v)) ∈ Rb × Rb
where ∂f (x) : Ra → Rb is the Jacobian of f at x. The VJP maps an input-cotangent pair (x, c) ∈
Ra × Rb to (f (x), ∂f (x)T c) ∈ Rb × Ra, where ∂f (x)T : Rb → Ra is the transpose of ∂f (x).

The major contribution of Lineax over existing linear solve and least-squares software is the efficient
computation of JVPs for pseudoinverse solutions. That is, differentiation through both well-posed
linear solves and ill-posed least-squares solves are performed in the same manner as each other.
In particular, we may special case when operators have full row or column rank in order to obtain
improved performance, as we now show.

4.1

JVPs and forward-mode autodifferentiation

In this section, let L(A, b) denote the linear solve lineax.linear_solve. For a primal problem
Ax = b, then L(A, b) = A†b where A† be the Moore–Penrose pseudoinverse of A, as mentioned in
section 2. Here we discuss how the to compute the JVP ∂L(A, b)(V, v), where (V, v) is the tangent
pair consisting of a tangent operator V and tangent vector v.

It is possible to compute the JVP through either argument. For example, the tangent computation of a
linear solve as a function of v alone is

∂L(A, b)(0, v) = A†v

3

(2)

(3)

(4)

where 0 represents the 0 tangent operator.

Meanwhile, computing the JVP for ∂L(A, b)(V, 0) requires differentiating through a pseudoinverse,
which has the explicit formula [13]

∂L(A, b)(V, 0) = (−A†V A† + A†(A†)T V T (I − AA†) + (I − A†A)V T (A†)T A†)A†b.

Letting

x = A†b
z = V T (A†)T x,

and adding the above two equations together and using the linearity of the Jacobian, we have the total
JVP with respect the primal pair (A, b) and tangent pair (V, v) for lineax.linear_solve is

∂L(A, b)(V, v) = A† (cid:0)−V x + (A†)T V T (b − Ax) − Az + v(cid:1) + z.

(1)

If A has linearly independent columns, then A†A = I [14, section 5.5.2] and the term z − A†Az = 0,
giving

∂L(A, b)(V, v) = A† (cid:0)−V x + (A†)T V T (b − Ax) + v(cid:1) .

When A has linearly independent rows, then (b − Ax) = 0 and

∂L(A, b)(V, v) = A† (−V x − Az + v) + z.

Together, if A has linearly independent rows and columns, then A is well-posed, A† = A−1 is a true
inverse, and

∂L(A, b)(V, v) = A−1 (−V x + v) .

We then select between equations (4), (3), (2), or (1) depending on whether we know at compile time
that A has linearly independent rows and columns, has only independent rows, has only independent
columns, or has both dependent rows and columns. Despite being a property of the operator, at
compile time the main way the JVP rule is dispatched via the choice of solver. This is because not
every solver supports dependent rows/columns (section 5), and will return nan values if used in a
solve with an unsupported operator. So, if a solver does not support dependent rows/columns, we can
be sure we will not get a solution given an operator with dependent rows/columns in the JVP.

For example, lineax.QR [27, section 2] can handle dependent rows if the number of rows is greater
than the number of columns (or dependent columns if the number of columns is greater than the
number of rows) and will dispatch to either equation (2) or (3). If the number of columns and rows of
A are the same, then lineax.QR will dispatch to equation (4).

4.2 VJPs and backpropogation via transposition

Reverse-mode autodifferentiation of a function f : Ra → Rb is not built on JVPs, but rather on
vector-Jacobian products (VJPs) vT ∂f (x) = ∂f (x)T v. As suggested by the definition, VJPs are
constructed via a JVP and transposition [12]. This is how VJPs are implemented in JAX, and thus in
Lineax as well. The Jacobian ∂L(A, b) : Rm×n × Rm → Rn is a linear function, see also the explicit
form in equation (1). Therefore, it has a transpose ∂L(A, b)T : Rn → Rm×n × Rm.

The transpose rule for the linear solve is implemented as a custom JAX primitive. See [12] for more
details.

5 User-defined solvers

A user can implement a custom solver by subclassing

lineax . Ab stra ct Li ne ar So lver

which requires the methods: init, compute, transpose, allow_dependent_rows, and
allow_dependent_columns.

4

Many direct linear solvers for Ax = b use two stages of computation. First, factor A into a form
amenable to computation (eg. LU factorisation [14, section 3], QR factorisation, SVD factorisation,
etc.) Then, use this factorisation to solve for a given right hand side b. The factorisation of A does not
depend on the right hand side b, and can be reused with various choices of b. This saves computation
cost when solving Ax = b for many right hands b.

For a solver using such a two-stage factorisation approach, init computes the factorisation, and
compute performs the solve for the specific right hand b. transpose computes the transpose of the
factorisation provided by init, which allows us to skip computing the factorisation of the transpose
operator directly (init(transpose(operator))), as it is commonly the case that we can cheaply
derive this from init(operator) alone. This is needed when computing VJPs as discussed in
the previous section. The methods allow_dependent_rows and allow_dependent_columns
determine which equation (1-4) is used in the differentiation, as discussed in section 4.

This greatly simplifies the process of writing a differentiable linear solver or least-squares solver. In
the core JAX library, it is somewhat cumbersome to write a differentiable solver. It requires using
jax.lax.custom_linear_solve and implementing a solver transposition rule transpose_solve
if the user would like to use reverse-mode autodifferentiation. In Lineax, differentiation comes for
free once a solver is implemented, whether the solver is a linear solve or a least-squares algorithm.

6 The AutoLinearSolver polyalgorithm

If the user does not provide a solver to lineax.linear_solve, then the default linear solve
is lineax.AutoLinearSolver. lineax.AutoLinearSolver is a polyalgorithm which selects a
solver automatically at compile time depending on the structure of A, as indicated through its operator
tag (discussed in section 3.1). lineax.AutoLinearSolver takes the argument well_posed, which
indicates whether the system is expected to solve a least-squares/minimum norm problem, or only
handle well-posed linear solves.

lineax.AutoLinearSolver(well_posed=True) selects a solver depending upon the oper-
ator structure, and throws an error when it encounters an underdeteremined or overdeter-
mined system.
lineax.AutoLinearSolver(well_posed=False) solves well-posed linear
solves as well as linear least squares, but often at an additional computation cost. Finally,
lineax.AutoLinearSolver(well_posed=None) solves a least-squares problem only if it is not
expensive to do so.

The specific polyalgorithms for well_posed=True, well_posed=False, and well_posed=None
are shown in figure 1.

6.1 Choosing a solver at compile time

We must choose between two paradigms for the implementation of lineax.AutoLinearSolver:
make the algorithm selection at run time, or make the algorithm selection at compile time. This is a
trade-off, as determining which solver to use at run time means checking the elements of the matrix.
This incurs a run time overhead of O(n2) for an n × n matrix, which is relatively small compared to
the O(n3) run time of most linear solve algorithms. However, run time checking also incurs a greater
cost in compile times. Since it is not known at compile time which branch of the polyalgorithm will
run, the compiler is forced to compile all branches. Thus, compilation cost scales with the logic
of the polyalgorithm: as more branches are included, compile times increase. This can limit the
extensibility of the polyalgorithm.

Compile time selection avoids these performance issues, and is faster both in run time and compile
time when used correctly. Further, it simplifies tying solves to GPU hardware, as there is no possibility
of taking separate branches for different batch elements. However, compile time selection requires
the user to a-priori know the structure of the operator, and can result in using a suboptimal solver if
the operator has exploitable structure which the user does not indicate.

We choose the compile time approach, and require the user to pass the structure of an operator explic-
itly via the operator tag (section 3.1.) We choose this approach primarily to minimise compilation
times, and to avoid the tradeoff between extensibility and compile time inherent in run time checking.

5

The AutoLinearSolver polyalgorithm.

Figure 1:
so that
well_posed=True starts at ""A is square?"", well_posed=None starts at ""A is diagonal?"",
and well_posed=False starts at ""A is diagonal?"" in the well_posed=False section.

Read from left-to-right,

6

Our approach is in contrast to MATLAB’s mldivide, a (nondifferentiable) unified linear solve and
least-squares solver which uses a run time approach [17]. Both the Julia and MATLAB languages
offer methods for nonsingular linear solves – the infix \ operation in Julia and linsolve in MATLAB
– which accept compile time tags, but still perform run time checks if the user passes no tags [2, 7, 17].
Therefore, both suffer from the additional overhead of the run time approach in many cases.

7 Conclusion

We have introduced Lineax, a differentiable JAX+Equinox library unifying linear solves and linear
least-squares. We have demonstrated that users can extend base Lineax operators and solvers and
use them within our unified API, without the need to write any custom derivative rules. We hope to
see adoption of Lineax solves within the JAX+Equinox scientific computing and machine learning
ecosystem.

8 Acknowledgements

This publication is based on work supported by the EPSRC Centre for Doctoral Training in Mathe-
matics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1)

References

[1] Adi Ben-Israel and Thomas N. E. Greville. Generalized Inverses: Theory and Applications.

Springer New York, 2003.

[2] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to
numerical computing. SIAM Review, 59(1):65–98, 2017. URL: https://epubs.siam.org/
doi/10.1137/141000671, doi:10.1137/141000671.

[3] Deniz A. Bezgin, Aaron B. Buhendwa, and Nikolaus A. Adams.

JAX-fluids: A fully-
differentiable high-order computational fluid dynamics solver for compressible two-phase
flows. Computer Physics Communications, 282:108527, 2023.

[4] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James, Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. Jax: composable transformations of python+numpy programs, 2018. URL:
http://github.com/google/jax.

[5] Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and
Olivier Teboul. Optimal transport tools (ott): A jax toolbox for all things wasserstein. arXiv
preprint arXiv:2201.12324, 2022.

[6] Gideon Dresdner, Dmitrii Kochkov, Peter Norgaard, Leonardo Zepeda-Núñez, Jamie A. Smith,
Michael P. Brenner, and Stephan Hoyer. Learning to correct spectral methods for simulating
turbulent flows. arXiv preprint arXiv:2207.00556, 2022.

[7] Chris Rackauckas et al. Linearsolve.jl: High-performance unified linear solvers, 2021. Accessed

2023. URL: https://github.com/SciML/LinearSolve.jl.

[8] David Hall et al. Haliax. Accessed 2023, 2023. URL: https://github.com/

stanford-crfm/haliax.

[9] Igor Babuschkin et al. The deepmind jax ecosystem, 2020. URL: http://github.com/

deepmind.

[10] John Jumper et al. Highly accurate protein structure prediction with alphafold. Nature, 596:583–

589, 8 2021.

[11] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier
Bachem. Brax - a differentiable physics engine for large scale rigid body simulation, 2021.

7

[12] Roy Frostig, Matthew J. Johnson, Dougal Maclaurin, Adam Paszke, and Alexey Radul. Decom-

posing reverse-mode automatic differentiation, 2021. arXiv:2105.09469.

[13] Gene H. Golub and V. Pereyra. The differentiation of pseudo-inverses and nonlinear least squares
problems whose variables separate. SIAM Journal on Numerical Analysis, 10(2):413–432, 1973.

[14] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University

Press, third edition, 1996.

[15] David Hall, Ivan Zhou, and Percy Liang. Levanter — legible, scalable, reproducible foundation
models with jax. Accessed 2023, 2023. URL: https://github.com/stanford-crfm/
levanter.

[16] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL:
http://github.com/google/flax.

[17] The MathWorks Inc. Matlab version: 9.13.0 (r2022b), 1984. URL: https://www.mathworks.

com.

[18] J. Emmanuel Johnson and Takaya Uchida. Jaxsw: Jax approximate ocean models, 2023.

Accessed 2023. URL: https://github.com/jejjohnson/jaxsw.

[19] Patrick Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021.

[20] Patrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTrees
and filtered transformations. Differentiable Programming workshop at Neural Information
Processing Systems 2021, 2021.

[21] Jorge Nocedal and Stephen Wright. Numerical Optimization (Second Edition). Springer New

York, 2006.

[22] Alan V. Oppenheim and Ronald W. Schafer. Digital Signal Processing. Prentice-Hall, 1975.

[23] R. Penrose. A generalized inverse for matrices. Mathematical Proceedings of the Cambridge

Philosophical Society, 51(3):406–413, 1955.

[24] Youcef Saad and Martin H. Schultz. Gmres: A generalized minimal residual algorithm for
solving nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing,
7(3):856–869, 1986.

[25] Miloš Stanojevi´c and Laurent Sartran. SynJax: Structured Probability Distributions for JAX.

arXiv preprint arXiv:2308.03291, 2023.

[26] J. W. Thomas. Numerical Partial Differential Equations: Finite Difference Methods. Springer

New York, 1995.

[27] Lloyd N. Trefethen and David Bau. Numerical Linear Algebra. Society for Industrial and

Applied Mathematics, 1997.

[28] H. A. van der Vorst. Bi-cgstab: A fast and smoothly converging variant of bi-cg for the
solution of nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing,
13(2):631–644, 1992.

[29] Jake Vanderplas. Jep 18137: Scope of jax numpy & scipy wrappers. Github pull request, created

and accessed 2023, 2023. URL: https://github.com/google/jax/pull/18137.

8

","3 2 0 2 v o N 8 2 ] S M . s c [ 1 v 3 8 2 7 1 . 1 1 3 2 : v i X r a Lineax : unified linear solves and linear least-squares in JAX and Equinox Jason Rader Oxford University rader @ maths.ox.ac.uk Terry Lyons Oxford University Patrick Kidger Google X math @ kidger.site Abstract We introduce Lineax , a library bringing linear solves and linear least-squares to the JAX+Equinox scientific computing ecosystem . Lineax uses general linear operators , and unifies linear solves and least-squares into a single , autodifferen- tiable API . Solvers and operators are user-extensible , without requiring the user to implement any custom derivative rules to get differentiability . Lineax is available at https : //github.com/google/lineax . 1 Introduction JAX is an autodifferentiatiable Python framework popular for machine learning and scientific comput- ing [ 4 , 9 , 12 , 16 ] . Equinox [ 20 ] is a popular JAX library [ 8 , 15 ] , targeting the same use cases , that adds additional support for parameterised functions . Solving linear systems , whether well-posed linear solves or ill-posed linear least-squares problems , is a central sub-problem in scientific computing [ 14 , 27 ] . For example , linear solves and least-squares appear as subroutines in nonlinear optimisation [ 21 ] , finite-difference schemes [ 26 ] , and signal processing [ 22 ] . As such , we introduce Lineax , a library built in JAX and Equinox for linear solves and linear least-squares . Lineax presents a single , differentiable interface for solving well-posed , underdetermined , and overdetermined linear systems . It also allows users to write custom differentiable linear solvers or least-squares solvers , and introduces a linear operator abstraction . Overall , we intend for Lineax to integrate well with the existing JAX scientific ecosystem . This ecosystem is growing , and includes packages for differentiable rigid-body physics simulation [ 11 ] , computational fluid dynamics [ 3 , 6 ] , protein structure prediction [ 10 ] , ordinary and stochastic differential equations [ 19 ] , and probabilistic modeling [ 25 ] . We are beginning to see some use of Lineax in this ecosystem already . This includes for linear subroutines in ocean dynamics [ 18 ] and optimal transport [ 5 ] . Further , Diffrax [ 19 ] plans to adopt Lineax in the near future for linear subroutines in differential equations solves . 1.1 Main contributions The main contributions of Lineax are : • A general linear operator abstraction , as implemented by dense matrices , linear functions , Jacobians , etc . • Stable and fast gradients through least-squares solves . This includes through user-defined solvers , without requiring extra effort from the user . • PyTree-valued 1 operators and vectors . 1JAX terminology for arbitrarily nested container ’ node ’ types ( tuples/dictionaries/lists/custom types ) con- taining ’ leaves ’ ( every other Python/JAX type . ) We exclusively consider PyTrees who ’ s leaves are JAX arrays . NeurIPS 2023 AI for Science Workshop . Comparisons to existing JAX APIs The operator abstraction introduced in Lineax offers a flexibility not found in core JAX , which supports only dense matrices or matrix-vector product representations of operators . Lineax introduces new solvers over core JAX , such as lineax.Tridiagonal . Lineax also offers a consistent API between operators and solvers , which is what allows for extensibility to user-specified custom operator and solvers . Compilation times for most Lineax solvers are essentially identical to JAX native solvers ; Lineax ’ s iterative solvers ( CG , GMRES , ... ) compile roughly twice as fast . The ‘ benchmarks ’ folder on GitHub provides a quantitative comparison . We emphasise the stable and fast gradients to contrast with the existing JAX implementation , which as of version 0.4.16 exhibits instability or incorrect gradients in some exceptional cases . For these reasons , JAX is actually considering deprecating some of its own APIs in favour of Lineax [ 29 ] . 1.2 Classical linear solve example Consider solving Ax = b for a random matrix A ∈ R10×10 against a random vector b ∈ R10 . This can be done via 1 2 3 4 5 6 7 import jax . random as jr import lineax as lx A_key , b_key = jr . split ( jr . PRNGKey ( 0 ) ) A = lx . M atr ix Li near Op er ator ( jr . normal ( A_key , ( 10 , 10 ) ) ) b = jr . normal ( b_key , ( 10 , ) ) solution = lx . linear_solve ( A , b ) 2 Performing linear solves and least-squares The main entry point to linear solves and least-squares in Lineax is lineax . linear_solve ( A , b , solver ) for a linear operator A and PyTree b . This performs a linear solve Ax = b ( for well-posed systems ) , or returns a least-squares solution minx ∥Ax − b∥2 ( for overdetermined systems ) , or returns a minimum norm solution minx ∥x∥2 subject to Ax = b ( for underdetermined systems ) . This is a lot of operations to unify together , and it may initially seem strange to do so . The common thread – and our justification for unifying these operations – is that mathematically , all the above operations correspond to the pseudoinverse solution to Ax = b , ie . the solution arising from using the Moore-Penrose pseudoinverse x = A†b [ 1 , 23 ] . The user can specify which solver they ’ d like to use via the solver argument . This is helpful when the user already knows which solvers should work well for a problem . Not every solver is capable of handling every problem . For example , lineax.CG handles positive definite operators [ 21 , section 5 ] . Using a solver with an incompatible problem will result in an error . 3 General linear operators In Lineax , we represent A more generally than as an n × m matrix . Instead , we represent A as a linear operator A : X → Y , where X and Y are spaces of PyTrees of arrays . At an implementation level , a linear operator is an object which subclasses lineax . A bs t ra c tL i ne a r Op e rat or . When A is a dense matrix , A ∈ Rdim ( Y ) ×dim ( X ) , it can be treated as a Lineax linear operator via lineax . Ma trix Li ne ar Op er ator ( A ) . 2 Lineax operators themselves form a vector space , and are closed under addition , scalar multiplication , and composition . Each linear operator A must implement a method to : • Compute the matrix-vector product : Ax for x ∈ X . • Compute the transpose of the operator : AT : Y → X . • Materialise the operator as a matrix : A.as_matrix ( ) ∈ Rdim ( X ) ×dim ( Y ) . • Retrieve the input/output PyTree structure , as well as the input/output dimensions . ie . the functions domain ( A ) = X and codomain ( A ) = Y . This increased generality comes with increased flexibility . For example : large , sparse matrices can use data-efficient formats and utilise linear solves which use only the matrix-vector product , such as GMRES [ 24 ] or BiCGStab [ 28 ] . For example , a linear function f : X → Y can be made into a linear operator with lineax . F un c ti o nL i ne a r Op e rat or ( f , in_structure ) where in_structure describes the PyTree structure of the input of f ( equivalently , the PyTree structure of the elements x ∈ X . ) Similarly , a nonlinear function g : X → Y can be linearised at a point x ∈ X and use its Jacobian at x as a linear operator via lineax . J ac o bi a nL i ne a r Op e rat or ( g , x ) The lineax.AbstractLinearOperator base class is available for users to subclass and create their own linear operator types . 3.1 Operator tags Tags are an optional argument to most linear operators , and indicate properties of the operator A . For example , if A ∈ Rn×n is positive semidefinite , then A can be marked as a positive semidefinite linear operator with lineax . Ma trix Li ne ar Op er ator ( A , lineax . p o s i t i ve _ s e m i d e f i ni t e _ t a g ) This indicates to any solver which uses A that it is positive semidefinite . For example , if A is also nonsingular , then it can be used safely with lineax.CG . Tags are also used to select the appropriate solver in the polyalgorithm lineax.AutoLinearSolver detailed in section 6 . 4 Computing gradients In JAX , derivatives are built from Jacobian-vector products ( JVPs ) and vector-Jacobian products ( VJPs ) for forward-mode and reverse-mode automatic differentiation respectively [ 12 ] . The JVP of a function f : Ra → Rb maps an input-tangent pair ( x , v ) ∈ Ra × Ra to ( f ( x ) , ∂f ( x ) ( v ) ) ∈ Rb × Rb where ∂f ( x ) : Ra → Rb is the Jacobian of f at x . The VJP maps an input-cotangent pair ( x , c ) ∈ Ra × Rb to ( f ( x ) , ∂f ( x ) T c ) ∈ Rb × Ra , where ∂f ( x ) T : Rb → Ra is the transpose of ∂f ( x ) . The major contribution of Lineax over existing linear solve and least-squares software is the efficient computation of JVPs for pseudoinverse solutions . That is , differentiation through both well-posed linear solves and ill-posed least-squares solves are performed in the same manner as each other . In particular , we may special case when operators have full row or column rank in order to obtain improved performance , as we now show . 4.1 JVPs and forward-mode autodifferentiation In this section , let L ( A , b ) denote the linear solve lineax.linear_solve . For a primal problem Ax = b , then L ( A , b ) = A†b where A† be the Moore–Penrose pseudoinverse of A , as mentioned in section 2 . Here we discuss how the to compute the JVP ∂L ( A , b ) ( V , v ) , where ( V , v ) is the tangent pair consisting of a tangent operator V and tangent vector v. It is possible to compute the JVP through either argument . For example , the tangent computation of a linear solve as a function of v alone is ∂L ( A , b ) ( 0 , v ) = A†v 3 ( 2 ) ( 3 ) ( 4 ) where 0 represents the 0 tangent operator . Meanwhile , computing the JVP for ∂L ( A , b ) ( V , 0 ) requires differentiating through a pseudoinverse , which has the explicit formula [ 13 ] ∂L ( A , b ) ( V , 0 ) = ( −A†V A† + A† ( A† ) T V T ( I − AA† ) + ( I − A†A ) V T ( A† ) T A† ) A†b . Letting x = A†b z = V T ( A† ) T x , and adding the above two equations together and using the linearity of the Jacobian , we have the total JVP with respect the primal pair ( A , b ) and tangent pair ( V , v ) for lineax.linear_solve is ∂L ( A , b ) ( V , v ) = A† ( cid:0 ) −V x + ( A† ) T V T ( b − Ax ) − Az + v ( cid:1 ) + z . ( 1 ) If A has linearly independent columns , then A†A = I [ 14 , section 5.5.2 ] and the term z − A†Az = 0 , giving ∂L ( A , b ) ( V , v ) = A† ( cid:0 ) −V x + ( A† ) T V T ( b − Ax ) + v ( cid:1 ) . When A has linearly independent rows , then ( b − Ax ) = 0 and ∂L ( A , b ) ( V , v ) = A† ( −V x − Az + v ) + z . Together , if A has linearly independent rows and columns , then A is well-posed , A† = A−1 is a true inverse , and ∂L ( A , b ) ( V , v ) = A−1 ( −V x + v ) . We then select between equations ( 4 ) , ( 3 ) , ( 2 ) , or ( 1 ) depending on whether we know at compile time that A has linearly independent rows and columns , has only independent rows , has only independent columns , or has both dependent rows and columns . Despite being a property of the operator , at compile time the main way the JVP rule is dispatched via the choice of solver . This is because not every solver supports dependent rows/columns ( section 5 ) , and will return nan values if used in a solve with an unsupported operator . So , if a solver does not support dependent rows/columns , we can be sure we will not get a solution given an operator with dependent rows/columns in the JVP . For example , lineax.QR [ 27 , section 2 ] can handle dependent rows if the number of rows is greater than the number of columns ( or dependent columns if the number of columns is greater than the number of rows ) and will dispatch to either equation ( 2 ) or ( 3 ) . If the number of columns and rows of A are the same , then lineax.QR will dispatch to equation ( 4 ) . 4.2 VJPs and backpropogation via transposition Reverse-mode autodifferentiation of a function f : Ra → Rb is not built on JVPs , but rather on vector-Jacobian products ( VJPs ) vT ∂f ( x ) = ∂f ( x ) T v. As suggested by the definition , VJPs are constructed via a JVP and transposition [ 12 ] . This is how VJPs are implemented in JAX , and thus in Lineax as well . The Jacobian ∂L ( A , b ) : Rm×n × Rm → Rn is a linear function , see also the explicit form in equation ( 1 ) . Therefore , it has a transpose ∂L ( A , b ) T : Rn → Rm×n × Rm . The transpose rule for the linear solve is implemented as a custom JAX primitive . See [ 12 ] for more details . 5 User-defined solvers A user can implement a custom solver by subclassing lineax . Ab stra ct Li ne ar So lver which requires the methods : init , compute , transpose , allow_dependent_rows , and allow_dependent_columns . 4 Many direct linear solvers for Ax = b use two stages of computation . First , factor A into a form amenable to computation ( eg . LU factorisation [ 14 , section 3 ] , QR factorisation , SVD factorisation , etc . ) Then , use this factorisation to solve for a given right hand side b . The factorisation of A does not depend on the right hand side b , and can be reused with various choices of b . This saves computation cost when solving Ax = b for many right hands b . For a solver using such a two-stage factorisation approach , init computes the factorisation , and compute performs the solve for the specific right hand b. transpose computes the transpose of the factorisation provided by init , which allows us to skip computing the factorisation of the transpose operator directly ( init ( transpose ( operator ) ) ) , as it is commonly the case that we can cheaply derive this from init ( operator ) alone . This is needed when computing VJPs as discussed in the previous section . The methods allow_dependent_rows and allow_dependent_columns determine which equation ( 1-4 ) is used in the differentiation , as discussed in section 4 . This greatly simplifies the process of writing a differentiable linear solver or least-squares solver . In the core JAX library , it is somewhat cumbersome to write a differentiable solver . It requires using jax.lax.custom_linear_solve and implementing a solver transposition rule transpose_solve if the user would like to use reverse-mode autodifferentiation . In Lineax , differentiation comes for free once a solver is implemented , whether the solver is a linear solve or a least-squares algorithm . 6 The AutoLinearSolver polyalgorithm If the user does not provide a solver to lineax.linear_solve , then the default linear solve is lineax.AutoLinearSolver . lineax.AutoLinearSolver is a polyalgorithm which selects a solver automatically at compile time depending on the structure of A , as indicated through its operator tag ( discussed in section 3.1 ) . lineax.AutoLinearSolver takes the argument well_posed , which indicates whether the system is expected to solve a least-squares/minimum norm problem , or only handle well-posed linear solves . lineax.AutoLinearSolver ( well_posed=True ) selects a solver depending upon the oper- ator structure , and throws an error when it encounters an underdeteremined or overdeter- mined system . lineax.AutoLinearSolver ( well_posed=False ) solves well-posed linear solves as well as linear least squares , but often at an additional computation cost . Finally , lineax.AutoLinearSolver ( well_posed=None ) solves a least-squares problem only if it is not expensive to do so . The specific polyalgorithms for well_posed=True , well_posed=False , and well_posed=None are shown in figure 1 . 6.1 Choosing a solver at compile time We must choose between two paradigms for the implementation of lineax.AutoLinearSolver : make the algorithm selection at run time , or make the algorithm selection at compile time . This is a trade-off , as determining which solver to use at run time means checking the elements of the matrix . This incurs a run time overhead of O ( n2 ) for an n × n matrix , which is relatively small compared to the O ( n3 ) run time of most linear solve algorithms . However , run time checking also incurs a greater cost in compile times . Since it is not known at compile time which branch of the polyalgorithm will run , the compiler is forced to compile all branches . Thus , compilation cost scales with the logic of the polyalgorithm : as more branches are included , compile times increase . This can limit the extensibility of the polyalgorithm . Compile time selection avoids these performance issues , and is faster both in run time and compile time when used correctly . Further , it simplifies tying solves to GPU hardware , as there is no possibility of taking separate branches for different batch elements . However , compile time selection requires the user to a-priori know the structure of the operator , and can result in using a suboptimal solver if the operator has exploitable structure which the user does not indicate . We choose the compile time approach , and require the user to pass the structure of an operator explic- itly via the operator tag ( section 3.1 . ) We choose this approach primarily to minimise compilation times , and to avoid the tradeoff between extensibility and compile time inherent in run time checking . 5 The AutoLinearSolver polyalgorithm . Figure 1 : so that well_posed=True starts at `` A is square ? `` , well_posed=None starts at `` A is diagonal ? `` , and well_posed=False starts at `` A is diagonal ? '' in the well_posed=False section . Read from left-to-right , 6 Our approach is in contrast to MATLAB ’ s mldivide , a ( nondifferentiable ) unified linear solve and least-squares solver which uses a run time approach [ 17 ] . Both the Julia and MATLAB languages offer methods for nonsingular linear solves – the infix \ operation in Julia and linsolve in MATLAB – which accept compile time tags , but still perform run time checks if the user passes no tags [ 2 , 7 , 17 ] . Therefore , both suffer from the additional overhead of the run time approach in many cases . 7 Conclusion We have introduced Lineax , a differentiable JAX+Equinox library unifying linear solves and linear least-squares . We have demonstrated that users can extend base Lineax operators and solvers and use them within our unified API , without the need to write any custom derivative rules . We hope to see adoption of Lineax solves within the JAX+Equinox scientific computing and machine learning ecosystem . 8 Acknowledgements This publication is based on work supported by the EPSRC Centre for Doctoral Training in Mathe- matics of Random Systems : Analysis , Modelling and Simulation ( EP/S023925/1 ) References [ 1 ] Adi Ben-Israel and Thomas N. E. Greville . Generalized Inverses : Theory and Applications . Springer New York , 2003 . [ 2 ] Jeff Bezanson , Alan Edelman , Stefan Karpinski , and Viral B Shah . Julia : A fresh approach to numerical computing . SIAM Review , 59 ( 1 ) :65–98 , 2017 . URL : https : //epubs.siam.org/ doi/10.1137/141000671 , doi:10.1137/141000671 . [ 3 ] Deniz A. Bezgin , Aaron B. Buhendwa , and Nikolaus A. Adams . JAX-fluids : A fully- differentiable high-order computational fluid dynamics solver for compressible two-phase flows . Computer Physics Communications , 282:108527 , 2023 . [ 4 ] James Bradbury , Roy Frostig , Peter Hawkins , Matthew James , Johnson , Chris Leary , Dougal Maclaurin , George Necula , Adam Paszke , Jake VanderPlas , Skye Wanderman-Milne , and Qiao Zhang . Jax : composable transformations of python+numpy programs , 2018 . URL : http : //github.com/google/jax . [ 5 ] Marco Cuturi , Laetitia Meng-Papaxanthos , Yingtao Tian , Charlotte Bunne , Geoff Davis , and Olivier Teboul . Optimal transport tools ( ott ) : A jax toolbox for all things wasserstein . arXiv preprint arXiv:2201.12324 , 2022 . [ 6 ] Gideon Dresdner , Dmitrii Kochkov , Peter Norgaard , Leonardo Zepeda-Núñez , Jamie A. Smith , Michael P. Brenner , and Stephan Hoyer . Learning to correct spectral methods for simulating turbulent flows . arXiv preprint arXiv:2207.00556 , 2022 . [ 7 ] Chris Rackauckas et al . Linearsolve.jl : High-performance unified linear solvers , 2021 . Accessed 2023 . URL : https : //github.com/SciML/LinearSolve.jl . [ 8 ] David Hall et al . Haliax . Accessed 2023 , 2023 . URL : https : //github.com/ stanford-crfm/haliax . [ 9 ] Igor Babuschkin et al . The deepmind jax ecosystem , 2020 . URL : http : //github.com/ deepmind . [ 10 ] John Jumper et al . Highly accurate protein structure prediction with alphafold . Nature , 596:583– 589 , 8 2021 . [ 11 ] C. Daniel Freeman , Erik Frey , Anton Raichuk , Sertan Girgin , Igor Mordatch , and Olivier Bachem . Brax - a differentiable physics engine for large scale rigid body simulation , 2021 . 7 [ 12 ] Roy Frostig , Matthew J. Johnson , Dougal Maclaurin , Adam Paszke , and Alexey Radul . Decom- posing reverse-mode automatic differentiation , 2021. arXiv:2105.09469 . [ 13 ] Gene H. Golub and V. Pereyra . The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate . SIAM Journal on Numerical Analysis , 10 ( 2 ) :413–432 , 1973 . [ 14 ] Gene H. Golub and Charles F. Van Loan . Matrix Computations . The Johns Hopkins University Press , third edition , 1996 . [ 15 ] David Hall , Ivan Zhou , and Percy Liang . Levanter — legible , scalable , reproducible foundation models with jax . Accessed 2023 , 2023 . URL : https : //github.com/stanford-crfm/ levanter . [ 16 ] Jonathan Heek , Anselm Levskaya , Avital Oliver , Marvin Ritter , Bertrand Rondepierre , Andreas Steiner , and Marc van Zee . Flax : A neural network library and ecosystem for JAX , 2023 . URL : http : //github.com/google/flax . [ 17 ] The MathWorks Inc. Matlab version : 9.13.0 ( r2022b ) , 1984 . URL : https : //www.mathworks . com . [ 18 ] J. Emmanuel Johnson and Takaya Uchida . Jaxsw : Jax approximate ocean models , 2023 . Accessed 2023 . URL : https : //github.com/jejjohnson/jaxsw . [ 19 ] Patrick Kidger . On Neural Differential Equations . PhD thesis , University of Oxford , 2021 . [ 20 ] Patrick Kidger and Cristian Garcia . Equinox : neural networks in JAX via callable PyTrees and filtered transformations . Differentiable Programming workshop at Neural Information Processing Systems 2021 , 2021 . [ 21 ] Jorge Nocedal and Stephen Wright . Numerical Optimization ( Second Edition ) . Springer New York , 2006 . [ 22 ] Alan V. Oppenheim and Ronald W. Schafer . Digital Signal Processing . Prentice-Hall , 1975 . [ 23 ] R. Penrose . A generalized inverse for matrices . Mathematical Proceedings of the Cambridge Philosophical Society , 51 ( 3 ) :406–413 , 1955 . [ 24 ] Youcef Saad and Martin H. Schultz . Gmres : A generalized minimal residual algorithm for solving nonsymmetric linear systems . SIAM Journal on Scientific and Statistical Computing , 7 ( 3 ) :856–869 , 1986 . [ 25 ] Miloš Stanojevi´c and Laurent Sartran . SynJax : Structured Probability Distributions for JAX . arXiv preprint arXiv:2308.03291 , 2023 . [ 26 ] J. W. Thomas . Numerical Partial Differential Equations : Finite Difference Methods . Springer New York , 1995 . [ 27 ] Lloyd N. Trefethen and David Bau . Numerical Linear Algebra . Society for Industrial and Applied Mathematics , 1997 . [ 28 ] H. A. van der Vorst . Bi-cgstab : A fast and smoothly converging variant of bi-cg for the solution of nonsymmetric linear systems . SIAM Journal on Scientific and Statistical Computing , 13 ( 2 ) :631–644 , 1992 . [ 29 ] Jake Vanderplas . Jep 18137 : Scope of jax numpy & scipy wrappers . Github pull request , created and accessed 2023 , 2023 . URL : https : //github.com/google/jax/pull/18137 . 8","['c', 'v', 'r', 'lineax', 'unified', 'linear', 'solve', 'linear', 'leastsquare', 'jax', 'math', 'abstract', 'introduce', 'lineax', 'library', 'bring', 'linear', 'solve', 'linear', 'leastsquare', 'jaxequinox', 'scientific', 'computing', 'ecosystem', 'lineax', 'use', 'general', 'linear', 'operator', 'unify', 'linear', 'solve', 'leastsquare', 'single', 'autodifferen', 'api', 'solver', 'operator', 'userextensible', 'require', 'user', 'implement', 'custom', 'derivative', 'rule', 'get', 'differentiability', 'lineax', 'available', 'githubcomgooglelineax', 'introduction', 'jax', 'autodifferentiatiable', 'python', 'framework', 'popular', 'machine', 'learning', 'scientific', 'comput', 'equinox', 'popular', 'jax', 'library', 'target', 'use', 'case', 'add', 'additional', 'support', 'parameterise', 'function', 'solve', 'linear', 'system', 'wellpose', 'linear', 'solve', 'illpose', 'linear', 'leastsquare', 'problem', 'central', 'subproblem', 'scientific', 'computing', 'example', 'linear', 'solve', 'leastsquare', 'appear', 'subroutine', 'nonlinear', 'optimisation', 'finitedifference', 'scheme', 'signal', 'processing', 'introduce', 'lineax', 'library', 'build', 'jax', 'equinox', 'linear', 'solve', 'linear', 'leastsquare', 'lineax', 'present', 'single', 'differentiable', 'interface', 'solve', 'wellpose', 'underdetermined', 'overdetermine', 'linear', 'system', 'also', 'allow', 'user', 'write', 'custom', 'differentiable', 'linear', 'solver', 'leastsquare', 'solver', 'introduce', 'linear', 'operator', 'abstraction', 'overall', 'intend', 'lineax', 'integrate', 'well', 'exist', 'jax', 'scientific', 'ecosystem', 'ecosystem', 'grow', 'include', 'package', 'differentiable', 'rigidbody', 'physics', 'simulation', 'computational', 'fluid', 'dynamic', 'protein', 'structure', 'prediction', 'ordinary', 'stochastic', 'differential', 'equation', 'probabilistic', 'modeling', 'begin', 'see', 'use', 'lineax', 'ecosystem', 'already', 'include', 'linear', 'subroutine', 'ocean', 'dynamic', 'optimal', 'transport', 'diffrax', 'plan', 'adopt', 'lineax', 'near', 'future', 'linear', 'subroutine', 'differential', 'equation', 'solve', 'main', 'contribution', 'main', 'contribution', 'lineax', 'general', 'linear', 'operator', 'abstraction', 'implement', 'dense', 'matrix', 'linear', 'function', 'jacobian', 'stable', 'fast', 'gradient', 'leastsquare', 'solve', 'include', 'userdefined', 'solver', 'require', 'extra', 'effort', 'user', 'pytreevalue', 'operator', 'vector', '1jax', 'terminology', 'arbitrarily', 'nest', 'container', 'type', 'type', 'taining', 'leave', 'pythonjax', 'type', 'exclusively', 'consider', 'pytree', 'leave', 'jax', 'array', 'neurip', 'ai', 'science', 'workshop', 'comparison', 'exist', 'jax', 'apis', 'operator', 'abstraction', 'introduce', 'lineax', 'offer', 'flexibility', 'find', 'support', 'dense', 'matrix', 'matrixvector', 'product', 'representation', 'operator', 'lineax', 'introduce', 'new', 'solver', 'lineaxtridiagonal', 'lineax', 'also', 'offer', 'consistent', 'api', 'operator', 'solver', 'allow', 'extensibility', 'userspecifie', 'custom', 'operator', 'solver', 'compilation', 'time', 'lineax', 'solver', 'essentially', 'identical', 'jax', 'native', 'solver', 'lineax', 'iterative', 'solver', 'cg', 'gmre', 'compile', 'roughly', 'twice', 'fast', 'benchmark', 'folder', 'provide', 'quantitative', 'comparison', 'emphasise', 'stable', 'fast', 'gradient', 'contrast', 'exist', 'jax', 'implementation', 'version', 'exhibit', 'instability', 'incorrect', 'gradient', 'exceptional', 'case', 'reason', 'actually', 'consider', 'deprecate', 'apis', 'favour', 'lineax', 'classical', 'linear', 'solve', 'example', 'consider', 'solve', 'ax', 'b', 'random', 'matrix', 'r10×10', 'random', 'vector', 'r10', 'import', 'jax', 'random', 'jr', 'import', 'lineax', 'lx', 'akey', 'bkey', 'jr', 'split', 'jr', 'prngkey', 'lx', 'atr', 'op', 'ator', 'normal', 'akey', 'b', 'jr', 'normal', 'bkey', 'solution', 'lx', 'linearsolve', 'b', 'perform', 'linear', 'solve', 'leastsquare', 'main', 'entry', 'point', 'linear', 'solve', 'leastsquare', 'lineax', 'linearsolve', 'b', 'solver', 'linear', 'operator', 'pytree', 'b', 'perform', 'linear', 'solve', 'ax', 'b', 'wellpose', 'system', 'return', 'leastsquare', 'solution', 'minx', 'b∥2', 'overdetermine', 'system', 'return', 'minimum', 'norm', 'solution', 'minx', 'subject', 'ax', 'b', 'underdetermined', 'system', 'lot', 'operation', 'unify', 'together', 'initially', 'seem', 'strange', 'common', 'thread', 'justification', 'unify', 'operation', 'mathematically', 'operation', 'correspond', 'pseudoinverse', 'solution', 'ax', 'b', 'solution', 'arise', 'use', 'moorepenrose', 'pseudoinverse', 'user', 'specify', 'solver', 'like', 'use', 'solver', 'argument', 'helpful', 'user', 'already', 'know', 'solver', 'work', 'well', 'problem', 'solver', 'capable', 'handle', 'problem', 'example', 'handle', 'positive', 'definite', 'operator', 'section', 'use', 'solver', 'incompatible', 'problem', 'result', 'error', 'general', 'linear', 'operator', 'lineax', 'represent', 'generally', 'n', '×', 'matrix', 'instead', 'represent', 'linear', 'operator', 'x', 'space', 'pytree', 'array', 'implementation', 'level', 'linear', 'operator', 'object', 'subclasse', 'lineax', 'bs', 'ne', 'r', 'op', 'e', 'rat', 'dense', 'matrix', 'rdim', 'treat', 'lineax', 'linear', 'operator', 'ator', 'lineax', 'operator', 'form', 'vector', 'space', 'close', 'addition', 'scalar', 'multiplication', 'composition', 'linear', 'operator', 'implement', 'method', 'compute', 'matrixvector', 'product', 'ax', 'compute', 'transpose', 'operator', 'materialise', 'operator', 'matrix', 'retrieve', 'inputoutput', 'pytree', 'structure', 'well', 'inputoutput', 'dimension', 'function', 'domain', 'x', 'codomain', 'increase', 'generality', 'come', 'increase', 'flexibility', 'example', 'large', 'sparse', 'matrix', 'use', 'dataefficient', 'format', 'utilise', 'linear', 'solve', 'use', 'matrixvector', 'product', 'gmre', 'bicgstab', 'example', 'linear', 'function', 'make', 'linear', 'operator', 'ne', 'r', 'op', 'e', 'rat', 'instructure', 'instructure', 'describe', 'pytree', 'structure', 'input', 'equivalently', 'pytree', 'structure', 'element', 'similarly', 'nonlinear', 'function', 'linearise', 'point', '∈', 'use', 'jacobian', 'linear', 'operator', 'ne', 'r', 'op', 'e', 'rat', 'g', 'lineaxabstractlinearoperator', 'base', 'class', 'available', 'user', 'subclass', 'create', 'linear', 'operator', 'type', 'operator', 'tag', 'tag', 'optional', 'argument', 'linear', 'operator', 'indicate', 'property', 'operator', 'example', '∈', 'rn×n', 'positive', 'semidefinite', 'mark', 'positive', 'semidefinite', 'operator', 'ator', 'lineax', 'p', 'e', 'e', 'e', 'g', 'indicate', 'solver', 'use', 'positive', 'semidefinite', 'example', 'also', 'nonsingular', 'use', 'safely', 'lineaxcg', 'tag', 'also', 'use', 'select', 'appropriate', 'solver', 'lineaxautolinearsolver', 'detail', 'section', 'computing', 'gradient', 'jax', 'derivative', 'build', 'product', 'vectorjacobian', 'product', 'vjps', 'reversemode', 'automatic', 'differentiation', 'respectively', 'jvp', 'function', 'rb', 'map', 'inputtangent', 'pair', '∈', 'rb', 'rb', 'jacobian', 'map', 'inputcotangent', 'pair', 'c', 'c', '∈', 'transpose', 'major', 'contribution', 'lineax', 'exist', 'linear', 'solve', 'leastsquare', 'software', 'efficient', 'computation', 'pseudoinverse', 'solution', 'differentiation', 'wellpose', 'linear', 'solve', 'illpose', 'leastsquare', 'solve', 'perform', 'manner', 'particular', 'special', 'case', 'operator', 'full', 'row', 'column', 'rank', 'order', 'obtain', 'improved', 'performance', 'show', 'forwardmode', 'autodifferentiation', 'section', 'let', 'l', 'b', 'denote', 'linear', 'solve', 'lineaxlinearsolve', 'primal', 'problem', 'ax', 'l', 'b', 'a†', 'moore', 'penrose', 'pseudoinverse', 'mention', 'section', 'discuss', 'compute', 'jvp', 'b', 'v', 'v', 'tangent', 'pair', 'consist', 'tangent', 'operator', 'v', 'tangent', 'vector', 'possible', 'compute', 'jvp', 'argument', 'example', 'tangent', 'computation', 'linear', 'solve', 'function', 'v', 'alone', 'b', 'represent', 'tangent', 'operator', 'meanwhile', 'compute', 'jvp', 'b', 'v', 'require', 'differentiate', 'pseudoinverse', 'explicit', 'formula', 'b', 'v', 'a†', 'a†', 'a†', 'a†', 'let', 'z', 'a†', 'add', 'equation', 'together', 'use', 'linearity', 'jacobian', 'total', 'jvp', 'respect', 'primal', 'pair', 'b', 'tangent', 'pair', 'v', 'lineaxlinearsolve', 'b', 'a†', 'cid0', '−v', 'ax', 'cid1', 'z', 'linearly', 'independent', 'column', 'section', 'term', 'z', 'give', '∂l', 'b', 'a†', 'cid0', '−v', 'ax', 'cid1', 'linearly', 'independent', 'row', 'ax', 'b', 'a†', 'z', 'together', 'linearly', 'independent', 'row', 'column', 'wellpose', 'a†', 'true', 'inverse', 'b', 'select', 'equation', 'depend', 'know', 'compile', 'time', 'linearly', 'independent', 'row', 'column', 'independent', 'row', 'independent', 'column', 'dependent', 'row', 'column', 'property', 'operator', 'compile', 'time', 'main', 'way', 'jvp', 'rule', 'dispatch', 'choice', 'solver', 'solver', 'support', 'dependent', 'rowscolumn', 'section', 'return', 'value', 'use', 'solve', 'unsupported', 'operator', 'solver', 'support', 'dependent', 'rowscolumn', 'sure', 'get', 'solution', 'give', 'operator', 'dependent', 'rowscolumn', 'jvp', 'example', 'section', 'handle', 'dependent', 'row', 'number', 'row', 'great', 'number', 'column', 'dependent', 'column', 'number', 'column', 'great', 'number', 'row', 'dispatch', 'equation', 'number', 'column', 'row', 'dispatch', 'equation', 'vjps', 'backpropogation', 'transposition', 'reversemode', 'autodifferentiation', 'function', 'rb', 'build', 'rather', 'vectorjacobian', 'product', 'vjps', 'vt', 'v', 'suggest', 'definition', 'vjps', 'construct', 'jvp', 'transposition', 'vjps', 'implement', 'thus', 'lineax', 'well', 'jacobian', 'b', 'rm×n', '×', 'rm', 'linear', 'function', 'see', 'also', 'explicit', 'form', 'equation', 'therefore', 'transpose', '∂l', 'b', 'rm×n', '×', 'rm', 'transpose', 'rule', 'linear', 'solve', 'implement', 'custom', 'jax', 'primitive', 'see', 'detail', 'userdefined', 'solver', 'user', 'implement', 'custom', 'solver', 'subclasse', 'lver', 'require', 'method', 'init', 'compute', 'transpose', 'allowdependentrow', 'allowdependentcolumn', 'many', 'direct', 'linear', 'solver', 'ax', 'b', 'use', 'stage', 'computation', 'first', 'factor', 'form', 'amenable', 'computation', 'factorisation', 'section', 'factorisation', 'factorisation', 'use', 'factorisation', 'solve', 'give', 'right', 'hand', 'side', 'b', 'factorisation', 'depend', 'right', 'hand', 'side', 'reuse', 'various', 'choice', 'b', 'save', 'computation', 'cost', 'solve', 'ax', 'b', 'many', 'right', 'hand', 'b', 'solver', 'use', 'twostage', 'factorisation', 'approach', 'compute', 'factorisation', 'compute', 'perform', 'solve', 'specific', 'right', 'hand', 'b', 'transpose', 'compute', 'transpose', 'factorisation', 'provide', 'init', 'allow', 'skip', 'compute', 'factorisation', 'transpose', 'operator', 'directly', 'init', 'transpose', 'operator', 'commonly', 'case', 'cheaply', 'derive', 'init', 'operator', 'alone', 'need', 'compute', 'discuss', 'previous', 'section', 'method', 'allowdependentrow', 'allowdependentcolumn', 'determine', 'equation', 'use', 'differentiation', 'discuss', 'section', 'greatly', 'simplify', 'process', 'write', 'differentiable', 'linear', 'solver', 'leastsquare', 'solver', 'library', 'somewhat', 'cumbersome', 'write', 'differentiable', 'solver', 'require', 'use', 'jaxlaxcustomlinearsolve', 'implement', 'solver', 'transposition', 'rule', 'transposesolve', 'user', 'like', 'use', 'reversemode', 'autodifferentiation', 'lineax', 'differentiation', 'come', 'free', 'solver', 'implement', 'solver', 'linear', 'solve', 'leastsquare', 'autolinearsolver', 'polyalgorithm', 'user', 'provide', 'solver', 'lineaxlinearsolve', 'default', 'linear', 'solve', 'lineaxautolinearsolver', 'lineaxautolinearsolver', 'polyalgorithm', 'select', 'solver', 'automatically', 'compile', 'time', 'depend', 'structure', 'indicate', 'operator', 'tag', 'discuss', 'section', 'lineaxautolinearsolver', 'take', 'argument', 'wellpose', 'indicate', 'system', 'expect', 'solve', 'leastsquaresminimum', 'norm', 'problem', 'handle', 'wellpose', 'linear', 'solve', 'lineaxautolinearsolver', 'wellposedtrue', 'select', 'solver', 'depend', 'oper', 'ator', 'structure', 'throw', 'error', 'encounter', 'underdeteremined', 'overdeter', 'mined', 'system', 'lineaxautolinearsolver', 'wellposedfalse', 'solve', 'wellpose', 'linear', 'solve', 'well', 'linear', 'least', 'square', 'often', 'additional', 'computation', 'cost', 'finally', 'lineaxautolinearsolver', 'wellposednone', 'solve', 'leastsquare', 'problem', 'expensive', 'specific', 'polyalgorithm', 'wellposedtrue', 'wellposedfalse', 'wellposednone', 'show', 'figure', 'choose', 'solver', 'compile', 'time', 'choose', 'paradigm', 'implementation', 'lineaxautolinearsolver', 'make', 'selection', 'run', 'time', 'make', 'selection', 'compile', 'time', 'tradeoff', 'determine', 'solver', 'use', 'run', 'time', 'mean', 'check', 'element', 'matrix', 'incur', 'run', 'time', 'overhead', 'n2', 'n', 'n', 'matrix', 'relatively', 'small', 'compare', 'n3', 'run', 'time', 'linear', 'solve', 'algorithm', 'however', 'run', 'time', 'check', 'also', 'incur', 'great', 'cost', 'compile', 'time', 'know', 'compile', 'time', 'branch', 'polyalgorithm', 'run', 'compiler', 'force', 'compile', 'branch', 'thus', 'compilation', 'cost', 'scale', 'logic', 'polyalgorithm', 'branch', 'include', 'compile', 'time', 'increase', 'limit', 'extensibility', 'compile', 'time', 'selection', 'avoid', 'performance', 'issue', 'fast', 'run', 'time', 'compile', 'time', 'use', 'correctly', 'simplify', 'tie', 'solve', 'hardware', 'possibility', 'take', 'separate', 'branch', 'different', 'batch', 'element', 'however', 'compile', 'time', 'selection', 'require', 'user', 'know', 'structure', 'operator', 'result', 'use', 'suboptimal', 'solver', 'operator', 'exploitable', 'structure', 'user', 'indicate', 'choose', 'compile', 'time', 'approach', 'require', 'user', 'pass', 'structure', 'operator', 'explic', 'itly', 'operator', 'tag', 'section', 'choose', 'approach', 'primarily', 'minimise', 'compilation', 'time', 'avoid', 'tradeoff', 'extensibility', 'compile', 'time', 'inherent', 'run', 'time', 'check', 'autolinearsolver', 'polyalgorithm', 'figure', 'wellposedtrue', 'start', 'square', 'wellposednone', 'start', 'diagonal', 'wellposedfalse', 'start', 'diagonal', 'section', 'read', 'lefttoright', 'approach', 'contrast', 'mldivide', 'nondifferentiable', 'unified', 'linear', 'solve', 'leastsquare', 'solver', 'use', 'run', 'time', 'approach', 'language', 'offer', 'method', 'nonsingular', 'linear', 'solve', 'infix', 'operation', 'linsolve', 'accept', 'compile', 'time', 'tag', 'still', 'perform', 'run', 'time', 'check', 'user', 'pass', 'tag', 'therefore', 'suffer', 'additional', 'overhead', 'run', 'time', 'approach', 'many', 'case', 'conclusion', 'introduce', 'lineax', 'differentiable', 'jaxequinox', 'library', 'unify', 'linear', 'solve', 'linear', 'leastsquare', 'demonstrate', 'user', 'extend', 'base', 'lineax', 'operator', 'solver', 'use', 'unified', 'api', 'need', 'write', 'custom', 'derivative', 'rule', 'hope', 'see', 'adoption', 'lineax', 'solve', 'jaxequinox', 'scientific', 'computing', 'machine', 'learn', 'ecosystem', 'acknowledgement', 'publication', 'base', 'work', 'support', 'epsrc', 'centre', 'doctoral', 'training', 'matic', 'random', 'system', 'analysis', 'modelling', 'simulation', 'reference', 'adi', 'generalize', 'inverse', 'theory', 'application', 'viral', 'shah', 'fresh', 'approach', 'url', 'https', 'epubssiamorg', 'doi101137141000671', 'doi101137141000671', 'deniz', 'bezgin', 'buhendwa', 'nikolaus', 'adam', 'jaxfluid', 'fully', 'differentiable', 'highorder', 'computational', 'fluid', 'dynamic', 'solver', 'compressible', 'twophase', 'flow', 'computer', 'physics', 'communication', 'frostig', 'dougal', 'vanderpla', 'skye', 'wandermanmilne', 'jax', 'composable', 'transformation', 'pythonnumpy', 'program', 'url', 'http', 'githubcomgooglejax', 'marco', 'olivi', 'teboul', 'optimal', 'transport', 'tool', 'ott', 'jax', 'toolbox', 'thing', 'wasserstein', 'arxiv', 'preprint', 'dresdner', 'dmitrii', 'jamie', 'brenner', 'hoyer', 'learn', 'correct', 'spectral', 'method', 'simulate', 'turbulent', 'flow', 'arxiv', 'preprint', 'arxiv220700556', 'highperformance', 'unify', 'linear', 'solver', 'access', 'url', 'https', 'haliax', 'access', 'url', 'stanfordcrfmhaliax', 'igor', 'babuschkin', 'ecosystem', 'url', 'deepmind', 'highly', 'accurate', 'protein', 'structure', 'prediction', 'alphafold', 'nature', 'c', 'anton', 'raichuk', 'sertan', 'girgin', 'igor', 'mordatch', 'olivi', 'bachem', 'brax', 'differentiable', 'physics', 'engine', 'large', 'scale', 'rigid', 'body', 'simulation', 'roy', 'frostig', 'paszke', 'pose', 'reversemode', 'automatic', 'differentiation', 'arxiv210509469', 'gene', 'h', 'golub', 'pereyra', 'differentiation', 'pseudoinverse', 'nonlinear', 'least', 'square', 'problem', 'variable', 'separate', 'numerical', 'analysis', 'gene', 'h', 'golub', 'loan', 'matrix', 'computation', 'press', 'third', 'edition', 'levanter', 'legible', 'scalable', 'reproducible', 'foundation', 'model', 'jax', 'access', 'url', 'https', 'githubcomstanfordcrfm', 'levanter', 'ritter', 'flax', 'neural', 'network', 'library', 'ecosystem', 'jax', 'url', 'http', 'githubcomgoogleflax', 'url', 'https', 'wwwmathwork', 'com', 'approximate', 'ocean', 'model', 'access', 'url', 'kidger', 'neural', 'differential', 'equation', 'phd', 'thesis', 'kidger', 'equinox', 'neural', 'network', 'callable', 'pytree', 'filter', 'transformation', 'differentiable', 'programming', 'workshop', 'neural', 'information', 'processing', 'system', 'jorge', 'nocedal', 'optimization', 'second', 'edition', 'digital', 'signal', 'processing', 'prenticehall', 'r', 'penrose', 'generalized', 'inverse', 'matrix', 'mathematical', 'proceeding', 'cambridge', 'philosophical', 'society', 'schultz', 'gmre', 'generalized', 'minimal', 'residual', 'algorithm', 'solve', 'nonsymmetric', 'scientific', 'statistical', 'computing', 'miloš', 'stanojevi´c', 'laurent', 'sartran', 'synjax', 'structured', 'probability', 'distribution', 'jax', 'arxiv', 'preprint', 'numerical', 'partial', 'differential', 'equation', 'finite', 'difference', 'method', 'algebra', 'society', 'industrial', 'apply', 'mathematic', 'h', 'der', 'bicgstab', 'fast', 'smoothly', 'converge', 'variant', 'bicg', 'solution', 'nonsymmetric', 'scientific', 'statistical', 'computing', 'jake', 'vanderpla', 'jep', 'scope', 'numpy', 'scipy', 'wrapper', 'pull', 'request', 'create', 'access', 'url']",
"Lineax: unified linear solves and linear least-squares in JAX and
  Equinox","[{'href': 'http://arxiv.org/abs/2311.17283v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17283v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-28 23:50:08,"3
2
0
2

v
o
N
2

]

G
L
.
s
c
[

1
v
0
5
4
1
0
.
1
1
3
2
:
v
i
X
r
a

DREAMSMOOTH: IMPROVING MODEL-BASED REIN-
FORCEMENT LEARNING VIA REWARD SMOOTHING

Vint Lee, Pieter Abbeel, Youngwoon Lee
University of California, Berkeley

ABSTRACT

Model-based reinforcement learning (MBRL) has gained much attention for its
ability to learn complex behaviors in a sample-efficient way: planning actions
by generating imaginary trajectories with predicted rewards. Despite its success,
we found that surprisingly, reward prediction is often a bottleneck of MBRL,
especially for sparse rewards that are challenging (or even ambiguous) to predict.
Motivated by the intuition that humans can learn from rough reward estimates, we
propose a simple yet effective reward smoothing approach, DreamSmooth, which
learns to predict a temporally-smoothed reward, instead of the exact reward at the
given timestep. We empirically show that DreamSmooth achieves state-of-the-art
performance on long-horizon sparse-reward tasks both in sample efficiency and
final performance without losing performance on common benchmarks, such as
Deepmind Control Suite and Atari benchmarks.

1

INTRODUCTION

Humans often plan actions with a rough estimate
of future rewards, instead of the exact reward at
the exact moment (Fiorillo et al., 2008; Klein-
Fl¨ugge et al., 2011). A rough reward estimate is
mostly sufficient to learn a task, and predicting
the exact reward is often challenging since it
can be ambiguous, delayed, or not observable.
Consider for instance the manipulation task il-
lustrated in Figure 1 (middle) of pushing a block
on a table into a bin, where a sparse reward is
given only on the timestep when the block first
touches the bin. Using the same image observa-
tions as the agent, it is challenging even for hu-
mans to predict the correct sequence of rewards.
Crucially, this issue is present in many environ-
ments, where states with no reward are almost
indistinguishable from those with rewards.

Figure 1: Predicting the exact sequence of rewards
is extremely difficult. These examples show the
sequences of image observations seen by the agent
just before and after it receives a sparse reward.
There is little to visually distinguish timesteps with
a large reward from those without, which creates a
significant challenge for reward prediction.

An accurate reward model is vital to model-
based reinforcement learning (MBRL) – reward
estimates that are too high will cause an agent
to choose actions that perform poorly in reality,
and estimates that are too low will lead an agent
to ignore high rewards. Despite its difficulty and
importance, the reward prediction problem in
MBRL has been largely overlooked. We find that even for the state-of-the-art MBRL algorithm,
DreamerV3 (Hafner et al., 2023), reward prediction is not only challenging, but is also a performance
bottleneck for many tasks. For instance, DreamerV3 fails to predict any reward for most objectives
in the Crafter environment (Hafner, 2022) with similar failure modes observed on variants of the
RoboDesk (Kannan et al., 2021) and Shadow Hand (Plappert et al., 2018) tasks with sparse rewards.

1

 
 
 
 
 
 
Inspired by the human intuition that only a rough estimate of rewards is sufficient, we propose a
simple yet effective solution, DreamSmooth, which learns to predict a temporally-smoothed reward
rather than the exact reward at each timestep. This makes reward prediction much easier – instead of
having to predict rewards exactly, now the model only needs to produce an estimate of when sparse
rewards are obtained, which is sufficient for policy learning.

Our experiments demonstrate that while extremely simple, this technique significantly improves
performance of different MBRL algorithms on many sparse-reward environments. Specifically, we
find that for DreamerV3 (Hafner et al., 2023) and TD-MPC (Hansen et al., 2022), our technique
is especially beneficial in environments with the following characteristics: sparse rewards, partial
observability, and stochastic rewards. Finally, we show that even on benchmarks where reward
prediction is not a significant issue, DreamSmooth does not degrade performance, which indicates
that our technique can be universally applied.

2 RELATED WORK

Model-based reinforcement learning (MBRL) leverages a dynamics model (i.e. world model) of an
environment and a reward model of a desired task to plan a sequence of actions that maximize the
total reward. The dynamics model predicts the future state of the environment after taking a specific
action and the reward model predicts the reward corresponding to the state-action transition. With
the dynamics and reward models, an agent can simulate a large number of candidate behaviors in
imagination instead of in the physical environment, allowing MBRL to tackle many challenging
tasks (Silver et al., 2016; 2017; 2018).

Instead of relying on the given dynamics and reward models, recent advances in MBRL have enabled
learning a world model of high-dimensional observations and complex dynamics (Ha & Schmidhuber,
2018; Schrittwieser et al., 2020; Hafner et al., 2019; 2021; 2023; Hansen et al., 2022), as well as a
temporally-extended world model (Shi et al., 2022). Specifically, DreamerV3 (Hafner et al., 2023)
has achieved the state-of-the-art performance across diverse domains of problems, e.g., both with
pixel and state observations as well as both with discrete and continuous actions.

For realistic imagination, MBRL requires an accurate world model. There have been significant
efforts in learning better world models by leveraging human videos (Mendonca et al., 2023), by
adopting a more performant architecture (Deng et al., 2023), and via representation learning, such
as prototype-based (Deng et al., 2022) and object-centric (Singh et al., 2021) state representations,
contrastive learning (Okada & Taniguchi, 2021), and masked auto-encoding (Seo et al., 2022; 2023).

However, compared to the efforts on learning a better world model, learning an accurate reward
model has been largely overlooked. Babaeizadeh et al. (2020) investigates the effects of various world
model designs and shows that reward prediction is strongly correlated to task performance when
trained on an offline dataset, while limited to dense-reward environments. In this paper, we point out
that accurate reward prediction is crucial for MBRL, especially in sparse-reward tasks and partially
observable environments, and propose a simple method to improve reward prediction in MBRL.

3 APPROACH

The main goal of this paper is to understand how challenging reward prediction is in model-based
reinforcement learning (MBRL) and propose a simple yet effective solution, reward smoothing, which
makes reward prediction easier to learn. In this section, we first provide a background about MBRL
in Section 3.1, then present experiments demonstrating the challenge of predicting sparse reward
signals in Section 3.2, and finally explain our approach, DreamSmooth, in Section 3.4.

3.1 BACKGROUND

We formulate a problem as a partially observable Markov decision process (POMDP), which is
defined as tuple (O, A, P, R, γ). O is an observation space, A is an action space, P (ot+1|o≤t, a≤t)
with timestep t is a transition dynamics, R is a reward function that maps previous observations and
actions to a reward rt = R(o≤t, a≤t), and γ ∈ [0, 1) is a discount factor (Sutton & Barto, 2018). RL
aims to find a policy π(at | o≤t, a<t) that maximizes the expected sum of rewards Eπ[(cid:80)T
t=1 γt−1rt].

2

This paper focuses on MBRL algorithms that learn a world model Pθ(zt+1|zt, at) and reward model
Rθ(rt|zt) from agent experience, where zt is a learned latent state at timestep t. The learned world
model and reward model can then generate imaginary rollouts {zτ , aτ , rτ }t+H−1
of the horizon H
starting from any zt, which can be used for planning (Argenson & Dulac-Arnold, 2021; Hansen et al.,
2022) or policy optimization (Ha & Schmidhuber, 2018; Hafner et al., 2019). Specifically, we use the
state-of-the-art algorithms, DreamerV3 (Hafner et al., 2023) and TD-MPC (Hansen et al., 2022).

τ =t

DreamerV3 (Hafner et al., 2023) uses the predicted rewards for computing new value targets to train
the critic. For learning a good policy, the reward model plays a vital role since the critic, from which
the actor learns a policy, receives its training signal exclusively through the reward model. Note that
the data collected from the environment is only used for training a world model and reward model.

On the other hand, TD-MPC (Hansen et al., 2022) learns a state-action value function Q(zt, at)
directly from agent experience, not from predicted rewards. However, the reward model is still
important for obtaining a good policy in TD-MPC because the algorithm uses both the reward model
and value function to obtain the policy through online planning.

3.2 REWARD PREDICTION IS DIFFICULT

Reward prediction is surprisingly challenging in many environments. Figure 1 shows sequences of
frames right before and after sparse rewards are received in diverse environments. Even for humans,
it is difficult to determine the exact timestep when the reward is received in all three environments.
We hypothesize that the mean squared error loss E(z,r)∼D[(Rθ(z) − r)2], typically used for reward
model training, deteriorates reward prediction accuracy when there exist sparse rewards. This is
because predicting a sparse reward a single step earlier or later results in a higher loss than simply
predicting 0 reward at every step. Thus, instead of trying to predict sparse rewards at the exact
timesteps, a reward model minimizes the loss by entirely omitting sparse rewards from its predictions.

To verify this hypothesis, we plot the ground-truth and DreamerV3’s predicted rewards in Figure 2.
The reward models struggle at predicting exact rewards and simply ignore sparse rewards unless they
are straightforward to predict on the four tasks described in Section 4.1. This hypothesis also holds in
a deterministic and fully-observable environment, Crafter, which has 24 sources of sparse rewards.
The reward model fails to predict most of these reward sources (Figure 2d).

The difficulty of reward prediction can be further exacerbated by partial observability, ambiguous
rewards, or stochastic dynamics of environments. As an example in the first (third) row in Figure 1,

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

Figure 2: Ground truth rewards and DreamerV3’s predicted rewards over an evaluation episode. The
reward model misses many sparse rewards, which is highlighted in yellow.

3

Ground Truth

Predicted

x-axis: Timestep

y-axis: Reward

330

0

−110

0

120

330

0

−110

0

120

2400

0

−800

0

360

1.2

0.0

−0.4

0

200

the sparse rewards are given when the block (the rocks in the third example) first contacts the bin (the
dumptruck). The exact moment of contact is not directly observable from the camera viewpoint, and
this makes reward prediction ambiguous. Moreover, stochastic environment dynamics, e.g., contact
between multiple rocks, can make predicting a future state and reward challenging.

3.3 REWARD PREDICTION IS A BOTTLENECK OF MBRL

The preceding section shows that reward prediction is challenging in many environments. More
importantly, this poor reward prediction can be a bottleneck of policy learning, as shown in Figure 3.
In RoboDesk, where the reward model does not reliably detect the completion of the second task
(Figure 2a), the policy gets stuck at solving the first task and fails on subsequent tasks. In Earthmoving,
where the reward model cannot capture rewards for successful dumping (Figure 2c), the policy
frequently drops the rocks outside the dumptruck. These consistent failure modes in reward prediction
and policy learning in DreamerV3 suggest that poor reward prediction can be a bottleneck of MBRL.

(a) RoboDesk

(b) Earthmoving

Figure 3: The reward model’s inability to predict sparse rewards for completing tasks leads to poor
task performance. (a) In RoboDesk, the agent gets stuck after learning the first task, and is unable to
learn to perform the subsequent tasks. (b) In Earthmoving, the policy often fails to dump the rocks
accurately into the dumptruck. The learning curves are averaged over 3 seeds.

3.4 DREAMSMOOTH: IMPROVING MBRL VIA REWARD SMOOTHING

To address the reward prediction problem,
we propose a simple yet effective solution,
DreamSmooth, which relaxes the require-
ment for the model to predict sparse rewards
at the exact timesteps by performing tempo-
ral smoothing. Allowing the reward model to
predict rewards that are off from the ground
truth by a few timesteps makes learning eas-
ier, especially when rewards are ambiguous
or sparse.

(a) Gaussian

(b) Uniform

(c) EMA

Figure 4: Reward smoothing on sparse reward 1 at
t = 4. σ, δ, and α are smoothing hyperparameters.

Specifically, DreamSmooth applies temporal smoothing to the rewards upon collecting each new
episode. DreamSmooth can work with any smoothing function f that preserves the sum of rewards:

˜rt ← f (rt−L:t+L) =

L
(cid:88)

i=−L

fi · rclip(t+i,0,T )

s.t.

L
(cid:88)

i=−L

fi = 1,

(1)

where T and L denote the episode and smoothing horizons, respectively. For simplicity, we omit the
discount factor in Equation (1); the full equation can be found in Appendix, Equation (6). Episodes
with the smoothed rewards are stored in the replay buffer and used to train the reward model. The
agent learns only from the smoothed rewards, without ever seeing the original rewards. The smoothed
rewards ease reward prediction by allowing the model to predict rewards several timesteps earlier or
later, without incurring large losses. In this paper, we investigate three popular smoothing functions:
Gaussian, uniform, and exponential moving average (EMA) smoothing, as illustrated in Figure 4.

While the main motivation for smoothing is to make it easier to learn reward models, we note that
reward smoothing in some cases preserves optimality – an optimal policy under smoothed rewards ˜r

4

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

8

16

24

Environment steps (×10⁶)

 
2

1

d
e
p
m
u
D
s
k
c
o
R

0

0

4

8

12

Environment steps (×10⁶)

 
1

d
r
a
w
e
R

0

0

Orginal
σ = 1
σ = 2

2

4
Timestep

6

8

1

d
r
a
w
e
R

0

0

Orginal
δ = 3
δ = 7

2

4
Timestep

6

8

1

d
r
a
w
e
R

0

0

Orginal
α = .3
α = .6

2

4
Timestep

6

8

is also optimal under the original rewards r. In particular, we provide a proof in Appendix A for the
optimality of EMA smoothing (and any smoothing function where ∀i > 0, fi = 0) by augmenting the
POMDP states with the history of past states. However, when future rewards are used for smoothing
(e.g. Gaussian smoothing), the smoothed rewards are conditioned on policy, and we can no longer
define an equivalent POMDP. In such cases, there is no theoretical guarantee. Even so, we empirically
show that reward models can adapt their predictions alongside the changing policy, and achieve
performance improvements.

The implementation of DreamSmooth is extremely simple, requiring only one additional line of
code to existing MBRL algorithms, as shown in Algorithm 1. The overhead of reward smoothing is
minimal, with time complexity O(T · L). More implementation details can be found in Appendix B.

Algorithm 1 COLLECT ROLLOUT (π: policy, D: replay buffer) in DREAMSMOOTH

t=1} ← ROLLOUT(π)

{(ot, at, rt)T
{rt}T
D ← D ∪ {(ot, at, rt)T

t=1 ← GAUSSIAN({rt}T

t=1}

t=1, σ) or EMA({rt}T

t=1, α)

▷ only one line needs to be added.

4 EXPERIMENTS

In this paper, we propose a simple reward smoothing method, DreamSmooth, which facilitates reward
prediction in model-based reinforcement learning (MBRL) and thus, improves the performance of
existing MBRL methods. Through our experiments, we aim to answer the following questions:
(1) Does reward smoothing improve reward prediction? (2) Does better reward prediction with reward
smoothing lead to better sample efficiency and asymptotic performance of MBRL in sparse-reward
tasks? (3) Does MBRL with reward smoothing also work in common dense-reward tasks?

4.1 TASKS

We evaluate DreamSmooth on four tasks with sparse subtask completion rewards and two common
RL benchmarks. Earthmoving uses two 64 × 64 images as an observation while all other tasks use a
single image. See Appendix C for environment details.

• RoboDesk: We use a modified version of RoboDesk (Kannan et al., 2021), where a sequence of ma-
nipulation tasks (flat block in bin, upright block off table, push green)
need to be completed in order (Figure 5a). We use the original dense rewards together with
a large sparse reward for each task completed.

• Hand: The Hand task (Plappert et al., 2018) requires a Shadow Hand to rotate a block in hand into
a specific orientation. We extend it to achieve a sequence of pre-defined goal orientations in order.
In addition to the original dense rewards, we provide a large sparse reward for each goal.

• Earthmoving: The Earthmoving task consists of a wheel loader, dump truck, and a pile of rocks
(Figure 5c). The agent controls the wheel loader to pick up rocks from the pile and dump them in
the dump truck. A large sparse reward is given for each rock picked up and for each rock dumped,
proportional to its mass. In addition, dense rewards are given for moving rocks towards the dump
truck. The environment is simulated using the AGX Dynamics physics engine (Algoryx, 2020).

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

(e) DMC

(f) Atari

Figure 5: We evaluate DreamSmooth on four tasks with sparse subtask completion rewards (a-d). We
also test on two popular benchmarks, (e) DeepMind Control Suite and (f) Atari.

5

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

Figure 6: We visualize the ground truth rewards, smoothed rewards with Gaussian smoothing, and
predicted rewards by DreamerV3 trained on the smoothed rewards over an evaluation episode. In
contrast to Figure 2, the reward models with reward smoothing capture most of sparse rewards.

• Crafter: Crafter (Hafner, 2022) is a minecraft-like 2D environment, where the agent tries to
collect, place, and craft items in order to survive. There are 22 achievements in the environment
(e.g. collecting water, mining diamonds) with a sparse reward 1 for obtaining each achievement for
the first time. A small reward is given (or lost) for each health point gained (or lost).

• DMC: We benchmark 7 DeepMind Control Suite continuous control tasks (Tassa et al., 2018).
• Atari: We benchmark 6 Atari tasks (Bellemare et al., 2013) at 100K steps.

4.2

IMPROVED REWARD PREDICTION WITH REWARD SMOOTHING

We first visualize the ground truth rewards,
smoothed rewards (Gaussian smoothing), and re-
ward prediction results of DreamerV3 trained with
DreamSmooth in Figure 6. We observe that re-
ward smoothing leads to a significant improve-
ment in reward prediction: DreamSmooth success-
fully predicts most of the (smoothed) sparse re-
wards and no longer omits vital signals for policy
learning or planning.

The improvement is especially notable in Crafter.
In Figure 7, we measure the accuracy of the re-
ward model, (i.e. predicting a reward larger than
half of the original or smoothed reward for Dream-
erV3 and DreamSmooth respectively) at the exact
timesteps for each subtask. The vanilla Dream-
erV3’s reward model (baseline) misses most of
the sparse rewards while DreamSmooth predicts
sparse rewards more accurately in 15/19 subtasks.

4.3 RESULTS

Figure 7: Reward prediction rates for 19 achieve-
ments in Crafter. The other 3 tasks have been
never achieved by both methods. With reward
smoothing, the prediction rates are better in
15/19 tasks.

We compare the vanilla DreamerV3 (Hafner et al., 2023) with DreamSmooth, whose backbone is
also DreamerV3. For DreamSmooth, we evaluate Gaussian, uniform, and EMA smoothing. The

6

Ground Truth

Smoothed

Predicted

x-axis: Timestep

y-axis: Reward

64

00

0

120

60

0

−20

0

120

800

0

−200

0

360

0.6

0.0

−0.2

0

200

Baseline (no smoothing)

Gaussian

1.0

0.5

e
t
a
R
n
o
i
t
c
i
d
e
r
P

0.0

w

k

al

d
e
ain
alth g
e
H

nt

e
n

ace

d
e
n
o
o

bie
alth loss
g
xe
xe
ord
n
ord
ollect drin
plin
at skeleto
at co
ollect co
Place pla
d picka
e picka
Place sto
ollect sto
Place ta
m
d sw
e sw
Place furn
ollect w
ollect sa
at zo
E
ke sto
n
efe
ke w
ke sto
ke w
D
a
M

efe
C
D

n
o
o

o
o

C

C

C

C

e
H

p

ble
ke u
Wa

a
M
a
M

a
M

 
(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

(e) DMC

(f) Atari

Figure 8: Comparison of learning curves of DreamSmooth (Gaussian, Uniform, EMA) and Dream-
erV3. The shaded regions in (a-d) show the maximum and minimum over 3 seeds. For DMC (e) and
Atari (f), we aggregate results over 7 and 6 tasks respectively, and display the standard deviation.

hyperparameters for DreamerV3 and smoothing functions can be found in Appendix B. As shown in
Figure 8, DreamSmooth-Gaussian and DreamSmooth-Uniform significantly improve the performance
as well as the sample efficiency of DreamerV3 on the Robodesk, Hand, and Earthmoving tasks. The
only change between DreamerV3 and ours is the improved reward prediction, as shown in Section 4.2.
This result suggests that reward prediction is one of major bottlenecks of the MBRL performance.

While all smoothing methods lead to improvements over DreamerV3, Gaussian smoothing generally
performs the best, except on Crafter, with uniform smoothing showing comparable performance. The
better performance of Gaussian and uniform smoothing could be because it allows predicting rewards
both earlier and later, whereas EMA smoothing only allows predicting rewards later.

Despite the improved reward prediction accuracy, DreamSmooth-Gaussian and DreamSmooth-
Uniform perform worse than the baseline in Crafter. This can be because more predicted task
rewards encourage more exploitation and less exploration. Further investigation on this trade-off is a
promising direction for future work.

Moreover, we observe that on the DMC and Atari benchmarks, where reward prediction is not particu-
larly challenging, our technique shows comparable performance with the unmodified algorithms (see
Appendix, Figure 16 for full results). This suggests that reward smoothing can be applied generally,
and does not hinder performance on most environments.

7

DreamSmooth-Gaussian

DreamSmooth-Uniform

DreamSmooth-EMA

DreamerV3

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

18
12
6
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

15
10
5
Environment steps (×10⁶)

20

 
d
e
p
m
u
D
s
k
c
o
R
f
o

r
e
b
m
u
N

2.0

1.5

1.0

0.5

0.0

0

3

6
Environment steps (×10⁶)

9

12

 
 
 
16

12

8

4

e
r
o
c
S

0
0.0

1.5
4.5
3.0
Environment steps (×10⁶)

6.0

1000

750

500

250

e
r
o
c
S

0
0.00

0.25
0.75
0.50
Environment steps (×10⁶)

1.00

e
r
o
c
S

d
e
z
i
l

a
m
r
o
N
n
a
m
u
H

0.6

0.4

0.2

0.0

0.0

0.1
0.3
0.2
Environment steps (×10⁶)

0.4

 
 
In Figure 9, DreamSmooth also improves
the performance of TD-MPC (Hansen et al.,
2022). In the Hand task, vanilla TD-MPC
is unable to consistently solve the first task,
even with proprioceptive state observations.
However, TD-MPC with DreamSmooth
learns to complete the tasks with not only
state observations but also pixel observa-
tions. This suggests that DreamSmooth can
be useful in a broad range of MBRL algo-
rithms that use a reward model. We only
demonstrate the Hand task since TD-MPC
fails on other sparse-reward tasks.

4.4 ABLATION STUDIES

(a) Hand (Pixel)

(b) Hand (State)

Figure 9: Learning curves for TD-MPC and TD-MPC
with DreamSmooth on the Hand task. The shaded re-
gions show the minimum and maximum over 3 seeds.

Data Imbalance. One possible cause of poor
reward predictions is data imbalance – because
sparse rewards are infrequent, sequences con-
taining sparse rewards are rarely sampled from
the replay buffer. The reward model therefore
trains on fewer examples of sparse rewards, po-
tentially leading to poor predictions. To test
this hypothesis, we conducted experiments with
oversampling: with probability p = 0.5, we
sample a sequence in which the agent receives
a sparse reward; otherwise, we sample uni-
formly from all sequences in the buffer. As
shown in Figure 10, oversampling performs bet-
ter than the baseline, but learns slower than
DreamSmooth. This suggests that while data
imbalance largely contributes to the difficulty
of reward prediction, it is not the only factor
hindering performance. Furthermore, this over-
sampling method requires domain knowledge about which reward signals to be oversampled while
DreamSmooth is agnostic to the scale and frequency of sparse rewards.

Figure 10: Using oversampling of sequences
with sparse rewards (p = 0.5) performs better
than DreamerV3 on RoboDesk, but worse than
DreamSmooth with Gaussian smoothing. The lines
show median task performance over 3 seeds, while
shaded regions show maximum and minimum.

Reward Model Size. Another hypothe-
sis for poor reward predictions is that the
reward model does not have enough ca-
pacity to capture sparse rewards. To test
this hypothesis, we increase the size of the
reward model from 4 layers of 768 units
to 5 layeres of 1024 units and 6 layers of
1280 units, while keeping the rest of the
world model the same. We observe in Fig-
ure 11 that without smoothing, changing
the reward model size has negligible impact
on performance, and DreamSmooth out-
performs all the reward model sizes tested.
This indicates that the reward prediction
problem is not simply caused by insuffi-
cient model capacity.

(a) RoboDesk

(b) Hand

Figure 11: Simply increasing the reward model size has
negligible impact on performance. DreamerV3-768 and
DreamSmooth use 4 layers of 768 units; DreamerV3-
1024 uses 5 layers of 1024 units; and DreamerV3-1280
uses 6 layers of 1280 units.

In Figure 12, we analyze the impact of the smoothing parameters σ and
Smoothing Parameter.
α for Gaussian and EMA, respectively, on RoboDesk and Hand. We observe that DreamSmooth is
insensitive to the smoothing parameters, performing well across a wide range of values.

8

DreamSmooth-Gaussian (TDMPC)

TD-MPC

l

d
e
t
e
p
m
o
C
s
k
s
a
T

1.2

0.8

0.4

0.0

0.0
0.5
1.0
Environment steps (×10⁶)

 
l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

0
4
2
Environment steps (×10⁶)

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

DreamSmooth
Oversampling
DreamerV3

6
18
12
Environment steps (×10⁶)

24

 
DreamSmooth
DreamerV3-1280

DreamerV3-1024
DreamerV3-768

l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

6

0
24
12
Environment steps (×10⁶)

18

 
l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

0
20
10
Environment steps (×10⁶)

 
(a) Gaussian Smoothing on RoboDesk

(b) Gaussian Smoothing on Hand

(c) Uniform Smoothing on RoboDesk

(d) Uniform Smoothing on Hand

(e) EMA Smoothing on RoboDesk

(f) EMA Smoothing on Hand

Figure 12: Parameter sweep over smoothing parameters σ, δ, and α. The lines show median task
performance over 3 seeds, while shaded regions show maximum and minimum.

5 CONCLUSION

In this paper, we identify the reward prediction problem in MBRL and provide a simple yet effective
solution, reward smoothing. Our approach, DreamSmooth, demonstrates superior performance in
sparse reward tasks where reward prediction is not trivial mainly due to the partial observability
or stochasticity of the environments. Moreover, DreamSmooth shows comparable results on the
commonly used benchmarks, DMC and Atari, showing its task-agnostic nature. Although we show
that our simple reward smoothing approach mitigates the difficulty in reward prediction, the improved
reward prediction does not always improve the task performance, e.g., in Crafter. This can be because
more predicted task rewards encourage more exploitation and less exploration. Further investigation
on this trade-off is a promising direction for future work.

9

DreamSmooth-Gaussian

(cid:15)(cid:3)(cid:8)(cid:3)(cid:4)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:6)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:7)

(cid:9)(cid:14)(cid:12)(cid:11)(cid:13)(cid:12)(cid:14)(cid:10)(cid:5)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
DreamSmooth-Uniform

(cid:15)(cid:3)(cid:8)(cid:3)(cid:6)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:7)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:4)(cid:5)

(cid:9)(cid:14)(cid:12)(cid:11)(cid:13)(cid:12)(cid:14)(cid:10)(cid:5)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
DreamSmooth-EMA

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:6)

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:7)(cid:8)

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:9)

(cid:11)(cid:16)(cid:14)(cid:13)(cid:15)(cid:14)(cid:16)(cid:12)(cid:6)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
ACKNOWLEDGMENTS

This work was supported in part by the BAIR Industrial Consortium, an ONR DURIP grant, Komatsu,
and InnoHK Centre for Logistics Robotics. We would like to thank all members of the Berkeley
Robot Learning lab for their insightful feedback.

REFERENCES

Algoryx. AGX dynamics, 2020. URL https://www.algoryx.se/agx-dynamics/.

Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
OMNB1G5xzd4.

Mohammad Babaeizadeh, Mohammad Taghi Saffar, Danijar Hafner, Harini Kannan, Chelsea Finn,
Sergey Levine, and Dumitru Erhan. Models, pixels, and rewards: Evaluating design trade-offs in
visual model-based reinforcement learning. arXiv preprint arXiv:2012.04603, 2020.

M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279,
jun 2013.

Fei Deng, Ingook Jang, and Sungjin Ahn. Dreamerpro: Reconstruction-free model-based reinforce-
ment learning with prototypical representations. In International Conference on Machine Learning,
pp. 4956–4975. PMLR, 2022.

Fei Deng, Junyeong Park, and Sungjin Ahn. Facing off world model backbones: Rnns, transformers,

and s4. arXiv preprint arXiv:2307.02064, 2023.

Christopher D Fiorillo, William T Newsome, and Wolfram Schultz. The temporal precision of reward

prediction in dopamine neurons. Nature neuroscience, 11(8):966–973, 2008.

David Ha and J¨urgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.

Danijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on

Learning Representations, 2022.

Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In International Conference on Learning Representations, 2019.

Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete

world models. In International Conference on Learning Representations, 2021.

Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains

through world models. arXiv preprint arXiv:2301.04104, 2023.

Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive

control. In International Conference on Machine Learning, 2022.

Harini Kannan, Danijar Hafner, Chelsea Finn, and Dumitru Erhan. Robodesk: A multi-task rein-
forcement learning benchmark. https://github.com/google-research/robodesk,
2021.

Miriam C Klein-Fl¨ugge, Laurence T Hunt, Dominik R Bach, Raymond J Dolan, and Timothy EJ
Behrens. Dissociable reward and timing signals in human midbrain and ventral striatum. Neuron,
72(4):654–664, 2011.

Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos.

In Robotics: Science and Systems, 2023.

Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In International Conference on Machine Learning,
volume 99, pp. 278–287, 1999.

10

Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by latent
imagination without reconstruction. In IEEE International Conference on Robotics and Automation,
pp. 4209–4215, 2021. doi: 10.1109/ICRA48506.2021.9560734.

Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learn-
ing: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464,
2018.

Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.

Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter

Abbeel. Masked world models for visual control. In Conference on Robot Learning, 2022.

Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. Multi-view
masked world models for visual robotic manipulation. In International Conference on Machine
Learning, 2023.

Lucy Xiaoyang Shi, Joseph J. Lim, and Youngwoon Lee. Skill-based model-based reinforcement

learning. In Conference on Robot Learning, 2022.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017.

David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140–1144, 2018.

Gautam Singh, Skand Peri, Junghyun Kim, Hyunseok Kim, and Sungjin Ahn. Structured world
belief for reinforcement learning in pomdp. In International Conference on Machine Learning, pp.
9744–9755. PMLR, 2021.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 2018.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.

11

A PROOFS

Let M = (S, A, P, R, γ) be the given MDP. Without loss of generality, we assume the augmented
form of the MDP M, where a state st includes the entire history of states, i.e., st = (s1, . . . , st),
and thus, reward functions R, ˜R have access to previous states, i.e., ˜R(st) = ˜R(s1, . . . , st).
Theorem A.1. An optimal policy ˜π∗ of the MDP with reward smoothing only with past rewards, e.g.,
EMA smoothing, ˜M = (S, A, P, ˜R, γ) is also optimal under the original MDP M, where

˜R(st) =

0
(cid:88)

i=−L

fi · γiR(st+i) and

0
(cid:88)

i=−L

fi = 1.

(2)

Proof. We will use the theorem of reward shaping that guarantees an optimal policy introduced in Ng
et al. (1999): if a modified reward function can be represented in the form of R(st)+γΦ(st+1)−Φ(st)
with any potential function Φ(st), the new reward function yields the same optimal policy with the
original reward function R.

Let the potential function for the EMA reward smoothing

Φ(st) = −

−1
(cid:88)

i=−L

γiR(st+i) +

0
(cid:88)

i=−L

γiR(st+i) ·

0
(cid:88)

fj.

j=i+1

(3)

Then, our reward shaping term in ˜R can be represented as the difference in the potential function
γΦ(st+1) − Φ(st) as follows:

γΦ(st+1) − Φ(st) = −R(st) +

0
(cid:88)

i=−L

fi · γiR(st+i).

R(st) + γΦ(st+1) − Φ(st) =

0
(cid:88)

i=−L

fi · γiR(st+i) = ˜R.

(4)

(5)

Hence, following Ng et al. (1999), reward shaping with our EMA smoothing guarantees the optimal
policy in the original MDP M.

However, Theorem A.1 does not apply to smoothing functions that require access to future rewards,
e.g., Gaussian smoothing. As in Gaussian smoothing, a smoothed reward function may require future
rewards, which are conditioned on the current policy; so is the reward model. In such cases, there is
no theoretical guarantee; but in our experiments, we empirically show that reward models can adapt
their predictions along the changes in policies and thus, improve MBRL.

Instead, we intuitively explain that an optimal policy under any reward smoothing (even though the
reward function is post hoc and cannot be defined for MDPs) is also optimal under the original reward
function.
Theorem A.2. An optimal policy ˜π∗ with the smoothed reward function ˜R is also optimal under the
original reward function R, where

˜R(st) =

L
(cid:88)

i=−L

γclip(i,−t,T −t) · fi · R(sclip(t+i,0,T )) and

L
(cid:88)

i=−L

fi = 1.

(6)

12

Proof. First, we show that the discounted sum of original rewards (cid:80)T
smoothed rewards (cid:80)T

t=0 γt ˜R(st) are the same for any trajectories (s0, s1, . . . , sT ):
T
(cid:88)

L
(cid:88)

γclip(i,−t,T −t) · fi · R(sclip(t+i,0,T ))

γt ˜R(st) =

γt

from Equation (6)

t=0 γtR(st) and the one of

T
(cid:88)

t=0

i=−L

γtR(st) ·

L
(cid:88)

fi

i=−L

γtR(st).

t=0

T
(cid:88)

t=0

T
(cid:88)

t=0

=

=

(7)

(8)

from

L
(cid:88)

i=−L

fi = 1

(9)

Let an optimal policy under the smoothed rewards ˜R be ˜π∗. Assume that ˜π∗ is not optimal under the
original reward R. Then,

∃π∗, s0

such that E(s0,...,sT )∼π∗

(cid:104) T
(cid:88)

t=0

(cid:105)
γtR(st)

> E(s0,...,sT )∼˜π∗

(cid:104) T
(cid:88)

t=0

γt ˜R(st)

(cid:105)
.

(10)

However,

E(s0,...,sT )∼π∗

(cid:104) T
(cid:88)

t=0

(cid:105)

γtR(st)

= E(s0,...,sT )∼π∗

> E(s0,...,sT )∼˜π∗

(cid:104) T
(cid:88)

t=0
(cid:104) T
(cid:88)

t=0

(cid:105)

γt ˜R(st)

γt ˜R(st)

(cid:105)
,

by Equation (9)

(11)

by Equation (10)

(12)

which contradicts that ˜π∗ is optimal under ˜R. Therefore, the optimal policy ˜π∗ under ˜R guarantees
its optimality under R.

B IMPLEMENTATION DETAILS

Models are trained on NVIDIA A5000, V100, RTX Titan, RTX 2080, and RTX 6000 GPUs. Each
experiment takes about 72 hours for RoboDesk, 100 hours for Hand, 150 hours for Earthmoving, 96
hours for Crafter, and 6 hours for Atari and DMC tasks.

B.1 DREAMSMOOTH SMOOTHING FUNCTIONS

Gaussian smoothing follows the Gaussian distribution with σ:

fi = ke

−i2
2σ2 ,

(13)

where k = 1/((cid:80)L

i=−L e
We implement this using

−i2
2σ2 ) is a normalization constant.

scipy.ndimage.gaussian_filter1d(rewards, sigma, mode=""nearest"")

Uniform smoothing distributes rewards equally across δ consecutive timesteps.
1
δ

δ − 1
2

δ − 1
2

fi =

∀i ∈

−

(cid:105)

(cid:104)

.

,

We implement this using

scipy.ndimage.convolve(rewards, filter, mode=""nearest"")

EMA smoothing uses the following smoothing function:

fi = α(1 − α)i ∀i ≤ 0,

which we implement by performing the following at each timestep:

reward[t] = alpha * reward[t - 1] + (1 - alpha) * reward[t]

13

(14)

(15)

B.2 MODEL-BASED REINFORCEMENT LEARNING BACKBONES

Hyperparameters for DreamerV3 experiments are shown in Table 1 and TD-MPC in Table 2.

Table 1: DreamerV3 hyperparameters. Episode length is measured in environment steps, which is the
number of agent steps multiplied by action repeat. Model sizes are as listed in Hafner et al. (2023),
which we also refer to for all other hyperparameters.

Environment Action Repeat Episode Length Train Ratio Model Size

Earthmoving
RoboDesk
Hand
Crafter
DMC
Atari

4
8
1
1
2
4

2000
2400
300
Variable
1000
Variable

64
64
64
64
512
1024

L
L
L
XL
S
S

σ

3
3
2
1
3
3

α

0.33
0.3
0.3
0.45
0.33
0.3

δ

9
9
9
9
9
9

Table 2: TD-MPC hyperparameters. Unless specified, we use the default hyperparameters in Hansen
et al. (2022).

Environment Latent Dimension CNN channels Planning Iterations

Hand-Pixel
Hand-Proprio

128
128

64
–

6
12

σ

3
3

C ENVIRONMENT DETAILS

C.1 ROBODESK ENVIRONMENT

We use a modified version of RoboDesk (Kannan et al., 2021), where a sequence of manipulation tasks
(flat block in bin, upright block off table, push green) need to be completed
in order. Figure 13 shows images of an agent successfully completing each of these tasks.

In the original environment, dense rewards are based on Euclidean distances of objects to their targets,
with additional terms to encourage the arm to reach the object. They typically range from 0 to 10
per timestep. We use these dense rewards together with a large sparse reward of 300 for each task
completed.

(a) Put the flat block into the bin

(b) Push the upright block off the
table

(c) Press the green button

Figure 13: Subtasks for RoboDesk.

C.2 HAND ENVIRONMENT

We modified the Shadow Hand environment (Plappert et al., 2018), so that the agent is required to
achieve a sequence of pre-defined goal orientations in order. The first 3 goals are shown in Figure 14,

14

while the subsequent goals are a repeat of the first 3. The goal orientations are chosen so that the
agent only has to rotate the cube along the z-axis, and we only require the agent to match the cube’s
rotation to the goal, not its position.

In the original environment, dense rewards are computed using r = −(10x + ∆θ), where x is the
Euclidean distance to some fixed position, and ∆θ is the angular difference to the target orientation.
In addition to these dense rewards, we provide a large sparse reward of 300 for each goal successfully
achieved by the agent.

(a) Goal 1

(b) Goal 2

(c) Goal 3

Figure 14: Subtasks for Hand.

C.3 AGX EARTHMOVING ENVIRONMENT

The Earthmoving environment consists of a wheel loader, dump
truck, a pile of dirt, with some rocks on top of the pile. The en-
vironment is simulated using the realistic AGX Dynamics physics
engine (Algoryx, 2020). The agent controls the wheel loader to pick
up rocks and dump them in the dump truck.

The starting positions of the dirt pile, wheel loader, and dump truck
are all randomized, as are the initial orientations of the dirt pile and
wheel loader.

The agent’s observations consist of 3 components: a wide-angle
egocentric RGB camera mounted on the cabin to allow navigation,
an RGB camera mounted on the bucket for observing interactions
with rocks, and proprioceptive observations (positions, velocity,
speed, force of actuators etc.). We use 64 × 64 × 3 images for all
cameras, while the proprioceptive observation has 21 dimensions.

Figure 15: The agent uses one
camera mounted on the cabin
(left) for navigation, and one
mounted on the bucket (right)
for observing interactions with
rocks and terrain.

The action space is 4-dimensional: 2 dimensions for driving and steering the loader, and 2 dimensions
for moving and tilting the bucket.

The reward consists of a large sparse reward for rocks picked up and dumped, and dense rewards
for moving rocks towards the dumptruck. The total reward rt at timestep t is computed using
Equation (16).
rt = λdump(mt
(cid:124)

load(max (2, dt) − max (2, dt−1))
(cid:125)

load − mt−1
load )
(cid:125)

dump) + λload(mt

dump − mt−1

+ λmovemt

(cid:124)

(cid:123)(cid:122)
dense reward

(cid:123)(cid:122)
sparse reward

Where mdump, mload are rock masses in the dumptruck and the bucket respectively, d is the distance
between the shovel and a point above the dumptruck, and λ are constants.

(16)

15

D DMC AND ATARI BENCHMARKING RESULTS

Figure 16: Full learning curves for the DMC and Atari benchmarks.

16

DreamSmooth-Gaussian

DreamSmooth-Uniform

DreamSmooth-EMA

DreamerV3

Hopper Hop

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Reacher Hard

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Cartpole Swingup Sparse

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Walker Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Finger Turn Hard

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Quadruped Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Cheetah Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Pong

3.00

−5.33

−13.67

e
r
o
c
S

−22.00

0.0
0.4
0.2
Environment steps (×10⁶)

Breakout

16.00

10.67

5.33

e
r
o
c
S

0.00

0.0
0.4
0.2
Environment steps (×10⁶)

Freeway

29

19

9

e
r
o
c
S

−1

0.0

0.2
Environment steps (×10⁶)

0.4

Assault

807.0

621.3

435.7

e
r
o
c
S

250.0

0.0
0.4
0.2
Environment steps (×10⁶)

Seaquest

545

378

211

e
r
o
c
S

44

0.0

0.2
Environment steps (×10⁶)

0.4

Hero

13200

e
r
o
c
S

8860

4520

180

0.0
0.4
0.2
Environment steps (×10⁶)

","3 2 0 2 v o N 2 ] G L . s c [ 1 v 0 5 4 1 0 . 1 1 3 2 : v i X r a DREAMSMOOTH : IMPROVING MODEL-BASED REIN- FORCEMENT LEARNING VIA REWARD SMOOTHING Vint Lee , Pieter Abbeel , Youngwoon Lee University of California , Berkeley ABSTRACT Model-based reinforcement learning ( MBRL ) has gained much attention for its ability to learn complex behaviors in a sample-efficient way : planning actions by generating imaginary trajectories with predicted rewards . Despite its success , we found that surprisingly , reward prediction is often a bottleneck of MBRL , especially for sparse rewards that are challenging ( or even ambiguous ) to predict . Motivated by the intuition that humans can learn from rough reward estimates , we propose a simple yet effective reward smoothing approach , DreamSmooth , which learns to predict a temporally-smoothed reward , instead of the exact reward at the given timestep . We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks , such as Deepmind Control Suite and Atari benchmarks . 1 INTRODUCTION Humans often plan actions with a rough estimate of future rewards , instead of the exact reward at the exact moment ( Fiorillo et al. , 2008 ; Klein- Fl¨ugge et al. , 2011 ) . A rough reward estimate is mostly sufficient to learn a task , and predicting the exact reward is often challenging since it can be ambiguous , delayed , or not observable . Consider for instance the manipulation task il- lustrated in Figure 1 ( middle ) of pushing a block on a table into a bin , where a sparse reward is given only on the timestep when the block first touches the bin . Using the same image observa- tions as the agent , it is challenging even for hu- mans to predict the correct sequence of rewards . Crucially , this issue is present in many environ- ments , where states with no reward are almost indistinguishable from those with rewards . Figure 1 : Predicting the exact sequence of rewards is extremely difficult . These examples show the sequences of image observations seen by the agent just before and after it receives a sparse reward . There is little to visually distinguish timesteps with a large reward from those without , which creates a significant challenge for reward prediction . An accurate reward model is vital to model- based reinforcement learning ( MBRL ) – reward estimates that are too high will cause an agent to choose actions that perform poorly in reality , and estimates that are too low will lead an agent to ignore high rewards . Despite its difficulty and importance , the reward prediction problem in MBRL has been largely overlooked . We find that even for the state-of-the-art MBRL algorithm , DreamerV3 ( Hafner et al. , 2023 ) , reward prediction is not only challenging , but is also a performance bottleneck for many tasks . For instance , DreamerV3 fails to predict any reward for most objectives in the Crafter environment ( Hafner , 2022 ) with similar failure modes observed on variants of the RoboDesk ( Kannan et al. , 2021 ) and Shadow Hand ( Plappert et al. , 2018 ) tasks with sparse rewards . 1 Inspired by the human intuition that only a rough estimate of rewards is sufficient , we propose a simple yet effective solution , DreamSmooth , which learns to predict a temporally-smoothed reward rather than the exact reward at each timestep . This makes reward prediction much easier – instead of having to predict rewards exactly , now the model only needs to produce an estimate of when sparse rewards are obtained , which is sufficient for policy learning . Our experiments demonstrate that while extremely simple , this technique significantly improves performance of different MBRL algorithms on many sparse-reward environments . Specifically , we find that for DreamerV3 ( Hafner et al. , 2023 ) and TD-MPC ( Hansen et al. , 2022 ) , our technique is especially beneficial in environments with the following characteristics : sparse rewards , partial observability , and stochastic rewards . Finally , we show that even on benchmarks where reward prediction is not a significant issue , DreamSmooth does not degrade performance , which indicates that our technique can be universally applied . 2 RELATED WORK Model-based reinforcement learning ( MBRL ) leverages a dynamics model ( i.e . world model ) of an environment and a reward model of a desired task to plan a sequence of actions that maximize the total reward . The dynamics model predicts the future state of the environment after taking a specific action and the reward model predicts the reward corresponding to the state-action transition . With the dynamics and reward models , an agent can simulate a large number of candidate behaviors in imagination instead of in the physical environment , allowing MBRL to tackle many challenging tasks ( Silver et al. , 2016 ; 2017 ; 2018 ) . Instead of relying on the given dynamics and reward models , recent advances in MBRL have enabled learning a world model of high-dimensional observations and complex dynamics ( Ha & Schmidhuber , 2018 ; Schrittwieser et al. , 2020 ; Hafner et al. , 2019 ; 2021 ; 2023 ; Hansen et al. , 2022 ) , as well as a temporally-extended world model ( Shi et al. , 2022 ) . Specifically , DreamerV3 ( Hafner et al. , 2023 ) has achieved the state-of-the-art performance across diverse domains of problems , e.g. , both with pixel and state observations as well as both with discrete and continuous actions . For realistic imagination , MBRL requires an accurate world model . There have been significant efforts in learning better world models by leveraging human videos ( Mendonca et al. , 2023 ) , by adopting a more performant architecture ( Deng et al. , 2023 ) , and via representation learning , such as prototype-based ( Deng et al. , 2022 ) and object-centric ( Singh et al. , 2021 ) state representations , contrastive learning ( Okada & Taniguchi , 2021 ) , and masked auto-encoding ( Seo et al. , 2022 ; 2023 ) . However , compared to the efforts on learning a better world model , learning an accurate reward model has been largely overlooked . Babaeizadeh et al . ( 2020 ) investigates the effects of various world model designs and shows that reward prediction is strongly correlated to task performance when trained on an offline dataset , while limited to dense-reward environments . In this paper , we point out that accurate reward prediction is crucial for MBRL , especially in sparse-reward tasks and partially observable environments , and propose a simple method to improve reward prediction in MBRL . 3 APPROACH The main goal of this paper is to understand how challenging reward prediction is in model-based reinforcement learning ( MBRL ) and propose a simple yet effective solution , reward smoothing , which makes reward prediction easier to learn . In this section , we first provide a background about MBRL in Section 3.1 , then present experiments demonstrating the challenge of predicting sparse reward signals in Section 3.2 , and finally explain our approach , DreamSmooth , in Section 3.4 . 3.1 BACKGROUND We formulate a problem as a partially observable Markov decision process ( POMDP ) , which is defined as tuple ( O , A , P , R , γ ) . O is an observation space , A is an action space , P ( ot+1|o≤t , a≤t ) with timestep t is a transition dynamics , R is a reward function that maps previous observations and actions to a reward rt = R ( o≤t , a≤t ) , and γ ∈ [ 0 , 1 ) is a discount factor ( Sutton & Barto , 2018 ) . RL aims to find a policy π ( at | o≤t , a < t ) that maximizes the expected sum of rewards Eπ [ ( cid:80 ) T t=1 γt−1rt ] . 2 This paper focuses on MBRL algorithms that learn a world model Pθ ( zt+1|zt , at ) and reward model Rθ ( rt|zt ) from agent experience , where zt is a learned latent state at timestep t. The learned world model and reward model can then generate imaginary rollouts { zτ , aτ , rτ } t+H−1 of the horizon H starting from any zt , which can be used for planning ( Argenson & Dulac-Arnold , 2021 ; Hansen et al. , 2022 ) or policy optimization ( Ha & Schmidhuber , 2018 ; Hafner et al. , 2019 ) . Specifically , we use the state-of-the-art algorithms , DreamerV3 ( Hafner et al. , 2023 ) and TD-MPC ( Hansen et al. , 2022 ) . τ =t DreamerV3 ( Hafner et al. , 2023 ) uses the predicted rewards for computing new value targets to train the critic . For learning a good policy , the reward model plays a vital role since the critic , from which the actor learns a policy , receives its training signal exclusively through the reward model . Note that the data collected from the environment is only used for training a world model and reward model . On the other hand , TD-MPC ( Hansen et al. , 2022 ) learns a state-action value function Q ( zt , at ) directly from agent experience , not from predicted rewards . However , the reward model is still important for obtaining a good policy in TD-MPC because the algorithm uses both the reward model and value function to obtain the policy through online planning . 3.2 REWARD PREDICTION IS DIFFICULT Reward prediction is surprisingly challenging in many environments . Figure 1 shows sequences of frames right before and after sparse rewards are received in diverse environments . Even for humans , it is difficult to determine the exact timestep when the reward is received in all three environments . We hypothesize that the mean squared error loss E ( z , r ) ∼D [ ( Rθ ( z ) − r ) 2 ] , typically used for reward model training , deteriorates reward prediction accuracy when there exist sparse rewards . This is because predicting a sparse reward a single step earlier or later results in a higher loss than simply predicting 0 reward at every step . Thus , instead of trying to predict sparse rewards at the exact timesteps , a reward model minimizes the loss by entirely omitting sparse rewards from its predictions . To verify this hypothesis , we plot the ground-truth and DreamerV3 ’ s predicted rewards in Figure 2 . The reward models struggle at predicting exact rewards and simply ignore sparse rewards unless they are straightforward to predict on the four tasks described in Section 4.1 . This hypothesis also holds in a deterministic and fully-observable environment , Crafter , which has 24 sources of sparse rewards . The reward model fails to predict most of these reward sources ( Figure 2d ) . The difficulty of reward prediction can be further exacerbated by partial observability , ambiguous rewards , or stochastic dynamics of environments . As an example in the first ( third ) row in Figure 1 , ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter Figure 2 : Ground truth rewards and DreamerV3 ’ s predicted rewards over an evaluation episode . The reward model misses many sparse rewards , which is highlighted in yellow . 3 Ground Truth Predicted x-axis : Timestep y-axis : Reward 330 0 −110 0 120 330 0 −110 0 120 2400 0 −800 0 360 1.2 0.0 −0.4 0 200 the sparse rewards are given when the block ( the rocks in the third example ) first contacts the bin ( the dumptruck ) . The exact moment of contact is not directly observable from the camera viewpoint , and this makes reward prediction ambiguous . Moreover , stochastic environment dynamics , e.g. , contact between multiple rocks , can make predicting a future state and reward challenging . 3.3 REWARD PREDICTION IS A BOTTLENECK OF MBRL The preceding section shows that reward prediction is challenging in many environments . More importantly , this poor reward prediction can be a bottleneck of policy learning , as shown in Figure 3 . In RoboDesk , where the reward model does not reliably detect the completion of the second task ( Figure 2a ) , the policy gets stuck at solving the first task and fails on subsequent tasks . In Earthmoving , where the reward model can not capture rewards for successful dumping ( Figure 2c ) , the policy frequently drops the rocks outside the dumptruck . These consistent failure modes in reward prediction and policy learning in DreamerV3 suggest that poor reward prediction can be a bottleneck of MBRL . ( a ) RoboDesk ( b ) Earthmoving Figure 3 : The reward model ’ s inability to predict sparse rewards for completing tasks leads to poor task performance . ( a ) In RoboDesk , the agent gets stuck after learning the first task , and is unable to learn to perform the subsequent tasks . ( b ) In Earthmoving , the policy often fails to dump the rocks accurately into the dumptruck . The learning curves are averaged over 3 seeds . 3.4 DREAMSMOOTH : IMPROVING MBRL VIA REWARD SMOOTHING To address the reward prediction problem , we propose a simple yet effective solution , DreamSmooth , which relaxes the require- ment for the model to predict sparse rewards at the exact timesteps by performing tempo- ral smoothing . Allowing the reward model to predict rewards that are off from the ground truth by a few timesteps makes learning eas- ier , especially when rewards are ambiguous or sparse . ( a ) Gaussian ( b ) Uniform ( c ) EMA Figure 4 : Reward smoothing on sparse reward 1 at t = 4. σ , δ , and α are smoothing hyperparameters . Specifically , DreamSmooth applies temporal smoothing to the rewards upon collecting each new episode . DreamSmooth can work with any smoothing function f that preserves the sum of rewards : ˜rt ← f ( rt−L : t+L ) = L ( cid:88 ) i=−L fi · rclip ( t+i,0 , T ) s.t . L ( cid:88 ) i=−L fi = 1 , ( 1 ) where T and L denote the episode and smoothing horizons , respectively . For simplicity , we omit the discount factor in Equation ( 1 ) ; the full equation can be found in Appendix , Equation ( 6 ) . Episodes with the smoothed rewards are stored in the replay buffer and used to train the reward model . The agent learns only from the smoothed rewards , without ever seeing the original rewards . The smoothed rewards ease reward prediction by allowing the model to predict rewards several timesteps earlier or later , without incurring large losses . In this paper , we investigate three popular smoothing functions : Gaussian , uniform , and exponential moving average ( EMA ) smoothing , as illustrated in Figure 4 . While the main motivation for smoothing is to make it easier to learn reward models , we note that reward smoothing in some cases preserves optimality – an optimal policy under smoothed rewards ˜r 4 3 2 1 l d e t e p m o C s k s a T 0 0 8 16 24 Environment steps ( ×10⁶ ) 2 1 d e p m u D s k c o R 0 0 4 8 12 Environment steps ( ×10⁶ ) 1 d r a w e R 0 0 Orginal σ = 1 σ = 2 2 4 Timestep 6 8 1 d r a w e R 0 0 Orginal δ = 3 δ = 7 2 4 Timestep 6 8 1 d r a w e R 0 0 Orginal α = .3 α = .6 2 4 Timestep 6 8 is also optimal under the original rewards r. In particular , we provide a proof in Appendix A for the optimality of EMA smoothing ( and any smoothing function where ∀i > 0 , fi = 0 ) by augmenting the POMDP states with the history of past states . However , when future rewards are used for smoothing ( e.g . Gaussian smoothing ) , the smoothed rewards are conditioned on policy , and we can no longer define an equivalent POMDP . In such cases , there is no theoretical guarantee . Even so , we empirically show that reward models can adapt their predictions alongside the changing policy , and achieve performance improvements . The implementation of DreamSmooth is extremely simple , requiring only one additional line of code to existing MBRL algorithms , as shown in Algorithm 1 . The overhead of reward smoothing is minimal , with time complexity O ( T · L ) . More implementation details can be found in Appendix B. Algorithm 1 COLLECT ROLLOUT ( π : policy , D : replay buffer ) in DREAMSMOOTH t=1 } ← ROLLOUT ( π ) { ( ot , at , rt ) T { rt } T D ← D ∪ { ( ot , at , rt ) T t=1 ← GAUSSIAN ( { rt } T t=1 } t=1 , σ ) or EMA ( { rt } T t=1 , α ) ▷ only one line needs to be added . 4 EXPERIMENTS In this paper , we propose a simple reward smoothing method , DreamSmooth , which facilitates reward prediction in model-based reinforcement learning ( MBRL ) and thus , improves the performance of existing MBRL methods . Through our experiments , we aim to answer the following questions : ( 1 ) Does reward smoothing improve reward prediction ? ( 2 ) Does better reward prediction with reward smoothing lead to better sample efficiency and asymptotic performance of MBRL in sparse-reward tasks ? ( 3 ) Does MBRL with reward smoothing also work in common dense-reward tasks ? 4.1 TASKS We evaluate DreamSmooth on four tasks with sparse subtask completion rewards and two common RL benchmarks . Earthmoving uses two 64 × 64 images as an observation while all other tasks use a single image . See Appendix C for environment details . • RoboDesk : We use a modified version of RoboDesk ( Kannan et al. , 2021 ) , where a sequence of ma- nipulation tasks ( flat block in bin , upright block off table , push green ) need to be completed in order ( Figure 5a ) . We use the original dense rewards together with a large sparse reward for each task completed . • Hand : The Hand task ( Plappert et al. , 2018 ) requires a Shadow Hand to rotate a block in hand into a specific orientation . We extend it to achieve a sequence of pre-defined goal orientations in order . In addition to the original dense rewards , we provide a large sparse reward for each goal . • Earthmoving : The Earthmoving task consists of a wheel loader , dump truck , and a pile of rocks ( Figure 5c ) . The agent controls the wheel loader to pick up rocks from the pile and dump them in the dump truck . A large sparse reward is given for each rock picked up and for each rock dumped , proportional to its mass . In addition , dense rewards are given for moving rocks towards the dump truck . The environment is simulated using the AGX Dynamics physics engine ( Algoryx , 2020 ) . ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter ( e ) DMC ( f ) Atari Figure 5 : We evaluate DreamSmooth on four tasks with sparse subtask completion rewards ( a-d ) . We also test on two popular benchmarks , ( e ) DeepMind Control Suite and ( f ) Atari . 5 ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter Figure 6 : We visualize the ground truth rewards , smoothed rewards with Gaussian smoothing , and predicted rewards by DreamerV3 trained on the smoothed rewards over an evaluation episode . In contrast to Figure 2 , the reward models with reward smoothing capture most of sparse rewards . • Crafter : Crafter ( Hafner , 2022 ) is a minecraft-like 2D environment , where the agent tries to collect , place , and craft items in order to survive . There are 22 achievements in the environment ( e.g . collecting water , mining diamonds ) with a sparse reward 1 for obtaining each achievement for the first time . A small reward is given ( or lost ) for each health point gained ( or lost ) . • DMC : We benchmark 7 DeepMind Control Suite continuous control tasks ( Tassa et al. , 2018 ) . • Atari : We benchmark 6 Atari tasks ( Bellemare et al. , 2013 ) at 100K steps . 4.2 IMPROVED REWARD PREDICTION WITH REWARD SMOOTHING We first visualize the ground truth rewards , smoothed rewards ( Gaussian smoothing ) , and re- ward prediction results of DreamerV3 trained with DreamSmooth in Figure 6 . We observe that re- ward smoothing leads to a significant improve- ment in reward prediction : DreamSmooth success- fully predicts most of the ( smoothed ) sparse re- wards and no longer omits vital signals for policy learning or planning . The improvement is especially notable in Crafter . In Figure 7 , we measure the accuracy of the re- ward model , ( i.e . predicting a reward larger than half of the original or smoothed reward for Dream- erV3 and DreamSmooth respectively ) at the exact timesteps for each subtask . The vanilla Dream- erV3 ’ s reward model ( baseline ) misses most of the sparse rewards while DreamSmooth predicts sparse rewards more accurately in 15/19 subtasks . 4.3 RESULTS Figure 7 : Reward prediction rates for 19 achieve- ments in Crafter . The other 3 tasks have been never achieved by both methods . With reward smoothing , the prediction rates are better in 15/19 tasks . We compare the vanilla DreamerV3 ( Hafner et al. , 2023 ) with DreamSmooth , whose backbone is also DreamerV3 . For DreamSmooth , we evaluate Gaussian , uniform , and EMA smoothing . The 6 Ground Truth Smoothed Predicted x-axis : Timestep y-axis : Reward 64 00 0 120 60 0 −20 0 120 800 0 −200 0 360 0.6 0.0 −0.2 0 200 Baseline ( no smoothing ) Gaussian 1.0 0.5 e t a R n o i t c i d e r P 0.0 w k al d e ain alth g e H nt e n ace d e n o o bie alth loss g xe xe ord n ord ollect drin plin at skeleto at co ollect co Place pla d picka e picka Place sto ollect sto Place ta m d sw e sw Place furn ollect w ollect sa at zo E ke sto n efe ke w ke sto ke w D a M efe C D n o o o o C C C C e H p ble ke u Wa a M a M a M ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter ( e ) DMC ( f ) Atari Figure 8 : Comparison of learning curves of DreamSmooth ( Gaussian , Uniform , EMA ) and Dream- erV3 . The shaded regions in ( a-d ) show the maximum and minimum over 3 seeds . For DMC ( e ) and Atari ( f ) , we aggregate results over 7 and 6 tasks respectively , and display the standard deviation . hyperparameters for DreamerV3 and smoothing functions can be found in Appendix B . As shown in Figure 8 , DreamSmooth-Gaussian and DreamSmooth-Uniform significantly improve the performance as well as the sample efficiency of DreamerV3 on the Robodesk , Hand , and Earthmoving tasks . The only change between DreamerV3 and ours is the improved reward prediction , as shown in Section 4.2 . This result suggests that reward prediction is one of major bottlenecks of the MBRL performance . While all smoothing methods lead to improvements over DreamerV3 , Gaussian smoothing generally performs the best , except on Crafter , with uniform smoothing showing comparable performance . The better performance of Gaussian and uniform smoothing could be because it allows predicting rewards both earlier and later , whereas EMA smoothing only allows predicting rewards later . Despite the improved reward prediction accuracy , DreamSmooth-Gaussian and DreamSmooth- Uniform perform worse than the baseline in Crafter . This can be because more predicted task rewards encourage more exploitation and less exploration . Further investigation on this trade-off is a promising direction for future work . Moreover , we observe that on the DMC and Atari benchmarks , where reward prediction is not particu- larly challenging , our technique shows comparable performance with the unmodified algorithms ( see Appendix , Figure 16 for full results ) . This suggests that reward smoothing can be applied generally , and does not hinder performance on most environments . 7 DreamSmooth-Gaussian DreamSmooth-Uniform DreamSmooth-EMA DreamerV3 3 2 1 l d e t e p m o C s k s a T 0 0 18 12 6 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 15 10 5 Environment steps ( ×10⁶ ) 20 d e p m u D s k c o R f o r e b m u N 2.0 1.5 1.0 0.5 0.0 0 3 6 Environment steps ( ×10⁶ ) 9 12 16 12 8 4 e r o c S 0 0.0 1.5 4.5 3.0 Environment steps ( ×10⁶ ) 6.0 1000 750 500 250 e r o c S 0 0.00 0.25 0.75 0.50 Environment steps ( ×10⁶ ) 1.00 e r o c S d e z i l a m r o N n a m u H 0.6 0.4 0.2 0.0 0.0 0.1 0.3 0.2 Environment steps ( ×10⁶ ) 0.4 In Figure 9 , DreamSmooth also improves the performance of TD-MPC ( Hansen et al. , 2022 ) . In the Hand task , vanilla TD-MPC is unable to consistently solve the first task , even with proprioceptive state observations . However , TD-MPC with DreamSmooth learns to complete the tasks with not only state observations but also pixel observa- tions . This suggests that DreamSmooth can be useful in a broad range of MBRL algo- rithms that use a reward model . We only demonstrate the Hand task since TD-MPC fails on other sparse-reward tasks . 4.4 ABLATION STUDIES ( a ) Hand ( Pixel ) ( b ) Hand ( State ) Figure 9 : Learning curves for TD-MPC and TD-MPC with DreamSmooth on the Hand task . The shaded re- gions show the minimum and maximum over 3 seeds . Data Imbalance . One possible cause of poor reward predictions is data imbalance – because sparse rewards are infrequent , sequences con- taining sparse rewards are rarely sampled from the replay buffer . The reward model therefore trains on fewer examples of sparse rewards , po- tentially leading to poor predictions . To test this hypothesis , we conducted experiments with oversampling : with probability p = 0.5 , we sample a sequence in which the agent receives a sparse reward ; otherwise , we sample uni- formly from all sequences in the buffer . As shown in Figure 10 , oversampling performs bet- ter than the baseline , but learns slower than DreamSmooth . This suggests that while data imbalance largely contributes to the difficulty of reward prediction , it is not the only factor hindering performance . Furthermore , this over- sampling method requires domain knowledge about which reward signals to be oversampled while DreamSmooth is agnostic to the scale and frequency of sparse rewards . Figure 10 : Using oversampling of sequences with sparse rewards ( p = 0.5 ) performs better than DreamerV3 on RoboDesk , but worse than DreamSmooth with Gaussian smoothing . The lines show median task performance over 3 seeds , while shaded regions show maximum and minimum . Reward Model Size . Another hypothe- sis for poor reward predictions is that the reward model does not have enough ca- pacity to capture sparse rewards . To test this hypothesis , we increase the size of the reward model from 4 layers of 768 units to 5 layeres of 1024 units and 6 layers of 1280 units , while keeping the rest of the world model the same . We observe in Fig- ure 11 that without smoothing , changing the reward model size has negligible impact on performance , and DreamSmooth out- performs all the reward model sizes tested . This indicates that the reward prediction problem is not simply caused by insuffi- cient model capacity . ( a ) RoboDesk ( b ) Hand Figure 11 : Simply increasing the reward model size has negligible impact on performance . DreamerV3-768 and DreamSmooth use 4 layers of 768 units ; DreamerV3- 1024 uses 5 layers of 1024 units ; and DreamerV3-1280 uses 6 layers of 1280 units . In Figure 12 , we analyze the impact of the smoothing parameters σ and Smoothing Parameter . α for Gaussian and EMA , respectively , on RoboDesk and Hand . We observe that DreamSmooth is insensitive to the smoothing parameters , performing well across a wide range of values . 8 DreamSmooth-Gaussian ( TDMPC ) TD-MPC l d e t e p m o C s k s a T 1.2 0.8 0.4 0.0 0.0 0.5 1.0 Environment steps ( ×10⁶ ) l d e t e p m o C s k s a T 3 2 1 0 0 4 2 Environment steps ( ×10⁶ ) 3 2 1 l d e t e p m o C s k s a T 0 0 DreamSmooth Oversampling DreamerV3 6 18 12 Environment steps ( ×10⁶ ) 24 DreamSmooth DreamerV3-1280 DreamerV3-1024 DreamerV3-768 l d e t e p m o C s k s a T 3 2 1 0 6 0 24 12 Environment steps ( ×10⁶ ) 18 l d e t e p m o C s k s a T 3 2 1 0 0 20 10 Environment steps ( ×10⁶ ) ( a ) Gaussian Smoothing on RoboDesk ( b ) Gaussian Smoothing on Hand ( c ) Uniform Smoothing on RoboDesk ( d ) Uniform Smoothing on Hand ( e ) EMA Smoothing on RoboDesk ( f ) EMA Smoothing on Hand Figure 12 : Parameter sweep over smoothing parameters σ , δ , and α . The lines show median task performance over 3 seeds , while shaded regions show maximum and minimum . 5 CONCLUSION In this paper , we identify the reward prediction problem in MBRL and provide a simple yet effective solution , reward smoothing . Our approach , DreamSmooth , demonstrates superior performance in sparse reward tasks where reward prediction is not trivial mainly due to the partial observability or stochasticity of the environments . Moreover , DreamSmooth shows comparable results on the commonly used benchmarks , DMC and Atari , showing its task-agnostic nature . Although we show that our simple reward smoothing approach mitigates the difficulty in reward prediction , the improved reward prediction does not always improve the task performance , e.g. , in Crafter . This can be because more predicted task rewards encourage more exploitation and less exploration . Further investigation on this trade-off is a promising direction for future work . 9 DreamSmooth-Gaussian ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:4 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:6 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:7 ) ( cid:9 ) ( cid:14 ) ( cid:12 ) ( cid:11 ) ( cid:13 ) ( cid:12 ) ( cid:14 ) ( cid:10 ) ( cid:5 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 DreamSmooth-Uniform ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:6 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:7 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:4 ) ( cid:5 ) ( cid:9 ) ( cid:14 ) ( cid:12 ) ( cid:11 ) ( cid:13 ) ( cid:12 ) ( cid:14 ) ( cid:10 ) ( cid:5 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 DreamSmooth-EMA ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:6 ) ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:7 ) ( cid:8 ) ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:9 ) ( cid:11 ) ( cid:16 ) ( cid:14 ) ( cid:13 ) ( cid:15 ) ( cid:14 ) ( cid:16 ) ( cid:12 ) ( cid:6 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 ACKNOWLEDGMENTS This work was supported in part by the BAIR Industrial Consortium , an ONR DURIP grant , Komatsu , and InnoHK Centre for Logistics Robotics . We would like to thank all members of the Berkeley Robot Learning lab for their insightful feedback . REFERENCES Algoryx . AGX dynamics , 2020 . URL https : //www.algoryx.se/agx-dynamics/ . Arthur Argenson and Gabriel Dulac-Arnold . Model-based offline planning . In International Confer- ence on Learning Representations , 2021 . URL https : //openreview.net/forum ? id= OMNB1G5xzd4 . Mohammad Babaeizadeh , Mohammad Taghi Saffar , Danijar Hafner , Harini Kannan , Chelsea Finn , Sergey Levine , and Dumitru Erhan . Models , pixels , and rewards : Evaluating design trade-offs in visual model-based reinforcement learning . arXiv preprint arXiv:2012.04603 , 2020 . M. G. Bellemare , Y. Naddaf , J. Veness , and M. Bowling . The arcade learning environment : An evaluation platform for general agents . Journal of Artificial Intelligence Research , 47:253–279 , jun 2013 . Fei Deng , Ingook Jang , and Sungjin Ahn . Dreamerpro : Reconstruction-free model-based reinforce- ment learning with prototypical representations . In International Conference on Machine Learning , pp . 4956–4975 . PMLR , 2022 . Fei Deng , Junyeong Park , and Sungjin Ahn . Facing off world model backbones : Rnns , transformers , and s4 . arXiv preprint arXiv:2307.02064 , 2023 . Christopher D Fiorillo , William T Newsome , and Wolfram Schultz . The temporal precision of reward prediction in dopamine neurons . Nature neuroscience , 11 ( 8 ) :966–973 , 2008 . David Ha and J¨urgen Schmidhuber . World models . arXiv preprint arXiv:1803.10122 , 2018 . Danijar Hafner . Benchmarking the spectrum of agent capabilities . In International Conference on Learning Representations , 2022 . Danijar Hafner , Timothy Lillicrap , Jimmy Ba , and Mohammad Norouzi . Dream to control : Learning behaviors by latent imagination . In International Conference on Learning Representations , 2019 . Danijar Hafner , Timothy Lillicrap , Mohammad Norouzi , and Jimmy Ba . Mastering atari with discrete world models . In International Conference on Learning Representations , 2021 . Danijar Hafner , Jurgis Pasukonis , Jimmy Ba , and Timothy Lillicrap . Mastering diverse domains through world models . arXiv preprint arXiv:2301.04104 , 2023 . Nicklas Hansen , Xiaolong Wang , and Hao Su . Temporal difference learning for model predictive control . In International Conference on Machine Learning , 2022 . Harini Kannan , Danijar Hafner , Chelsea Finn , and Dumitru Erhan . Robodesk : A multi-task rein- forcement learning benchmark . https : , 2021 . Miriam C Klein-Fl¨ugge , Laurence T Hunt , Dominik R Bach , Raymond J Dolan , and Timothy EJ Behrens . Dissociable reward and timing signals in human midbrain and ventral striatum . Neuron , 72 ( 4 ) :654–664 , 2011 . Russell Mendonca , Shikhar Bahl , and Deepak Pathak . Structured world models from human videos . In Robotics : Science and Systems , 2023 . Andrew Y Ng , Daishi Harada , and Stuart Russell . Policy invariance under reward transformations : Theory and application to reward shaping . In International Conference on Machine Learning , volume 99 , pp . 278–287 , 1999 . 10 Masashi Okada and Tadahiro Taniguchi . Dreaming : Model-based reinforcement learning by latent imagination without reconstruction . In IEEE International Conference on Robotics and Automation , pp . 4209–4215 , 2021. doi : 10.1109/ICRA48506.2021.9560734 . Matthias Plappert , Marcin Andrychowicz , Alex Ray , Bob McGrew , Bowen Baker , Glenn Powell , Jonas Schneider , Josh Tobin , Maciek Chociej , Peter Welinder , et al . Multi-goal reinforcement learn- ing : Challenging robotics environments and request for research . arXiv preprint arXiv:1802.09464 , 2018 . Julian Schrittwieser , Ioannis Antonoglou , Thomas Hubert , Karen Simonyan , Laurent Sifre , Simon Schmitt , Arthur Guez , Edward Lockhart , Demis Hassabis , Thore Graepel , et al . Mastering atari , go , chess and shogi by planning with a learned model . Nature , 588 ( 7839 ) :604–609 , 2020 . Younggyo Seo , Danijar Hafner , Hao Liu , Fangchen Liu , Stephen James , Kimin Lee , and Pieter Abbeel . Masked world models for visual control . In Conference on Robot Learning , 2022 . Younggyo Seo , Junsu Kim , Stephen James , Kimin Lee , Jinwoo Shin , and Pieter Abbeel . Multi-view masked world models for visual robotic manipulation . In International Conference on Machine Learning , 2023 . Lucy Xiaoyang Shi , Joseph J. Lim , and Youngwoon Lee . Skill-based model-based reinforcement learning . In Conference on Robot Learning , 2022 . David Silver , Aja Huang , Chris J Maddison , Arthur Guez , Laurent Sifre , George Van Den Driessche , Julian Schrittwieser , Ioannis Antonoglou , Veda Panneershelvam , Marc Lanctot , et al . Mastering the game of go with deep neural networks and tree search . nature , 529 ( 7587 ) :484–489 , 2016 . David Silver , Julian Schrittwieser , Karen Simonyan , Ioannis Antonoglou , Aja Huang , Arthur Guez , Thomas Hubert , Lucas Baker , Matthew Lai , Adrian Bolton , et al . Mastering the game of go without human knowledge . nature , 550 ( 7676 ) :354–359 , 2017 . David Silver , Thomas Hubert , Julian Schrittwieser , Ioannis Antonoglou , Matthew Lai , Arthur Guez , Marc Lanctot , Laurent Sifre , Dharshan Kumaran , Thore Graepel , et al . A general reinforcement learning algorithm that masters chess , shogi , and go through self-play . Science , 362 ( 6419 ) : 1140–1144 , 2018 . Gautam Singh , Skand Peri , Junghyun Kim , Hyunseok Kim , and Sungjin Ahn . Structured world belief for reinforcement learning in pomdp . In International Conference on Machine Learning , pp . 9744–9755 . PMLR , 2021 . Richard S Sutton and Andrew G Barto . Reinforcement learning : An introduction . MIT Press , 2018 . Yuval Tassa , Yotam Doron , Alistair Muldal , Tom Erez , Yazhe Li , Diego de Las Casas , David Budden , Abbas Abdolmaleki , Josh Merel , Andrew Lefrancq , Timothy P. Lillicrap , and Martin A. Riedmiller . Deepmind control suite . arXiv preprint arXiv:1801.00690 , 2018 . 11 A PROOFS Let M = ( S , A , P , R , γ ) be the given MDP . Without loss of generality , we assume the augmented form of the MDP M , where a state st includes the entire history of states , i.e. , st = ( s1 , . . . , st ) , and thus , reward functions R , ˜R have access to previous states , i.e. , ˜R ( st ) = ˜R ( s1 , . . . , st ) . Theorem A.1 . An optimal policy ˜π∗ of the MDP with reward smoothing only with past rewards , e.g. , EMA smoothing , ˜M = ( S , A , P , ˜R , γ ) is also optimal under the original MDP M , where ˜R ( st ) = 0 ( cid:88 ) i=−L fi · γiR ( st+i ) and 0 ( cid:88 ) i=−L fi = 1 . ( 2 ) Proof . We will use the theorem of reward shaping that guarantees an optimal policy introduced in Ng et al . ( 1999 ) : if a modified reward function can be represented in the form of R ( st ) +γΦ ( st+1 ) −Φ ( st ) with any potential function Φ ( st ) , the new reward function yields the same optimal policy with the original reward function R. Let the potential function for the EMA reward smoothing Φ ( st ) = − −1 ( cid:88 ) i=−L γiR ( st+i ) + 0 ( cid:88 ) i=−L γiR ( st+i ) · 0 ( cid:88 ) fj . j=i+1 ( 3 ) Then , our reward shaping term in ˜R can be represented as the difference in the potential function γΦ ( st+1 ) − Φ ( st ) as follows : γΦ ( st+1 ) − Φ ( st ) = −R ( st ) + 0 ( cid:88 ) i=−L fi · γiR ( st+i ) . R ( st ) + γΦ ( st+1 ) − Φ ( st ) = 0 ( cid:88 ) i=−L fi · γiR ( st+i ) = ˜R . ( 4 ) ( 5 ) Hence , following Ng et al . ( 1999 ) , reward shaping with our EMA smoothing guarantees the optimal policy in the original MDP M. However , Theorem A.1 does not apply to smoothing functions that require access to future rewards , e.g. , Gaussian smoothing . As in Gaussian smoothing , a smoothed reward function may require future rewards , which are conditioned on the current policy ; so is the reward model . In such cases , there is no theoretical guarantee ; but in our experiments , we empirically show that reward models can adapt their predictions along the changes in policies and thus , improve MBRL . Instead , we intuitively explain that an optimal policy under any reward smoothing ( even though the reward function is post hoc and can not be defined for MDPs ) is also optimal under the original reward function . Theorem A.2 . An optimal policy ˜π∗ with the smoothed reward function ˜R is also optimal under the original reward function R , where ˜R ( st ) = L ( cid:88 ) i=−L γclip ( i , −t , T −t ) · fi · R ( sclip ( t+i,0 , T ) ) and L ( cid:88 ) i=−L fi = 1 . ( 6 ) 12 Proof . First , we show that the discounted sum of original rewards ( cid:80 ) T smoothed rewards ( cid:80 ) T t=0 γt ˜R ( st ) are the same for any trajectories ( s0 , s1 , . . . , sT ) : T ( cid:88 ) L ( cid:88 ) γclip ( i , −t , T −t ) · fi · R ( sclip ( t+i,0 , T ) ) γt ˜R ( st ) = γt from Equation ( 6 ) t=0 γtR ( st ) and the one of T ( cid:88 ) t=0 i=−L γtR ( st ) · L ( cid:88 ) fi i=−L γtR ( st ) . t=0 T ( cid:88 ) t=0 T ( cid:88 ) t=0 = = ( 7 ) ( 8 ) from L ( cid:88 ) i=−L fi = 1 ( 9 ) Let an optimal policy under the smoothed rewards ˜R be ˜π∗ . Assume that ˜π∗ is not optimal under the original reward R. Then , ∃π∗ , s0 such that E ( s0 , ... , sT ) ∼π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γtR ( st ) > E ( s0 , ... , sT ) ∼˜π∗ ( cid:104 ) T ( cid:88 ) t=0 γt ˜R ( st ) ( cid:105 ) . ( 10 ) However , E ( s0 , ... , sT ) ∼π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γtR ( st ) = E ( s0 , ... , sT ) ∼π∗ > E ( s0 , ... , sT ) ∼˜π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γt ˜R ( st ) γt ˜R ( st ) ( cid:105 ) , by Equation ( 9 ) ( 11 ) by Equation ( 10 ) ( 12 ) which contradicts that ˜π∗ is optimal under ˜R . Therefore , the optimal policy ˜π∗ under ˜R guarantees its optimality under R. B IMPLEMENTATION DETAILS Models are trained on NVIDIA A5000 , V100 , RTX Titan , RTX 2080 , and RTX 6000 GPUs . Each experiment takes about 72 hours for RoboDesk , 100 hours for Hand , 150 hours for Earthmoving , 96 hours for Crafter , and 6 hours for Atari and DMC tasks . B.1 DREAMSMOOTH SMOOTHING FUNCTIONS Gaussian smoothing follows the Gaussian distribution with σ : fi = ke −i2 2σ2 , ( 13 ) where k = 1/ ( ( cid:80 ) L i=−L e We implement this using −i2 2σ2 ) is a normalization constant . scipy.ndimage.gaussian_filter1d ( rewards , sigma , mode= '' nearest '' ) Uniform smoothing distributes rewards equally across δ consecutive timesteps . 1 δ δ − 1 2 δ − 1 2 fi = ∀i ∈ − ( cid:105 ) ( cid:104 ) . , We implement this using scipy.ndimage.convolve ( rewards , filter , mode= '' nearest '' ) EMA smoothing uses the following smoothing function : fi = α ( 1 − α ) i ∀i ≤ 0 , which we implement by performing the following at each timestep : reward [ t ] = alpha * reward [ t - 1 ] + ( 1 - alpha ) * reward [ t ] 13 ( 14 ) ( 15 ) B.2 MODEL-BASED REINFORCEMENT LEARNING BACKBONES Hyperparameters for DreamerV3 experiments are shown in Table 1 and TD-MPC in Table 2 . Table 1 : DreamerV3 hyperparameters . Episode length is measured in environment steps , which is the number of agent steps multiplied by action repeat . Model sizes are as listed in Hafner et al . ( 2023 ) , which we also refer to for all other hyperparameters . Environment Action Repeat Episode Length Train Ratio Model Size Earthmoving RoboDesk Hand Crafter DMC Atari 4 8 1 1 2 4 2000 2400 300 Variable 1000 Variable 64 64 64 64 512 1024 L L L XL S S σ 3 3 2 1 3 3 α 0.33 0.3 0.3 0.45 0.33 0.3 δ 9 9 9 9 9 9 Table 2 : TD-MPC hyperparameters . Unless specified , we use the default hyperparameters in Hansen et al . ( 2022 ) . Environment Latent Dimension CNN channels Planning Iterations Hand-Pixel Hand-Proprio 128 128 64 – 6 12 σ 3 3 C ENVIRONMENT DETAILS C.1 ROBODESK ENVIRONMENT We use a modified version of RoboDesk ( Kannan et al. , 2021 ) , where a sequence of manipulation tasks ( flat block in bin , upright block off table , push green ) need to be completed in order . Figure 13 shows images of an agent successfully completing each of these tasks . In the original environment , dense rewards are based on Euclidean distances of objects to their targets , with additional terms to encourage the arm to reach the object . They typically range from 0 to 10 per timestep . We use these dense rewards together with a large sparse reward of 300 for each task completed . ( a ) Put the flat block into the bin ( b ) Push the upright block off the table ( c ) Press the green button Figure 13 : Subtasks for RoboDesk . C.2 HAND ENVIRONMENT We modified the Shadow Hand environment ( Plappert et al. , 2018 ) , so that the agent is required to achieve a sequence of pre-defined goal orientations in order . The first 3 goals are shown in Figure 14 , 14 while the subsequent goals are a repeat of the first 3 . The goal orientations are chosen so that the agent only has to rotate the cube along the z-axis , and we only require the agent to match the cube ’ s rotation to the goal , not its position . In the original environment , dense rewards are computed using r = − ( 10x + ∆θ ) , where x is the Euclidean distance to some fixed position , and ∆θ is the angular difference to the target orientation . In addition to these dense rewards , we provide a large sparse reward of 300 for each goal successfully achieved by the agent . ( a ) Goal 1 ( b ) Goal 2 ( c ) Goal 3 Figure 14 : Subtasks for Hand . C.3 AGX EARTHMOVING ENVIRONMENT The Earthmoving environment consists of a wheel loader , dump truck , a pile of dirt , with some rocks on top of the pile . The en- vironment is simulated using the realistic AGX Dynamics physics engine ( Algoryx , 2020 ) . The agent controls the wheel loader to pick up rocks and dump them in the dump truck . The starting positions of the dirt pile , wheel loader , and dump truck are all randomized , as are the initial orientations of the dirt pile and wheel loader . The agent ’ s observations consist of 3 components : a wide-angle egocentric RGB camera mounted on the cabin to allow navigation , an RGB camera mounted on the bucket for observing interactions with rocks , and proprioceptive observations ( positions , velocity , speed , force of actuators etc. ) . We use 64 × 64 × 3 images for all cameras , while the proprioceptive observation has 21 dimensions . Figure 15 : The agent uses one camera mounted on the cabin ( left ) for navigation , and one mounted on the bucket ( right ) for observing interactions with rocks and terrain . The action space is 4-dimensional : 2 dimensions for driving and steering the loader , and 2 dimensions for moving and tilting the bucket . The reward consists of a large sparse reward for rocks picked up and dumped , and dense rewards for moving rocks towards the dumptruck . The total reward rt at timestep t is computed using Equation ( 16 ) . rt = λdump ( mt ( cid:124 ) load ( max ( 2 , dt ) − max ( 2 , dt−1 ) ) ( cid:125 ) load − mt−1 load ) ( cid:125 ) dump ) + λload ( mt dump − mt−1 + λmovemt ( cid:124 ) ( cid:123 ) ( cid:122 ) dense reward ( cid:123 ) ( cid:122 ) sparse reward Where mdump , mload are rock masses in the dumptruck and the bucket respectively , d is the distance between the shovel and a point above the dumptruck , and λ are constants . ( 16 ) 15 D DMC AND ATARI BENCHMARKING RESULTS Figure 16 : Full learning curves for the DMC and Atari benchmarks . 16 DreamSmooth-Gaussian DreamSmooth-Uniform DreamSmooth-EMA DreamerV3 Hopper Hop 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Reacher Hard 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Cartpole Swingup Sparse 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Walker Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Finger Turn Hard 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Quadruped Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Cheetah Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Pong 3.00 −5.33 −13.67 e r o c S −22.00 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Breakout 16.00 10.67 5.33 e r o c S 0.00 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Freeway 29 19 9 e r o c S −1 0.0 0.2 Environment steps ( ×10⁶ ) 0.4 Assault 807.0 621.3 435.7 e r o c S 250.0 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Seaquest 545 378 211 e r o c S 44 0.0 0.2 Environment steps ( ×10⁶ ) 0.4 Hero 13200 e r o c S 8860 4520 180 0.0 0.4 0.2 Environment steps ( ×10⁶ )","['n', 'l', 'c', 'v', 'r', 'dreamsmooth', 'improve', 'modelbase', 'rein', 'forcement', 'learning', 'reward', 'smoothing', 'vint', 'pieter', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'gain', 'much', 'attention', 'ability', 'learn', 'complex', 'behavior', 'sampleefficient', 'way', 'planning', 'action', 'generate', 'imaginary', 'trajectory', 'predict', 'reward', 'success', 'find', 'surprisingly', 'reward', 'prediction', 'often', 'bottleneck', 'mbrl', 'especially', 'sparse', 'reward', 'challenge', 'even', 'ambiguous', 'predict', 'motivate', 'intuition', 'human', 'learn', 'rough', 'reward', 'estimate', 'propose', 'simple', 'yet', 'effective', 'reward', 'smooth', 'approach', 'dreamsmooth', 'learn', 'predict', 'temporallysmoothe', 'reward', 'instead', 'exact', 'reward', 'give', 'timestep', 'empirically', 'show', 'dreamsmooth', 'achieve', 'stateoftheart', 'performance', 'task', 'sample', 'efficiency', 'final', 'performance', 'lose', 'performance', 'common', 'benchmark', 'deepmind', 'control', 'suite', 'atari', 'benchmark', 'introduction', 'human', 'often', 'plan', 'action', 'rough', 'estimate', 'future', 'reward', 'instead', 'exact', 'reward', 'exact', 'moment', 'fiorillo', 'rough', 'reward', 'estimate', 'mostly', 'sufficient', 'learn', 'task', 'predict', 'exact', 'reward', 'often', 'challenge', 'ambiguous', 'delay', 'observable', 'consider', 'instance', 'manipulation', 'lustrate', 'figure', 'middle', 'push', 'block', 'table', 'sparse', 'reward', 'give', 'timestep', 'block', 'first', 'touch', 'bin', 'use', 'image', 'observa', 'tion', 'agent', 'challenge', 'even', 'man', 'predict', 'correct', 'sequence', 'reward', 'crucially', 'issue', 'present', 'many', 'environ', 'ment', 'state', 'reward', 'almost', 'indistinguishable', 'reward', 'figure', 'predict', 'exact', 'sequence', 'reward', 'extremely', 'difficult', 'example', 'show', 'sequence', 'image', 'observation', 'see', 'agent', 'receive', 'sparse', 'reward', 'little', 'visually', 'distinguish', 'timestep', 'large', 'reward', 'create', 'significant', 'challenge', 'reward', 'prediction', 'accurate', 'reward', 'model', 'vital', 'model', 'base', 'reinforcement', 'learning', 'mbrl', 'reward', 'estimate', 'high', 'cause', 'agent', 'choose', 'action', 'perform', 'poorly', 'reality', 'estimate', 'low', 'lead', 'agent', 'ignore', 'high', 'reward', 'difficulty', 'importance', 'reward', 'prediction', 'problem', 'mbrl', 'largely', 'overlook', 'find', 'even', 'stateoftheart', 'reward', 'prediction', 'challenge', 'also', 'performance', 'bottleneck', 'many', 'task', 'instance', 'dreamerv3', 'fail', 'predict', 'reward', 'objective', 'crafter', 'environment', 'similar', 'failure', 'mode', 'observe', 'variant', 'robodesk', 'shadow', 'hand', 'plappert', 'task', 'sparse', 'reward', 'inspire', 'human', 'intuition', 'rough', 'estimate', 'reward', 'sufficient', 'propose', 'simple', 'yet', 'effective', 'solution', 'dreamsmooth', 'learn', 'predict', 'temporallysmoothe', 'reward', 'rather', 'exact', 'reward', 'timestep', 'make', 'reward', 'prediction', 'much', 'easy', 'instead', 'predict', 'reward', 'exactly', 'model', 'need', 'produce', 'estimate', 'sparse', 'reward', 'obtain', 'sufficient', 'policy', 'learn', 'experiment', 'demonstrate', 'extremely', 'simple', 'technique', 'significantly', 'improve', 'performance', 'different', 'mbrl', 'algorithm', 'many', 'sparsereward', 'environment', 'specifically', 'find', 'dreamerv3', 'hansen', 'technique', 'especially', 'beneficial', 'environment', 'follow', 'characteristic', 'sparse', 'reward', 'partial', 'observability', 'stochastic', 'reward', 'finally', 'show', 'even', 'benchmark', 'reward', 'prediction', 'significant', 'issue', 'dreamsmooth', 'degrade', 'performance', 'indicate', 'technique', 'universally', 'apply', 'relate', 'work', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'leverage', 'dynamic', 'model', 'ie', 'world', 'model', 'environment', 'reward', 'model', 'desire', 'task', 'plan', 'sequence', 'action', 'maximize', 'total', 'reward', 'dynamic', 'model', 'predict', 'future', 'state', 'environment', 'take', 'specific', 'action', 'reward', 'model', 'predict', 'reward', 'correspond', 'stateaction', 'transition', 'dynamic', 'reward', 'model', 'agent', 'simulate', 'large', 'number', 'candidate', 'behavior', 'imagination', 'instead', 'physical', 'environment', 'allow', 'mbrl', 'tackle', 'many', 'challenging', 'task', 'silver', 'instead', 'rely', 'give', 'dynamic', 'reward', 'model', 'recent', 'advance', 'mbrl', 'enable', 'learn', 'world', 'model', 'highdimensional', 'observation', 'complex', 'dynamic', 'schmidhuber', 'schrittwieser', 'hafner', 'hansen', 'well', 'temporallyextende', 'world', 'model', 'specifically', 'dreamerv3', 'achieve', 'stateoftheart', 'performance', 'diverse', 'domain', 'problem', 'eg', 'pixel', 'state', 'observation', 'well', 'discrete', 'continuous', 'action', 'realistic', 'imagination', 'mbrl', 'require', 'accurate', 'world', 'model', 'significant', 'effort', 'learn', 'well', 'world', 'model', 'leverage', 'human', 'video', 'adopt', 'performant', 'architecture', 'deng', 'representation', 'learning', 'prototypebase', 'deng', 'objectcentric', 'state', 'representation', 'contrastive', 'learn', 'taniguchi', 'mask', 'autoencoding', 'seo', 'however', 'compare', 'effort', 'learn', 'well', 'world', 'model', 'learn', 'accurate', 'reward', 'model', 'largely', 'overlook', 'investigate', 'effect', 'various', 'world', 'model', 'design', 'show', 'reward', 'prediction', 'strongly', 'correlate', 'task', 'performance', 'train', 'offline', 'dataset', 'limit', 'densereward', 'environment', 'paper', 'point', 'accurate', 'reward', 'prediction', 'crucial', 'mbrl', 'especially', 'sparsereward', 'task', 'partially', 'observable', 'environment', 'propose', 'simple', 'method', 'improve', 'reward', 'prediction', 'mbrl', 'approach', 'main', 'goal', 'paper', 'understand', 'challenging', 'reward', 'prediction', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'propose', 'simple', 'yet', 'effective', 'solution', 'reward', 'smooth', 'make', 'reward', 'prediction', 'easy', 'learn', 'section', 'first', 'provide', 'background', 'mbrl', 'section', 'present', 'experiment', 'demonstrate', 'challenge', 'predict', 'sparse', 'reward', 'signal', 'section', 'finally', 'explain', 'approach', 'dreamsmooth', 'section', 'background', 'formulate', 'problem', 'partially', 'observable', 'markov', 'decision', 'process', 'pomdp', 'define', 'tuple', 'p', 'r', 'observation', 'space', 'action', 'space', 'p', 'timestep', 'transition', 'dynamic', 'r', 'reward', 'function', 'map', 'previous', 'observation', 'action', 'reward', 'r', 'o≤t', 'discount', 'factor', 'sutton', 'rl', 'aim', 'find', 'policy', 'π', 'o≤t', 'maximize', 'expect', 'sum', 'reward', 'paper', 'focus', 'mbrl', 'algorithm', 'learn', 'world', 'model', 'reward', 'model', 'rtzt', 'agent', 'experience', 'learned', 'latent', 'state', 'timestep', 'learned', 'world', 'model', 'reward', 'model', 'generate', 'imaginary', 'rollout', 'aτ', 'horizon', 'start', 'zt', 'use', 'planning', 'argenson', 'dulacarnold', 'hansen', 'policy', 'optimization', 'schmidhuber', 'hafner', 'specifically', 'use', 'stateoftheart', 'algorithm', 'dreamerv3', 'hansen', 'use', 'predict', 'reward', 'compute', 'new', 'value', 'target', 'train', 'critic', 'learn', 'good', 'policy', 'reward', 'model', 'play', 'vital', 'role', 'critic', 'actor', 'learn', 'policy', 'receive', 'training', 'signal', 'exclusively', 'reward', 'model', 'note', 'datum', 'collect', 'environment', 'use', 'train', 'world', 'model', 'reward', 'model', 'hand', 'learn', 'stateaction', 'value', 'function', 'zt', 'directly', 'agent', 'experience', 'predict', 'reward', 'however', 'reward', 'model', 'still', 'important', 'obtain', 'good', 'policy', 'tdmpc', 'use', 'reward', 'model', 'value', 'function', 'obtain', 'policy', 'online', 'planning', 'reward', 'prediction', 'difficult', 'reward', 'prediction', 'surprisingly', 'challenge', 'many', 'environment', 'figure', 'show', 'sequence', 'frame', 'right', 'sparse', 'reward', 'receive', 'diverse', 'environment', 'even', 'human', 'difficult', 'determine', 'exact', 'timestep', 'reward', 'receive', 'environment', 'hypothesize', 'mean', 'square', 'error', 'loss', 'e', 'z', 'r', 'rθ', 'z', 'r', 'typically', 'use', 'reward', 'model', 'training', 'deteriorate', 'reward', 'prediction', 'accuracy', 'exist', 'sparse', 'reward', 'predict', 'sparse', 'reward', 'single', 'step', 'early', 'later', 'result', 'high', 'loss', 'simply', 'predict', 'reward', 'step', 'thus', 'instead', 'try', 'predict', 'sparse', 'reward', 'exact', 'timestep', 'reward', 'model', 'minimize', 'loss', 'entirely', 'omitting', 'sparse', 'reward', 'prediction', 'verify', 'hypothesis', 'plot', 'groundtruth', 'dreamerv3', 'predict', 'reward', 'figure', 'reward', 'model', 'struggle', 'predict', 'exact', 'reward', 'simply', 'ignore', 'sparse', 'reward', 'straightforward', 'predict', 'task', 'describe', 'section', 'hypothesis', 'also', 'hold', 'deterministic', 'fullyobservable', 'environment', 'crafter', 'source', 'sparse', 'reward', 'reward', 'model', 'fail', 'predict', 'reward', 'source', 'figure', '2d', 'difficulty', 'reward', 'prediction', 'far', 'exacerbate', 'partial', 'observability', 'ambiguous', 'reward', 'stochastic', 'dynamic', 'environment', 'example', 'first', 'third', 'row', 'figure', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'figure', 'ground', 'truth', 'reward', 'dreamerv3', 'predict', 'reward', 'evaluation', 'episode', 'reward', 'model', 'miss', 'many', 'sparse', 'reward', 'highlight', 'yellow', 'ground', 'truth', 'predict', 'timestep', 'yaxis', 'reward', '−110', '−110', '−800', '−04', 'sparse', 'reward', 'give', 'block', 'rock', 'third', 'example', 'first', 'contact', 'dumptruck', 'exact', 'moment', 'contact', 'directly', 'observable', 'camera', 'viewpoint', 'make', 'reward', 'prediction', 'ambiguous', 'moreover', 'stochastic', 'environment', 'dynamic', 'contact', 'multiple', 'rock', 'make', 'predict', 'future', 'state', 'reward', 'challenge', 'reward', 'prediction', 'bottleneck', 'mbrl', 'precede', 'section', 'show', 'reward', 'prediction', 'challenge', 'many', 'environment', 'importantly', 'poor', 'reward', 'prediction', 'bottleneck', 'policy', 'learning', 'show', 'figure', 'robodesk', 'reward', 'model', 'reliably', 'detect', 'completion', 'second', 'task', 'figure', '2a', 'policy', 'get', 'stick', 'solve', 'first', 'task', 'fail', 'subsequent', 'task', 'earthmove', 'reward', 'model', 'capture', 'reward', 'successful', 'dumping', 'figure', 'policy', 'frequently', 'drop', 'rock', 'dumptruck', 'consistent', 'failure', 'mode', 'reward', 'prediction', 'policy', 'learning', 'dreamerv3', 'suggest', 'poor', 'reward', 'prediction', 'bottleneck', 'mbrl', 'robodesk', 'b', 'earthmove', 'figure', 'reward', 'model', 'inability', 'predict', 'sparse', 'reward', 'complete', 'task', 'lead', 'poor', 'task', 'performance', 'robodesk', 'agent', 'get', 'stick', 'learn', 'first', 'task', 'unable', 'learn', 'perform', 'subsequent', 'task', 'b', 'earthmove', 'policy', 'often', 'fail', 'dump', 'rock', 'accurately', 'dumptruck', 'learn', 'curve', 'average', 'seed', 'dreamsmooth', 'improve', 'mbrl', 'reward', 'smooth', 'address', 'reward', 'prediction', 'problem', 'propose', 'simple', 'yet', 'effective', 'solution', 'dreamsmooth', 'relax', 'require', 'ment', 'model', 'predict', 'sparse', 'reward', 'exact', 'timestep', 'perform', 'ral', 'smoothing', 'allow', 'reward', 'model', 'predict', 'reward', 'ground', 'truth', 'timestep', 'make', 'learn', 'ier', 'especially', 'reward', 'ambiguous', 'sparse', 'gaussian', 'uniform', 'c', 'figure', 'reward', 'smooth', 'sparse', 'reward', 'σ', 'δ', 'smooth', 'hyperparameter', 'specifically', 'dreamsmooth', 'apply', 'temporal', 'smoothing', 'reward', 'collect', 'new', 'episode', 'dreamsmooth', 'work', 'smoothing', 'function', 'preserve', 'sum', 'reward', 'rt−l', 'fi', 'rclip', 'ti0', 'i−l', 'fi', 'l', 'denote', 'episode', 'smooth', 'horizon', 'respectively', 'simplicity', 'omit', 'discount', 'factor', 'equation', 'full', 'equation', 'find', 'equation', 'episode', 'smoothed', 'reward', 'store', 'replay', 'buffer', 'use', 'train', 'reward', 'model', 'agent', 'learn', 'smoothed', 'reward', 'ever', 'see', 'original', 'reward', 'smoothed', 'reward', 'ease', 'reward', 'prediction', 'allow', 'model', 'predict', 'reward', 'several', 'timestep', 'early', 'later', 'incur', 'large', 'loss', 'paper', 'investigate', 'popular', 'smoothing', 'function', 'gaussian', 'uniform', 'exponential', 'move', 'average', 'smoothing', 'illustrate', 'figure', 'main', 'motivation', 'smoothing', 'make', 'easy', 'learn', 'reward', 'model', 'note', 'reward', 'smooth', 'case', 'preserve', 'optimality', 'optimal', 'policy', 'smoothed', 'reward', 'l', 'e', 'c', 'environment', 'step', '×10⁶', 'e', 'k', 'r', 'environment', 'step', 'r', 'w', 'e', 'r', 'orginal', 'σ', 'timestep', 'r', 'w', 'e', 'r', 'orginal', 'δ', 'δ', 'timestep', 'r', 'w', 'e', 'r', 'orginal', 'α', 'timestep', 'also', 'optimal', 'original', 'reward', 'r', 'particular', 'provide', 'proof', 'optimality', 'smoothing', 'smoothing', 'function', 'fi', 'augment', 'pomdp', 'state', 'history', 'past', 'state', 'however', 'future', 'reward', 'use', 'smooth', 'gaussian', 'smooth', 'smoothed', 'reward', 'condition', 'policy', 'long', 'define', 'equivalent', 'pomdp', 'case', 'theoretical', 'guarantee', 'even', 'empirically', 'show', 'reward', 'model', 'adapt', 'prediction', 'change', 'policy', 'achieve', 'performance', 'improvement', 'implementation', 'dreamsmooth', 'extremely', 'simple', 'require', 'additional', 'line', 'code', 'exist', 'mbrl', 'algorithm', 'show', 'overhead', 'reward', 'smoothing', 'minimal', 'time', 'complexity', 'l', 'implementation', 'detail', 'find', 'collect', 'rollout', 'π', 'policy', 'replay', 'buffer', 'dreamsmooth', 'π', '∪', 'ot', 't1', 't1', '▷', 'line', 'need', 'add', 'experiment', 'paper', 'propose', 'simple', 'reward', 'smooth', 'method', 'dreamsmooth', 'facilitate', 'reward', 'prediction', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'thus', 'improve', 'performance', 'exist', 'mbrl', 'method', 'experiment', 'aim', 'answer', 'follow', 'question', 'reward', 'smooth', 'improve', 'reward', 'prediction', 'well', 'reward', 'prediction', 'reward', 'smooth', 'lead', 'well', 'sample', 'efficiency', 'asymptotic', 'performance', 'mbrl', 'sparsereward', 'task', 'mbrl', 'reward', 'smoothing', 'also', 'work', 'common', 'densereward', 'task', 'task', 'evaluate', 'dreamsmooth', 'task', 'sparse', 'subtask', 'completion', 'reward', 'common', 'rl', 'benchmark', 'earthmove', 'use', 'image', 'observation', 'task', 'use', 'single', 'image', 'see', 'c', 'environment', 'detail', 'robodesk', 'use', 'modify', 'version', 'robodesk', 'sequence', 'nipulation', 'task', 'flat', 'block', 'upright', 'block', 'table', 'push', 'green', 'need', 'complete', 'order', 'figure', 'use', 'original', 'dense', 'reward', 'together', 'large', 'sparse', 'reward', 'task', 'complete', 'hand', 'hand', 'task', 'plappert', 'require', 'shadow', 'hand', 'rotate', 'block', 'hand', 'specific', 'orientation', 'extend', 'achieve', 'sequence', 'predefine', 'goal', 'orientation', 'order', 'addition', 'original', 'dense', 'reward', 'provide', 'large', 'sparse', 'reward', 'goal', '•', 'earthmove', 'earthmove', 'task', 'consist', 'wheel', 'loader', 'dump', 'truck', 'pile', 'rock', 'figure', 'agent', 'control', 'wheel', 'loader', 'pick', 'rock', 'pile', 'dump', 'dump', 'truck', 'large', 'sparse', 'reward', 'give', 'rock', 'pick', 'rock', 'dump', 'proportional', 'mass', 'addition', 'dense', 'reward', 'give', 'move', 'rock', 'dump', 'truck', 'environment', 'simulate', 'use', 'dynamic', 'physics', 'engine', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'e', 'dmc', 'atari', 'figure', 'evaluate', 'dreamsmooth', 'task', 'sparse', 'subtask', 'completion', 'reward', 'ad', 'also', 'test', 'popular', 'benchmark', 'e', 'deepmind', 'control', 'suite', 'atari', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'figure', 'visualize', 'ground', 'truth', 'reward', 'smoothed', 'reward', 'gaussian', 'smoothing', 'predict', 'reward', 'dreamerv3', 'train', 'smoothed', 'reward', 'evaluation', 'episode', 'contrast', 'figure', 'reward', 'model', 'reward', 'smoothing', 'capture', 'sparse', 'reward', 'crafter', 'crafter', 'hafner', 'minecraftlike', 'environment', 'agent', 'try', 'collect', 'place', 'craft', 'item', 'order', 'survive', 'achievement', 'environment', 'collect', 'water', 'mining', 'diamond', 'sparse', 'reward', 'obtain', 'achievement', 'first', 'time', 'small', 'reward', 'give', 'lose', 'health', 'point', 'gain', 'lose', 'dmc', 'benchmark', 'deepmind', 'control', 'continuous', 'control', 'task', 'tassa', 'atari', 'benchmark', 'atari', 'task', 'bellemare', 'step', 'improve', 'reward', 'prediction', 'reward', 'smooth', 'first', 'visualize', 'ground', 'truth', 'reward', 'smoothed', 'reward', 'gaussian', 'smoothing', 'ward', 'prediction', 'result', 'dreamerv3', 'train', 'dreamsmooth', 'figure', 'observe', 'ward', 'smoothing', 'lead', 'significant', 'improve', 'ment', 'reward', 'prediction', 'dreamsmooth', 'success', 'fully', 'predict', 'smoothed', 'sparse', 'ward', 'long', 'omit', 'vital', 'signal', 'policy', 'learning', 'planning', 'improvement', 'especially', 'notable', 'crafter', 'figure', 'measure', 'accuracy', 'ward', 'model', 'predict', 'reward', 'large', 'half', 'original', 'smoothed', 'reward', 'dream', 'erv3', 'dreamsmooth', 'respectively', 'exact', 'timestep', 'subtask', 'vanilla', 'dream', 'reward', 'model', 'baseline', 'miss', 'sparse', 'reward', 'predict', 'sparse', 'reward', 'accurately', 'subtask', 'result', 'figure', 'reward', 'prediction', 'rate', 'achieve', 'ment', 'crafter', 'task', 'never', 'achieve', 'method', 'reward', 'smooth', 'prediction', 'rate', 'well', 'task', 'compare', 'vanilla', 'dreamsmooth', 'backbone', 'also', 'dreamerv3', 'dreamsmooth', 'evaluate', 'gaussian', 'uniform', 'smooth', 'ground', 'truth', 'smooth', 'predict', 'timestep', 'yaxis', 'reward', '−200', 'baseline', 'smooth', 'gaussian', 'e', 'r', 'n', 'c', 'e', 'r', 'p', 'e', 'bie', 'alth', 'loss', 'ollect', 'drin', 'picka', 'ollect', 'sto', 'place', 'e', 'efe', 'efe', 'c', 'c', 'ble', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'e', 'dmc', 'atari', 'figure', 'comparison', 'learn', 'curve', 'dreamsmooth', 'gaussian', 'uniform', 'dream', 'erv3', 'shaded', 'region', 'ad', 'show', 'maximum', 'minimum', 'seed', 'dmc', 'e', 'aggregate', 'result', 'task', 'respectively', 'display', 'standard', 'deviation', 'hyperparameter', 'dreamerv3', 'smoothing', 'function', 'find', 'show', 'figure', 'dreamsmoothgaussian', 'dreamsmoothuniform', 'significantly', 'improve', 'performance', 'well', 'sample', 'efficiency', 'dreamerv3', 'robodesk', 'hand', 'earthmoving', 'task', 'change', 'improved', 'reward', 'prediction', 'show', 'section', 'result', 'suggest', 'reward', 'prediction', 'major', 'bottleneck', 'mbrl', 'performance', 'smoothing', 'method', 'lead', 'improvement', 'dreamerv3', 'gaussian', 'smoothing', 'generally', 'perform', 'good', 'crafter', 'uniform', 'smooth', 'show', 'comparable', 'performance', 'well', 'performance', 'gaussian', 'uniform', 'smoothing', 'allow', 'predict', 'reward', 'early', 'later', 'smoothing', 'allow', 'predict', 'reward', 'later', 'improved', 'reward', 'prediction', 'accuracy', 'dreamsmoothgaussian', 'dreamsmooth', 'uniform', 'perform', 'bad', 'baseline', 'crafter', 'predict', 'task', 'reward', 'encourage', 'exploitation', 'less', 'exploration', 'investigation', 'tradeoff', 'promising', 'direction', 'future', 'work', 'moreover', 'observe', 'dmc', 'atari', 'benchmark', 'reward', 'prediction', 'particu', 'larly', 'challenge', 'technique', 'show', 'comparable', 'performance', 'unmodified', 'algorithm', 'see', 'figure', 'full', 'result', 'suggest', 'reward', 'smoothing', 'apply', 'generally', 'hinder', 'performance', 'environment', 'dreamsmoothgaussian', 'dreamerv3', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'e', 'k', 'r', 'r', 'e', 'environment', 'step', 'e', 'r', 'c', 'environment', 'step', 'e', 'r', 'c', 'environment', 'step', 'e', 'r', 'e', 'z', 'l', 'r', 'h', 'environment', 'step', 'figure', 'dreamsmooth', 'also', 'improve', 'performance', 'tdmpc', 'hand', 'task', 'vanilla', 'tdmpc', 'unable', 'consistently', 'solve', 'first', 'task', 'even', 'proprioceptive', 'state', 'observation', 'however', 'tdmpc', 'dreamsmooth', 'learn', 'complete', 'task', 'state', 'observation', 'also', 'pixel', 'observa', 'tion', 'suggest', 'dreamsmooth', 'useful', 'broad', 'range', 'mbrl', 'rithms', 'use', 'reward', 'model', 'demonstrate', 'hand', 'task', 'tdmpc', 'fail', 'sparsereward', 'task', 'ablation', 'study', 'hand', 'hand', 'state', 'figure', 'learn', 'curve', 'tdmpc', 'tdmpc', 'dreamsmooth', 'hand', 'task', 'shaded', 'gion', 'show', 'minimum', 'maximum', 'seed', 'datum', 'imbalance', 'possible', 'cause', 'poor', 'reward', 'prediction', 'datum', 'imbalance', 'sparse', 'reward', 'infrequent', 'sequence', 'taine', 'sparse', 'reward', 'rarely', 'sample', 'replay', 'buffer', 'reward', 'model', 'therefore', 'train', 'example', 'sparse', 'reward', 'tentially', 'lead', 'poor', 'prediction', 'test', 'hypothesis', 'conduct', 'experiment', 'oversample', 'probability', 'p', 'sample', 'sequence', 'agent', 'receive', 'sparse', 'reward', 'otherwise', 'sample', 'formly', 'sequence', 'buffer', 'show', 'figure', 'oversampling', 'perform', 'bet', 'ter', 'baseline', 'learn', 'slow', 'dreamsmooth', 'suggest', 'datum', 'imbalance', 'largely', 'contribute', 'difficulty', 'reward', 'prediction', 'factor', 'hinder', 'performance', 'furthermore', 'sample', 'method', 'require', 'domain', 'knowledge', 'reward', 'signal', 'oversample', 'dreamsmooth', 'agnostic', 'scale', 'frequency', 'sparse', 'reward', 'figure', 'use', 'oversampling', 'sequence', 'sparse', 'reward', 'p', 'perform', 'well', 'dreamerv3', 'robodesk', 'bad', 'dreamsmooth', 'gaussian', 'smooth', 'line', 'show', 'median', 'task', 'performance', 'seed', 'shaded', 'region', 'show', 'maximum', 'minimum', 'reward', 'model', 'size', 'sis', 'poor', 'reward', 'prediction', 'reward', 'model', 'enough', 'pacity', 'capture', 'sparse', 'reward', 'test', 'hypothesis', 'increase', 'size', 'reward', 'model', 'layer', 'unit', 'layere', 'unit', 'layer', 'unit', 'keep', 'rest', 'world', 'model', 'observe', 'fig', 'ure', 'smooth', 'change', 'reward', 'model', 'size', 'negligible', 'impact', 'performance', 'dreamsmooth', 'perform', 'reward', 'model', 'size', 'test', 'indicate', 'reward', 'prediction', 'problem', 'simply', 'cause', 'cient', 'model', 'capacity', 'robodesk', 'b', 'hand', 'figure', 'simply', 'increase', 'reward', 'model', 'size', 'negligible', 'impact', 'performance', 'dreamsmooth', 'use', 'layer', 'unit', 'dreamerv3', 'use', 'layer', 'unit', 'dreamerv31280', 'use', 'layer', 'unit', 'figure', 'analyze', 'impact', 'smooth', 'parameter', 'smoothing', 'parameter', 'gaussian', 'respectively', 'robodesk', 'hand', 'observe', 'dreamsmooth', 'insensitive', 'smooth', 'parameter', 'perform', 'well', 'wide', 'range', 'value', 'dreamsmoothgaussian', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'dreamsmooth', 'oversample', 'environment', 'step', 'dreamsmooth', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'gaussian', 'smoothing', 'robodesk', 'gaussian', 'smoothing', 'hand', 'c', 'uniform', 'smooth', 'robodesk', 'uniform', 'smoothing', 'hand', 'e', 'smoothing', 'robodesk', 'smoothing', 'hand', 'figure', 'parameter', 'sweep', 'smooth', 'parameter', 'δ', 'line', 'show', 'median', 'task', 'performance', 'seed', 'shaded', 'region', 'show', 'maximum', 'minimum', 'conclusion', 'paper', 'identify', 'reward', 'prediction', 'problem', 'mbrl', 'provide', 'simple', 'yet', 'effective', 'solution', 'reward', 'smooth', 'approach', 'dreamsmooth', 'demonstrate', 'superior', 'performance', 'sparse', 'reward', 'task', 'reward', 'prediction', 'trivial', 'mainly', 'partial', 'observability', 'stochasticity', 'environment', 'moreover', 'dreamsmooth', 'show', 'comparable', 'result', 'commonly', 'use', 'benchmark', 'dmc', 'atari', 'show', 'taskagnostic', 'nature', 'show', 'simple', 'reward', 'smooth', 'approach', 'mitigate', 'difficulty', 'reward', 'prediction', 'improved', 'reward', 'prediction', 'always', 'improve', 'task', 'performance', 'eg', 'crafter', 'predict', 'task', 'reward', 'encourage', 'exploitation', 'less', 'exploration', 'investigation', 'tradeoff', 'promising', 'direction', 'future', 'work', 'dreamsmoothgaussian', 'cid3', 'cid8', 'cid3', 'cid4', 'cid3', 'cid8', 'cid3', 'cid3', 'cid8', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', '×10⁶', 'cid3', 'cid8', 'cid3', 'cid3', 'cid8', 'cid3', 'cid8', 'cid3', 'cid4', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'dreamsmoothema', 'cid10', 'cid3', 'cid4', 'cid10', 'cid3', 'cid4', 'cid8', 'cid17', 'cid10', 'cid3', 'cid4', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'acknowledgment', 'work', 'support', 'part', 'bair', 'industrial', 'consortium', 'logistic', 'robotic', 'like', 'thank', 'member', 'robot', 'learn', 'lab', 'insightful', 'feedback', 'reference', 'dynamic', 'url', 'https', 'modelbase', 'offline', 'planning', 'international', 'ence', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'omnb1g5xzd4', 'mohammad', 'saffar', 'model', 'pixel', 'reward', 'evaluate', 'design', 'tradeoff', 'visual', 'modelbase', 'reinforcement', 'learning', 'arxiv', 'preprint', 'bellemare', 'veness', 'bowl', 'arcade', 'learn', 'environment', 'evaluation', 'platform', 'general', 'agent', 'artificial', 'intelligence', 'research', 'dreamerpro', 'reconstructionfree', 'modelbase', 'reinforce', 'ment', 'learn', 'prototypical', 'representation', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'park', 'face', 'world', 'model', 'backbone', 'rnns', 'transformer', 'arxiv', 'preprint', 'christopher', 'fiorillo', 'newsome', 'wolfram', 'schultz', 'temporal', 'precision', 'reward', 'prediction', 'dopamine', 'neuron', 'nature', 'neuroscience', 'schmidhuber', 'world', 'model', 'arxiv', 'preprint', 'hafner', 'benchmarke', 'spectrum', 'agent', 'capability', 'international', 'conference', 'learn', 'representation', 'dream', 'control', 'learn', 'behavior', 'latent', 'imagination', 'international', 'conference', 'learn', 'representation', 'master', 'atari', 'discrete', 'world', 'model', 'international', 'conference', 'learn', 'representation', 'master', 'diverse', 'domain', 'world', 'model', 'arxiv', 'preprint', 'temporal', 'difference', 'learn', 'model', 'predictive', 'control', 'international', 'conference', 'machine', 'learn', 'robodesk', 'multitask', 'rein', 'learning', 'benchmark', 'https', 'hunt', 'r', 'raymond', 'dissociable', 'reward', 'time', 'signal', 'human', 'midbrain', 'ventral', 'striatum', 'bahl', 'deepak', 'pathak', 'structured', 'world', 'model', 'human', 'video', 'robotic', 'science', 'system', 'policy', 'invariance', 'reward', 'transformation', 'theory', 'application', 'reward', 'shape', 'international', 'conference', 'machine', 'learn', 'volume', 'pp', 'taniguchi', 'dream', 'modelbase', 'reinforcement', 'learning', 'latent', 'imagination', 'reconstruction', 'international', 'conference', 'robotic', 'automation', 'pp', 'doi', 'matthia', 'plappert', 'bowen', 'schneider', 'welinder', 'reinforcement', 'learn', 'challenging', 'robotic', 'environment', 'request', 'research', 'arxiv', 'preprint', 'schrittwieser', 'ioannis', 'laurent', 'sifre', 'schmitt', 'hassabis', 'thore', 'et', 'master', 'atari', 'go', 'chess', 'shogi', 'plan', 'learned', 'model', 'nature', 'younggyo', 'seo', 'pieter', 'abbeel', 'mask', 'world', 'model', 'visual', 'control', 'conference', 'robot', 'learn', 'seo', 'pieter', 'abbeel', 'multiview', 'mask', 'world', 'model', 'visual', 'robotic', 'manipulation', 'international', 'conference', 'machine', 'learn', 'skillbase', 'modelbase', 'reinforcement', 'learning', 'conference', 'robot', 'learn', 'aja', 'maddison', 'schrittwieser', 'ioannis', 'et', 'master', 'game', 'go', 'deep', 'neural', 'network', 'tree', 'search', 'nature', 'schrittwieser', 'ioannis', 'antonoglou', 'aja', 'et', 'master', 'game', 'go', 'human', 'knowledge', 'nature', 'schrittwieser', 'ioannis', 'lanctot', 'laurent', 'sifre', 'thore', 'et', 'general', 'reinforcement', 'learning', 'master', 'chess', 'go', 'selfplay', 'science', 'gautam', 'structured', 'world', 'belief', 'reinforcement', 'learning', 'pomdp', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'reinforcement', 'learn', 'introduction', 'mit', 'press', 'yuval', 'tassa', 'martin', 'riedmiller', 'deepmind', 'control', 'suite', 'arxiv', 'preprint', 'proof', 'let', 'p', 'r', 'give', 'mdp', 'loss', 'generality', 'assume', 'augment', 'form', 'mdp', 'state', 'include', 'entire', 'history', 'state', 'thus', 'reward', 'function', 'r', '˜r', 'access', 'previous', 'state', 'theorem', 'optimal', 'policy', '˜π∗', 'mdp', 'reward', 'smooth', 'past', 'reward', 'smoothing', 'p', 'also', 'optimal', 'original', 'mdp', 'i−l', 'fi', 'i−l', 'fi', 'proof', 'use', 'theorem', 'reward', 'shape', 'guarantee', 'optimal', 'policy', 'introduce', 'modify', 'reward', 'function', 'represent', 'form', 'r', 'st1', '−φ', 'potential', 'function', 'φ', 'new', 'reward', 'function', 'yield', 'optimal', 'policy', 'original', 'reward', 'function', 'r', 'let', 'potential', 'function', 'reward', 'smooth', 'φ', '−1', 'i−l', 'i−l', 'ji1', 'reward', 'shape', 'term', 'represent', 'difference', 'potential', 'function', 'st1', 'follow', 'st1', 'i−l', 'fi', 'st1', 'i−l', 'fi', 'hence', 'follow', 'reward', 'shape', 'smoothing', 'guarantee', 'optimal', 'policy', 'original', 'mdp', 'however', 'theorem', 'apply', 'smooth', 'function', 'require', 'access', 'future', 'reward', 'gaussian', 'smoothing', 'gaussian', 'smooth', 'smoothed', 'reward', 'function', 'require', 'future', 'reward', 'condition', 'current', 'policy', 'reward', 'model', 'case', 'theoretical', 'guarantee', 'experiment', 'empirically', 'show', 'reward', 'model', 'adapt', 'prediction', 'change', 'policy', 'thus', 'improve', 'mbrl', 'instead', 'intuitively', 'explain', 'optimal', 'policy', 'reward', 'smooth', 'even', 'reward', 'function', 'hoc', 'define', 'mdps', 'also', 'optimal', 'original', 'reward', 'function', 'theorem', 'optimal', 'policy', '˜π∗', 'smoothed', 'reward', 'function', 'also', 'optimal', 'original', 'reward', 'function', 'r', 'l', 'γclip', '−t', '−t', 'fi', 'sclip', 'ti0', 'l', 'i−l', 'fi', 'proof', 'first', 'show', 'discount', 'sum', 'original', 'reward', 'smooth', 'reward', 'trajectory', 'l', 'γclip', '−t', '−t', 'fi', 'sclip', 'ti0', 'equation', 'γtr', 'cid88', 'γtr', 'l', 'cid88', 'fi', 'i−l', 'γtr', 'cid88', 'l', 'i−l', 'fi', 'let', 'optimal', 'policy', 'smoothed', 'reward', '˜π∗', 'assume', '˜π∗', 'optimal', 'original', 'reward', 'r', 's0', 'e', 'cid104', 'cid88', 'e', 'cid104', 'γt', 'however', 'e', 'cid104', 'cid88', 'e', 'e', 'cid104', 'cid88', 'cid104', 'cid88', 'equation', 'equation', 'contradict', '˜π∗', 'optimal', 'therefore', 'optimal', 'policy', '˜π∗', 'guarantee', 'optimality', 'r', 'b', 'implementation', 'detail', 'model', 'train', 'rtx', 'rtx', 'rtx', 'gpu', 'experiment', 'take', 'hour', 'robodesk', 'hour', 'hand', 'hour', 'earthmove', 'hour', 'crafter', 'hour', 'atari', 'dmc', 'task', 'dreamsmooth', 'smoothing', 'function', 'gaussian', 'smoothing', 'follow', 'gaussian', 'distribution', 'fi', 'l', 'e', 'implement', 'use', 'normalization', 'constant', 'scipyndimagegaussianfilter1d', 'reward', 'mode', 'near', 'uniform', 'smoothing', 'distribute', 'reward', 'equally', 'consecutive', 'timestep', 'δ', 'δ', 'δ', 'fi', 'cid104', 'implement', 'use', 'scipyndimageconvolve', 'reward', 'filter', 'mode', 'near', 'smoothing', 'use', 'follow', 'smoothing', 'function', 'fi', '≤', 'implement', 'perform', 'following', 'timestep', 'reward', 'alpha', 'reward', 'alpha', 'reward', 'b2', 'modelbase', 'reinforcement', 'learning', 'backbone', 'hyperparameter', 'dreamerv3', 'experiment', 'show', 'table', 'tdmpc', 'table', 'table', 'dreamerv3', 'hyperparameter', 'episode', 'length', 'measure', 'environment', 'step', 'number', 'agent', 'step', 'multiply', 'action', 'repeat', 'model', 'size', 'list', 'hafner', 'also', 'refer', 'hyperparameter', 'environment', 'action', 'repeat', 'episode', 'length', 'train', 'ratio', 'model', 'size', 'earthmove', 'robodesk', 'hand', 'crafter', 'dmc', 'atari', 'variable', 'variable', 'l', 'δ', 'table', 'tdmpc', 'hyperparameter', 'specify', 'use', 'default', 'hyperparameter', 'environment', 'latent', 'dimension', 'channel', 'plan', 'iteration', 'σ', 'c', 'environment', 'detail', 'c1', 'robodesk', 'environment', 'use', 'modify', 'version', 'robodesk', 'sequence', 'manipulation', 'task', 'flat', 'block', 'upright', 'block', 'table', 'push', 'green', 'need', 'complete', 'order', 'figure', 'show', 'image', 'agent', 'successfully', 'complete', 'task', 'original', 'environment', 'dense', 'reward', 'base', 'euclidean', 'distance', 'object', 'target', 'additional', 'term', 'encourage', 'arm', 'reach', 'object', 'typically', 'range', 'timestep', 'use', 'dense', 'reward', 'together', 'large', 'sparse', 'reward', 'task', 'complete', 'put', 'flat', 'block', 'push', 'upright', 'block', 'table', 'press', 'figure', 'subtask', 'robodesk', 'hand', 'environment', 'modify', 'shadow', 'hand', 'environment', 'agent', 'require', 'achieve', 'sequence', 'predefine', 'goal', 'orientation', 'order', 'first', 'goal', 'show', 'figure', 'subsequent', 'goal', 'repeat', 'first', 'goal', 'orientation', 'choose', 'agent', 'rotate', 'cube', 'zaxis', 'require', 'agent', 'match', 'cube', 'rotation', 'goal', 'position', 'original', 'environment', 'dense', 'reward', 'compute', 'use', 'r', '10x', 'euclidean', 'distance', 'fix', 'position', 'angular', 'difference', 'target', 'orientation', 'addition', 'dense', 'reward', 'provide', 'large', 'sparse', 'reward', 'goal', 'successfully', 'achieve', 'agent', 'goal', 'b', 'goal', 'c', 'goal', 'figure', 'subtask', 'hand', 'c3', 'earthmove', 'environment', 'earthmove', 'environment', 'consist', 'wheel', 'loader', 'dump', 'truck', 'pile', 'dirt', 'rock', 'top', 'pile', 'en', 'vironment', 'simulate', 'use', 'realistic', 'dynamic', 'physics', 'engine', 'agent', 'control', 'wheel', 'loader', 'pick', 'rock', 'dump', 'dump', 'truck', 'start', 'position', 'dirt', 'pile', 'wheel', 'loader', 'dump', 'truck', 'randomize', 'initial', 'orientation', 'dirt', 'pile', 'wheel', 'loader', 'agent', 'observation', 'consist', 'component', 'wideangle', 'egocentric', 'camera', 'mount', 'cabin', 'allow', 'navigation', 'camera', 'mount', 'bucket', 'observe', 'interaction', 'rock', 'proprioceptive', 'observation', 'position', 'velocity', 'speed', 'force', 'actuator', 'use', '×', 'image', 'camera', 'proprioceptive', 'observation', 'dimension', 'figure', 'agent', 'use', 'camera', 'mount', 'cabin', 'leave', 'navigation', 'mount', 'bucket', 'right', 'observe', 'interaction', 'rock', 'terrain', 'action', 'space', 'dimension', 'drive', 'steer', 'loader', 'dimension', 'move', 'tilt', 'bucket', 'reward', 'consist', 'large', 'sparse', 'reward', 'rock', 'pick', 'dump', 'dense', 'reward', 'move', 'rock', 'dumptruck', 'total', 'reward', 'timestep', 'compute', 'use', 'equation', 'λdump', 'load', 'cid125', 'load', 'load', 'cid125', 'dump', 'λload', 'dump', 'dense', 'reward', 'reward', 'mdump', 'mload', 'rock', 'masse', 'dumptruck', 'bucket', 'respectively', 'distance', 'shovel', 'point', 'dumptruck', 'constant', 'dmc', 'atari', 'benchmarking', 'result', 'figure', 'full', 'learning', 'curve', 'dmc', 'atari', 'benchmark', 'dreamsmoothgaussian', 'e', 'r', 'c', 'environment', 'step', 'reacher', 'hard', 'e', 'r', 'c', 'environment', 'step', '×10⁶', 'cartpole', 'sparse', 'e', 'r', 'c', 'environment', 'step', 'walker', 'run', 'e', 'r', 'c', 'environment', 'step', 'finger', 'turn', 'hard', 'e', 'r', 'c', 'environment', 'step', '×10⁶', 'quadrupe', 'run', 'e', 'r', 'c', 'environment', 'step', 'run', 'e', 'r', 'c', 'environment', 'step', 'pong', 'r', 'c', 'environment', 'step', 'breakout', 'e', 'r', 'c', 'environment', 'step', 'freeway', 'e', 'r', '−1', 'environment', 'step', 'assault', 'c', 'environment', 'step', 'seaqu', 'e', 'r', 'c', 'environment', 'step', 'hero', 'r', 'c', 'environment', 'step']",
RETSim: Resilient and Efficient Text Similarity,"[{'href': 'http://arxiv.org/abs/2311.17264v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17264v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-28 22:54:33,"3
2
0
2

v
o
N
7

]

G
L
.
s
c
[

1
v
1
1
7
3
0
.
1
1
3
2
:
v
i
X
r
a

MITIGATING ESTIMATION ERRORS BY TWIN TD-
REGULARIZED ACTOR AND CRITIC FOR DEEP REIN-
FORCEMENT LEARNING

Junmin Zhong
Arizona State University

Ruofan Wu
Arizona State University

Jennie Si ∗
Arizona State University

ABSTRACT

We address the issue of estimation bias in deep reinforcement learning (DRL) by
introducing solution mechanisms that include a new, twin TD-regularized actor-
critic (TDR) method. It aims at reducing both over and under estimation errors.
With TDR and by combining good DRL improvements, such as distributional
learning and long N -step surrogate stage reward (LNSS) method, we show that
our new TDR-based actor-critic learning has enabled DRL methods to outper-
form their respective baselines in challenging environments in DeepMind Control
Suite. Furthermore, they elevate TD3 and SAC respectively to a level of perfor-
mance comparable to that of D4PG (the current SOTA), and they also improve the
performance of D4PG to a new SOTA level measured by mean reward, conver-
gence speed, learning success rate, and learning variance.

1

INTRODUCTION

Reinforcement learning (RL) has been developed for decades to provide a mathematical formal-
ism for learning-based control. Recently, significant progress has been made to attain excellent
results for a wide range of high-dimensional and continuous state-action space problems especially
in robotics applications, such as robot manipulation (Andrychowicz et al., 2017), and human-robotic
interaction (Liu et al., 2022; Wu et al., 2022).

However, the fundamental issue of estimation error associated with actor-critic RL (Van Hasselt
et al., 2016; Duan et al., 2021) still poses great challenge. Overestimation due to, for example, using
the max operator in updates has been identified and studied (Thrun & Schwartz, 1993; Duan et al.,
2021). To reduce it, most efforts have focused on attaining more accurate and stable critic networks.
TD3 (Fujimoto et al., 2018) applies clipped double Q-learning by taking the minimum between the
two Q estimates. SAC (Haarnoja et al., 2018) utilizes the double Q network and incorporates entropy
regularization in the critic objective function to ensure more exploratory behavior to help alleviate
the overestimation problem. However, directly taking the minimum value of the target networks
such as that in TD3 and SAC has been reported to result in an underestimation bias (Fujimoto et al.,
2018).

Evaluations have revealed multiple roles of over and under estimation errors in learning. On one
hand, overestimation may not always be harmful (Lan et al., 2020) as it is considered playing a
role of encouraging exploration by overestimated actions. Along this line, underestimation bias
may discourage exploration. If the overestimation bias occurs in a high-value region containing the
optimal policy, then encouraging exploration is a good thing (Hailu & Sommer, 1999). On the other
hand, overestimation bias may also cause an agent to overly explore a low-value region. This may
lead to a suboptimal policy. Accordingly, an underestimation bias may discourage an agent from
exploring high-value regions or avoiding low-value regions. All things considered, if estimation
errors are left unchecked, they may accumulate to negatively impact policy updates as suboptimal
actions may be highly rated by a suboptimal critic, reinforcing the suboptimal action in the next
policy update (Fujimoto et al., 2018). Aside from the anecdotal evidence on the roles of over and
under estimation, how to mitigate both of them in a principled way remains an open issue.

∗si@asu.edu

1

 
 
 
 
 
 
While several methods and evaluations have been performed and shown promising, a major tool has
been mostly left out thus far. That is, it is still not clear how, and if it is possible, to further reduce
estimation errors by considering the actor given the interplay between the actor and the critic. Only
a handful of approaches have been examined. As shown in (Wu et al., 2023) with demonstrated
performance improvement, PAAC uses a phased actor to account for both a Q value and a TD error
in actor update. A double actor idea was proposed and evaluated in (Lyu et al., 2022). It takes the
minimum value estimate associated with one of the two actor networks. However, directly using the
minimum of the estimated values was shown resulting in an underestimation error, similar to that in
TD3. Other methods, such as Entropy (Haarnoja et al., 2018; Fox et al., 2015), mutual-information
(MI) (Leibfried & Grau-Moya, 2020), and Kullback-Leibler (KL) (Vieillard et al., 2020; Rudner
et al., 2021) regularization, are also used to enhance policy exploration, robustness, and stability.
TD-regularized actor-critic (Parisi et al., 2019) regularizes the actor only aiming to enhance the
stability of the actor learning by applying a TD error (same as that in online critic updates) as a
regularization term in actor updates. However, none of these methods have shown how regularization
in actor may help reduce estimation error in the critic.

In this paper, we propose a new, TD-regularized (TDR) learning mechanism which includes TD-
regularized double critic networks and TD-regularized actor network. This new architecture has
several properties that make it ideal for the enhancements we consider. For the TD-regularized
double critic network, instead of directly selecting the minimum value from twin target networks,
we select the target based on the minimum TD error, which then addresses not only overestimation
but underestimation problems. For the TD-regularized actor network, we formulate a new TD error
to regularize actor updates to avoid a misleading critic. This regularization term helps further reduce
the estimation error in critic updates. Additionally, we apply TDR combined with distributional
RL (Barth-Maron et al., 2018; Bellemare et al., 2017) and LNSS reward estimation method (Zhong
et al., 2022) to further improve learning stability and performance.

2 RELATED WORK

To shed light on the novelty of the TDR method, here we discuss double critic networks and TD
error-based actor learning to provide a backdrop. We include reviews of distributional RL (Barth-
Maron et al., 2018; Bellemare et al., 2017) and long-N -step surrogate stage (LNSS) method (Zhong
et al., 2022) in Appendix A.

Double critic networks have been used in both RL (Hasselt, 2010; Zhang et al., 2017; Weng et al.,
2020) and DRL (Fujimoto et al., 2018; Haarnoja et al., 2018; Van Hasselt et al., 2016). Double Q
learning (Hasselt, 2010; Van Hasselt et al., 2016) was the first to show reduction of overestimation
bias. TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018) also were shown effective by
applying clipped double Q-learning by using the minimum between the two Q estimates. However,
these methods have induced an underestimation bias problem. (Hasselt, 2010; Zhang et al., 2017;
Fujimoto et al., 2018). Consequently, weighted double Q learning (Zhang et al., 2017) was proposed
to deal with both overestimation and underestimation biases. However, this method has not been
tested in DRL context and therefore, it lacks a systematic approach to designing the weighting
function.

TD error-based actor learning is expected to be effective in reducing overestimation error since it is
a consistent estimate of the advantage function with lower variance, and it discriminates feedback
instead of directly using Q estimates. Some actor-critic variants (Crites & Barto, 1994; Bhatnagar
et al., 2007) update the actor based on the sign of a TD error with a positive error preferred in
policy updates. However, TD error only measures the discrepancy between the predicted value
and the target value, which may not guide exploration effectively, and using TD error alone in actor
update may discourage exploration and cause slow learning, especially in high-dimensional complex
problems. TD-regularized actor-critic (Parisi et al., 2019) enhanced the stability of the actor update
by using the same TD error (as that in online critic update) as a regularization term. However, such
use of TD error may not sufficiently evaluate the critic update because it only uses the temporal
difference between target and online Q estimates. Additionally, the time-varying regularization
coefficient was shown leading to poor convergence (Chen et al., 2017). Note also that the TD-
regularized actor-critic only considered TD-regularized actor but not the critic.

2

Contributions. 1) We introduce a novel TDR mechanism that includes TD-regularized double critic
networks and TD-regularized actor network. 2) Extensive experiments using DMC benchmarks
show that TDR enables SOTA performance (measureed by learning speed, success rate, variance,
and converged reward) across a wide variety of control tasks, such as locomotion, classical control,
and tasks with sparse rewards. 3) We also provide qualitative analysis to show that each component
of TDR contributes to mitigating both over and under estimation errors.

3 METHOD

3.1 DOUBLE Q IN ACTOR-CRITIC METHOD

For a general double Q actor-critic method (Fujimoto et al., 2018; Haarnoja et al., 2018). The policy
(πϕ) is called an actor and the state-action value function (Qθ(sk, ak)) is called a critic where both
the actor and the critic are estimated by deep neural networks with parameters ϕ and θ, respectively.

First, consider a policy π that is evaluated by the state-action value function below:

Qπ(sk, ak) = E[Rk|sk, ak],

(1)

where Rk = (cid:80)∞
t=k γt−krt, sk ∼ p (· | sk−1, ak−1), ak = πϕ (sk), and γ ∈ (0, 1). Most actor-
critic methods are based on temporal difference (TD) learning (Sutton & Barto, 2018) that updates
Q estimates by minimizing the TD error, which is obtained from the the difference between a target
and a critic estimated value.

Next, consider typical double Q methods which entail twin Q networks denoted as Qθ1 and Qθ2.
The respective twin target networks are denoted as Qθ′
. In the upcoming discussions, we
also use θ to denote parameters in both Q networks, i.e., θ={θ1, θ2}. The target value yk is the lesser
of the two target values,

and Qθ′

1

2

yk = rk + γ min
ζ=1,2

Qθ′

ζ

(sk+1, πϕ′(sk+1)),

(2)

where by taking the minimum of the two target values, it aims to curtail overestimation of Q value
frequently experienced by using a single target. Thus the critic value Qθ is updated by minimizing
the loss function (L (θ)) with respect to the critic weights θ:

L (θ) = Es∼pπ,a∼π[

(cid:88)

(yk − Qθζ (sk, ak))2].

(3)

ζ=1,2

The actor weights can be updated by the deterministic policy gradient algorithm below (Silver et al.,
2014), where by convention (Fujimoto et al., 2018; Haarnoja et al., 2018), Qθ1 is used to update the
actor weights.

∇ϕJ(ϕ) = Es∼pπϕ

(cid:104)

(cid:105)
∇aQθ1(sk, ak)|a=πϕ(s) ∇ϕπϕ(s)

.

(4)

Figure 1: Twin TD-regularized Actor-Critic (TDR) Architecture

3

3.2 TWIN TD-REGULARIZED ACTOR-CRITIC (TDR) ARCHITECTURE

Figure 1 depicts our TDR-based solution mechanisms, which include twin Q networks as in TD3
(Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018), and an actor network. The TDR-based
actor and critic updates are different from currently existing methods. In the following, we show
how the new TDR selects target value yk different from Equation (2) as used in SAC and TD3, and
how that helps reduce both overestimation and underestimation errors. We also show how the new
TD-regularized actor helps further reduce the estimation bias in the critic. Our TDR-based solutions
in Figure 1 include two additional good improvements: distributional learning as in D4PG and long
N -step surrogate stage (LNSS) method (Zhong et al., 2022) as described in Appendix A.

3.3 TD-REGULARIZED DOUBLE Q NETWORKS

To overcome overestimation, TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018) train
their critic networks to minimize the loss function in Equation (3) where the target value yk is from
Equation (2). While this helps reduce overestimation error, it promotes a new problem of underes-
timation, which usually occurs during the early stage of learning, or when subjected to corrupted
reward feedback or inaccurate states.

Our TDR method aims at minimizing the same loss function as in Equation (3), but with a different
target value yk. Instead of directly choosing the lesser from the two target values as in Equation (2),
we use the TD errors of the two target networks to set the target value. First, the two TD errors from
the respective target networks are determined from:
δ′
1 = rk + γQθ′
δ′
2 = rk + γQθ′

(sk+1, πϕ′(sk+1)) − Qθ′

(sk+1, πϕ′(sk+1)) − Qθ′

(sk, ak),

(sk, ak).

(5)

(6)

1

1

2

2

The target value for TDR is then selected from the following:

1

yk =

(cid:26) rk + γQθ′
rk + γQθ′

(sk+1, πϕ′(sk+1))
(sk+1, πϕ′(sk+1))
Note from Equation (7) that TDR always uses a target value associated with a smaller target TD
value (regardless of the error sign) between the two. As the ultimate objective of a target network is
to converge to Qπ, such choice by TDR pushes the critic via Equation (3) toward reaching the target
no matter the estimation error is from above or below, but with a smaller TD value. Thus, TDR is
naturally positioned to address both overesdiation and underestimation errors.

1| ≤ |δ′
1| > |δ′

if |δ′
if |δ′

2|,
2|.

(7)

2

3.4 TD-REGULARIZED ACTOR NETWORK

Our TD-regularized actor network directly penalizes the actor’s learning objective whenever there
is a critic estimation error. The estimation error ∆i+1 of the first critic (Qθ1 chosen by convention
of double Q-based actor-critic methods) is determined from the following:

∆i+1 = Qθi+1

1

(sk, ak) − (rk + γQθi+1

1

(sk+1, πϕ(sk+1))),

(8)

where i + 1 represents the iteration number during critic update. Then the actor can be updated in
the direction of maximizing Q while keeping the TD error small,
(cid:20)

(cid:21)

∇ϕJ(ϕ) = Es∼pπϕ

∇a(Qθi+1

1

(cid:12)
(sk, ak) − ρ(∆i+1))
(cid:12)
(cid:12)a=πϕ(s)

∇ϕπϕ(s)

.

(9)

where ρ ∈ (0, 1) is the regularization coefficient to balance the role of TD error in the actor learning
objective. Thus, we expect the TD-regularized actor to help further reduce estimation error in the
critic. With TDR actor and cirtic working together hand-in-hand, TDR is positioned to help avoid
bad policy updates due to a misleading Q value estimate.

Remark 1. There are a few key differences between TDR and TD-regularized Actor Network
(Parisi et al., 2019). 1) In Equation (8), they use the target critic Qθi′
(sk+1, πϕ(sk+1)) to construct
TD error, the same as in critic updates. This TD error evaluates the temporal difference between
target and online Q estimates. To more accurately evaluate critic estimations, we construct the TD
error by only using online critics which directly affects actor updates. 2) Their TD error does not
sufficiently evaluate how the critic updates. Instead in Equation (8), we use the updated critic (θi+1
)
to construct the TD error to directly measure critic estimation.

1

1

4

4 MITIGATING ESTIMATION BIAS BY TDR

Let Qπ be the true Q value obtained by following the current target policy π, and let Qθ be the
estimated value using neural networks. Let Ψk
θ be a random estimation bias. Then for state-action
pairs (sk, ak). we have,

Qθ(sk, ak) = Qπ(sk, ak) + Ψk
θ .
(10)
The same holds for the target networks, i.e., when θ is replaced by θ′ in the above equation. An
overestimation problem refers to when the estimation bias E[Ψk
θ ] > 0, and an underestimation
problem when the estimation bias E[Ψk

θ ] < 0.

4.1 MITIGATING ESTIMATION BIAS USING TD-REGULARIZED DOUBLE CRITIC NETWORKS

Theorem 1. Let Qπ be the true Q value following the current target policy π, and Qθ′
and Qθ′
be the target network estimates using double Q neural networks. We assume that there exists a
step random estimation bias ψk
(i.e., estimation bias at the kth stage), and that it is independent of
θ′
ζ
(sk, ak) with mean E[ψk
ζ, µ′
] = µ′
ζ < ∞, for all k, and ζ = 1, 2. Additionally, let δYk denote
θ′
ζ
the target value estimation error. Accordingly, we denote this error for TDR as δY T DR
, and DQ as
δY DQ
k

. We then have the following,

k

1

2

Where E[δY T DR

k

] = E[Qπ − yT DR

k

]| ≤ |E[δY DQ

k

|E[δY T DR
k
], and E[δY DQ

k

] = E[Qπ − yDQ

k

].

]|,

(11)

Proof. The proof of Theorem 1 is provided in Appendix B

Remark 2. By selecting a target value with less TD error, our TD-regularized double critic networks
mitigate both overestimation and underestimation errors. However, vanilla double Q methods usu-
ally push the target toward the lower value no matter the estimation error is over or under. Although
this estimation error may not be detrimental as they may be small at each update, the presence of
unchecked underestimation bias raises two concerns. Firstly, if there is no sufficient reward feed-
back from the environment, (e.g., for a noisy reward or sparse reward), underestimation bias may
not get a chance to make corrections and may develop into a more significant bias over several up-
dates. Secondly, this inaccurate value estimate may lead to poor policy updates in which suboptimal
actions might be highly rated by the suboptimal critic, reinforcing the suboptimal action in the next
policy update.

4.2 ADDRESSING A MISGUIDING CRITIC IN POLICY UPDATES USING TD-REGULARIZED

ACTOR

Theorem 2. Let Qπ denote the true Q value following the current target policy π, Qθ1 be the
estimated value. We assume that there exists a step random estimation bias ψk
that is independent
θ1
of (sk, ak) with mean E[ψk
] = µ1, µ1 < ∞, for all k. We assume the policy is updated based
θ1
on critic Qθ1 using the deterministic policy gradient (DPG) as in Equation (4). Let δϕk denote the
change in actor parameter ϕ updates at stage k. Accordingly, we denote this change for TDR as
, and true change without any approximation error in Q as δϕtrue
δϕT DR
.
k
k
We then have the following,

, vanilla DPG as δϕDP G

k

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(12)

Where δϕtrue
k
Appendix B

, δϕDP G
k

, and δϕT DR

k

are defined as Equation (55),(56), and (57) respectively in

Proof. The proof of Theorem 2 is provided in Appendix B.
Remark 3. Theorem 2, holds for ρ ∈ (0, 1). If the regularization factor ρ = 1
1−γ , from Equation
(59), we have E[Ψk
] = E[δϕT DR
]. By using TDR,
θ1
the actor will always update the same way as using the true value. While this is not realistic, the
following relationship still preserves |E[Ψk
]| to help ease the negative effect of
θ1
critic estimation bias.

− ρ∆] = 0 which implies that E[δϕtrue

− ρ∆]| ≤ |E[Ψk
θ1

k

k

5

4.3 MITIGATING CRITIC ESTIMATION ERROR BY TD-REGULARIZED ACTOR

Theorem 3. Suboptimal actor updates negatively affect the critic. Specifically, consider actor up-
dates as in Theorem 2, in the overestimation case, we have:

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1 (sk, πT DR(sk))] ≥ E[Qπ(sk, πT rue(sk))],

and in the underestimation case,

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(13)

(14)

Proof The proof of Theorem 3 is provided in Appendix B.

Remark 4. For both cases, by using TD-regularized actors, it is expected to result in less estimation
bias in the critic.

5 EXPERIMENTS AND RESULTS

In this section, we provide a comprehensive evaluation of our TDR enabled actor-critic learning
methods based on three commonly used, well-behaved baseline algorithms including SAC, TD3
and D4PG. Additional evaluations are also provided for popular DRL algorithms such as DDPG and
PPO to provide a broader perspective on the effectiveness of TDR-based methods. All evaluations
are performed based on several benchmarks in Deepmind Control Suite (Tassa et al., 2018).

In reporting evaluation results, we use the following short-form names:

1) Base: the original DRL algorithms including SAC, TD3, D4PG, DDPG and PPO.

2) TDR-TD3: Applied TD regularized double critic (TD Critic) networks, TD regularized actor (TD
Actor) network, with regularization factor ρ = 0.7, and LNSS with N = 100.

3) TDR-SAC: Applied TD regularized double critic (TD Critic) networks, and LNSS with N = 100.

4) dTDR (TDR-D4PG): Applied TD regularized double critic (TD Critic) network, TD regularized
actor (TD Actor) network, with regularization factor ρ = 0.7, and LNSS with N = 100.

Our evaluations aim to quantitatively address the following questions:
Q1. How does TDR improve over Base and other common methods?
Q2. How does the performance of TDR methods compare to that of SOTA algorithms (D4PG)?
Q3. Is TDR method robust enough to handle both dense stochastic reward and sparse reward?
Q4. How does each component in TDR-based learning mechanisms affect performance?
Q5. How does TD regularized actor make policy updates in situations of misguiding critics?
Q6. How does the regularization coefficient ρ in Equation (9) affect TD Actor performance?

Details of the implementation, training, and evaluation procedures are provided in Appendix C and
D where links to all implementation codes are also provided.

5.1 MAIN EVALUATION

In obtaining comprehensive evaluation results summarized in Table 1, we included a 10% noise
respectively in state, action, and reward in each of the considered DMC environments in order
to make the evaluations more realistic. In “Cheetah Run sparse”, we sparsified the reward in the
environment. All details of the environment setup can be found in Appendix C. In Table 1, “Success”
is shorthand for learning success rate, “Avg. Rwd” for average reward, and “Rank” (%) is the
“percent of reward difference” between the evaluated method and the SOTA D4PG, which is (the
average reward of the evaluated method over that of the D4PG - 1), the more positive the better.
Note that, in computing the success rate, only those trials that have achieved a reward of at least
10 are accounted for as successful learning. The results are based on the last 50 evaluations of 10
different random seeds (same for all compared algorithms). Best performances are boldfaced for
average reward (Avg. Rwd). Note that we did not implement our TD Actor into SAC because SAC
already has a max entropy-regulated actor.

Q1 TDR improves over respective Base methods. The learning curves for six benchmark environ-
ments are shown in Figure 2. Overall, TDR methods (solid lines) outperform their respective Base

6

Figure 2: Systematic evaluation of TDR realized in three DRL algorithms (SAC, TD3, D4PG) in
DMC environments with 10% uniform random noise in state, action, and reward. The shaded regions
represent the 95 % confidence range of the evaluations over 10 seeds. The x-axis is the number of
steps.

Envirinoment

D4PG
DDPG
PPO
SAC
TD3
TDR-SAC
TDR-TD3
dTDR

Envirinoment

D4PG
DDPG
PPO
SAC
TD3
TDR-SAC
TDR-TD3
dTDR

Success
[%]
100
100
100
90
100
100
100
100

Success
[%]
100
100
20
0
0
100
100
100

Finger Turn Hard
Avg. Rwd
[µ ± 2σ]
400.9 ± 173.4
222.1 ± 160.4
85.9 ± 50
65.6 ± 30.2
205.9 ± 108.5
601.5 ± 147.4
569.8 ± 142.1
841.02 ± 148.3
Acrobot Swingup
Avg. Rwd
[µ ± 2σ]
26.8 ± 8.9
17.2 ± 3.8
7.9 ± 7.8
4 ± 2.2
5.2 ± 4.2
42.9 ± 5.1
50 ± 7.9
62.6 ± 14.4

Rank
[%]
0
-44.6
-78.6
-83.6
-48.6
49.9
42.3
109.8

Rank
[%]
0
-35.8
-70.5
-85.1
-80.6
60.1
86.5
133.6

Quadruped Walk

Success
[%]
100
100
100
100
100
100
100
100

Avg. Rwd
[µ ± 2σ]
858.5 ± 11.4
226.8 ± 133.6
173.1 ± 60.4
196.6 ± 73.7
334.8 ± 76.4
479.5 ± 126.9
475.4 ± 45.4
888.6 ± 15.7
Cartpole Swingup Sparse

Rank
[%]
0
-73.6
-79.8
-77.2
-61
-44.2
-44.6
3.46

Success
[%]
100
0
80
0
0
100
100
100

Avg. Rwd
[µ ± 2σ]
493.5 ± 15.9
3.6 ± 5.8
99.2 ± 172.9
1.7 ± 3.4
1.3 ± 2.3
774.2 ± 51.1
790.13 ± 33.0
810.3 ± 34.9

Rank
[%]
0
-99
-79.9
-99.7
-99.7
56.8
60.1
64.2

Success
[%]
100
100
100
100
100
100
100
100

Fish Swim
Avg. Rwd
[µ ± 2σ]
153.7 ± 68.1
109.7 ± 27.1
78.67 ± 6.28
73.2 ± 9.87
85.3 ± 21.7
212.3 ± 51.2
204.2 ± 41.5
249.9 ± 45.5
Cheetah Run Sparse

Success
[%]
60
50
0
0
50
100
100
100

Avg. Rwd
[µ ± 2σ]
532.8 ± 388.4
160.7 ± 284.7
0 ± 0
0 ± 0
220.5 ± 354.7
930.2 ± 18.7
827.8 ± 62.2
900.1 ± 30.8

Rank
[%]
0
-28.6
-48.8
-52.4
-44.5
37.9
32.7
62

Rank
[%]
0
-69.8
-100
-100
-58.6
74.6
55.4
68.9

Table 1: Systematic evaluations of TDR respectively augmented Base algorithms. “Rank” (%) is
the “percent of reward difference” between the SOTA D4PG, the more positive the better.

methods TD3, SAC and D4PG (dash lines) in terms of episode reward, learning speed, learning
variance and success rate. In Table 1,among the measures, the Avg. Rwd of TDR methods outper-
formed respective baseline algorithms. Notice from the table that the learning success rates for
all TDR methods are now 100%, a significant improvement over the Base methods. In comparison,
DDPG, SAC and TD3 Base methods struggle with Acrobot Swingup, Cartpole Swingup Sparse,
and Cheetah Run Sparse. Moreover, TDR methods also outperform DDPG and PPO in terms of
averaged reward (Awg.Rwd), learning speed, learning variance, and success rate. Thus, TDR has
helped succesfully address the random initialization challenge caused by random seeds (Henderson
et al., 2018).

Q2 TDR brings performance of Base methods close to or better than that of the SOTA D4PG.
From Figure 2, and according to the “Rank” measure in Table 1, for all environments but Quadruped
walk, TDR (TDR-SAC and TDR-TD3) helped enhance the performances of the respective Base
methods. Additionally, it even outperformed the SOTA D4PG by around 40% in the “Rank” mea-
sure. For Quadruped walk, even though TDR-SAC and TDR-TD3 did not outperform D4PG, they
still are the two methods, among all evaluated, that provided closest performance to D4PG. It is also

7

worth noting that TDR brings the performance of D4PG to a new SOTA level measured by mean
reward, convergence speed, and learning success rate.

Q3 TDR is robust under both dense stochastic reward and sparse reward. From Figure 2 and
Table 2, TDR methods outperformed their respective baselines in both dense stochastic and sparse
reward in terms of average reward, learning variance, success rate, and converge speed. In particular,
baseline algorithms such as TD3 and SAC struggle with sparse reward benchmark environments
(cartpole swingup sparse and cheetah run sparse). However, by using TDR, they not only learned,
but also achieved SOTA performance.

Methods

TD3+TD Critic
TD3+LNSS
TD3+TD Actor
TD3+TDR
SAC+TD Critic
SAC+LNSS
SAC+TDR
D4PG+TD Critic
D4PG+LNSS
D4PG+TD Actor
dTDR

Acrobot Swingup

Finger TurnHard

Avg. Rwd
[µ ± 2σ]
24.9 ± 11.7
24.2 ± 9.2
6.9 ± 2.9
42.9 ± 5.1
28.8 ± 12.2
9.7 ± 2.9
42.9 ± 5.1
32.8 ± 6.9
43.9 ± 16.7
29.9 ± 13.8
62.6 ± 14.4

Enhancement
[%]
378.8
365.4
32.7
725
620
142.5
972.5
22.4
63.8
11.6
133.6

Avg. Rwd
[µ ± 2σ]
556.2 ± 239.8
547.5 ± 120.5
212.3 ± 45.7
569.8 ± 142.1
588 ± 223.8
573 ± 156.5
601.5 ± 147.4
835.7 ± 140.9
675.1 ± 217.6
532.5 ± 235.7
841.1 ± 148.3

Enhancement
[%]
170.1
165.9
3.1
176.7
796.3
773.5
816.9
108.5
68.4
33.5
109.8

Cartpole Swingup Sparse
Avg. Rwd
[µ ± 2σ]
766.2 ± 86.1
766.6 ± 38.3
339.6 ± 231.9
790.1 ± 33.0
766.7 ± 126.4
722.8 ± 162.4
774.2 ± 51.1
678.7 ± 246.2
759.1 ± 31.1
600 ± 129.3
810.3 ± 34.9

Enhancement
[%]
588.4
588.7
260.2
606.7
449.6
423.7
454.4
37.5
53.8
21.6
64.2

Table 2: Systematic evaluations of each component of TDR compared to their respective Base al-
gorithms. “Enhancement” (%) is the “percent of reward difference” between the respective Base
algorithms, the larger the better. Note that TD Actor was not considered for SAC as SAC already
has a max entropy-regularized actor.

Figure 3: Evaluation of TD Actor with different ρ (ρ = 0, 0.1, 0.3, 0.5, 0.7, 0.9) in Equations (9,
21) based on two DRL algorithms (TD3, D4PG) in DMC environments with 10% uniform random
noise in state, action, and reward. The shaded regions represent the 95 % confidence range of the
evaluations over 10 seeds. The x-axis is the number of steps.

5.2 ABLATION STUDY

To perform the ablation study, we examined TDR by removing each of the following three compo-
nents. The respective short-form descriptions are:

1) “TD Critic”: the TD regularized double Q networks.
2) “TD Actor”: the TD regularized actor network.
3) “LNSS”: LNSS method with N = 100.

In Table 2, “Enhancement” (%) is the “percent of reward difference” between the evaluated method
and its Base method, the larger the better.

8

Q4 TD Critic, TD Actor, and LNSS effectively improved the Base algorithms. In Table 2, TD
Critic, LNSS, and TD Actor all effectively improved the Base algorithms. From the table, TD Critic
and LNSS have provided comparable and significant enhancement over Base algorithms. As our TD
Critic methods outperform respective Base algorithms, this suggests that mitigating estimation errors
both over and under from vanilla double Q network is an effective way to improve performance
which has also been shown in our theoretical analysis (Theorem 1). The LNSS method helped
improve learning performance by reducing variances in value estimation for noisy rewards as shown
both theoretically and empirically (Zhong et al., 2022). By including LNSS, our TDR is more robust
under noisy and sparse rewards.

The TD Actor element also helped make appreciable improvements on learning performance as
shown in Table 2. More importantly, TD Actor plays an importantly role in TDR since it not only
stabilizes the policy updates as shown theoretically in Theorem 2 but also addresses the estimation
error in critic as shown theoretically in Theorem 3.

5.3 HYPER PARAMETER STUDY

Hyperparameter study results are summarized in Figure 3 where two DRL methods (D4PG and
TD3) with TD Actor are evaluated for different regularization factor ρ (ρ = 0, 0.1, 0.3, 0.5, 0.7, 0.9).
What is reported is the 10-seed averaged performance, i.e., the average of the approximate es-
timation error which is the difference between the true accumulated reward and the critic value:
Ψ = 1
10

t=0 γtrt − Q(s0, a0)).

eval=0((cid:80)999

(cid:80)9

Q5 TD regularized Actor helps reduce the estimation error in critic.

From Figure 3, with TD regularized Actor (TD Actor), the estimation errors in the critic are re-
duced from those without. For example, in Finger Turn hard, D4PG + TD Actor results in less
overestimation error compared with ρ = 0 at the later stage of training. TD3 + TD Actor has less
underestimation error compared with ρ = 0. Similarly in cartpole swingup sparse, D4PG + TD
Actor results in less overestimation error compared with ρ = 0.

A policy can be evaluated by the “epois reward” where a higher epois reward generally results from a
better policy. From Figure 3, policy updates are improved by selecting a suitable regularization fac-
tor ρ. Especially, in cartpole swingup sparse, TD3 + TD Actor enables successful learning whereas
the Base method struggled and stuck to 0 or no learning for the entire training period.

Q6 A range of ρ (ρ = 0.3, 0.5, 0.7) generally are good choices. From Figure 3, a small regular-
ization factor ρ = 0.1 in TDR will result in less regularization which may not provide sufficient
estimation error reduction in the critic. A larger regularization factor ρ = 0.9 in TDR will result in
more regularization and may have a negative effect on learning. Therefore, ρ = 0.3, 0.5, 0.7 may be
good choices. Therefore in this work, we have consistently used ρ = 0.7 in obtaining all results.

6 CONCLUSION, DISCUSSION, AND LIMITATION OF THE STUDY

1) In this work, we introduce a novel TDR mechanism that includes TD-regularized double critic
networks and TD-regularized actor network. Both components are shown to help mitigate both
over and under estimation errors. TDR has been shown to consistently outperform respective Base
algorithms in solving benchmark tasks in terms of average reward, learning success rate, learning
speed, and most times, learning variance. 2) Our analytical results also show that each component of
TDR helps mitigate both over and under estimation errors. 3) As shown in Figure 2, for five out of
the six environments (except quadruped walk) evaluated, our TDR combined with distributional and
LNSS elements has significantly elevated the current SOTA performance of D4PG to a new level
with an increase of at least 60%.

Even though we have identified a range of generally good regularization coefficient ρ values
(0.3, 0.5, 0.7), as Figure 3 shows, different algorithms in different environments have responded
somewhat differently to ρ. Therefore, how to effectively determine a regularization factor to have
the most improvement remains a question, and thus, it is the limitation of this study. Additionally,
the promising performances of TDR come after extensive training with millions of learning steps.
How TDR performs under limited training time and training steps need to be further investigated.

9

REFERENCES

Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. arXiv
preprint arXiv:1707.01495, 2017.

Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. arXiv preprint arXiv:1804.08617, 2018.

Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement

learning. In International conference on machine learning, pp. 449–458. PMLR, 2017.

Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. Incremental nat-

ural actor-critic algorithms. Advances in neural information processing systems, 20, 2007.

Renzhi Chen, Ke Li, and Xin Yao. Dynamic multiobjectives optimization with a changing number

of objectives. IEEE Transactions on Evolutionary Computation, 22(1):157–171, 2017.

Robert Crites and Andrew Barto. An actor/critic algorithm that is equivalent to q-learning. Advances

in Neural Information Processing Systems, 7, 1994.

Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos.

Implicit quantile networks for
distributional reinforcement learning. In International conference on machine learning, pp. 1096–
1105. PMLR, 2018a.

Will Dabney, Mark Rowland, Marc Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, volume 32, 2018b.

Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng. Distributional
soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors. IEEE
transactions on neural networks and learning systems, 33(11):6584–6598, 2021.

Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft

updates. arXiv preprint arXiv:1512.08562, 2015.

Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587–1596. PMLR, 2018.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018.

G Hailu and G Sommer. On amount and quality of bias in reinforcement learning. In IEEE SMC’99
Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics
(Cat. No. 99CH37028), volume 2, pp. 728–733. IEEE, 1999.

Hado Hasselt. Double q-learning. Advances in neural information processing systems, 23, 2010.

Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018.

Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin q-learning: Controlling

the estimation bias of q-learning. arXiv preprint arXiv:2002.06487, 2020.

Felix Leibfried and Jordi Grau-Moya. Mutual-information regularization in markov decision pro-
cesses and actor-critic learning. In Conference on Robot Learning, pp. 360–373. PMLR, 2020.

Wentao Liu, Junmin Zhong, Ruofan Wu, Bretta L Fylstra, Jennie Si, and He Helen Huang. Inferring
human-robot performance objectives during locomotion using inverse reinforcement learning and
inverse optimal control. IEEE Robotics and Automation Letters, 7(2):2549–2556, 2022.

10

Jiafei Lyu, Xiaoteng Ma, Jiangpeng Yan, and Xiu Li. Efficient continuous control with double
actors and regularized critics. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 36, pp. 7655–7663, 2022.

Fabio Pardo. Tonic: A deep reinforcement learning library for fast prototyping and benchmarking.

arXiv preprint arXiv:2011.07537, 2020.

Simone Parisi, Voot Tangkaratt, Jan Peters, and Mohammad Emtiyaz Khan. Td-regularized actor-

critic methods. Machine Learning, 108:1467–1501, 2019.

Tim GJ Rudner, Cong Lu, Michael A Osborne, Yarin Gal, and Yee Teh. On pathologies in kl-
regularized reinforcement learning from expert demonstrations. Advances in Neural Information
Processing Systems, 34:28376–28389, 2021.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387–395. Pmlr, 2014.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.

Sebastian Thrun and Anton Schwartz.

Issues in using function approximation for reinforcement
learning. In Proceedings of the Fourth Connectionist Models Summer School, volume 255, pp.
263. Hillsdale, NJ, 1993.

Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.

Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R´emi Munos, and Matthieu Geist.
Leverage the average: an analysis of kl regularization in reinforcement learning. Advances in
Neural Information Processing Systems, 33:12163–12174, 2020.

Wentao Weng, Harsh Gupta, Niao He, Lei Ying, and R Srikant. The mean-squared error of double

q-learning. Advances in Neural Information Processing Systems, 33:6815–6826, 2020.

Ruofan Wu, Junmin Zhong, Brent Wallace, Xiang Gao, He Huang, and Jennie Si. Human-robotic
prosthesis as collaborating agents for symmetrical walking. Advances in Neural Information
Processing Systems, 35:27306–27320, 2022.

Ruofan Wu, Junmin Zhong, and Jennie Si. Phased actor in actor-critic reinforcement learning. 2023.

Zongzhang Zhang, Zhiyuan Pan, and Mykel J Kochenderfer. Weighted double q-learning. In IJCAI,

pp. 3455–3461, 2017.

Junmin Zhong, Ruofan Wu, and Jennie Si. Long n-step surrogate stage reward to reduce variances
of deep reinforcement learning in complex problems. arXiv preprint arXiv:2210.04820, 2022.

A DISTRIBUTIONAL TDR AND LNSS

The distributional RL (Bellemare et al., 2017) represents value function in terms of probability
distribution rather than function estimates. This distribution provides a more comprehensive rep-
resentation of the uncertainty associated with a range of different possible reward returns and state
action pairs which can provide more informative value function estimation. Many distributional RL
algorithms (Bellemare et al., 2017; Dabney et al., 2018b;a) has been achieved great performance im-
provements on many discrete problems such as Atari benchmarks. D4PG (Barth-Maron et al., 2018)
applied distributional RL into continuous control problem by combining the distributional return
function within an actor-critic framework. DSAC (Duan et al., 2021) address overestimation error
by applying distributional RL piggyback on SAC. Although, D4PG and DSAC can provide more
accurate critic, the overestimation of actor still exists since the actor is still updated by maximizing
the expectation of value function distribution. How to regulate actors in distributional RL in solving
overestimations was barely discussed before.

11

A.1 DISTRIBUTIONAL TD-REGULARIZED ACTOR-CRITIC (DTDR)

Here we tailor a distributional TDR (dTDR) method based on the original distributional conceptu-
alization developed in D4PG (Barth-Maron et al., 2018; Bellemare et al., 2017). We show a number
of enhancements in the meantime.

Distributional Critic. The distributional critic (Bellemare et al., 2017 )treated the return in Equa-
tion 1 as a random variable Z(sk, ak) whose expectation is used as the Q value estimate, namely,
Q(sk, ak) = E[Z(sk, ak)].
In dTDR however, we use TD errors to evaluate distributional critics. Similar to Equation 5 and 6,
distributional TD errors of the two target networks can be written as:
(sk+1, πϕ′(sk+1))] − E[Zθ′

1 = rk + γE[Zθ′
d′

(sk, ak)],

(15)

1

1

2 = rk + γE[Zθ′
d′

2

(sk+1, πϕ′(sk+1))] − E[Zθ′

2

(sk, ak)].

The twin TD-regularized target distributional Bellman operator is thus defined as:

T Zk

D=

(cid:26) rk + γZθ′
rk + γZθ′

1

2

(sk+1, πϕ′(sk+1))
(sk+1, πϕ′(sk+1))

if |d′
if |d′

1| ≤ |d′
2|
1| > |d′
2|

(16)

(17)

where A D= B denotes that two random variables A and B follow the same probability laws. Al-
though the distributional Bellman operator appears similar to Equation 1, it maps state-action pairs
to distributions. As such, we need to define a new TD error measure for the distribution as in D4PG
(Barth-Maron et al., 2018). We consider using the following distributional loss,

L(θ) = Es∼pπ,a∼π[

(cid:88)

ζ=1,2

l(T Zk, Zθζ (sk, ak))],

(18)

where l measures the distance between two distributions. Many distributional RL algorithms use
Kullback-Leibler (KL) divergence as the distance metric (Duan et al., 2021; Barth-Maron et al.,
2018. We adopt the same metric.

Distributional Actor. In most distributional methods (Barth-Maron et al., 2018; Bellemare et al.,
2017), policy updates are performed based on the policy gradient below,

∇ϕJ(ϕ) = Es∼pπϕ

[E[∇aZθ(sk, ak)]|a=πϕ(s)∇ϕπϕ(s)].

(19)

In our dTDR, we need to use critic evaluation metrics to evaluate the quality of the current distribu-
tional critic and the regularized distributional actor. We first formulate the following loss metric:

Lz(ϕ) = E[l(rk + γZθi+1

1

(sk+1, πϕ(sk+1)), Zθi+1

1

(sk, πϕ(sk))].

(20)

Similar to TD-regularized actor network, the distributional actor is updated in the direction of max-
imizing the expected critic while keeping the expected distance between the projected critic and the
critic, namely,

∇ϕJ(ϕ) = Es∼pπϕ

[(E[∇aZθi+1

1

(sk, ak)] − ∇aρLz(ϕ))|a=πϕ(s)∇ϕπϕ(s)],

(21)

where ρ ∈ (0, 1) is a regularization coefficient.

A.2 LONG N-STEP SURROGATE STAGE (LNSS) REWARD

LNSS (Zhong et al., 2022) utilizes a long reward trajectory of N future steps in the estimation
of stage reward rk. Using the LNSS-resulted reward r′
k in place of the original rk was shown
to effectively reduce learning variance with significant performance improvements for off-policy
methods. Given a reward trajectory of N steps from time step k, let G(sk:k+N −1, ak:k+N −1) ∈ R
(with shorthand notation Gk) denote the discounted N -step return, i.e.,

k+N −1
(cid:88)

γt−krt,

Gk =

t=k

12

(22)

where rt is the tth stage reward and t is from k to k + N − 1. In LNSS, r′
reward in place of rk in Equation (2). To determine r′
N -step reward sequence, namely

k is a surrogate stage
k, LNSS treat it as a weighted average of the

r′
k =

γt−krt

(cid:80)k+N −1
t=k
(cid:80)N −1

n=0 γn

.

(23)

As Figure 1 shows, Once r′
(sk, ak, r′
as discussed.

k is obtained, it is simply used in place of rk to form a new tuple
k, sk+1), which is then stored into the memory buffer D. The TDR method proceeds

B ESTIMATION ANALYSIS

Lemma 1. Let Qπ be the true Q value following the current target policy π, and Qθ′
be the
target network estimates using double Q neural networks. We assume that there exists a step random
estimation bias ψk
(i.e., estimation bias at the kth stage), and that it is independent of (sk, ak) with
θ′
ζ
mean E[ψk
ζ, µ′
] = µ′
2 respectively defined in
θ′
ζ
Equations (5) and (6), we have,

ζ < ∞, for all k, and ζ = 1, 2. Then for δ′

1 and δ′

and Qθ′

2

1

E[δ′
E[δ′

1] = −µ′
1,
2] = −µ′
2.

(24)

(25)

(26)

Proof. With the step random estimation bias ψk
θ′
ζ

, We can rewrite the expectation of Ψk
θ′
ζ

as

E[Ψk+1
θ′
ζ

] =

∞
(cid:88)

t=k+1

γt−k−1E[ψt
θ′
ζ

] =

1
1 − γ

µ′
ζ.

Then the expectation of the target can be written as,

E[yk] = E[rk] + γE[(Qπ(sk+1, ak+1) + Ψk+1
∞
(cid:88)

θ′
ζ

)]

= E[rk] + γ(E[

γt−k−1rt]) +

γ
1 − γ

µ′
ζ

= Qπ(sk, ak) +

t=k+1
γ
1 − γ

µ′
ζ.

By using Equations (10), and (26), the TD errors of the two target critics (Equations 5 and 6),
respectably are:

E[δ′

1] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))] − E[Qθ′
1
1
γ
1 − γ
1 − γ

1 − Qπ(sk, ak) −
µ′

(sk, ak)]

µ′
1

(27)

= Qπ(sk, ak) +

Similarly, E[δ′

= −µ′
1.
2] = −µ′
2.

Thus Lemma 1 holds.

With Lemma 1 in place, we are now ready to analyze the estimation errors by using TDR and the
double Q (DQ) method as in TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018).

Theorem 1. Let assumptions in Lemma 1 hold, and let δYk denote the target value estimation
error. Accordingly, we denote this error for TDR as δY T DR
. We then have the
following,

, and DQ as δY DQ

k

k

|E[δY T DR
k

]| ≤ |E[δY DQ

k

]|.

(28)

Proof. The proof is based on enumerating a total of 8 possible scenarios of estimation errors which
are determined from the relationships among the two target Q values and the true Qπ value . We
provide proofs for the 4 out of 8 unique scenarios below.

13

First note that, E[δY T DR
Case 1: If the target critic values and the true value Qπ have the following relationship:

] = E[Qπ − yT DR

] = E[Qπ − yDQ

], and E[δY DQ

].

k

k

k

k

i.e, Qθ′

1

is more underestimated as

that implies

E[Qθ′

1

] < E[Qθ′

2

] < Qπ,

|E[Ψk
θ′
1

]| > |E[Ψk
θ′
2

]|,

|µ′

1| > |µ′

2|.

Based on Lemma 1 and Equation (7), our TDR will use Qθ′

2

in the target value,

E[yT DR
k

] = E[rk] + γE[Qθ′

2

(sk+1, πϕ′(sk+1))].

However for a vanilla double Q network, the target value will be Qθ′

1

,

] = E[rk] + γE[Qθ′
Thus based on Equation (26), the two estimation errors of the respective target values are

(sk+1, πϕ′(sk+1))].

E[yDQ
k

1

|E[δY T DR
k

]| = |E[Qπ − yT DR

k

]| = |

|E[δY DQ
k

]| = |E[Qπ − yDQ

k

]| = |

µ′

2|,

γ
1 − γ
γ
µ′
1 − γ

1|.

Since |µ′

1| > |µ′

2|, we have

|E[δY T DR
k

]| < |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 2: If the target critic values and the true value Qπ have the following relationship:

E[Qθ′
|E[Qπ − Qθ′

1

] < Qπ < E[Qθ′

2],
]| > |E[Qπ − Qθ′

1

]|,

2

is expected to be underestimated and Qθ′

is overestimated. Since |E[Qπ − Qθ′

1

]| >

2

then Qθ′
|E[Qπ − Qθ′

1

2

we thus have

]| which implies

|E[Ψk
θ′
1

]| > |E[Ψk
θ′
2

]|,

|µ′

1| > |µ′

2|.

Based on Lemma 1 and Equation (7), our TDR will use Qθ′

2

in the target value:

E[yT DR
k

] = E[rk] + γE[Qθ′

2

(sk+1, πϕ′(sk+1))].

However for a vanilla double Q network, the target value will use Qθ′

1

,

] = E[rk] + γE[Qθ′
Based on Equation (26), the two estimation errors of the respective target values are:

(sk+1, πϕ′(sk+1))].

E[yDQ
k

1

|E[δY T DR
k

]| = |E[Qπ − yT DR

k

]| = |

|E[δY DQ
k

]| = |E[Qπ − yDQ

k

]| = |

µ′

2|,

γ
1 − γ
γ
µ′
1 − γ

1|,

Since |µ′

1| > |µ′

2|, we have

|E[δY T DR
k

]| < |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 3: If the target critic values and the true value Qπ has the following relationship:

] < Qπ < E[Qθ′

],

2

E[Qθ′
|E[Qπ − Qθ′

1

1

]| < |E[Qπ − Qθ′

2

]|,

14

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

is expected to be underestimated and Qθ′

is overestimated. Since |E[Qπ − Qθ′

1

]| <

2

then Qθ′
|E[Qπ − Qθ′

1

2

]|, it implies

thus we have

|E[Ψk
θ′
1

]| < |E[Ψk
θ′
2

]|,

(44)

1| < |µ′
Based on Lemma 1 and Equation (7), both vanilla double Q network and our TDR will pick Qθ′
the target value:

|µ′

2|.

1

(45)

in

E[yT DR
k

] = E[yDQ

k

] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))].

Then based on Equation (26), the two estimation errors of the respective target values are:

|E[δY T DR
k

]| = |E[δY DQ

k

]| = |E[Qπ − yT DR

k

]| = |

γ
1 − γ

µ′

1|.

We thus have

|E[δY T DR
k

]| = |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 4: If the target critic values and the true value Qπ has the following relationship

where E[Qθ′

2

] is expected more overestimated ie |E[Ψk
θ′
1

]| < |E[Ψk
θ′
2

]| that implies

Qπ < E[Qθ′

1

] < E[Qθ′

2

],

|µ′

1| < |µ′

2|.

(46)

(47)

(48)

(49)

(50)

Based on Equation (24) and (7), same with vanilla double Q network, our Twin TD-regularized
Critic will pick the target value using Qθ′

which both mitigates the larger overestimation bias as:

1

E[yT DR
k

] = E[yDQ

k

] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))],

which based on Equation (26), the two estimation errors of the target value are

|E[δY T DR
k

]| = |E[δY DQ

k

]| = |E[Qπ − yT DR

k

]| = |

γ
1 − γ

µ′

1|.

We have

|E[δY T DR
k

]| = |E[δY DQ

k

]|.

(51)

(52)

(53)

1

1

2

k

] > E[Qθ′

] < E[Qθ′

]| ≤ |E[δY DQ

Thus identity (28) holds. Both methods can mitigate the overestimation error.
Note, the above cases study the relationship of E[Qθ′
for E[Qθ′
], |E[δY T DR
Theorem 2. Let Qπ denote the true Q value following the current target policy π, Qθ1 be the
estimated value. We assume that there exists a step random estimation bias ψk
that is independent
θ1
of (sk, ak) with mean E[ψk
] = µ1, µ1 < ∞, for all k. We assume the policy is updated based on
θ1
critic Qθ1 using the deterministic policy gradient (DPG) as in Equation 4. Let δϕk denote the change
in actor parameter ϕ updates at stage k. Accordingly, we denote this change for TDR as δϕT DR
,
k
vanilla DPG as δϕDP G
. We then
k
have the following,

, and true change without any approximation error in Q as δϕtrue

]| still valid. Thus Theorem 1 holds.

] and by applying same procedure

k

k

2

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(54)

Proof. With learning rate α, the true change of the actor parameters in case without any approxima-
tion error in Q:

E[δϕtrue
k

] = αEs∼pπ

(cid:104)

ϕj

∇aQπ(sk, ak)|a=πϕj (s) ∇ϕj πϕj (s)

(cid:105)

.

(55)

15

Consider the estimated critic and the true value follow the relationship in Equation 10. Given the
same current policy parameters ϕj, the updated parameters using DPG are:

ϕj+1
DP G = ϕj + αEs∼pπ
(cid:104)

E[δϕDP G
k

] = αEs∼pπ

ϕj

(cid:104)

ϕj

∇a(Qπ(sk, ak) + Ψk
θ1

(cid:105)
)(cid:12)
(cid:12)a=πϕj (s) ∇ϕj πϕj (s)

,

∇a(Qπ(sk, ak) + Ψk
θ1

)(cid:12)
(cid:12)a=πϕj (s) ∇ϕj πϕj (s)

(cid:105)

.

(56)

With an overestimation bias E[Ψk
θ1
mated actions, and with an underestimation bias E[Ψk
θ1
the underestimated actions. Both result in suboptimal policies.
However, by using TD-regularized actor, and given the same current policy parameters ϕj, the actor
updates with Equation (9) are:

] > 0, the updates encourage more exploration for the overesti-
] < 0, the updates discourage exploration for

ϕj+1
T DR = ϕj + αEs∼pπ
] = αEs∼pπ

E[δϕT DR
k

ϕj

[∇a(Qπ(sk, ak) + Ψk
θ1

ϕj

− ρ(∆))|a=πϕj (s)∇ϕj πϕj (s)],

[∇a(Qπ(sk, ak) + Ψk
θ1

− ρ(∆))|a=πϕj (s)∇ϕj πϕj (s)].

Similar to Lemma 1, E[Ψk
θ1

] = 1

1−γ µ1, and from Equations (8) and (9) we have:

E[∆] = E[Qθi+1
= µ1

1

(sk, ak)] − E[(rk + γQθi+1

1

(sk+1, πϕ(sk+1)))]

by selecting ρ ≤ 1

1−γ , we have the following:

(cid:26) 0 ≥ E[Ψk
θ1
0 ≤ E[Ψk
θ1

− ρ∆] > E[Ψk
θ1
− ρ∆] ≤ E[Ψk
θ1

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

Therefore by inspecting Equations (55), (56) and (57), we have:

(cid:26) E[δϕtrue
k
E[δϕtrue
k

Thus Theorem 2 holds.

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(57)

(58)

(59)

(60)

Theorem 3. Suboptimal actor updates negatively affect the critic. Specifically, consider actor up-
dates as in Theorem 2, in the overestimation case, we have:

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1 (sk, πT DR(sk))] ≥ E[Qπ(sk, πT rue(sk))],

(61)

and in the underestimation case,

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(62)

Proof Following the analysis of the TD3 (Fujimoto et al., 2018), consider Equation (12) in Theorem
2, we have

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0 Underestimate,
] ≥ 0 Overestimate.

(63)

In the overestimation case, the approximate value using TDR and vanilla DPG must be

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1(sk, πT DR(sk))] ≥ E[Qπ(sk, πtrue(sk))].

(64)

Similarly, in the underestimation case, the approximate value using TDR and vanilla DPG must be

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(65)

Thus Theorem 3 holds.

16

C IMPLEMENTATION DETAILS

We use PyTorch for all implementations. All results were obtained using our internal server consist-
ing of AMD Ryzen Threadripper 3970X Processor, a desktop with Intel Core i7-9700K processor,
and two desktops with Intel Core i9-12900K processor.

Training Procedure.

An episode is initialized by resetting the environment, and terminated at max step T = 1000. A trial
is a complete training process that contains a series of consecutive episodes. Each trial is run for a
maximum of 1 × 106 time steps with evaluations at every 2 × 104 time steps. Each task is reported
over 10 trials where the environment and the network were initialized by 10 random seeds, (0 − 9)
in this study.

For each training trial, to remove the dependency on the initial parameters of a policy, we use a
purely exploratory policy for the first 8000 time steps (start timesteps). Afterwards, we use an
off-policy exploration strategy, adding Gaussian noise N (0, 0.1) to each action.

Evaluation Procedure.
Every 1 × 104 time steps training, we have an evaluation section and each evaluation reports the
average reward over 5 evaluation episodes, with no exploration noise and with fixed policy weights.
The random seeds for evaluation are different from those in training which each trial, evaluations
were performed using seeds (seeds + 100).

Network Structure and optimizer.

TD3.The actor-critic networks in TD3 are implemented by feedforward neural networks with three
layers of weights. Each layer has 256 hidden nodes with rectified linear units (ReLU) for both the
actor and critic. The input layer of actor has the same dimension as observation state. The output
layer of the actor has the same dimension as action requirement with a tanh unit. Critic receives both
state and action as input to THE first layer and the output layer of critic has 1 linear unit to produce
Q value. Network parameters are updated using Adam optimizer with a learning rate of 10−3 for
simple control problems. After each time step k, the networks are trained with a mini-batch of a
256 transitions (s, a, r, s′), (s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D
containing the entire history of the agent.

D4PG. Same with the actor-critic networks in D4PG are implemented by feedforward neural net-
works with three layers of weights. Each layer has 256 hidden nodes with rectified linear units
(ReLU) for both the actor and critic. The input layer of actor has the same dimension as observa-
tion state. The output layer of the actor has the same dimension as action requirement with a tanh
unit. Critic receives both state and action as input to THE first layer and the output layer of critic
has a distribution with hyperparameters for the number of atoms l, and the bounds on the support
(Vmin, Vmax). Network parameters are updated using Adam optimizer with a learning rate of 10−3.
After each time step k, the networks are trained with a mini-batch of 256 transitions (s, a, r, s′),
(s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D containing the entire his-
tory of the agent.

SAC. The actor-critic networks in SAC are implemented by feedforward neural networks with three
layers of weights. Each layer has 256 hidden nodes with rectified linear units (ReLU) for both the
actor and critic. The input layer of actor has the same dimension as observation state. The output
layer of the actor has the same dimension as action requirement with a tanh unit. Critic receives both
state and action as input to the first layer and the output layer of critic has 1 linear unit to produce
Q value. Network parameters are updated using Adam optimizer with a learning rate of 10−3 for
simple control problems. After each time step k, the networks are trained with a mini-batch of a
256 transitions (s, a, r, s′), (s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D
containing the entire history of the agent.

Hyperparameters. To keep comparisons in this work fair, we set all common hyperparameters
(network layers, batch size, learning rate, discount factor, number of agents, etc) to be the same for
comparison within the same methods and different methods.

For TD3, target policy smoothing is implemented by adding ϵ ∼ N (0, 0.2) to the actions chosen
by the target actor-network, clipped to (−0.5, 0.5), delayed policy updates consist of only updating

17

the actor and target critic network every d iterations, with d = 2. While a larger d would result in
a larger benefit with respect to accumulating errors, for fair comparison, the critics are only trained
once per time step, and training the actor for too few iterations would cripple learning. Both target
networks are updated with τ = 0.005.

The TD3 and TD3+TDR used in this study are based on the paper (Fujimoto et al., 2018) and the
code from the authors (https://github.com/sfujim/TD3).

Hyperparameter TD3
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Policy noise
Noise clip
Policy update frequency
Batch size
Buffer size
γ
τ
Number of parallel actor
LNSS-N
Adam Learning rate
regularization factor

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
N (0, 0.2)
±0.5
2
256
1e6
0.99
0.005
1
100
1e-3
0.7

Table 3: TD3 + TDR hyper parameters used for DMC benckmark tasks

The SAC used in this study is based on paper (Haarnoja et al., 2018) and the code is from GitHub
(https://github.com/pranz24/pytorch-soft-actor-critic). and the hyperparameter is from Table 4.

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
N (0, 0.2)
±0.5
2
256
1e6
0.99
0.005

Hyperparameter SAC
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Policy noise
Noise clip
Policy update frequency
Batch size
Buffer size
γ
τ
Temperature parameter α 0.2
Number of parallel actor
LNSS-N
Adam Learning rate

1
100
1e-3

Table 4: SAC hyper parameters used for the DMC benckmark tasks

The D4PG used in this study is based on paper (Barth-Maron et al., 2018) and the code is modified
from TD3. The hyperparameter is from Table 5.

All Other algorithms are from the same DRL training platform (Tonic RL) (Pardo, 2020) with the
same evaluation as the above algorithms.

Sparse Reward Setup. 1) Cheetah Run Sparse: Cheetah needs to run forward as fast as possible.
The agent gets a reward only after speed exceeds 2.5 m/s, making the reward sparse. r = 1. That
is, if v >= 2.5 else r = 0.

18

Hyperparameter D4PG
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Noise clip
Batch size
Buffer size
γ
τ
Number of parallel actor
LNSS-N
Adam Learning rate
Vmax
Vmin
l
regularization factor

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
±0.5
256
1e6
0.99
0.005
1
100
1e-3
100
0
51
0.7

Table 5: D4PG + TDR hyper parameters used for the DMC benckmark tasks

19

D TDR ALGORITHMS DETAILS

In this section, we show our TDR-based algorithms. TD3-TDR is shown in Algorithm 1, SAC-
TDR is shown in Algorithm 2, and D4PG-TDR is shown in Algorithm 3. We mainly add LNSS
reward to the sample collection part. In algorithm update part, we mainly modify the target value
selection using Equation 7 for regular DRL and Equation (17) for distributional DRL. Additionally,
if applicable, we modify the actor gradient based on Equation 9 for regular DRL and Equation (21)
for distributional DRL. All codes will be released to GitHub once the paper get accepted.

Algorithm 1 TD3-TDR
Initialize:

1 ← θ1, θ′

2 ← θ2, ϕ′ ← ϕ,

• Critic networks Qθ1,Qθ2 and actor-network πϕ with random parameters, θ1, θ2, ϕ
• Target networks θ′
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
0, a′
Store the transition (s′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(66)

t, st+1) from D

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target value yt based on Equation (7)
Update Critics based on Equation 3
if k mod Policy Update frequency then

Update ϕ by Equation 9
Update target networks:
θ′
ζ ← τ θζ + (1 − τ )θ′
ζ
ϕ′ ← τ ϕ + (1 − τ )ϕ′

end if

end for

26.
27. end for

20

Algorithm 2 SAC-TDR
Initialize:

• Soft value function VΞ, target Soft value function V ′

Ξ, Critic networks Qθ1,Qθ2 and actor-

network πϕ with random parameters, θ1, θ2, ϕ

• Target networks Ξ′ ← Ξ
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.
9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
Store the transition (s′
0, a′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(67)

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target value yt based on Equation (7)
Update Critics based on Equation 3

t, st+1) from D

Update Soft value function based on original SAC formualtion
Update ϕ by original SAC formulation
Update target networks:
Ξ′ ← τ Ξ + (1 − τ )Ξ′

end for

24.
25. end for

21

Algorithm 3 D4PG-TDR
Initialize:

1 ← θ1, θ′

2 ← θ2, ϕ′ ← ϕ,

• Critic networks Zθ1,Zθ2 and actor-network πϕ with random parameters, θ1, θ2, ϕ
• Target networks θ′
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
0, a′
Store the transition (s′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(68)

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target distribution based on Equation (17)

t, st+1) from D

Update Critics based on Equation 18
if k mod Policy Update frequency then

Update ϕ by Equation 21
Update target networks:
θ′
ζ ← τ θζ + (1 − τ )θ′
ζ
ϕ′ ← τ ϕ + (1 − τ )ϕ′

end if

end for

26.
27. end for

22

","3 2 0 2 v o N 7 ] G L . s c [ 1 v 1 1 7 3 0 . 1 1 3 2 : v i X r a MITIGATING ESTIMATION ERRORS BY TWIN TD- REGULARIZED ACTOR AND CRITIC FOR DEEP REIN- FORCEMENT LEARNING Junmin Zhong Arizona State University Ruofan Wu Arizona State University Jennie Si ∗ Arizona State University ABSTRACT We address the issue of estimation bias in deep reinforcement learning ( DRL ) by introducing solution mechanisms that include a new , twin TD-regularized actor- critic ( TDR ) method . It aims at reducing both over and under estimation errors . With TDR and by combining good DRL improvements , such as distributional learning and long N -step surrogate stage reward ( LNSS ) method , we show that our new TDR-based actor-critic learning has enabled DRL methods to outper- form their respective baselines in challenging environments in DeepMind Control Suite . Furthermore , they elevate TD3 and SAC respectively to a level of perfor- mance comparable to that of D4PG ( the current SOTA ) , and they also improve the performance of D4PG to a new SOTA level measured by mean reward , conver- gence speed , learning success rate , and learning variance . 1 INTRODUCTION Reinforcement learning ( RL ) has been developed for decades to provide a mathematical formal- ism for learning-based control . Recently , significant progress has been made to attain excellent results for a wide range of high-dimensional and continuous state-action space problems especially in robotics applications , such as robot manipulation ( Andrychowicz et al. , 2017 ) , and human-robotic interaction ( Liu et al. , 2022 ; Wu et al. , 2022 ) . However , the fundamental issue of estimation error associated with actor-critic RL ( Van Hasselt et al. , 2016 ; Duan et al. , 2021 ) still poses great challenge . Overestimation due to , for example , using the max operator in updates has been identified and studied ( Thrun & Schwartz , 1993 ; Duan et al. , 2021 ) . To reduce it , most efforts have focused on attaining more accurate and stable critic networks . TD3 ( Fujimoto et al. , 2018 ) applies clipped double Q-learning by taking the minimum between the two Q estimates . SAC ( Haarnoja et al. , 2018 ) utilizes the double Q network and incorporates entropy regularization in the critic objective function to ensure more exploratory behavior to help alleviate the overestimation problem . However , directly taking the minimum value of the target networks such as that in TD3 and SAC has been reported to result in an underestimation bias ( Fujimoto et al. , 2018 ) . Evaluations have revealed multiple roles of over and under estimation errors in learning . On one hand , overestimation may not always be harmful ( Lan et al. , 2020 ) as it is considered playing a role of encouraging exploration by overestimated actions . Along this line , underestimation bias may discourage exploration . If the overestimation bias occurs in a high-value region containing the optimal policy , then encouraging exploration is a good thing ( Hailu & Sommer , 1999 ) . On the other hand , overestimation bias may also cause an agent to overly explore a low-value region . This may lead to a suboptimal policy . Accordingly , an underestimation bias may discourage an agent from exploring high-value regions or avoiding low-value regions . All things considered , if estimation errors are left unchecked , they may accumulate to negatively impact policy updates as suboptimal actions may be highly rated by a suboptimal critic , reinforcing the suboptimal action in the next policy update ( Fujimoto et al. , 2018 ) . Aside from the anecdotal evidence on the roles of over and under estimation , how to mitigate both of them in a principled way remains an open issue . ∗si @ asu.edu 1 While several methods and evaluations have been performed and shown promising , a major tool has been mostly left out thus far . That is , it is still not clear how , and if it is possible , to further reduce estimation errors by considering the actor given the interplay between the actor and the critic . Only a handful of approaches have been examined . As shown in ( Wu et al. , 2023 ) with demonstrated performance improvement , PAAC uses a phased actor to account for both a Q value and a TD error in actor update . A double actor idea was proposed and evaluated in ( Lyu et al. , 2022 ) . It takes the minimum value estimate associated with one of the two actor networks . However , directly using the minimum of the estimated values was shown resulting in an underestimation error , similar to that in TD3 . Other methods , such as Entropy ( Haarnoja et al. , 2018 ; Fox et al. , 2015 ) , mutual-information ( MI ) ( Leibfried & Grau-Moya , 2020 ) , and Kullback-Leibler ( KL ) ( Vieillard et al. , 2020 ; Rudner et al. , 2021 ) regularization , are also used to enhance policy exploration , robustness , and stability . TD-regularized actor-critic ( Parisi et al. , 2019 ) regularizes the actor only aiming to enhance the stability of the actor learning by applying a TD error ( same as that in online critic updates ) as a regularization term in actor updates . However , none of these methods have shown how regularization in actor may help reduce estimation error in the critic . In this paper , we propose a new , TD-regularized ( TDR ) learning mechanism which includes TD- regularized double critic networks and TD-regularized actor network . This new architecture has several properties that make it ideal for the enhancements we consider . For the TD-regularized double critic network , instead of directly selecting the minimum value from twin target networks , we select the target based on the minimum TD error , which then addresses not only overestimation but underestimation problems . For the TD-regularized actor network , we formulate a new TD error to regularize actor updates to avoid a misleading critic . This regularization term helps further reduce the estimation error in critic updates . Additionally , we apply TDR combined with distributional RL ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) and LNSS reward estimation method ( Zhong et al. , 2022 ) to further improve learning stability and performance . 2 RELATED WORK To shed light on the novelty of the TDR method , here we discuss double critic networks and TD error-based actor learning to provide a backdrop . We include reviews of distributional RL ( Barth- Maron et al. , 2018 ; Bellemare et al. , 2017 ) and long-N -step surrogate stage ( LNSS ) method ( Zhong et al. , 2022 ) in Appendix A . Double critic networks have been used in both RL ( Hasselt , 2010 ; Zhang et al. , 2017 ; Weng et al. , 2020 ) and DRL ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ; Van Hasselt et al. , 2016 ) . Double Q learning ( Hasselt , 2010 ; Van Hasselt et al. , 2016 ) was the first to show reduction of overestimation bias . TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) also were shown effective by applying clipped double Q-learning by using the minimum between the two Q estimates . However , these methods have induced an underestimation bias problem . ( Hasselt , 2010 ; Zhang et al. , 2017 ; Fujimoto et al. , 2018 ) . Consequently , weighted double Q learning ( Zhang et al. , 2017 ) was proposed to deal with both overestimation and underestimation biases . However , this method has not been tested in DRL context and therefore , it lacks a systematic approach to designing the weighting function . TD error-based actor learning is expected to be effective in reducing overestimation error since it is a consistent estimate of the advantage function with lower variance , and it discriminates feedback instead of directly using Q estimates . Some actor-critic variants ( Crites & Barto , 1994 ; Bhatnagar et al. , 2007 ) update the actor based on the sign of a TD error with a positive error preferred in policy updates . However , TD error only measures the discrepancy between the predicted value and the target value , which may not guide exploration effectively , and using TD error alone in actor update may discourage exploration and cause slow learning , especially in high-dimensional complex problems . TD-regularized actor-critic ( Parisi et al. , 2019 ) enhanced the stability of the actor update by using the same TD error ( as that in online critic update ) as a regularization term . However , such use of TD error may not sufficiently evaluate the critic update because it only uses the temporal difference between target and online Q estimates . Additionally , the time-varying regularization coefficient was shown leading to poor convergence ( Chen et al. , 2017 ) . Note also that the TD- regularized actor-critic only considered TD-regularized actor but not the critic . 2 Contributions . 1 ) We introduce a novel TDR mechanism that includes TD-regularized double critic networks and TD-regularized actor network . 2 ) Extensive experiments using DMC benchmarks show that TDR enables SOTA performance ( measureed by learning speed , success rate , variance , and converged reward ) across a wide variety of control tasks , such as locomotion , classical control , and tasks with sparse rewards . 3 ) We also provide qualitative analysis to show that each component of TDR contributes to mitigating both over and under estimation errors . 3 METHOD 3.1 DOUBLE Q IN ACTOR-CRITIC METHOD For a general double Q actor-critic method ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ) . The policy ( πϕ ) is called an actor and the state-action value function ( Qθ ( sk , ak ) ) is called a critic where both the actor and the critic are estimated by deep neural networks with parameters ϕ and θ , respectively . First , consider a policy π that is evaluated by the state-action value function below : Qπ ( sk , ak ) = E [ Rk|sk , ak ] , ( 1 ) where Rk = ( cid:80 ) ∞ t=k γt−krt , sk ∼ p ( · | sk−1 , ak−1 ) , ak = πϕ ( sk ) , and γ ∈ ( 0 , 1 ) . Most actor- critic methods are based on temporal difference ( TD ) learning ( Sutton & Barto , 2018 ) that updates Q estimates by minimizing the TD error , which is obtained from the the difference between a target and a critic estimated value . Next , consider typical double Q methods which entail twin Q networks denoted as Qθ1 and Qθ2 . The respective twin target networks are denoted as Qθ′ . In the upcoming discussions , we also use θ to denote parameters in both Q networks , i.e. , θ= { θ1 , θ2 } . The target value yk is the lesser of the two target values , and Qθ′ 1 2 yk = rk + γ min ζ=1,2 Qθ′ ζ ( sk+1 , πϕ′ ( sk+1 ) ) , ( 2 ) where by taking the minimum of the two target values , it aims to curtail overestimation of Q value frequently experienced by using a single target . Thus the critic value Qθ is updated by minimizing the loss function ( L ( θ ) ) with respect to the critic weights θ : L ( θ ) = Es∼pπ , a∼π [ ( cid:88 ) ( yk − Qθζ ( sk , ak ) ) 2 ] . ( 3 ) ζ=1,2 The actor weights can be updated by the deterministic policy gradient algorithm below ( Silver et al. , 2014 ) , where by convention ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ) , Qθ1 is used to update the actor weights . ∇ϕJ ( ϕ ) = Es∼pπϕ ( cid:104 ) ( cid:105 ) ∇aQθ1 ( sk , ak ) |a=πϕ ( s ) ∇ϕπϕ ( s ) . ( 4 ) Figure 1 : Twin TD-regularized Actor-Critic ( TDR ) Architecture 3 3.2 TWIN TD-REGULARIZED ACTOR-CRITIC ( TDR ) ARCHITECTURE Figure 1 depicts our TDR-based solution mechanisms , which include twin Q networks as in TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) , and an actor network . The TDR-based actor and critic updates are different from currently existing methods . In the following , we show how the new TDR selects target value yk different from Equation ( 2 ) as used in SAC and TD3 , and how that helps reduce both overestimation and underestimation errors . We also show how the new TD-regularized actor helps further reduce the estimation bias in the critic . Our TDR-based solutions in Figure 1 include two additional good improvements : distributional learning as in D4PG and long N -step surrogate stage ( LNSS ) method ( Zhong et al. , 2022 ) as described in Appendix A . 3.3 TD-REGULARIZED DOUBLE Q NETWORKS To overcome overestimation , TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) train their critic networks to minimize the loss function in Equation ( 3 ) where the target value yk is from Equation ( 2 ) . While this helps reduce overestimation error , it promotes a new problem of underes- timation , which usually occurs during the early stage of learning , or when subjected to corrupted reward feedback or inaccurate states . Our TDR method aims at minimizing the same loss function as in Equation ( 3 ) , but with a different target value yk . Instead of directly choosing the lesser from the two target values as in Equation ( 2 ) , we use the TD errors of the two target networks to set the target value . First , the two TD errors from the respective target networks are determined from : δ′ 1 = rk + γQθ′ δ′ 2 = rk + γQθ′ ( sk+1 , πϕ′ ( sk+1 ) ) − Qθ′ ( sk+1 , πϕ′ ( sk+1 ) ) − Qθ′ ( sk , ak ) , ( sk , ak ) . ( 5 ) ( 6 ) 1 1 2 2 The target value for TDR is then selected from the following : 1 yk = ( cid:26 ) rk + γQθ′ rk + γQθ′ ( sk+1 , πϕ′ ( sk+1 ) ) ( sk+1 , πϕ′ ( sk+1 ) ) Note from Equation ( 7 ) that TDR always uses a target value associated with a smaller target TD value ( regardless of the error sign ) between the two . As the ultimate objective of a target network is to converge to Qπ , such choice by TDR pushes the critic via Equation ( 3 ) toward reaching the target no matter the estimation error is from above or below , but with a smaller TD value . Thus , TDR is naturally positioned to address both overesdiation and underestimation errors . 1| ≤ |δ′ 1| > |δ′ if |δ′ if |δ′ 2| , 2| . ( 7 ) 2 3.4 TD-REGULARIZED ACTOR NETWORK Our TD-regularized actor network directly penalizes the actor ’ s learning objective whenever there is a critic estimation error . The estimation error ∆i+1 of the first critic ( Qθ1 chosen by convention of double Q-based actor-critic methods ) is determined from the following : ∆i+1 = Qθi+1 1 ( sk , ak ) − ( rk + γQθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) ) , ( 8 ) where i + 1 represents the iteration number during critic update . Then the actor can be updated in the direction of maximizing Q while keeping the TD error small , ( cid:20 ) ( cid:21 ) ∇ϕJ ( ϕ ) = Es∼pπϕ ∇a ( Qθi+1 1 ( cid:12 ) ( sk , ak ) − ρ ( ∆i+1 ) ) ( cid:12 ) ( cid:12 ) a=πϕ ( s ) ∇ϕπϕ ( s ) . ( 9 ) where ρ ∈ ( 0 , 1 ) is the regularization coefficient to balance the role of TD error in the actor learning objective . Thus , we expect the TD-regularized actor to help further reduce estimation error in the critic . With TDR actor and cirtic working together hand-in-hand , TDR is positioned to help avoid bad policy updates due to a misleading Q value estimate . Remark 1 . There are a few key differences between TDR and TD-regularized Actor Network ( Parisi et al. , 2019 ) . 1 ) In Equation ( 8 ) , they use the target critic Qθi′ ( sk+1 , πϕ ( sk+1 ) ) to construct TD error , the same as in critic updates . This TD error evaluates the temporal difference between target and online Q estimates . To more accurately evaluate critic estimations , we construct the TD error by only using online critics which directly affects actor updates . 2 ) Their TD error does not sufficiently evaluate how the critic updates . Instead in Equation ( 8 ) , we use the updated critic ( θi+1 ) to construct the TD error to directly measure critic estimation . 1 1 4 4 MITIGATING ESTIMATION BIAS BY TDR Let Qπ be the true Q value obtained by following the current target policy π , and let Qθ be the estimated value using neural networks . Let Ψk θ be a random estimation bias . Then for state-action pairs ( sk , ak ) . we have , Qθ ( sk , ak ) = Qπ ( sk , ak ) + Ψk θ . ( 10 ) The same holds for the target networks , i.e. , when θ is replaced by θ′ in the above equation . An overestimation problem refers to when the estimation bias E [ Ψk θ ] > 0 , and an underestimation problem when the estimation bias E [ Ψk θ ] < 0 . 4.1 MITIGATING ESTIMATION BIAS USING TD-REGULARIZED DOUBLE CRITIC NETWORKS Theorem 1 . Let Qπ be the true Q value following the current target policy π , and Qθ′ and Qθ′ be the target network estimates using double Q neural networks . We assume that there exists a step random estimation bias ψk ( i.e. , estimation bias at the kth stage ) , and that it is independent of θ′ ζ ( sk , ak ) with mean E [ ψk ζ , µ′ ] = µ′ ζ < ∞ , for all k , and ζ = 1 , 2 . Additionally , let δYk denote θ′ ζ the target value estimation error . Accordingly , we denote this error for TDR as δY T DR , and DQ as δY DQ k . We then have the following , k 1 2 Where E [ δY T DR k ] = E [ Qπ − yT DR k ] | ≤ |E [ δY DQ k |E [ δY T DR k ] , and E [ δY DQ k ] = E [ Qπ − yDQ k ] . ] | , ( 11 ) Proof . The proof of Theorem 1 is provided in Appendix B Remark 2 . By selecting a target value with less TD error , our TD-regularized double critic networks mitigate both overestimation and underestimation errors . However , vanilla double Q methods usu- ally push the target toward the lower value no matter the estimation error is over or under . Although this estimation error may not be detrimental as they may be small at each update , the presence of unchecked underestimation bias raises two concerns . Firstly , if there is no sufficient reward feed- back from the environment , ( e.g. , for a noisy reward or sparse reward ) , underestimation bias may not get a chance to make corrections and may develop into a more significant bias over several up- dates . Secondly , this inaccurate value estimate may lead to poor policy updates in which suboptimal actions might be highly rated by the suboptimal critic , reinforcing the suboptimal action in the next policy update . 4.2 ADDRESSING A MISGUIDING CRITIC IN POLICY UPDATES USING TD-REGULARIZED ACTOR Theorem 2 . Let Qπ denote the true Q value following the current target policy π , Qθ1 be the estimated value . We assume that there exists a step random estimation bias ψk that is independent θ1 of ( sk , ak ) with mean E [ ψk ] = µ1 , µ1 < ∞ , for all k. We assume the policy is updated based θ1 on critic Qθ1 using the deterministic policy gradient ( DPG ) as in Equation ( 4 ) . Let δϕk denote the change in actor parameter ϕ updates at stage k. Accordingly , we denote this change for TDR as , and true change without any approximation error in Q as δϕtrue δϕT DR . k k We then have the following , , vanilla DPG as δϕDP G k ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 12 ) Where δϕtrue k Appendix B , δϕDP G k , and δϕT DR k are defined as Equation ( 55 ) , ( 56 ) , and ( 57 ) respectively in Proof . The proof of Theorem 2 is provided in Appendix B . Remark 3 . Theorem 2 , holds for ρ ∈ ( 0 , 1 ) . If the regularization factor ρ = 1 1−γ , from Equation ( 59 ) , we have E [ Ψk ] = E [ δϕT DR ] . By using TDR , θ1 the actor will always update the same way as using the true value . While this is not realistic , the following relationship still preserves |E [ Ψk ] | to help ease the negative effect of θ1 critic estimation bias . − ρ∆ ] = 0 which implies that E [ δϕtrue − ρ∆ ] | ≤ |E [ Ψk θ1 k k 5 4.3 MITIGATING CRITIC ESTIMATION ERROR BY TD-REGULARIZED ACTOR Theorem 3 . Suboptimal actor updates negatively affect the critic . Specifically , consider actor up- dates as in Theorem 2 , in the overestimation case , we have : E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πT rue ( sk ) ) ] , and in the underestimation case , E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 13 ) ( 14 ) Proof The proof of Theorem 3 is provided in Appendix B . Remark 4 . For both cases , by using TD-regularized actors , it is expected to result in less estimation bias in the critic . 5 EXPERIMENTS AND RESULTS In this section , we provide a comprehensive evaluation of our TDR enabled actor-critic learning methods based on three commonly used , well-behaved baseline algorithms including SAC , TD3 and D4PG . Additional evaluations are also provided for popular DRL algorithms such as DDPG and PPO to provide a broader perspective on the effectiveness of TDR-based methods . All evaluations are performed based on several benchmarks in Deepmind Control Suite ( Tassa et al. , 2018 ) . In reporting evaluation results , we use the following short-form names : 1 ) Base : the original DRL algorithms including SAC , TD3 , D4PG , DDPG and PPO . 2 ) TDR-TD3 : Applied TD regularized double critic ( TD Critic ) networks , TD regularized actor ( TD Actor ) network , with regularization factor ρ = 0.7 , and LNSS with N = 100 . 3 ) TDR-SAC : Applied TD regularized double critic ( TD Critic ) networks , and LNSS with N = 100 . 4 ) dTDR ( TDR-D4PG ) : Applied TD regularized double critic ( TD Critic ) network , TD regularized actor ( TD Actor ) network , with regularization factor ρ = 0.7 , and LNSS with N = 100 . Our evaluations aim to quantitatively address the following questions : Q1 . How does TDR improve over Base and other common methods ? Q2 . How does the performance of TDR methods compare to that of SOTA algorithms ( D4PG ) ? Q3 . Is TDR method robust enough to handle both dense stochastic reward and sparse reward ? Q4 . How does each component in TDR-based learning mechanisms affect performance ? Q5 . How does TD regularized actor make policy updates in situations of misguiding critics ? Q6 . How does the regularization coefficient ρ in Equation ( 9 ) affect TD Actor performance ? Details of the implementation , training , and evaluation procedures are provided in Appendix C and D where links to all implementation codes are also provided . 5.1 MAIN EVALUATION In obtaining comprehensive evaluation results summarized in Table 1 , we included a 10 % noise respectively in state , action , and reward in each of the considered DMC environments in order to make the evaluations more realistic . In “ Cheetah Run sparse ” , we sparsified the reward in the environment . All details of the environment setup can be found in Appendix C. In Table 1 , “ Success ” is shorthand for learning success rate , “ Avg . Rwd ” for average reward , and “ Rank ” ( % ) is the “ percent of reward difference ” between the evaluated method and the SOTA D4PG , which is ( the average reward of the evaluated method over that of the D4PG - 1 ) , the more positive the better . Note that , in computing the success rate , only those trials that have achieved a reward of at least 10 are accounted for as successful learning . The results are based on the last 50 evaluations of 10 different random seeds ( same for all compared algorithms ) . Best performances are boldfaced for average reward ( Avg . Rwd ) . Note that we did not implement our TD Actor into SAC because SAC already has a max entropy-regulated actor . Q1 TDR improves over respective Base methods . The learning curves for six benchmark environ- ments are shown in Figure 2 . Overall , TDR methods ( solid lines ) outperform their respective Base 6 Figure 2 : Systematic evaluation of TDR realized in three DRL algorithms ( SAC , TD3 , D4PG ) in DMC environments with 10 % uniform random noise in state , action , and reward . The shaded regions represent the 95 % confidence range of the evaluations over 10 seeds . The x-axis is the number of steps . Envirinoment D4PG DDPG PPO SAC TD3 TDR-SAC TDR-TD3 dTDR Envirinoment D4PG DDPG PPO SAC TD3 TDR-SAC TDR-TD3 dTDR Success [ % ] 100 100 100 90 100 100 100 100 Success [ % ] 100 100 20 0 0 100 100 100 Finger Turn Hard Avg . Rwd [ µ ± 2σ ] 400.9 ± 173.4 222.1 ± 160.4 85.9 ± 50 65.6 ± 30.2 205.9 ± 108.5 601.5 ± 147.4 569.8 ± 142.1 841.02 ± 148.3 Acrobot Swingup Avg . Rwd [ µ ± 2σ ] 26.8 ± 8.9 17.2 ± 3.8 7.9 ± 7.8 4 ± 2.2 5.2 ± 4.2 42.9 ± 5.1 50 ± 7.9 62.6 ± 14.4 Rank [ % ] 0 -44.6 -78.6 -83.6 -48.6 49.9 42.3 109.8 Rank [ % ] 0 -35.8 -70.5 -85.1 -80.6 60.1 86.5 133.6 Quadruped Walk Success [ % ] 100 100 100 100 100 100 100 100 Avg . Rwd [ µ ± 2σ ] 858.5 ± 11.4 226.8 ± 133.6 173.1 ± 60.4 196.6 ± 73.7 334.8 ± 76.4 479.5 ± 126.9 475.4 ± 45.4 888.6 ± 15.7 Cartpole Swingup Sparse Rank [ % ] 0 -73.6 -79.8 -77.2 -61 -44.2 -44.6 3.46 Success [ % ] 100 0 80 0 0 100 100 100 Avg . Rwd [ µ ± 2σ ] 493.5 ± 15.9 3.6 ± 5.8 99.2 ± 172.9 1.7 ± 3.4 1.3 ± 2.3 774.2 ± 51.1 790.13 ± 33.0 810.3 ± 34.9 Rank [ % ] 0 -99 -79.9 -99.7 -99.7 56.8 60.1 64.2 Success [ % ] 100 100 100 100 100 100 100 100 Fish Swim Avg . Rwd [ µ ± 2σ ] 153.7 ± 68.1 109.7 ± 27.1 78.67 ± 6.28 73.2 ± 9.87 85.3 ± 21.7 212.3 ± 51.2 204.2 ± 41.5 249.9 ± 45.5 Cheetah Run Sparse Success [ % ] 60 50 0 0 50 100 100 100 Avg . Rwd [ µ ± 2σ ] 532.8 ± 388.4 160.7 ± 284.7 0 ± 0 0 ± 0 220.5 ± 354.7 930.2 ± 18.7 827.8 ± 62.2 900.1 ± 30.8 Rank [ % ] 0 -28.6 -48.8 -52.4 -44.5 37.9 32.7 62 Rank [ % ] 0 -69.8 -100 -100 -58.6 74.6 55.4 68.9 Table 1 : Systematic evaluations of TDR respectively augmented Base algorithms . “ Rank ” ( % ) is the “ percent of reward difference ” between the SOTA D4PG , the more positive the better . methods TD3 , SAC and D4PG ( dash lines ) in terms of episode reward , learning speed , learning variance and success rate . In Table 1 , among the measures , the Avg . Rwd of TDR methods outper- formed respective baseline algorithms . Notice from the table that the learning success rates for all TDR methods are now 100 % , a significant improvement over the Base methods . In comparison , DDPG , SAC and TD3 Base methods struggle with Acrobot Swingup , Cartpole Swingup Sparse , and Cheetah Run Sparse . Moreover , TDR methods also outperform DDPG and PPO in terms of averaged reward ( Awg.Rwd ) , learning speed , learning variance , and success rate . Thus , TDR has helped succesfully address the random initialization challenge caused by random seeds ( Henderson et al. , 2018 ) . Q2 TDR brings performance of Base methods close to or better than that of the SOTA D4PG . From Figure 2 , and according to the “ Rank ” measure in Table 1 , for all environments but Quadruped walk , TDR ( TDR-SAC and TDR-TD3 ) helped enhance the performances of the respective Base methods . Additionally , it even outperformed the SOTA D4PG by around 40 % in the “ Rank ” mea- sure . For Quadruped walk , even though TDR-SAC and TDR-TD3 did not outperform D4PG , they still are the two methods , among all evaluated , that provided closest performance to D4PG . It is also 7 worth noting that TDR brings the performance of D4PG to a new SOTA level measured by mean reward , convergence speed , and learning success rate . Q3 TDR is robust under both dense stochastic reward and sparse reward . From Figure 2 and Table 2 , TDR methods outperformed their respective baselines in both dense stochastic and sparse reward in terms of average reward , learning variance , success rate , and converge speed . In particular , baseline algorithms such as TD3 and SAC struggle with sparse reward benchmark environments ( cartpole swingup sparse and cheetah run sparse ) . However , by using TDR , they not only learned , but also achieved SOTA performance . Methods TD3+TD Critic TD3+LNSS TD3+TD Actor TD3+TDR SAC+TD Critic SAC+LNSS SAC+TDR D4PG+TD Critic D4PG+LNSS D4PG+TD Actor dTDR Acrobot Swingup Finger TurnHard Avg . Rwd [ µ ± 2σ ] 24.9 ± 11.7 24.2 ± 9.2 6.9 ± 2.9 42.9 ± 5.1 28.8 ± 12.2 9.7 ± 2.9 42.9 ± 5.1 32.8 ± 6.9 43.9 ± 16.7 29.9 ± 13.8 62.6 ± 14.4 Enhancement [ % ] 378.8 365.4 32.7 725 620 142.5 972.5 22.4 63.8 11.6 133.6 Avg . Rwd [ µ ± 2σ ] 556.2 ± 239.8 547.5 ± 120.5 212.3 ± 45.7 569.8 ± 142.1 588 ± 223.8 573 ± 156.5 601.5 ± 147.4 835.7 ± 140.9 675.1 ± 217.6 532.5 ± 235.7 841.1 ± 148.3 Enhancement [ % ] 170.1 165.9 3.1 176.7 796.3 773.5 816.9 108.5 68.4 33.5 109.8 Cartpole Swingup Sparse Avg . Rwd [ µ ± 2σ ] 766.2 ± 86.1 766.6 ± 38.3 339.6 ± 231.9 790.1 ± 33.0 766.7 ± 126.4 722.8 ± 162.4 774.2 ± 51.1 678.7 ± 246.2 759.1 ± 31.1 600 ± 129.3 810.3 ± 34.9 Enhancement [ % ] 588.4 588.7 260.2 606.7 449.6 423.7 454.4 37.5 53.8 21.6 64.2 Table 2 : Systematic evaluations of each component of TDR compared to their respective Base al- gorithms . “ Enhancement ” ( % ) is the “ percent of reward difference ” between the respective Base algorithms , the larger the better . Note that TD Actor was not considered for SAC as SAC already has a max entropy-regularized actor . Figure 3 : Evaluation of TD Actor with different ρ ( ρ = 0 , 0.1 , 0.3 , 0.5 , 0.7 , 0.9 ) in Equations ( 9 , 21 ) based on two DRL algorithms ( TD3 , D4PG ) in DMC environments with 10 % uniform random noise in state , action , and reward . The shaded regions represent the 95 % confidence range of the evaluations over 10 seeds . The x-axis is the number of steps . 5.2 ABLATION STUDY To perform the ablation study , we examined TDR by removing each of the following three compo- nents . The respective short-form descriptions are : 1 ) “ TD Critic ” : the TD regularized double Q networks . 2 ) “ TD Actor ” : the TD regularized actor network . 3 ) “ LNSS ” : LNSS method with N = 100 . In Table 2 , “ Enhancement ” ( % ) is the “ percent of reward difference ” between the evaluated method and its Base method , the larger the better . 8 Q4 TD Critic , TD Actor , and LNSS effectively improved the Base algorithms . In Table 2 , TD Critic , LNSS , and TD Actor all effectively improved the Base algorithms . From the table , TD Critic and LNSS have provided comparable and significant enhancement over Base algorithms . As our TD Critic methods outperform respective Base algorithms , this suggests that mitigating estimation errors both over and under from vanilla double Q network is an effective way to improve performance which has also been shown in our theoretical analysis ( Theorem 1 ) . The LNSS method helped improve learning performance by reducing variances in value estimation for noisy rewards as shown both theoretically and empirically ( Zhong et al. , 2022 ) . By including LNSS , our TDR is more robust under noisy and sparse rewards . The TD Actor element also helped make appreciable improvements on learning performance as shown in Table 2 . More importantly , TD Actor plays an importantly role in TDR since it not only stabilizes the policy updates as shown theoretically in Theorem 2 but also addresses the estimation error in critic as shown theoretically in Theorem 3 . 5.3 HYPER PARAMETER STUDY Hyperparameter study results are summarized in Figure 3 where two DRL methods ( D4PG and TD3 ) with TD Actor are evaluated for different regularization factor ρ ( ρ = 0 , 0.1 , 0.3 , 0.5 , 0.7 , 0.9 ) . What is reported is the 10-seed averaged performance , i.e. , the average of the approximate es- timation error which is the difference between the true accumulated reward and the critic value : Ψ = 1 10 t=0 γtrt − Q ( s0 , a0 ) ) . eval=0 ( ( cid:80 ) 999 ( cid:80 ) 9 Q5 TD regularized Actor helps reduce the estimation error in critic . From Figure 3 , with TD regularized Actor ( TD Actor ) , the estimation errors in the critic are re- duced from those without . For example , in Finger Turn hard , D4PG + TD Actor results in less overestimation error compared with ρ = 0 at the later stage of training . TD3 + TD Actor has less underestimation error compared with ρ = 0 . Similarly in cartpole swingup sparse , D4PG + TD Actor results in less overestimation error compared with ρ = 0 . A policy can be evaluated by the “ epois reward ” where a higher epois reward generally results from a better policy . From Figure 3 , policy updates are improved by selecting a suitable regularization fac- tor ρ . Especially , in cartpole swingup sparse , TD3 + TD Actor enables successful learning whereas the Base method struggled and stuck to 0 or no learning for the entire training period . Q6 A range of ρ ( ρ = 0.3 , 0.5 , 0.7 ) generally are good choices . From Figure 3 , a small regular- ization factor ρ = 0.1 in TDR will result in less regularization which may not provide sufficient estimation error reduction in the critic . A larger regularization factor ρ = 0.9 in TDR will result in more regularization and may have a negative effect on learning . Therefore , ρ = 0.3 , 0.5 , 0.7 may be good choices . Therefore in this work , we have consistently used ρ = 0.7 in obtaining all results . 6 CONCLUSION , DISCUSSION , AND LIMITATION OF THE STUDY 1 ) In this work , we introduce a novel TDR mechanism that includes TD-regularized double critic networks and TD-regularized actor network . Both components are shown to help mitigate both over and under estimation errors . TDR has been shown to consistently outperform respective Base algorithms in solving benchmark tasks in terms of average reward , learning success rate , learning speed , and most times , learning variance . 2 ) Our analytical results also show that each component of TDR helps mitigate both over and under estimation errors . 3 ) As shown in Figure 2 , for five out of the six environments ( except quadruped walk ) evaluated , our TDR combined with distributional and LNSS elements has significantly elevated the current SOTA performance of D4PG to a new level with an increase of at least 60 % . Even though we have identified a range of generally good regularization coefficient ρ values ( 0.3 , 0.5 , 0.7 ) , as Figure 3 shows , different algorithms in different environments have responded somewhat differently to ρ . Therefore , how to effectively determine a regularization factor to have the most improvement remains a question , and thus , it is the limitation of this study . Additionally , the promising performances of TDR come after extensive training with millions of learning steps . How TDR performs under limited training time and training steps need to be further investigated . 9 REFERENCES Marcin Andrychowicz , Filip Wolski , Alex Ray , Jonas Schneider , Rachel Fong , Peter Welinder , Bob McGrew , Josh Tobin , Pieter Abbeel , and Wojciech Zaremba . Hindsight experience replay . arXiv preprint arXiv:1707.01495 , 2017 . Gabriel Barth-Maron , Matthew W Hoffman , David Budden , Will Dabney , Dan Horgan , Dhruva Tb , Alistair Muldal , Nicolas Heess , and Timothy Lillicrap . Distributed distributional deterministic policy gradients . arXiv preprint arXiv:1804.08617 , 2018 . Marc G Bellemare , Will Dabney , and R´emi Munos . A distributional perspective on reinforcement learning . In International conference on machine learning , pp . 449–458 . PMLR , 2017 . Shalabh Bhatnagar , Mohammad Ghavamzadeh , Mark Lee , and Richard S Sutton . Incremental nat- ural actor-critic algorithms . Advances in neural information processing systems , 20 , 2007 . Renzhi Chen , Ke Li , and Xin Yao . Dynamic multiobjectives optimization with a changing number of objectives . IEEE Transactions on Evolutionary Computation , 22 ( 1 ) :157–171 , 2017 . Robert Crites and Andrew Barto . An actor/critic algorithm that is equivalent to q-learning . Advances in Neural Information Processing Systems , 7 , 1994 . Will Dabney , Georg Ostrovski , David Silver , and R´emi Munos . Implicit quantile networks for distributional reinforcement learning . In International conference on machine learning , pp . 1096– 1105 . PMLR , 2018a . Will Dabney , Mark Rowland , Marc Bellemare , and R´emi Munos . Distributional reinforcement learning with quantile regression . In Proceedings of the AAAI Conference on Artificial Intelli- gence , volume 32 , 2018b . Jingliang Duan , Yang Guan , Shengbo Eben Li , Yangang Ren , Qi Sun , and Bo Cheng . Distributional soft actor-critic : Off-policy reinforcement learning for addressing value estimation errors . IEEE transactions on neural networks and learning systems , 33 ( 11 ) :6584–6598 , 2021 . Roy Fox , Ari Pakman , and Naftali Tishby . Taming the noise in reinforcement learning via soft updates . arXiv preprint arXiv:1512.08562 , 2015 . Scott Fujimoto , Herke Hoof , and David Meger . Addressing function approximation error in actor- critic methods . In International Conference on Machine Learning , pp . 1587–1596 . PMLR , 2018 . Tuomas Haarnoja , Aurick Zhou , Kristian Hartikainen , George Tucker , Sehoon Ha , Jie Tan , Vikash Kumar , Henry Zhu , Abhishek Gupta , Pieter Abbeel , et al . Soft actor-critic algorithms and appli- cations . arXiv preprint arXiv:1812.05905 , 2018 . G Hailu and G Sommer . On amount and quality of bias in reinforcement learning . In IEEE SMC ’ 99 Conference Proceedings . 1999 IEEE International Conference on Systems , Man , and Cybernetics ( Cat . No . 99CH37028 ) , volume 2 , pp . 728–733 . IEEE , 1999 . Hado Hasselt . Double q-learning . Advances in neural information processing systems , 23 , 2010 . Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger . Deep reinforcement learning that matters . In Proceedings of the AAAI conference on artificial intelligence , volume 32 , 2018 . Qingfeng Lan , Yangchen Pan , Alona Fyshe , and Martha White . Maxmin q-learning : Controlling the estimation bias of q-learning . arXiv preprint arXiv:2002.06487 , 2020 . Felix Leibfried and Jordi Grau-Moya . Mutual-information regularization in markov decision pro- cesses and actor-critic learning . In Conference on Robot Learning , pp . 360–373 . PMLR , 2020 . Wentao Liu , Junmin Zhong , Ruofan Wu , Bretta L Fylstra , Jennie Si , and He Helen Huang . Inferring human-robot performance objectives during locomotion using inverse reinforcement learning and inverse optimal control . IEEE Robotics and Automation Letters , 7 ( 2 ) :2549–2556 , 2022 . 10 Jiafei Lyu , Xiaoteng Ma , Jiangpeng Yan , and Xiu Li . Efficient continuous control with double actors and regularized critics . In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36 , pp . 7655–7663 , 2022 . Fabio Pardo . Tonic : A deep reinforcement learning library for fast prototyping and benchmarking . arXiv preprint arXiv:2011.07537 , 2020 . Simone Parisi , Voot Tangkaratt , Jan Peters , and Mohammad Emtiyaz Khan . Td-regularized actor- critic methods . Machine Learning , 108:1467–1501 , 2019 . Tim GJ Rudner , Cong Lu , Michael A Osborne , Yarin Gal , and Yee Teh . On pathologies in kl- regularized reinforcement learning from expert demonstrations . Advances in Neural Information Processing Systems , 34:28376–28389 , 2021 . David Silver , Guy Lever , Nicolas Heess , Thomas Degris , Daan Wierstra , and Martin Riedmiller . Deterministic policy gradient algorithms . In International conference on machine learning , pp . 387–395 . Pmlr , 2014 . Richard S Sutton and Andrew G Barto . Reinforcement learning : An introduction . MIT press , 2018 . Yuval Tassa , Yotam Doron , Alistair Muldal , Tom Erez , Yazhe Li , Diego de Las Casas , David Bud- den , Abbas Abdolmaleki , Josh Merel , Andrew Lefrancq , et al . Deepmind control suite . arXiv preprint arXiv:1801.00690 , 2018 . Sebastian Thrun and Anton Schwartz . Issues in using function approximation for reinforcement learning . In Proceedings of the Fourth Connectionist Models Summer School , volume 255 , pp . 263 . Hillsdale , NJ , 1993 . Hado Van Hasselt , Arthur Guez , and David Silver . Deep reinforcement learning with double q- learning . In Proceedings of the AAAI conference on artificial intelligence , volume 30 , 2016 . Nino Vieillard , Tadashi Kozuno , Bruno Scherrer , Olivier Pietquin , R´emi Munos , and Matthieu Geist . Leverage the average : an analysis of kl regularization in reinforcement learning . Advances in Neural Information Processing Systems , 33:12163–12174 , 2020 . Wentao Weng , Harsh Gupta , Niao He , Lei Ying , and R Srikant . The mean-squared error of double q-learning . Advances in Neural Information Processing Systems , 33:6815–6826 , 2020 . Ruofan Wu , Junmin Zhong , Brent Wallace , Xiang Gao , He Huang , and Jennie Si . Human-robotic prosthesis as collaborating agents for symmetrical walking . Advances in Neural Information Processing Systems , 35:27306–27320 , 2022 . Ruofan Wu , Junmin Zhong , and Jennie Si . Phased actor in actor-critic reinforcement learning . 2023 . Zongzhang Zhang , Zhiyuan Pan , and Mykel J Kochenderfer . Weighted double q-learning . In IJCAI , pp . 3455–3461 , 2017 . Junmin Zhong , Ruofan Wu , and Jennie Si . Long n-step surrogate stage reward to reduce variances of deep reinforcement learning in complex problems . arXiv preprint arXiv:2210.04820 , 2022 . A DISTRIBUTIONAL TDR AND LNSS The distributional RL ( Bellemare et al. , 2017 ) represents value function in terms of probability distribution rather than function estimates . This distribution provides a more comprehensive rep- resentation of the uncertainty associated with a range of different possible reward returns and state action pairs which can provide more informative value function estimation . Many distributional RL algorithms ( Bellemare et al. , 2017 ; Dabney et al. , 2018b ; a ) has been achieved great performance im- provements on many discrete problems such as Atari benchmarks . D4PG ( Barth-Maron et al. , 2018 ) applied distributional RL into continuous control problem by combining the distributional return function within an actor-critic framework . DSAC ( Duan et al. , 2021 ) address overestimation error by applying distributional RL piggyback on SAC . Although , D4PG and DSAC can provide more accurate critic , the overestimation of actor still exists since the actor is still updated by maximizing the expectation of value function distribution . How to regulate actors in distributional RL in solving overestimations was barely discussed before . 11 A.1 DISTRIBUTIONAL TD-REGULARIZED ACTOR-CRITIC ( DTDR ) Here we tailor a distributional TDR ( dTDR ) method based on the original distributional conceptu- alization developed in D4PG ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) . We show a number of enhancements in the meantime . Distributional Critic . The distributional critic ( Bellemare et al. , 2017 ) treated the return in Equa- tion 1 as a random variable Z ( sk , ak ) whose expectation is used as the Q value estimate , namely , Q ( sk , ak ) = E [ Z ( sk , ak ) ] . In dTDR however , we use TD errors to evaluate distributional critics . Similar to Equation 5 and 6 , distributional TD errors of the two target networks can be written as : ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Zθ′ 1 = rk + γE [ Zθ′ d′ ( sk , ak ) ] , ( 15 ) 1 1 2 = rk + γE [ Zθ′ d′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Zθ′ 2 ( sk , ak ) ] . The twin TD-regularized target distributional Bellman operator is thus defined as : T Zk D= ( cid:26 ) rk + γZθ′ rk + γZθ′ 1 2 ( sk+1 , πϕ′ ( sk+1 ) ) ( sk+1 , πϕ′ ( sk+1 ) ) if |d′ if |d′ 1| ≤ |d′ 2| 1| > |d′ 2| ( 16 ) ( 17 ) where A D= B denotes that two random variables A and B follow the same probability laws . Al- though the distributional Bellman operator appears similar to Equation 1 , it maps state-action pairs to distributions . As such , we need to define a new TD error measure for the distribution as in D4PG ( Barth-Maron et al. , 2018 ) . We consider using the following distributional loss , L ( θ ) = Es∼pπ , a∼π [ ( cid:88 ) ζ=1,2 l ( T Zk , Zθζ ( sk , ak ) ) ] , ( 18 ) where l measures the distance between two distributions . Many distributional RL algorithms use Kullback-Leibler ( KL ) divergence as the distance metric ( Duan et al. , 2021 ; Barth-Maron et al. , 2018 . We adopt the same metric . Distributional Actor . In most distributional methods ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) , policy updates are performed based on the policy gradient below , ∇ϕJ ( ϕ ) = Es∼pπϕ [ E [ ∇aZθ ( sk , ak ) ] |a=πϕ ( s ) ∇ϕπϕ ( s ) ] . ( 19 ) In our dTDR , we need to use critic evaluation metrics to evaluate the quality of the current distribu- tional critic and the regularized distributional actor . We first formulate the following loss metric : Lz ( ϕ ) = E [ l ( rk + γZθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) , Zθi+1 1 ( sk , πϕ ( sk ) ) ] . ( 20 ) Similar to TD-regularized actor network , the distributional actor is updated in the direction of max- imizing the expected critic while keeping the expected distance between the projected critic and the critic , namely , ∇ϕJ ( ϕ ) = Es∼pπϕ [ ( E [ ∇aZθi+1 1 ( sk , ak ) ] − ∇aρLz ( ϕ ) ) |a=πϕ ( s ) ∇ϕπϕ ( s ) ] , ( 21 ) where ρ ∈ ( 0 , 1 ) is a regularization coefficient . A.2 LONG N-STEP SURROGATE STAGE ( LNSS ) REWARD LNSS ( Zhong et al. , 2022 ) utilizes a long reward trajectory of N future steps in the estimation of stage reward rk . Using the LNSS-resulted reward r′ k in place of the original rk was shown to effectively reduce learning variance with significant performance improvements for off-policy methods . Given a reward trajectory of N steps from time step k , let G ( sk : k+N −1 , ak : k+N −1 ) ∈ R ( with shorthand notation Gk ) denote the discounted N -step return , i.e. , k+N −1 ( cid:88 ) γt−krt , Gk = t=k 12 ( 22 ) where rt is the tth stage reward and t is from k to k + N − 1 . In LNSS , r′ reward in place of rk in Equation ( 2 ) . To determine r′ N -step reward sequence , namely k is a surrogate stage k , LNSS treat it as a weighted average of the r′ k = γt−krt ( cid:80 ) k+N −1 t=k ( cid:80 ) N −1 n=0 γn . ( 23 ) As Figure 1 shows , Once r′ ( sk , ak , r′ as discussed . k is obtained , it is simply used in place of rk to form a new tuple k , sk+1 ) , which is then stored into the memory buffer D. The TDR method proceeds B ESTIMATION ANALYSIS Lemma 1 . Let Qπ be the true Q value following the current target policy π , and Qθ′ be the target network estimates using double Q neural networks . We assume that there exists a step random estimation bias ψk ( i.e. , estimation bias at the kth stage ) , and that it is independent of ( sk , ak ) with θ′ ζ mean E [ ψk ζ , µ′ ] = µ′ 2 respectively defined in θ′ ζ Equations ( 5 ) and ( 6 ) , we have , ζ < ∞ , for all k , and ζ = 1 , 2 . Then for δ′ 1 and δ′ and Qθ′ 2 1 E [ δ′ E [ δ′ 1 ] = −µ′ 1 , 2 ] = −µ′ 2 . ( 24 ) ( 25 ) ( 26 ) Proof . With the step random estimation bias ψk θ′ ζ , We can rewrite the expectation of Ψk θ′ ζ as E [ Ψk+1 θ′ ζ ] = ∞ ( cid:88 ) t=k+1 γt−k−1E [ ψt θ′ ζ ] = 1 1 − γ µ′ ζ . Then the expectation of the target can be written as , E [ yk ] = E [ rk ] + γE [ ( Qπ ( sk+1 , ak+1 ) + Ψk+1 ∞ ( cid:88 ) θ′ ζ ) ] = E [ rk ] + γ ( E [ γt−k−1rt ] ) + γ 1 − γ µ′ ζ = Qπ ( sk , ak ) + t=k+1 γ 1 − γ µ′ ζ . By using Equations ( 10 ) , and ( 26 ) , the TD errors of the two target critics ( Equations 5 and 6 ) , respectably are : E [ δ′ 1 ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Qθ′ 1 1 γ 1 − γ 1 − γ 1 − Qπ ( sk , ak ) − µ′ ( sk , ak ) ] µ′ 1 ( 27 ) = Qπ ( sk , ak ) + Similarly , E [ δ′ = −µ′ 1 . 2 ] = −µ′ 2 . Thus Lemma 1 holds . With Lemma 1 in place , we are now ready to analyze the estimation errors by using TDR and the double Q ( DQ ) method as in TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) . Theorem 1 . Let assumptions in Lemma 1 hold , and let δYk denote the target value estimation error . Accordingly , we denote this error for TDR as δY T DR . We then have the following , , and DQ as δY DQ k k |E [ δY T DR k ] | ≤ |E [ δY DQ k ] | . ( 28 ) Proof . The proof is based on enumerating a total of 8 possible scenarios of estimation errors which are determined from the relationships among the two target Q values and the true Qπ value . We provide proofs for the 4 out of 8 unique scenarios below . 13 First note that , E [ δY T DR Case 1 : If the target critic values and the true value Qπ have the following relationship : ] = E [ Qπ − yT DR ] = E [ Qπ − yDQ ] , and E [ δY DQ ] . k k k k i.e , Qθ′ 1 is more underestimated as that implies E [ Qθ′ 1 ] < E [ Qθ′ 2 ] < Qπ , |E [ Ψk θ′ 1 ] | > |E [ Ψk θ′ 2 ] | , |µ′ 1| > |µ′ 2| . Based on Lemma 1 and Equation ( 7 ) , our TDR will use Qθ′ 2 in the target value , E [ yT DR k ] = E [ rk ] + γE [ Qθ′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] . However for a vanilla double Q network , the target value will be Qθ′ 1 , ] = E [ rk ] + γE [ Qθ′ Thus based on Equation ( 26 ) , the two estimation errors of the respective target values are ( sk+1 , πϕ′ ( sk+1 ) ) ] . E [ yDQ k 1 |E [ δY T DR k ] | = |E [ Qπ − yT DR k ] | = | |E [ δY DQ k ] | = |E [ Qπ − yDQ k ] | = | µ′ 2| , γ 1 − γ γ µ′ 1 − γ 1| . Since |µ′ 1| > |µ′ 2| , we have |E [ δY T DR k ] | < |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 2 : If the target critic values and the true value Qπ have the following relationship : E [ Qθ′ |E [ Qπ − Qθ′ 1 ] < Qπ < E [ Qθ′ 2 ] , ] | > |E [ Qπ − Qθ′ 1 ] | , 2 is expected to be underestimated and Qθ′ is overestimated . Since |E [ Qπ − Qθ′ 1 ] | > 2 then Qθ′ |E [ Qπ − Qθ′ 1 2 we thus have ] | which implies |E [ Ψk θ′ 1 ] | > |E [ Ψk θ′ 2 ] | , |µ′ 1| > |µ′ 2| . Based on Lemma 1 and Equation ( 7 ) , our TDR will use Qθ′ 2 in the target value : E [ yT DR k ] = E [ rk ] + γE [ Qθ′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] . However for a vanilla double Q network , the target value will use Qθ′ 1 , ] = E [ rk ] + γE [ Qθ′ Based on Equation ( 26 ) , the two estimation errors of the respective target values are : ( sk+1 , πϕ′ ( sk+1 ) ) ] . E [ yDQ k 1 |E [ δY T DR k ] | = |E [ Qπ − yT DR k ] | = | |E [ δY DQ k ] | = |E [ Qπ − yDQ k ] | = | µ′ 2| , γ 1 − γ γ µ′ 1 − γ 1| , Since |µ′ 1| > |µ′ 2| , we have |E [ δY T DR k ] | < |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 3 : If the target critic values and the true value Qπ has the following relationship : ] < Qπ < E [ Qθ′ ] , 2 E [ Qθ′ |E [ Qπ − Qθ′ 1 1 ] | < |E [ Qπ − Qθ′ 2 ] | , 14 ( 29 ) ( 30 ) ( 31 ) ( 32 ) ( 33 ) ( 34 ) ( 35 ) ( 36 ) ( 37 ) ( 38 ) ( 39 ) ( 40 ) ( 41 ) ( 42 ) ( 43 ) is expected to be underestimated and Qθ′ is overestimated . Since |E [ Qπ − Qθ′ 1 ] | < 2 then Qθ′ |E [ Qπ − Qθ′ 1 2 ] | , it implies thus we have |E [ Ψk θ′ 1 ] | < |E [ Ψk θ′ 2 ] | , ( 44 ) 1| < |µ′ Based on Lemma 1 and Equation ( 7 ) , both vanilla double Q network and our TDR will pick Qθ′ the target value : |µ′ 2| . 1 ( 45 ) in E [ yT DR k ] = E [ yDQ k ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] . Then based on Equation ( 26 ) , the two estimation errors of the respective target values are : |E [ δY T DR k ] | = |E [ δY DQ k ] | = |E [ Qπ − yT DR k ] | = | γ 1 − γ µ′ 1| . We thus have |E [ δY T DR k ] | = |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 4 : If the target critic values and the true value Qπ has the following relationship where E [ Qθ′ 2 ] is expected more overestimated ie |E [ Ψk θ′ 1 ] | < |E [ Ψk θ′ 2 ] | that implies Qπ < E [ Qθ′ 1 ] < E [ Qθ′ 2 ] , |µ′ 1| < |µ′ 2| . ( 46 ) ( 47 ) ( 48 ) ( 49 ) ( 50 ) Based on Equation ( 24 ) and ( 7 ) , same with vanilla double Q network , our Twin TD-regularized Critic will pick the target value using Qθ′ which both mitigates the larger overestimation bias as : 1 E [ yT DR k ] = E [ yDQ k ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] , which based on Equation ( 26 ) , the two estimation errors of the target value are |E [ δY T DR k ] | = |E [ δY DQ k ] | = |E [ Qπ − yT DR k ] | = | γ 1 − γ µ′ 1| . We have |E [ δY T DR k ] | = |E [ δY DQ k ] | . ( 51 ) ( 52 ) ( 53 ) 1 1 2 k ] > E [ Qθ′ ] < E [ Qθ′ ] | ≤ |E [ δY DQ Thus identity ( 28 ) holds . Both methods can mitigate the overestimation error . Note , the above cases study the relationship of E [ Qθ′ for E [ Qθ′ ] , |E [ δY T DR Theorem 2 . Let Qπ denote the true Q value following the current target policy π , Qθ1 be the estimated value . We assume that there exists a step random estimation bias ψk that is independent θ1 of ( sk , ak ) with mean E [ ψk ] = µ1 , µ1 < ∞ , for all k. We assume the policy is updated based on θ1 critic Qθ1 using the deterministic policy gradient ( DPG ) as in Equation 4 . Let δϕk denote the change in actor parameter ϕ updates at stage k. Accordingly , we denote this change for TDR as δϕT DR , k vanilla DPG as δϕDP G . We then k have the following , , and true change without any approximation error in Q as δϕtrue ] | still valid . Thus Theorem 1 holds . ] and by applying same procedure k k 2 ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 54 ) Proof . With learning rate α , the true change of the actor parameters in case without any approxima- tion error in Q : E [ δϕtrue k ] = αEs∼pπ ( cid:104 ) ϕj ∇aQπ ( sk , ak ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ( cid:105 ) . ( 55 ) 15 Consider the estimated critic and the true value follow the relationship in Equation 10 . Given the same current policy parameters ϕj , the updated parameters using DPG are : ϕj+1 DP G = ϕj + αEs∼pπ ( cid:104 ) E [ δϕDP G k ] = αEs∼pπ ϕj ( cid:104 ) ϕj ∇a ( Qπ ( sk , ak ) + Ψk θ1 ( cid:105 ) ) ( cid:12 ) ( cid:12 ) a=πϕj ( s ) ∇ϕj πϕj ( s ) , ∇a ( Qπ ( sk , ak ) + Ψk θ1 ) ( cid:12 ) ( cid:12 ) a=πϕj ( s ) ∇ϕj πϕj ( s ) ( cid:105 ) . ( 56 ) With an overestimation bias E [ Ψk θ1 mated actions , and with an underestimation bias E [ Ψk θ1 the underestimated actions . Both result in suboptimal policies . However , by using TD-regularized actor , and given the same current policy parameters ϕj , the actor updates with Equation ( 9 ) are : ] > 0 , the updates encourage more exploration for the overesti- ] < 0 , the updates discourage exploration for ϕj+1 T DR = ϕj + αEs∼pπ ] = αEs∼pπ E [ δϕT DR k ϕj [ ∇a ( Qπ ( sk , ak ) + Ψk θ1 ϕj − ρ ( ∆ ) ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ] , [ ∇a ( Qπ ( sk , ak ) + Ψk θ1 − ρ ( ∆ ) ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ] . Similar to Lemma 1 , E [ Ψk θ1 ] = 1 1−γ µ1 , and from Equations ( 8 ) and ( 9 ) we have : E [ ∆ ] = E [ Qθi+1 = µ1 1 ( sk , ak ) ] − E [ ( rk + γQθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) ) ] by selecting ρ ≤ 1 1−γ , we have the following : ( cid:26 ) 0 ≥ E [ Ψk θ1 0 ≤ E [ Ψk θ1 − ρ∆ ] > E [ Ψk θ1 − ρ∆ ] ≤ E [ Ψk θ1 ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . Therefore by inspecting Equations ( 55 ) , ( 56 ) and ( 57 ) , we have : ( cid:26 ) E [ δϕtrue k E [ δϕtrue k Thus Theorem 2 holds . ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 57 ) ( 58 ) ( 59 ) ( 60 ) Theorem 3 . Suboptimal actor updates negatively affect the critic . Specifically , consider actor up- dates as in Theorem 2 , in the overestimation case , we have : E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πT rue ( sk ) ) ] , ( 61 ) and in the underestimation case , E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 62 ) Proof Following the analysis of the TD3 ( Fujimoto et al. , 2018 ) , consider Equation ( 12 ) in Theorem 2 , we have ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 Underestimate , ] ≥ 0 Overestimate . ( 63 ) In the overestimation case , the approximate value using TDR and vanilla DPG must be E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πtrue ( sk ) ) ] . ( 64 ) Similarly , in the underestimation case , the approximate value using TDR and vanilla DPG must be E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 65 ) Thus Theorem 3 holds . 16 C IMPLEMENTATION DETAILS We use PyTorch for all implementations . All results were obtained using our internal server consist- ing of AMD Ryzen Threadripper 3970X Processor , a desktop with Intel Core i7-9700K processor , and two desktops with Intel Core i9-12900K processor . Training Procedure . An episode is initialized by resetting the environment , and terminated at max step T = 1000 . A trial is a complete training process that contains a series of consecutive episodes . Each trial is run for a maximum of 1 × 106 time steps with evaluations at every 2 × 104 time steps . Each task is reported over 10 trials where the environment and the network were initialized by 10 random seeds , ( 0 − 9 ) in this study . For each training trial , to remove the dependency on the initial parameters of a policy , we use a purely exploratory policy for the first 8000 time steps ( start timesteps ) . Afterwards , we use an off-policy exploration strategy , adding Gaussian noise N ( 0 , 0.1 ) to each action . Evaluation Procedure . Every 1 × 104 time steps training , we have an evaluation section and each evaluation reports the average reward over 5 evaluation episodes , with no exploration noise and with fixed policy weights . The random seeds for evaluation are different from those in training which each trial , evaluations were performed using seeds ( seeds + 100 ) . Network Structure and optimizer . TD3.The actor-critic networks in TD3 are implemented by feedforward neural networks with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observation state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to THE first layer and the output layer of critic has 1 linear unit to produce Q value . Network parameters are updated using Adam optimizer with a learning rate of 10−3 for simple control problems . After each time step k , the networks are trained with a mini-batch of a 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire history of the agent . D4PG . Same with the actor-critic networks in D4PG are implemented by feedforward neural net- works with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observa- tion state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to THE first layer and the output layer of critic has a distribution with hyperparameters for the number of atoms l , and the bounds on the support ( Vmin , Vmax ) . Network parameters are updated using Adam optimizer with a learning rate of 10−3 . After each time step k , the networks are trained with a mini-batch of 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire his- tory of the agent . SAC . The actor-critic networks in SAC are implemented by feedforward neural networks with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observation state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to the first layer and the output layer of critic has 1 linear unit to produce Q value . Network parameters are updated using Adam optimizer with a learning rate of 10−3 for simple control problems . After each time step k , the networks are trained with a mini-batch of a 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire history of the agent . Hyperparameters . To keep comparisons in this work fair , we set all common hyperparameters ( network layers , batch size , learning rate , discount factor , number of agents , etc ) to be the same for comparison within the same methods and different methods . For TD3 , target policy smoothing is implemented by adding ϵ ∼ N ( 0 , 0.2 ) to the actions chosen by the target actor-network , clipped to ( −0.5 , 0.5 ) , delayed policy updates consist of only updating 17 the actor and target critic network every d iterations , with d = 2 . While a larger d would result in a larger benefit with respect to accumulating errors , for fair comparison , the critics are only trained once per time step , and training the actor for too few iterations would cripple learning . Both target networks are updated with τ = 0.005 . The TD3 and TD3+TDR used in this study are based on the paper ( Fujimoto et al. , 2018 ) and the code from the authors ( https : //github.com/sfujim/TD3 ) . Hyperparameter TD3 Start timesteps Evaluation frequency Max timesteps Exploration noise Policy noise Noise clip Policy update frequency Batch size Buffer size γ τ Number of parallel actor LNSS-N Adam Learning rate regularization factor Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) N ( 0 , 0.2 ) ±0.5 2 256 1e6 0.99 0.005 1 100 1e-3 0.7 Table 3 : TD3 + TDR hyper parameters used for DMC benckmark tasks The SAC used in this study is based on paper ( Haarnoja et al. , 2018 ) and the code is from GitHub ( https : ) . and the hyperparameter is from Table 4 . Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) N ( 0 , 0.2 ) ±0.5 2 256 1e6 0.99 0.005 Hyperparameter SAC Start timesteps Evaluation frequency Max timesteps Exploration noise Policy noise Noise clip Policy update frequency Batch size Buffer size γ τ Temperature parameter α 0.2 Number of parallel actor LNSS-N Adam Learning rate 1 100 1e-3 Table 4 : SAC hyper parameters used for the DMC benckmark tasks The D4PG used in this study is based on paper ( Barth-Maron et al. , 2018 ) and the code is modified from TD3 . The hyperparameter is from Table 5 . All Other algorithms are from the same DRL training platform ( Tonic RL ) ( Pardo , 2020 ) with the same evaluation as the above algorithms . Sparse Reward Setup . 1 ) Cheetah Run Sparse : Cheetah needs to run forward as fast as possible . The agent gets a reward only after speed exceeds 2.5 m/s , making the reward sparse . r = 1 . That is , if v > = 2.5 else r = 0 . 18 Hyperparameter D4PG Start timesteps Evaluation frequency Max timesteps Exploration noise Noise clip Batch size Buffer size γ τ Number of parallel actor LNSS-N Adam Learning rate Vmax Vmin l regularization factor Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) ±0.5 256 1e6 0.99 0.005 1 100 1e-3 100 0 51 0.7 Table 5 : D4PG + TDR hyper parameters used for the DMC benckmark tasks 19 D TDR ALGORITHMS DETAILS In this section , we show our TDR-based algorithms . TD3-TDR is shown in Algorithm 1 , SAC- TDR is shown in Algorithm 2 , and D4PG-TDR is shown in Algorithm 3 . We mainly add LNSS reward to the sample collection part . In algorithm update part , we mainly modify the target value selection using Equation 7 for regular DRL and Equation ( 17 ) for distributional DRL . Additionally , if applicable , we modify the actor gradient based on Equation 9 for regular DRL and Equation ( 21 ) for distributional DRL . All codes will be released to GitHub once the paper get accepted . Algorithm 1 TD3-TDR Initialize : 1 ← θ1 , θ′ 2 ← θ2 , ϕ′ ← ϕ , • Critic networks Qθ1 , Qθ2 and actor-network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks θ′ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) 0 , a′ Store the transition ( s′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 66 ) t , st+1 ) from D end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target value yt based on Equation ( 7 ) Update Critics based on Equation 3 if k mod Policy Update frequency then Update ϕ by Equation 9 Update target networks : θ′ ζ ← τ θζ + ( 1 − τ ) θ′ ζ ϕ′ ← τ ϕ + ( 1 − τ ) ϕ′ end if end for 26 . 27. end for 20 Algorithm 2 SAC-TDR Initialize : • Soft value function VΞ , target Soft value function V ′ Ξ , Critic networks Qθ1 , Qθ2 and actor- network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks Ξ′ ← Ξ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) Store the transition ( s′ 0 , a′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 67 ) end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target value yt based on Equation ( 7 ) Update Critics based on Equation 3 t , st+1 ) from D Update Soft value function based on original SAC formualtion Update ϕ by original SAC formulation Update target networks : Ξ′ ← τ Ξ + ( 1 − τ ) Ξ′ end for 24 . 25. end for 21 Algorithm 3 D4PG-TDR Initialize : 1 ← θ1 , θ′ 2 ← θ2 , ϕ′ ← ϕ , • Critic networks Zθ1 , Zθ2 and actor-network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks θ′ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) 0 , a′ Store the transition ( s′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 68 ) end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target distribution based on Equation ( 17 ) t , st+1 ) from D Update Critics based on Equation 18 if k mod Policy Update frequency then Update ϕ by Equation 21 Update target networks : θ′ ζ ← τ θζ + ( 1 − τ ) θ′ ζ ϕ′ ← τ ϕ + ( 1 − τ ) ϕ′ end if end for 26 . 27. end for 22","['n', 'l', 'c', 'r', 'mitigating', 'estimation', 'error', 'twin', 'td', 'regularize', 'actor', 'critic', 'deep', 'rein', 'learn', 'abstract', 'address', 'issue', 'estimation', 'bias', 'deep', 'reinforcement', 'learning', 'introduce', 'solution', 'mechanism', 'include', 'new', 'twin', 'tdregularize', 'actor', 'critic', 'method', 'aim', 'reduce', 'estimation', 'error', 'combine', 'good', 'drl', 'improvement', 'distributional', 'learning', 'long', 'n', 'step', 'surrogate', 'stage', 'reward', 'method', 'show', 'new', 'tdrbased', 'actorcritic', 'learning', 'enable', 'drl', 'method', 'outper', 'form', 'respective', 'baseline', 'challenge', 'environment', 'deepmind', 'control', 'suite', 'furthermore', 'elevate', 'td3', 'sac', 'respectively', 'level', 'perfor', 'mance', 'comparable', 'current', 'sota', 'also', 'improve', 'performance', 'new', 'sota', 'level', 'measure', 'mean', 'reward', 'conver', 'gence', 'speed', 'learn', 'success', 'rate', 'learn', 'variance', 'introduction', 'reinforcement', 'learning', 'rl', 'develop', 'decade', 'provide', 'mathematical', 'formal', 'ism', 'learningbased', 'control', 'recently', 'significant', 'progress', 'make', 'attain', 'excellent', 'result', 'wide', 'range', 'highdimensional', 'continuous', 'stateaction', 'space', 'problem', 'especially', 'robotic', 'application', 'robot', 'manipulation', 'andrychowicz', 'humanrobotic', 'interaction', 'however', 'fundamental', 'issue', 'estimation', 'error', 'associate', 'actorcritic', 'hasselt', 'still', 'pose', 'great', 'challenge', 'overestimation', 'example', 'use', 'operator', 'update', 'identify', 'study', 'thrun', 'schwartz', 'reduce', 'effort', 'focus', 'attain', 'accurate', 'stable', 'critic', 'network', 'td3', 'fujimoto', 'apply', 'clip', 'double', 'qlearning', 'take', 'minimum', 'estimate', 'sac', 'utilize', 'double', 'q', 'network', 'incorporate', 'entropy', 'regularization', 'critic', 'objective', 'function', 'ensure', 'exploratory', 'behavior', 'help', 'alleviate', 'overestimation', 'problem', 'however', 'directly', 'take', 'minimum', 'value', 'target', 'network', 'td3', 'sac', 'report', 'result', 'underestimation', 'bias', 'fujimoto', 'evaluation', 'reveal', 'multiple', 'role', 'estimation', 'error', 'learn', 'hand', 'overestimation', 'always', 'harmful', 'lan', 'consider', 'play', 'role', 'encourage', 'exploration', 'overestimated', 'action', 'line', 'underestimation', 'bias', 'discourage', 'exploration', 'overestimation', 'bias', 'occur', 'highvalue', 'region', 'contain', 'optimal', 'policy', 'encourage', 'exploration', 'good', 'thing', 'hailu', 'sommer', 'hand', 'overestimation', 'bias', 'also', 'cause', 'agent', 'overly', 'explore', 'lowvalue', 'region', 'lead', 'suboptimal', 'policy', 'accordingly', 'underestimation', 'bias', 'discourage', 'agent', 'explore', 'highvalue', 'region', 'avoid', 'lowvalue', 'region', 'thing', 'consider', 'estimation', 'error', 'leave', 'unchecked', 'accumulate', 'negatively', 'impact', 'policy', 'update', 'suboptimal', 'action', 'highly', 'rate', 'suboptimal', 'critic', 'reinforce', 'suboptimal', 'action', 'next', 'policy', 'update', 'fujimoto', 'aside', 'anecdotal', 'evidence', 'role', 'estimation', 'mitigate', 'principled', 'way', 'remain', 'open', 'issue', 'asuedu', 'several', 'method', 'evaluation', 'perform', 'show', 'promise', 'major', 'tool', 'mostly', 'leave', 'thus', 'far', 'still', 'clear', 'possible', 'far', 'reduce', 'estimation', 'error', 'consider', 'actor', 'give', 'interplay', 'actor', 'critic', 'handful', 'approach', 'examine', 'show', 'demonstrate', 'performance', 'improvement', 'paac', 'use', 'phase', 'actor', 'account', 'q', 'value', 'td', 'error', 'actor', 'update', 'double', 'actor', 'idea', 'propose', 'evaluate', 'take', 'minimum', 'value', 'estimate', 'associate', 'actor', 'network', 'however', 'directly', 'use', 'minimum', 'estimate', 'value', 'show', 'result', 'underestimation', 'error', 'similar', 'td3', 'method', 'fox', 'mutualinformation', 'leibfrie', 'graumoya', 'vieillard', 'rudner', 'regularization', 'also', 'use', 'enhance', 'policy', 'exploration', 'robustness', 'stability', 'tdregularize', 'actorcritic', 'parisi', 'regularize', 'actor', 'aim', 'enhance', 'stability', 'actor', 'learn', 'apply', 'td', 'error', 'online', 'critic', 'update', 'regularization', 'term', 'actor', 'update', 'however', 'none', 'method', 'show', 'regularization', 'actor', 'help', 'reduce', 'estimation', 'error', 'critic', 'paper', 'propose', 'new', 'tdregularize', 'learning', 'mechanism', 'include', 'td', 'regularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'new', 'architecture', 'several', 'property', 'make', 'ideal', 'enhancement', 'consider', 'tdregularize', 'double', 'critic', 'network', 'instead', 'directly', 'select', 'minimum', 'value', 'twin', 'target', 'network', 'select', 'target', 'base', 'minimum', 'td', 'error', 'address', 'overestimation', 'underestimation', 'problem', 'tdregularize', 'actor', 'network', 'formulate', 'new', 'td', 'error', 'regularize', 'actor', 'update', 'avoid', 'misleading', 'critic', 'regularization', 'term', 'help', 'far', 'reduce', 'estimation', 'error', 'critic', 'update', 'additionally', 'apply', 'combine', 'distributional', 'bellemare', 'lns', 'reward', 'estimation', 'far', 'improve', 'learn', 'stability', 'performance', 'relate', 'work', 'shed', 'light', 'novelty', 'method', 'discuss', 'double', 'critic', 'network', 'errorbase', 'actor', 'learn', 'provide', 'backdrop', 'include', 'review', 'distributional', 'bellemare', 'longn', 'step', 'surrogate', 'stage', 'lnss', 'method', 'double', 'critic', 'network', 'use', 'rl', 'hasselt', 'drl', 'fujimoto', 'hasselt', 'double', 'q', 'learn', 'hasselt', 'hasselt', 'first', 'show', 'reduction', 'overestimation', 'bias', 'td3', 'fujimoto', 'sac', 'also', 'show', 'effective', 'apply', 'clip', 'double', 'qlearning', 'use', 'minimum', 'q', 'estimate', 'however', 'method', 'induce', 'underestimation', 'bias', 'problem', 'hasselt', 'fujimoto', 'consequently', 'weight', 'double', 'q', 'learn', 'propose', 'deal', 'overestimation', 'underestimation', 'bias', 'however', 'method', 'test', 'drl', 'context', 'therefore', 'lack', 'systematic', 'approach', 'design', 'weighting', 'function', 'errorbase', 'actor', 'learning', 'expect', 'effective', 'reduce', 'overestimation', 'error', 'consistent', 'estimate', 'advantage', 'function', 'low', 'variance', 'discriminate', 'feedback', 'instead', 'directly', 'use', 'q', 'estimate', 'actorcritic', 'variant', 'crite', 'bhatnagar', 'update', 'actor', 'base', 'sign', 'error', 'positive', 'error', 'prefer', 'policy', 'update', 'however', 'error', 'measure', 'discrepancy', 'predict', 'value', 'target', 'value', 'guide', 'exploration', 'effectively', 'use', 'td', 'error', 'alone', 'actor', 'update', 'discourage', 'exploration', 'cause', 'slow', 'learning', 'especially', 'highdimensional', 'complex', 'problem', 'tdregularize', 'actorcritic', 'parisi', 'enhance', 'stability', 'actor', 'update', 'use', 'error', 'online', 'critic', 'update', 'regularization', 'term', 'however', 'use', 'td', 'error', 'sufficiently', 'evaluate', 'critic', 'update', 'use', 'temporal', 'difference', 'target', 'online', 'q', 'estimate', 'additionally', 'timevarying', 'regularization', 'coefficient', 'show', 'lead', 'poor', 'convergence', 'note', 'also', 'td', 'regularize', 'actorcritic', 'consider', 'tdregularize', 'actor', 'critic', 'contribution', 'introduce', 'novel', 'tdr', 'mechanism', 'tdregularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'extensive', 'experiment', 'use', 'dmc', 'benchmark', 'show', 'enable', 'sota', 'performance', 'measureed', 'learn', 'speed', 'success', 'rate', 'variance', 'converge', 'reward', 'wide', 'variety', 'control', 'task', 'locomotion', 'classical', 'control', 'task', 'sparse', 'reward', 'also', 'provide', 'qualitative', 'analysis', 'show', 'component', 'contribute', 'mitigate', 'estimation', 'error', 'method', 'double', 'q', 'actorcritic', 'method', 'general', 'double', 'q', 'actorcritic', 'method', 'fujimoto', 'policy', 'call', 'actor', 'stateaction', 'value', 'function', 'call', 'critic', 'actor', 'critic', 'estimate', 'deep', 'neural', 'network', 'parameter', 'respectively', 'first', 'consider', 'policy', 'π', 'evaluate', 'stateaction', 'value', 'function', 'e', 'rk', 'tk', 'γt−krt', 'sk', '∼', 'p', 'sk', 'actor', 'critic', 'method', 'base', 'temporal', 'difference', 'learn', 'sutton', 'update', 'q', 'estimate', 'minimize', 'error', 'obtain', 'difference', 'target', 'critic', 'estimate', 'value', 'next', 'consider', 'typical', 'double', 'q', 'method', 'entail', 'network', 'denote', 'qθ1', 'qθ2', 'respective', 'twin', 'target', 'network', 'denote', 'upcoming', 'discussion', 'also', 'use', 'θ', 'denote', 'parameter', 'q', 'network', 'θ', 'target', 'value', 'yk', 'less', 'target', 'value', 'yk', 'rk', 'sk1', 'πϕ′', 'sk1', 'take', 'minimum', 'target', 'value', 'aim', 'curtail', 'overestimation', 'q', 'value', 'frequently', 'experience', 'use', 'single', 'target', 'thus', 'critic', 'value', 'qθ', 'update', 'minimize', 'loss', 'function', 'l', 'θ', 'respect', 'critic', 'weight', 'θ', 'l', 'θ', 'ζ12', 'actor', 'weight', 'update', 'deterministic', 'policy', 'gradient', 'silver', 'convention', 'fujimoto', 'qθ1', 'use', 'update', 'actor', 'weight', 'es∼pπϕ', 'cid104', '∇aqθ1', 'aπϕ', 'figure', 'twin', 'tdregularize', 'actorcritic', 'tdr', 'architecture', 'twin', 'tdregularize', 'actorcritic', 'tdr', 'architecture', 'figure', 'depict', 'tdrbased', 'solution', 'mechanism', 'include', 'twin', 'q', 'network', 'td3', 'fujimoto', 'sac', 'actor', 'network', 'tdrbased', 'actor', 'critic', 'update', 'different', 'currently', 'exist', 'method', 'following', 'show', 'new', 'select', 'target', 'value', 'yk', 'different', 'equation', 'use', 'sac', 'td3', 'help', 'reduce', 'overestimation', 'underestimation', 'error', 'also', 'show', 'new', 'tdregularize', 'actor', 'help', 'far', 'reduce', 'estimation', 'bias', 'critic', 'tdrbased', 'solution', 'figure', 'include', 'additional', 'good', 'improvement', 'distributional', 'learning', 'long', 'n', 'step', 'surrogate', 'stage', 'lnss', 'method', 'describe', 'tdregularize', 'double', 'q', 'network', 'overcome', 'overestimation', 'td3', 'fujimoto', 'sac', 'train', 'critic', 'network', 'minimize', 'loss', 'function', 'equation', 'target', 'value', 'yk', 'equation', 'help', 'reduce', 'overestimation', 'error', 'promote', 'new', 'problem', 'underes', 'timation', 'usually', 'occur', 'early', 'stage', 'learn', 'subject', 'corrupted', 'reward', 'feedback', 'inaccurate', 'state', 'method', 'aim', 'minimize', 'loss', 'function', 'equation', 'different', 'target', 'value', 'yk', 'instead', 'directly', 'choose', 'less', 'target', 'value', 'equation', 'use', 'error', 'target', 'network', 'set', 'target', 'value', 'first', 'error', 'respective', 'target', 'network', 'determine', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'πϕ′', 'sk1', 'target', 'value', 'select', 'following', 'yk', 'cid26', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'πϕ′', 'sk1', 'note', 'equation', 'always', 'use', 'target', 'value', 'associate', 'small', 'target', 'value', 'regardless', 'error', 'sign', 'ultimate', 'objective', 'target', 'network', 'converge', 'choice', 'push', 'critic', 'equation', 'reach', 'target', 'matter', 'estimation', 'error', 'small', 'td', 'value', 'thus', 'naturally', 'position', 'address', 'overesdiation', 'underestimation', 'error', '≤', 'δ′', 'δ′', 'tdregularize', 'actor', 'network', 'tdregularize', 'actor', 'network', 'directly', 'penalize', 'actor', 'learn', 'objective', 'critic', 'estimation', 'error', 'estimation', 'error', 'first', 'critic', 'qθ1', 'choose', 'convention', 'double', 'qbase', 'actorcritic', 'method', 'determine', 'follow', 'qθi1', 'rk', 'γqθi1', 'sk1', 'sk1', 'represent', 'iteration', 'number', 'critic', 'update', 'actor', 'update', 'direction', 'maximize', 'q', 'keep', 'td', 'error', 'small', 'cid20', 'qθi1', 'aπϕ', 'ρ', 'regularization', 'coefficient', 'balance', 'role', 'td', 'error', 'actor', 'learn', 'objective', 'thus', 'expect', 'tdregularize', 'actor', 'help', 'far', 'reduce', 'estimation', 'error', 'critic', 'actor', 'cirtic', 'work', 'together', 'position', 'help', 'avoid', 'bad', 'policy', 'update', 'misleading', 'q', 'value', 'estimate', 'remark', 'key', 'difference', 'tdregularize', 'actor', 'network', 'parisi', 'equation', 'use', 'target', 'critic', 'sk1', 'sk1', 'construct', 'error', 'critic', 'update', 'error', 'evaluate', 'temporal', 'difference', 'target', 'online', 'q', 'estimate', 'accurately', 'evaluate', 'critic', 'estimation', 'construct', 'td', 'error', 'use', 'online', 'critic', 'directly', 'affect', 'actor', 'update', 'error', 'sufficiently', 'evaluate', 'critic', 'update', 'instead', 'equation', 'use', 'update', 'critic', 'θi1', 'construct', 'td', 'error', 'directly', 'measure', 'critic', 'estimation', 'mitigating', 'estimation', 'bias', 'let', 'qπ', 'true', 'q', 'value', 'obtain', 'follow', 'current', 'target', 'policy', 'π', 'let', 'qθ', 'estimate', 'value', 'use', 'neural', 'network', 'let', 'random', 'estimation', 'bias', 'stateaction', 'pair', 'θ', 'hold', 'target', 'network', 'replace', 'θ′', 'equation', 'overestimation', 'problem', 'refer', 'estimation', 'underestimation', 'problem', 'estimation', 'mitigate', 'estimation', 'bias', 'use', 'tdregularize', 'double', 'critic', 'network', 'theorem', 'let', 'qπ', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'target', 'network', 'estimate', 'use', 'double', 'q', 'neural', 'network', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ie', 'estimation', 'bias', 'kth', 'stage', 'independent', 'θ′', 'ζ', 'e', '∞', 'k', 'ζ', 'additionally', 'let', 'δyk', 'denote', 'θ′', 'ζ', 'target', 'value', 'estimation', 'error', 'accordingly', 'denote', 'error', 'follow', 'k', 'e', 'e', 'e', 'e', 'e', 'k', 'proof', 'proof', 'theorem', 'provide', 'b', 'remark', 'select', 'target', 'value', 'less', 'error', 'tdregularize', 'double', 'critic', 'network', 'mitigate', 'overestimation', 'underestimation', 'error', 'however', 'vanilla', 'double', 'q', 'method', 'ally', 'push', 'target', 'low', 'value', 'matter', 'estimation', 'error', 'estimation', 'error', 'detrimental', 'small', 'update', 'presence', 'unchecked', 'underestimation', 'bias', 'raise', 'concern', 'firstly', 'sufficient', 'reward', 'feed', 'back', 'environment', 'noisy', 'reward', 'sparse', 'reward', 'underestimation', 'bias', 'get', 'chance', 'make', 'correction', 'develop', 'significant', 'bias', 'several', 'date', 'secondly', 'inaccurate', 'value', 'estimate', 'lead', 'poor', 'policy', 'update', 'suboptimal', 'action', 'highly', 'rate', 'suboptimal', 'critic', 'reinforce', 'suboptimal', 'action', 'next', 'policy', 'update', 'address', 'misguide', 'critic', 'policy', 'update', 'use', 'tdregularize', 'actor', 'theorem', 'let', 'denote', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ1', 'estimate', 'value', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ψk', 'independent', 'θ1', 'e', 'ψk', '∞', 'k', 'assume', 'policy', 'update', 'base', 'critic', 'qθ1', 'use', 'deterministic', 'policy', 'gradient', 'dpg', 'equation', 'let', 'δϕk', 'denote', 'change', 'actor', 'update', 'stage', 'accordingly', 'denote', 'change', 'true', 'change', 'approximation', 'error', 'q', 'follow', 'vanilla', 'dpg', 'δϕdp', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'δϕdp', 'define', 'equation', 'respectively', 'proof', 'proof', 'theorem', 'provide', 'remark', 'theorem', 'hold', 'regularization', 'factor', 'equation', 'e', 'ψk', 'e', 'use', 'θ1', 'actor', 'always', 'update', 'way', 'use', 'true', 'value', 'realistic', 'follow', 'relationship', 'still', 'preserve', 'e', 'ψk', 'help', 'ease', 'negative', 'effect', 'critic', 'estimation', 'bias', 'imply', 'e', 'δϕtrue', '≤', 'e', 'mitigate', 'critic', 'estimation', 'error', 'tdregularize', 'actor', 'theorem', 'suboptimal', 'actor', 'update', 'negatively', 'affect', 'critic', 'specifically', 'consider', 'actor', 'date', 'theorem', 'overestimation', 'case', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πt', 'underestimation', 'case', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'proof', 'proof', 'theorem', 'provide', 'remark', 'case', 'use', 'tdregularize', 'actor', 'expect', 'result', 'less', 'estimation', 'bias', 'critic', 'experiment', 'result', 'section', 'provide', 'comprehensive', 'evaluation', 'enable', 'actorcritic', 'learning', 'method', 'base', 'commonly', 'use', 'wellbehave', 'baseline', 'algorithm', 'include', 'sac', 'td3', 'additional', 'evaluation', 'also', 'provide', 'popular', 'drl', 'algorithm', 'ddpg', 'ppo', 'provide', 'broad', 'perspective', 'effectiveness', 'tdrbase', 'method', 'evaluation', 'perform', 'base', 'several', 'benchmark', 'deepmind', 'control', 'tassa', 'report', 'evaluation', 'result', 'use', 'follow', 'shortform', 'name', 'base', 'original', 'drl', 'algorithm', 'include', 'sac', 'td3', 'ddpg', 'ppo', 'tdrtd3', 'apply', 'td', 'regularize', 'double', 'critic', 'critic', 'network', 'td', 'regularize', 'actor', 'actor', 'network', 'regularization', 'factor', 'lns', 'n', 'tdrsac', 'apply', 'regularize', 'double', 'critic', 'critic', 'network', 'lns', 'n', 'dtdr', 'apply', 'td', 'regularize', 'double', 'critic', 'critic', 'network', 'td', 'regularize', 'actor', 'actor', 'network', 'regularization', 'factor', 'lns', 'n', 'evaluation', 'aim', 'quantitatively', 'address', 'follow', 'question', 'q1', 'improve', 'base', 'common', 'method', 'q2', 'performance', 'method', 'compare', 'sota', 'algorithm', 'method', 'robust', 'enough', 'handle', 'dense', 'stochastic', 'reward', 'sparse', 'reward', 'component', 'tdrbased', 'learning', 'mechanism', 'affect', 'performance', 'td', 'regularize', 'actor', 'make', 'policy', 'update', 'situation', 'misguide', 'critic', 'q6', 'regularization', 'coefficient', 'equation', 'affect', 'actor', 'performance', 'detail', 'implementation', 'training', 'evaluation', 'procedure', 'provide', 'c', 'link', 'implementation', 'code', 'also', 'provide', 'main', 'evaluation', 'obtain', 'comprehensive', 'evaluation', 'result', 'summarize', 'table', 'include', 'noise', 'respectively', 'state', 'action', 'reward', 'consider', 'dmc', 'environment', 'order', 'make', 'evaluation', 'realistic', 'run', 'sparse', 'sparsifie', 'reward', 'environment', 'detail', 'environment', 'setup', 'find', 'c', 'table', 'success', 'shorthand', 'learn', 'success', 'rate', 'avg', 'rwd', 'average', 'reward', 'rank', 'percent', 'reward', 'difference', 'evaluate', 'method', 'sota', 'average', 'reward', 'evaluate', 'method', 'positive', 'well', 'note', 'compute', 'success', 'rate', 'trial', 'achieve', 'reward', 'least', 'account', 'successful', 'learning', 'result', 'base', 'last', 'evaluation', 'different', 'random', 'seed', 'compare', 'algorithm', 'good', 'performance', 'boldface', 'average', 'reward', 'avg', 'rwd', 'note', 'implement', 'td', 'actor', 'sac', 'sac', 'already', 'entropyregulated', 'actor', 'q1', 'improve', 'respective', 'base', 'method', 'learn', 'curve', 'benchmark', 'environ', 'ment', 'show', 'figure', 'overall', 'method', 'solid', 'line', 'outperform', 'respective', 'base', 'figure', 'systematic', 'evaluation', 'realize', 'drl', 'algorithm', 'sac', 'td3', 'environment', 'uniform', 'random', 'noise', 'state', 'action', 'reward', 'shaded', 'region', 'represent', 'confidence', 'range', 'evaluation', 'seed', 'number', 'step', 'envirinoment', 'ddpg', 'sac', 'td3', 'tdrsac', 'tdrtd3', 'envirinoment', 'ddpg', 'sac', 'td3', 'tdrsac', 'tdrtd3', 'success', 'success', 'finger', 'turn', 'hard', 'avg', 'rwd', 'rwd', 'rank', 'rank', 'quadrupe', 'walk', 'success', 'avg', 'rwd', 'cartpole', 'sparse', 'rank', 'success', 'avg', 'rwd', 'rank', 'success', 'fish', 'swim', 'avg', 'rwd', 'run', 'sparse', 'success', 'avg', 'rwd', 'rank', 'rank', 'table', 'systematic', 'evaluation', 'respectively', 'augment', 'base', 'algorithm', 'rank', 'percent', 'reward', 'difference', 'sota', 'positive', 'well', 'method', 'td3', 'sac', 'dash', 'line', 'term', 'episode', 'reward', 'learn', 'speed', 'learn', 'variance', 'success', 'rate', 'table', 'measure', 'avg', 'rwd', 'method', 'form', 'respective', 'baseline', 'algorithm', 'notice', 'table', 'learn', 'success', 'rate', 'method', 'significant', 'improvement', 'base', 'method', 'comparison', 'ddpg', 'sac', 'td3', 'base', 'method', 'struggle', 'cartpole', 'sparse', 'run', 'sparse', 'moreover', 'method', 'also', 'outperform', 'ddpg', 'ppo', 'term', 'average', 'reward', 'awgrwd', 'learn', 'speed', 'learn', 'variance', 'success', 'rate', 'thus', 'help', 'succesfully', 'address', 'random', 'initialization', 'challenge', 'cause', 'random', 'seed', 'tdr', 'bring', 'performance', 'base', 'method', 'close', 'well', 'sota', 'figure', 'accord', 'rank', 'measure', 'table', 'environment', 'quadrupe', 'walk', 'tdrsac', 'tdrtd3', 'help', 'enhance', 'performance', 'respective', 'base', 'method', 'additionally', 'even', 'outperform', 'sota', 'around', 'rank', 'mea', 'sure', 'quadrupe', 'walk', 'even', 'tdrsac', 'tdrtd3', 'outperform', 'still', 'method', 'evaluate', 'provide', 'close', 'performance', 'also', 'worth', 'note', 'bring', 'performance', 'new', 'sota', 'level', 'measure', 'mean', 'reward', 'convergence', 'speed', 'learn', 'success', 'rate', 'robust', 'dense', 'stochastic', 'reward', 'sparse', 'reward', 'figure', 'table', 'method', 'outperform', 'respective', 'baseline', 'dense', 'stochastic', 'sparse', 'reward', 'term', 'average', 'reward', 'learn', 'variance', 'success', 'rate', 'converge', 'speed', 'particular', 'baseline', 'algorithm', 'td3', 'sac', 'struggle', 'reward', 'benchmark', 'environment', 'cartpole', 'sparse', 'run', 'sparse', 'however', 'use', 'learn', 'also', 'achieve', 'sota', 'performance', 'method', 'td3td', 'critic', 'actor', 'sactd', 'critic', 'actor', 'rwd', 'enhancement', 'avg', 'rwd', 'enhancement', 'cartpole', 'sparse', 'avg', 'rwd', 'enhancement', 'table', 'systematic', 'evaluation', 'component', 'tdr', 'compare', 'respective', 'base', 'gorithm', 'enhancement', 'percent', 'reward', 'difference', 'respective', 'base', 'algorithm', 'large', 'well', 'note', 'actor', 'consider', 'sac', 'sac', 'already', 'entropyregularize', 'actor', 'figure', 'evaluation', 'td', 'actor', 'different', 'ρ', 'ρ', 'equation', 'base', 'drl', 'algorithm', 'td3', 'environment', 'uniform', 'random', 'noise', 'state', 'action', 'reward', 'shaded', 'region', 'represent', 'confidence', 'range', 'evaluation', 'seed', 'number', 'step', 'ablation', 'study', 'perform', 'ablation', 'study', 'examine', 'remove', 'follow', 'compo', 'nent', 'respective', 'shortform', 'description', 'critic', 'td', 'regularize', 'double', 'q', 'network', 'actor', 'td', 'regularize', 'actor', 'network', 'lnss', 'method', 'n', 'table', 'enhancement', 'percent', 'reward', 'difference', 'evaluate', 'method', 'base', 'method', 'large', 'well', 'td', 'critic', 'actor', 'effectively', 'improve', 'base', 'algorithm', 'table', 'critic', 'lnss', 'actor', 'effectively', 'improve', 'base', 'algorithm', 'table', 'critic', 'lns', 'provide', 'comparable', 'significant', 'enhancement', 'base', 'algorithm', 'td', 'critic', 'method', 'outperform', 'respective', 'base', 'algorithm', 'suggest', 'mitigate', 'estimation', 'error', 'vanilla', 'double', 'q', 'network', 'effective', 'way', 'improve', 'performance', 'also', 'show', 'theoretical', 'analysis', 'theorem', 'method', 'help', 'improve', 'learn', 'performance', 'reduce', 'variance', 'value', 'estimation', 'noisy', 'reward', 'show', 'theoretically', 'empirically', 'include', 'lns', 'tdr', 'robust', 'noisy', 'sparse', 'reward', 'actor', 'element', 'also', 'help', 'make', 'appreciable', 'improvement', 'learn', 'performance', 'show', 'table', 'importantly', 'td', 'actor', 'play', 'importantly', 'role', 'stabilize', 'policy', 'update', 'show', 'theoretically', 'also', 'address', 'estimation', 'error', 'critic', 'show', 'theoretically', 'theorem', 'hyper', 'parameter', 'study', 'hyperparameter', 'study', 'result', 'summarize', 'figure', 'drl', 'method', 'td3', 'actor', 'evaluate', 'different', 'regularization', 'factor', 'ρ', 'report', 'average', 'performance', 'average', 'approximate', 'timation', 'error', 'difference', 'true', 'accumulate', 'reward', 'critic', 'value', 'γtrt', 'eval0', 'q5', 'td', 'regularize', 'actor', 'help', 'reduce', 'estimation', 'error', 'critic', 'figure', 'td', 'regularize', 'actor', 'actor', 'estimation', 'error', 'critic', 'duce', 'example', 'finger', 'turn', 'hard', 'actor', 'result', 'less', 'overestimation', 'error', 'compare', 'ρ', 'later', 'stage', 'training', 'td3', 'actor', 'less', 'underestimation', 'error', 'compare', 'ρ', 'similarly', 'cartpole', 'sparse', 'actor', 'result', 'less', 'overestimation', 'error', 'compare', 'ρ', 'policy', 'evaluate', 'epois', 'reward', 'high', 'epois', 'reward', 'generally', 'result', 'well', 'policy', 'figure', 'policy', 'update', 'improve', 'select', 'suitable', 'regularization', 'fac', 'tor', 'ρ', 'especially', 'cartpole', 'td3', 'actor', 'enable', 'successful', 'learning', 'base', 'method', 'struggle', 'stick', 'learning', 'entire', 'training', 'period', 'q6', 'range', 'ρ', 'ρ', 'generally', 'good', 'choice', 'figure', 'small', 'regular', 'ization', 'factor', 'result', 'less', 'regularization', 'provide', 'sufficient', 'estimation', 'error', 'reduction', 'critic', 'large', 'regularization', 'factor', 'result', 'regularization', 'negative', 'effect', 'learn', 'therefore', 'ρ', 'good', 'choice', 'therefore', 'work', 'consistently', 'use', 'ρ', 'obtain', 'result', 'conclusion', 'discussion', 'limitation', 'study', 'work', 'introduce', 'novel', 'tdr', 'mechanism', 'tdregularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'component', 'show', 'help', 'mitigate', 'estimation', 'error', 'show', 'consistently', 'outperform', 'respective', 'base', 'algorithm', 'solve', 'benchmark', 'task', 'term', 'average', 'reward', 'learn', 'success', 'rate', 'learn', 'speed', 'time', 'learn', 'variance', 'analytical', 'result', 'also', 'show', 'component', 'mitigate', 'estimation', 'error', 'show', 'figure', 'environment', 'quadrupe', 'walk', 'evaluate', 'tdr', 'combine', 'distributional', 'lnss', 'element', 'significantly', 'elevate', 'current', 'sota', 'performance', 'new', 'level', 'increase', 'least', 'even', 'identify', 'range', 'generally', 'good', 'regularization', 'coefficient', 'value', 'figure', 'show', 'different', 'algorithm', 'different', 'environment', 'respond', 'somewhat', 'differently', 'ρ', 'therefore', 'effectively', 'determine', 'regularization', 'factor', 'improvement', 'remain', 'question', 'thus', 'limitation', 'study', 'additionally', 'promising', 'performance', 'come', 'extensive', 'training', 'million', 'learn', 'step', 'perform', 'limited', 'training', 'time', 'training', 'step', 'need', 'far', 'investigate', 'reference', 'schneider', 'welinder', 'pieter', 'abbeel', 'wojciech', 'hindsight', 'experience', 'replay', 'arxiv', 'preprint', 'horgan', 'distribute', 'distributional', 'deterministic', 'policy', 'gradient', 'arxiv', 'preprint', 'dabney', 'muno', 'distributional', 'perspective', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'incremental', 'nat', 'ural', 'actorcritic', 'algorithm', 'advance', 'neural', 'information', 'processing', 'system', 'dynamic', 'multiobjective', 'optimization', 'change', 'number', 'objective', 'ieee', 'transaction', 'evolutionary', 'computation', 'actorcritic', 'equivalent', 'qlearne', 'advance', 'neural', 'information', 'processing', 'system', 'dabney', 'georg', 'ostrovski', 'muno', 'implicit', 'quantile', 'network', 'distributional', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', '2018a', 'dabney', 'munos', 'distributional', 'reinforcement', 'learning', 'quantile', 'regression', 'proceeding', 'conference', 'artificial', 'intelli', 'gence', 'volume', 'sun', 'distributional', 'soft', 'actorcritic', 'offpolicy', 'reinforcement', 'learning', 'address', 'value', 'estimation', 'error', 'ieee', 'transaction', 'neural', 'network', 'learn', 'system', 'roy', 'pakman', 'tame', 'noise', 'reinforcement', 'learning', 'soft', 'update', 'arxiv', 'preprint', 'arxiv151208562', 'herke', 'hoof', 'address', 'function', 'approximation', 'error', 'actor', 'critic', 'method', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sehoon', 'vikash', 'pieter', 'abbeel', 'et', 'soft', 'actorcritic', 'algorithm', 'appli', 'cation', 'arxiv', 'preprint', 'arxiv181205905', 'sommer', 'amount', 'quality', 'bias', 'reinforcement', 'learning', 'conference', 'proceeding', 'ieee', 'international', 'conference', 'system', 'man', 'cybernetic', 'cat', 'volume', 'pp', 'ieee', 'double', 'qlearning', 'advance', 'neural', 'information', 'processing', 'system', 'deep', 'reinforcement', 'learning', 'matter', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'qingfeng', 'white', 'qlearning', 'control', 'estimation', 'bias', 'qlearne', 'arxiv', 'preprint', 'leibfrie', 'mutualinformation', 'regularization', 'decision', 'pro', 'cesse', 'actorcritic', 'learning', 'conference', 'robot', 'learn', 'pp', 'pmlr', 'infer', 'humanrobot', 'performance', 'objective', 'locomotion', 'use', 'inverse', 'reinforcement', 'learning', 'inverse', 'optimal', 'control', 'ieee', 'robotic', 'automation', 'letter', 'efficient', 'continuous', 'control', 'double', 'actor', 'regularized', 'critic', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'pp', 'pardo', 'deep', 'reinforcement', 'learning', 'library', 'fast', 'prototyping', 'benchmarke', 'arxiv', 'preprint', 'simone', 'parisi', 'voot', 'tdregularize', 'actor', 'critic', 'method', 'machine', 'learn', 'cong', 'osborne', 'yarin', 'gal', 'teh', 'pathology', 'regularize', 'reinforcement', 'learning', 'expert', 'demonstration', 'advance', 'neural', 'information', 'processing', 'system', '3428376–28389', 'guy', 'riedmiller', 'deterministic', 'policy', 'gradient', 'algorithm', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'reinforcement', 'learn', 'introduction', 'mit', 'press', 'yuval', 'tassa', 'deepmind', 'control', 'suite', 'arxiv', 'preprint', 'sebastian', 'thrun', 'anton', 'schwartz', 'issue', 'use', 'function', 'approximation', 'reinforcement', 'learning', 'proceeding', 'fourth', 'connectionist', 'model', 'summer', 'school', 'volume', 'pp', 'hillsdale', 'nj', 'deep', 'reinforcement', 'learning', 'double', 'q', 'learning', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'scherrer', 'olivi', 'pietquin', 'muno', 'geist', 'leverage', 'average', 'analysis', 'regularization', 'reinforcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'harsh', 'gupta', 'r', 'srikant', 'meansquared', 'error', 'double', 'qlearning', 'advance', 'neural', 'information', 'processing', 'system', 'ruofan', 'brent', 'humanrobotic', 'prosthesis', 'collaborate', 'agent', 'symmetrical', 'walking', 'advance', 'neural', 'information', 'processing', 'system', 'ruofan', 'phase', 'actor', 'actorcritic', 'reinforcement', 'learning', 'weight', 'double', 'qlearning', 'pp', 'ruofan', 'long', 'nstep', 'surrogate', 'stage', 'reward', 'reduce', 'variance', 'deep', 'reinforcement', 'learning', 'complex', 'problem', 'arxiv', 'preprint', 'arxiv221004820', 'distributional', 'tdr', 'lnss', 'distributional', 'rl', 'bellemare', 'represent', 'value', 'function', 'term', 'probability', 'distribution', 'rather', 'function', 'estimate', 'distribution', 'provide', 'comprehensive', 'rep', 'resentation', 'uncertainty', 'associate', 'range', 'different', 'possible', 'reward', 'return', 'state', 'action', 'pair', 'provide', 'informative', 'value', 'function', 'estimation', 'many', 'distributional', 'rl', 'algorithm', 'bellemare', 'dabney', 'achieve', 'great', 'performance', 'provement', 'many', 'discrete', 'problem', 'benchmark', 'barthmaron', 'apply', 'distributional', 'rl', 'continuous', 'control', 'problem', 'combine', 'distributional', 'return', 'function', 'actorcritic', 'framework', 'address', 'overestimation', 'error', 'apply', 'distributional', 'rl', 'piggyback', 'sac', 'provide', 'accurate', 'critic', 'overestimation', 'actor', 'still', 'exist', 'actor', 'still', 'update', 'maximize', 'expectation', 'value', 'function', 'distribution', 'regulate', 'actor', 'distributional', 'rl', 'solve', 'overestimation', 'barely', 'discuss', 'a1', 'distributional', 'tdregularize', 'actorcritic', 'dtdr', 'tailor', 'distributional', 'method', 'base', 'original', 'distributional', 'conceptu', 'alization', 'develop', 'barthmaron', 'bellemare', 'show', 'number', 'enhancement', 'meantime', 'distributional', 'critic', 'distributional', 'critic', 'bellemare', 'treat', 'return', 'tion', 'random', 'variable', 'z', 'expectation', 'use', 'q', 'value', 'estimate', 'namely', 'q', 'e', 'z', 'however', 'use', 'td', 'error', 'evaluate', 'distributional', 'critic', 'similar', 'equation', 'distributional', 'error', 'target', 'network', 'write', 'sk1', 'πϕ′', 'sk1', 'e', 'rk', 'γe', 'zθ′', 'd′', 'rk', 'γe', 'zθ′', 'd′', 'sk1', 'πϕ′', 'sk1', 'e', 'twin', 'tdregularize', 'target', 'distributional', 'operator', 'thus', 'define', 'cid26', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'sk1', 'd′', '≤', 'd′', 'd′', 'b', 'denote', 'random', 'variable', 'follow', 'probability', 'law', 'distributional', 'bellman', 'operator', 'appear', 'similar', 'equation', 'map', 'stateaction', 'pair', 'distribution', 'need', 'define', 'new', 'td', 'error', 'measure', 'distribution', 'barthmaron', 'consider', 'use', 'follow', 'distributional', 'loss', 'l', 'θ', 'zθζ', 'l', 'measure', 'distance', 'distribution', 'many', 'distributional', 'rl', 'algorithm', 'use', 'kullbackleibler', 'divergence', 'distance', 'metric', 'barthmaron', 'adopt', 'metric', 'distributional', 'actor', 'distributional', 'method', 'barthmaron', 'bellemare', 'policy', 'update', 'perform', 'base', 'policy', 'gradient', 'e', 'aπϕ', 'dtdr', 'need', 'use', 'critic', 'evaluation', 'metric', 'evaluate', 'quality', 'current', 'distribu', 'tional', 'critic', 'regularize', 'distributional', 'actor', 'first', 'formulate', 'follow', 'loss', 'metric', 'e', 'l', 'rk', 'γzθi1', 'sk1', 'sk1', 'zθi1', 'similar', 'tdregularize', 'actor', 'network', 'distributional', 'actor', 'update', 'direction', 'imize', 'expect', 'critic', 'keep', 'expect', 'distance', 'project', 'critic', 'critic', 'namely', 'e', '∇aρlz', 'aπϕ', 'ρ', 'regularization', 'coefficient', 'a2', 'long', 'nstep', 'surrogate', 'stage', 'lnss', 'reward', 'lns', 'utilize', 'long', 'reward', 'trajectory', 'n', 'future', 'step', 'estimation', 'stage', 'reward', 'rk', 'use', 'lnssresulte', 'reward', 'place', 'original', 'rk', 'show', 'effectively', 'reduce', 'learn', 'variance', 'significant', 'performance', 'improvement', 'offpolicy', 'method', 'give', 'reward', 'trajectory', 'n', 'step', 'time', 'step', 'k', 'let', 'g', '−1', '−1', '∈', 'r', 'shorthand', 'notation', 'denote', 'discount', 'n', 'step', 'return', '−1', 'γt−krt', 'tk', 'stage', 'reward', 'n', 'reward', 'place', 'rk', 'equation', 'determine', 'r′', 'step', 'reward', 'sequence', 'namely', 'surrogate', 'stage', 'treat', 'weighted', 'average', 'k', 'γt−krt', '−1', 'figure', 'show', 'r′', 'discuss', 'obtain', 'simply', 'use', 'place', 'rk', 'form', 'new', 'tuple', 'k', 'sk1', 'store', 'memory', 'buffer', 'method', 'proceed', 'b', 'estimation', 'analysis', 'lemma', 'let', 'qπ', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ′', 'target', 'network', 'estimate', 'use', 'double', 'q', 'neural', 'network', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ie', 'estimation', 'bias', 'kth', 'stage', 'independent', 'θ′', 'mean', 'e', 'respectively', 'define', 'θ′', 'ζ', 'equation', 'ζ', '∞', 'k', 'ζ', 'δ′', 'δ′', 'e', 'e', '−µ′', 'proof', 'step', 'random', 'estimation', 'bias', 'θ′', 'ζ', 'rewrite', 'expectation', 'ψk', 'θ′', 'ζ', 'e', 'ψk1', 'θ′', 'ζ', '∞', 'tk1', 'ψt', 'θ′', 'ζ', 'expectation', 'target', 'write', 'e', 'e', 'rk', 'sk1', 'ψk1', '∞', 'θ′', 'e', 'rk', 'e', 'γt−k−1rt', 'tk1', 'use', 'equation', 'td', 'error', 'target', 'critic', 'equation', 'respectably', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'e', '−', '−', 'qπ', 'similarly', 'e', 'thus', 'hold', 'place', 'ready', 'analyze', 'estimation', 'error', 'use', 'double', 'q', 'dq', 'method', 'td3', 'fujimoto', 'sac', 'theorem', 'let', 'assumption', 'lemma', 'hold', 'let', 'δyk', 'denote', 'target', 'value', 'estimation', 'error', 'accordingly', 'denote', 'error', 'following', 'e', 'proof', 'proof', 'base', 'enumerate', 'total', 'possible', 'scenario', 'estimation', 'error', 'determine', 'relationship', 'target', 'q', 'value', 'true', 'value', 'provide', 'proof', 'unique', 'scenario', 'first', 'note', 'e', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'underestimated', 'imply', 'e', 'e', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'tdr', 'use', 'target', 'value', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'however', 'vanilla', 'double', 'q', 'network', 'target', 'value', 'e', 'rk', 'γe', 'thus', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'sk1', 'πϕ′', 'sk1', 'e', 'k', 'e', 'e', 'e', 'γ', 'e', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'e', 'expect', 'underestimate', 'overestimate', 'e', 'thus', 'imply', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'tdr', 'use', 'target', 'value', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'however', 'vanilla', 'double', 'q', 'network', 'target', 'value', 'use', 'e', 'rk', 'γe', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'sk1', 'πϕ′', 'sk1', 'e', 'k', 'e', 'e', 'e', 'γ', 'e', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'e', 'expect', 'underestimate', 'overestimate', 'e', 'imply', 'thus', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'vanilla', 'double', 'q', 'network', 'tdr', 'pick', 'qθ′', 'target', 'value', 'e', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'e', 'e', 'γ', 'thus', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'expect', 'overestimated', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'imply', 'e', 'e', 'base', 'equation', 'vanilla', 'double', 'q', 'network', 'twin', 'tdregularize', 'critic', 'pick', 'target', 'value', 'use', 'qθ′', 'mitigate', 'large', 'overestimation', 'bias', 'e', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'base', 'equation', 'estimation', 'error', 'target', 'value', 'e', 'γ', 'e', 'e', 'e', 'thus', 'identity', 'hold', 'method', 'mitigate', 'overestimation', 'error', 'note', 'case', 'study', 'relationship', 'e', 'e', 'e', 'let', 'denote', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ1', 'estimate', 'value', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ψk', 'independent', 'θ1', 'e', 'ψk', '∞', 'k', 'assume', 'policy', 'update', 'base', 'critic', 'qθ1', 'use', 'deterministic', 'policy', 'gradient', 'dpg', 'equation', 'let', 'δϕk', 'denote', 'change', 'actor', 'update', 'stage', 'accordingly', 'denote', 'change', 'vanilla', 'δϕdp', 'follow', 'true', 'change', 'approximation', 'error', 'q', 'δϕtrue', 'still', 'valid', 'thus', 'theorem', 'hold', 'apply', 'procedure', 'k', 'cid26', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'proof', 'learn', 'rate', 'true', 'change', 'actor', 'parameter', 'case', 'approxima', 'tion', 'error', 'q', 'e', 'δϕtrue', 'k', 'αes∼pπ', 'cid104', 'aπϕj', 'πϕj', 'consider', 'estimate', 'critic', 'true', 'value', 'follow', 'relationship', 'equation', 'give', 'current', 'policy', 'parameter', 'ϕj', 'update', 'parameter', 'use', 'dpg', 'ϕj1', 'dp', 'αes∼pπ', 'cid104', 'e', 'δϕdp', 'αes∼pπ', 'aπϕj', 'πϕj', 'qπ', 'aπϕj', 'πϕj', 'overestimation', 'bias', 'e', 'mate', 'action', 'underestimation', 'bias', 'e', 'ψk', 'underestimated', 'action', 'result', 'suboptimal', 'policy', 'however', 'use', 'tdregularize', 'actor', 'give', 'current', 'policy', 'parameter', 'ϕj', 'actor', 'update', 'equation', 'update', 'encourage', 'exploration', 'overesti', 'update', 'discourage', 'exploration', 'αes∼pπ', 'αes∼pπ', 'qπ', 'aπϕj', 'πϕj', 'qπ', 'aπϕj', 'πϕj', 'similar', 'e', 'ψk', 'equation', 'e', 'e', 'rk', 'γqθi1', 'sk1', 'sk1', 'select', '≤', 'follow', 'cid26', 'ψk', 'e', 'ψk', 'e', 'e', 'ψk', 'e', 'ψk', 'e', '≥', 'therefore', 'inspect', 'equation', 'cid26', 'e', 'thus', 'theorem', 'hold', '≥', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'theorem', 'suboptimal', 'actor', 'update', 'negatively', 'affect', 'critic', 'specifically', 'consider', 'actor', 'date', 'theorem', 'overestimation', 'case', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πt', 'underestimation', 'case', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'proof', 'follow', 'analysis', 'td3', 'fujimoto', 'consider', 'equation', 'theorem', 'cid26', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', 'underestimate', '≥', 'overestimate', 'overestimation', 'case', 'approximate', 'value', 'use', 'vanilla', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πtrue', 'similarly', 'underestimation', 'case', 'approximate', 'value', 'use', 'vanilla', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'thus', 'theorem', 'hold', 'c', 'implementation', 'detail', 'use', 'pytorch', 'implementation', 'result', 'obtain', 'use', 'internal', 'server', 'consist', 'ing', 'ryzen', 'threadripper', 'processor', 'desktop', 'core', 'processor', 'desktop', 'core', 'processor', 'training', 'procedure', 'episode', 'initialize', 'reset', 'environment', 'terminate', 'step', 'trial', 'complete', 'training', 'process', 'contain', 'series', 'consecutive', 'episode', 'trial', 'run', 'maximum', 'time', 'step', 'evaluation', '×', 'time', 'step', 'task', 'report', 'trial', 'environment', 'network', 'initialize', 'random', 'seed', 'study', 'training', 'trial', 'remove', 'dependency', 'initial', 'parameter', 'policy', 'use', 'purely', 'exploratory', 'policy', 'first', 'time', 'step', 'start', 'timestep', 'afterwards', 'use', 'offpolicy', 'exploration', 'strategy', 'add', 'gaussian', 'noise', 'action', 'evaluation', 'procedure', '×', 'time', 'step', 'train', 'evaluation', 'section', 'evaluation', 'report', 'average', 'reward', 'evaluation', 'episode', 'exploration', 'noise', 'fix', 'policy', 'weight', 'random', 'seed', 'evaluation', 'different', 'training', 'trial', 'evaluation', 'perform', 'use', 'seed', 'seed', 'network', 'structure', 'optimizer', 'actorcritic', 'network', 'td3', 'implement', 'neural', 'network', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observation', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'linear', 'unit', 'produce', 'q', 'value', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'simple', 'control', 'problem', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'history', 'agent', 'actorcritic', 'network', 'implement', 'neural', 'net', 'work', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observa', 'tion', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'distribution', 'hyperparameter', 'number', 'atom', 'l', 'bound', 'support', 'vmin', 'vmax', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'tory', 'agent', 'sac', 'actorcritic', 'network', 'sac', 'implement', 'neural', 'network', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observation', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'linear', 'unit', 'produce', 'q', 'value', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'simple', 'control', 'problem', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'history', 'agent', 'hyperparameter', 'keep', 'comparison', 'work', 'fair', 'set', 'common', 'hyperparameter', 'network', 'layer', 'batch', 'size', 'learning', 'rate', 'discount', 'factor', 'number', 'agent', 'comparison', 'method', 'different', 'method', 'td3', 'target', 'policy', 'smoothing', 'implement', 'add', 'ϵ', 'n', 'action', 'choose', 'target', 'actornetwork', 'clip', 'delay', 'policy', 'update', 'consist', 'update', 'actor', 'target', 'critic', 'network', 'iteration', 'large', 'result', 'large', 'benefit', 'respect', 'accumulate', 'error', 'fair', 'comparison', 'critic', 'train', 'time', 'step', 'train', 'actor', 'iteration', 'cripple', 'learn', 'target', 'network', 'update', 'td3', 'use', 'study', 'base', 'paper', 'fujimoto', 'code', 'author', 'https', 'githubcomsfujimtd3', 'hyperparameter', 'td3', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'policy', 'noise', 'noise', 'clip', 'policy', 'update', 'frequency', 'batch', 'size', 'buffer', 'size', 'τ', 'number', 'parallel', 'actor', 'lnssn', 'learning', 'rate', 'regularization', 'factor', 'value', 'step', 'step', 'step', 'n', 'n', 'table', 'td3', 'hyper', 'parameter', 'use', 'task', 'sac', 'use', 'study', 'base', 'paper', 'code', 'https', 'hyperparameter', 'table', 'value', 'step', 'step', 'step', 'n', 'n', 'hyperparameter', 'sac', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'policy', 'noise', 'noise', 'clip', 'policy', 'update', 'frequency', 'batch', 'size', 'buffer', 'size', 'temperature', 'parameter', 'number', 'parallel', 'actor', 'lnssn', 'learning', 'rate', 'table', 'sac', 'hyper', 'parameter', 'use', 'task', 'use', 'study', 'base', 'paper', 'barthmaron', 'code', 'modify', 'td3', 'hyperparameter', 'table', 'algorithm', 'drl', 'training', 'platform', 'evaluation', 'algorithm', 'sparse', 'reward', 'setup', 'sparse', 'need', 'run', 'forward', 'fast', 'possible', 'agent', 'get', 'reward', 'speed', 'exceed', 'ms', 'make', 'reward', 'sparse', 'r', 'v', 'else', 'r', 'hyperparameter', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'noise', 'batch', 'size', 'buffer', 'size', 'τ', 'number', 'parallel', 'actor', 'lnssn', 'learn', 'regularization', 'factor', 'value', 'step', 'step', 'step', 'n', 'table', 'hyper', 'parameter', 'use', 'task', 'tdr', 'algorithm', 'detail', 'section', 'show', 'tdrbased', 'algorithm', 'show', 'sac', 'show', 'show', 'mainly', 'add', 'lnss', 'reward', 'sample', 'collection', 'part', 'update', 'part', 'mainly', 'modify', 'target', 'value', 'selection', 'use', 'equation', 'regular', 'drl', 'equation', 'distributional', 'additionally', 'applicable', 'modify', 'actor', 'gradient', 'base', 'equation', 'regular', 'drl', 'equation', 'distributional', 'code', 'release', 'paper', 'accept', 'initialize', 'θ′', 'ϕ′', 'critic', 'network', 'qθ1', 'qθ2', 'actornetwork', 'random', 'parameter', '•', 'target', 'network', '•', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'st1', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'value', 'base', 'equation', 'update', 'critic', 'base', 'equation', 'policy', 'update', 'frequency', 'update', 'equation', 'update', 'target', 'network', 'θ′', 'τ', 'θ′', 'end', 'end', 'end', 'initialize', 'soft', 'value', 'function', 'vξ', 'target', 'soft', 'value', 'function', '′', 'ξ', 'critic', 'network', 'qθ1', 'qθ2', 'actor', 'network', 'πϕ', 'random', 'parameter', '•', 'target', 'network', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'value', 'base', 'equation', 'update', 'critic', 'base', 'equation', 'st1', 'update', 'soft', 'value', 'function', 'base', 'original', 'sac', 'formualtion', 'update', 'original', 'sac', 'formulation', 'update', 'target', 'network', 'τ', 'end', 'end', 'initialize', 'θ′', 'ϕ′', 'critic', 'network', 'actornetwork', 'random', 'parameter', '•', 'target', 'network', '•', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'distribution', 'base', 'equation', 'st1', 'update', 'critic', 'base', 'equation', 'policy', 'update', 'frequency', 'update', 'equation', 'update', 'target', 'network', 'θ′', 'τ', 'θ′', 'end', 'end', 'end']",
Adobe’s DMV3D Achieves SOTA Performance for High-Fidelity 3D Objects Generation Within Seconds,https://syncedreview.com/2023/11/28/adobes-dmv3d-achieves-sota-performance-for-high-fidelity-3d-objects-generation-within-seconds/,2023-11-28,"The recent surge in the popularity of 3D diffusion models is transforming the landscape of 3D asset generation, particularly in applications such as Augmented Reality (AR), Virtual Reality (VR), robotics, and gaming. These models excel in simplifying the complex 3D asset creation process, significantly reducing the manual workload involved. However, a common challenge with these models is the need for access to ground-truth 3D models or point clouds for training, which can be challenging to obtain for real images. Additionally, the latent 3D diffusion approach often results in an intricate and challenging-to-denoise latent space on highly diverse, category-free 3D datasets, posing a hurdle for achieving high-quality rendering. In a new paper DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model, a research team from Adobe Research, Stanford University, HKU, TTIC and HKUST proposes DMV3D, an innovative single-stage category-agnostic diffusion model. This model can generate 3D Neural Radiance Fields (NeRFs) from either text or a single-image input condition through direct model inference, enabling the creation of diverse high-fidelity 3D objects in just 30 seconds per asset. The team summarizes their main contributions as follows: The primary objective of this research is to realize rapid, realistic, and generic 3D generation. The proposed DMV3D integrates 3D NeRF reconstruction and rendering into its denoiser, creating a 2D multi-view image diffusion model trained without direct 3D supervision. This approach avoids the need for separately training 3D NeRF encoders for latent-space diffusion and eliminates the laborious per-asset optimization process. Essentially, DMV3D incorporates a 3D reconstruction model as the 2D multi-view denoiser within a multi-view diffusion framework. The team strategically considers a sparse set of four multi-view images surrounding an object, effectively describing a 3D object without significant self-occlusions. Leveraging large transformer models, the researchers address the challenging task of sparse-view 3D reconstruction. Built upon the recent 3D Large Reconstruction Model (LRM), they introduce a novel model for joint reconstruction and denoising, capable of handling various noise levels in the diffusion process. This model can be seamlessly integrated as the multi-view image denoiser in a multi-view image diffusion framework. The team trained their model on large-scale datasets comprising synthetic renderings from Objaverse and real captures from MVImgNet, using only image-space supervision. DMV3D not only demonstrates the ability for single-stage 3D generation in approximately 30 seconds on a single A100 GPU but also achieves state-of-the-art results in single-image 3D reconstruction, surpassing prior methods based on SDS (Self-Supervised Depth Sensing) and other 3D diffusion models. In summary, this work provides a fresh perspective on addressing 3D generation tasks by bridging the realms of 2D and 3D generative models, unifying 3D reconstruction and generation. The implications extend beyond the immediate applications, opening doors for the development of foundational models to tackle a diverse array of challenges in 3D vision and graphics. The project website is at: https: //justimyhxu.github.io/projects/dmv3d/. The paper DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The recent surge in the popularity of 3D diffusion models is transforming the landscape of 3D asset generation , particularly in applications such as Augmented Reality ( AR ) , Virtual Reality ( VR ) , robotics , and gaming . These models excel in simplifying the complex 3D asset creation process , significantly reducing the manual workload involved . However , a common challenge with these models is the need for access to ground-truth 3D models or point clouds for training , which can be challenging to obtain for real images . Additionally , the latent 3D diffusion approach often results in an intricate and challenging-to-denoise latent space on highly diverse , category-free 3D datasets , posing a hurdle for achieving high-quality rendering . In a new paper DMV3D : Denoising Multi-View Diffusion using 3D Large Reconstruction Model , a research team from Adobe Research , Stanford University , HKU , TTIC and HKUST proposes DMV3D , an innovative single-stage category-agnostic diffusion model . This model can generate 3D Neural Radiance Fields ( NeRFs ) from either text or a single-image input condition through direct model inference , enabling the creation of diverse high-fidelity 3D objects in just 30 seconds per asset . The team summarizes their main contributions as follows : The primary objective of this research is to realize rapid , realistic , and generic 3D generation . The proposed DMV3D integrates 3D NeRF reconstruction and rendering into its denoiser , creating a 2D multi-view image diffusion model trained without direct 3D supervision . This approach avoids the need for separately training 3D NeRF encoders for latent-space diffusion and eliminates the laborious per-asset optimization process . Essentially , DMV3D incorporates a 3D reconstruction model as the 2D multi-view denoiser within a multi-view diffusion framework . The team strategically considers a sparse set of four multi-view images surrounding an object , effectively describing a 3D object without significant self-occlusions . Leveraging large transformer models , the researchers address the challenging task of sparse-view 3D reconstruction . Built upon the recent 3D Large Reconstruction Model ( LRM ) , they introduce a novel model for joint reconstruction and denoising , capable of handling various noise levels in the diffusion process . This model can be seamlessly integrated as the multi-view image denoiser in a multi-view image diffusion framework . The team trained their model on large-scale datasets comprising synthetic renderings from Objaverse and real captures from MVImgNet , using only image-space supervision . DMV3D not only demonstrates the ability for single-stage 3D generation in approximately 30 seconds on a single A100 GPU but also achieves state-of-the-art results in single-image 3D reconstruction , surpassing prior methods based on SDS ( Self-Supervised Depth Sensing ) and other 3D diffusion models . In summary , this work provides a fresh perspective on addressing 3D generation tasks by bridging the realms of 2D and 3D generative models , unifying 3D reconstruction and generation . The implications extend beyond the immediate applications , opening doors for the development of foundational models to tackle a diverse array of challenges in 3D vision and graphics . The project website is at : https : . The paper DMV3D : Denoising Multi-View Diffusion using 3D Large Reconstruction Model on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['recent', 'surge', 'popularity', '3d', 'diffusion', 'model', 'transform', 'landscape', '3d', 'asset', 'generation', 'particularly', 'application', 'augment', 'reality', 'ar', 'virtual', 'reality', 'vr', 'robotic', 'game', 'model', 'excel', 'simplify', 'complex', '3d', 'asset', 'creation', 'process', 'significantly', 'reduce', 'manual', 'workload', 'involve', 'however', 'common', 'challenge', 'model', 'need', 'access', 'groundtruth', '3d', 'model', 'point', 'cloud', 'training', 'challenge', 'obtain', 'real', 'image', 'additionally', 'latent', '3d', 'diffusion', 'approach', 'often', 'result', 'intricate', 'challengingtodenoise', 'latent', 'space', 'highly', 'diverse', 'categoryfree', '3d', 'dataset', 'pose', 'hurdle', 'achieve', 'highquality', 'rendering', 'new', 'paper', 'denoise', 'multiview', 'diffusion', 'use', '3d', 'large', 'reconstruction', 'model', 'research', 'team', 'hku', 'ttic', 'hkust', 'propose', 'innovative', 'singlestage', 'categoryagnostic', 'diffusion', 'model', 'model', 'generate', '3d', 'neural', 'radiance', 'field', 'nerf', 'text', 'singleimage', 'input', 'condition', 'direct', 'model', 'inference', 'enable', 'creation', 'diverse', 'highfidelity', '3d', 'object', 'second', 'asset', 'team', 'summarize', 'main', 'contribution', 'follow', 'primary', 'objective', 'research', 'realize', 'rapid', 'realistic', 'generic', '3d', 'generation', 'propose', 'integrate', '3d', 'nerf', 'reconstruction', 'render', 'denoiser', 'create', 'multiview', 'image', 'diffusion', 'model', 'train', 'direct', '3d', 'supervision', 'approach', 'avoid', 'need', 'separately', 'train', '3d', 'nerf', 'encoder', 'latentspace', 'diffusion', 'eliminate', 'laborious', 'perasset', 'optimization', 'process', 'essentially', 'incorporate', '3d', 'reconstruction', 'model', 'multiview', 'denoiser', 'multiview', 'diffusion', 'framework', 'team', 'strategically', 'consider', 'sparse', 'set', 'multiview', 'image', 'surround', 'object', 'effectively', 'describe', '3d', 'object', 'significant', 'selfocclusion', 'leverage', 'large', 'transformer', 'model', 'researcher', 'address', 'challenging', 'task', 'sparseview', '3d', 'reconstruction', 'build', 'recent', '3d', 'large', 'reconstruction', 'model', 'introduce', 'novel', 'model', 'joint', 'reconstruction', 'denoise', 'capable', 'handle', 'various', 'noise', 'level', 'diffusion', 'process', 'model', 'seamlessly', 'integrate', 'multiview', 'image', 'denoiser', 'multiview', 'image', 'diffusion', 'framework', 'team', 'train', 'model', 'largescale', 'dataset', 'comprise', 'synthetic', 'rendering', 'real', 'capture', 'mvimgnet', 'use', 'imagespace', 'supervision', 'demonstrate', 'ability', 'singlestage', '3d', 'generation', 'approximately', 'second', 'single', 'also', 'achieve', 'stateoftheart', 'result', 'singleimage', '3d', 'reconstruction', 'surpass', 'prior', 'method', 'base', 'sds', 'selfsupervise', 'depth', 'sensing', '3d', 'diffusion', 'model', 'summary', 'work', 'provide', 'fresh', 'perspective', 'address', '3d', 'generation', 'task', 'bridge', 'realm', '2d', '3d', 'generative', 'model', 'unify', '3d', 'reconstruction', 'generation', 'implication', 'extend', 'immediate', 'application', 'opening', 'door', 'development', 'foundational', 'model', 'tackle', 'diverse', 'array', 'challenge', '3d', 'vision', 'graphic', 'project', 'website', 'https', 'paper', 'denoise', 'multiview', 'diffusion', 'use', '3d', 'large', 'reconstruction', 'model', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
A research team innovative single-stage category-agnostic diffusion model. This model can generate 3D Neural Radiance Fields (NeRFs) from either text or a single-image input condition through direct model inference, enabling the creation of diverse high-fidelity 3D objects in just 30s/asset. 

 "
DeepMind’s DiLoCo Revolutionizes Language Model Training with 500× Less Communication,https://syncedreview.com/2023/11/27/deepminds-diloco-revolutionizes-language-model-training-with-500x-less-communication/,2023-11-27,"Language models have demonstrated exceptional performance across various real-world applications. However, at contemporary scales, the conventional training method employing standard backpropagation introduces formidable engineering and infrastructure challenges. The difficulty lies in co-locating and synchronizing a large number of accelerators. In response to this challenge, in a new paper DiLoCo: Distributed Low-Communication Training of Language Models, a Google DeepMind research team presents Distributed Low-Communication (DiLoCo). DiLoCo employs a distributed optimization algorithm that facilitates the training of language models on islands of poorly connected devices, surpassing the performance of fully synchronous models while reducing communication by 500 times. Drawing inspiration from Federated Learning literature, the researchers propose a variant of the widely-used Federated Averaging (FedAvg) algorithm. They introduce a specific instantiation with a momentum-based optimizer, akin to the FedOpt algorithm. Notably, DiLoCo replaces the inner optimizer with AdamW and the outer optimizer with Nesterov Momentum for optimal performance. This innovative combination effectively addresses the challenges posed by conventional training approaches. The DiLoCo approach mitigates the aforementioned challenges through three key elements: a) Limited co-location requirements: While each worker requires co-located devices, their number is smaller than the total, easing the logistical burden. b) Reduced communication frequency: Workers need not communicate at each step, but only every 𝐻 steps, potentially in the order of hundreds or even thousands, significantly lowering communication overhead. c) Device heterogeneity: While devices within an island need to be homogeneous, different islands can operate with different types of devices, enhancing flexibility. In the DiLoCo training process, a pretrained model 𝜃 (0) is replicated 𝑘 times. Each worker independently and in parallel trains a model replica on its own shard of data for 𝐻 steps. Subsequently, workers average their outer gradients, and an outer optimizer updates the global parameter copy 𝜃 (1), which is then redistributed to the workers. This process repeats 𝑇 times. Notably, each replica can be trained in different global locations using various accelerators. On the widely-used C4 dataset, DiLoCo with 8 workers demonstrates performance on par with fully synchronous optimization while reducing communication by 500 times. Furthermore, DiLoCo exhibits remarkable robustness to variations in the data distribution of each worker and adapts seamlessly to changes in resource availability during training. In summary, DiLoCo presents a robust and effective solution for distributing the training of transformer language models when multiple machines are available but poorly connected. This innovative approach not only overcomes infrastructure challenges but also showcases superior performance and adaptability, marking a significant advancement in the field of language model optimization. The paper DiLoCo: Distributed Low-Communication Training of Language Models on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Language models have demonstrated exceptional performance across various real-world applications . However , at contemporary scales , the conventional training method employing standard backpropagation introduces formidable engineering and infrastructure challenges . The difficulty lies in co-locating and synchronizing a large number of accelerators . In response to this challenge , in a new paper DiLoCo : Distributed Low-Communication Training of Language Models , a Google DeepMind research team presents Distributed Low-Communication ( DiLoCo ) . DiLoCo employs a distributed optimization algorithm that facilitates the training of language models on islands of poorly connected devices , surpassing the performance of fully synchronous models while reducing communication by 500 times . Drawing inspiration from Federated Learning literature , the researchers propose a variant of the widely-used Federated Averaging ( FedAvg ) algorithm . They introduce a specific instantiation with a momentum-based optimizer , akin to the FedOpt algorithm . Notably , DiLoCo replaces the inner optimizer with AdamW and the outer optimizer with Nesterov Momentum for optimal performance . This innovative combination effectively addresses the challenges posed by conventional training approaches . The DiLoCo approach mitigates the aforementioned challenges through three key elements : a ) Limited co-location requirements : While each worker requires co-located devices , their number is smaller than the total , easing the logistical burden . b ) Reduced communication frequency : Workers need not communicate at each step , but only every 𝐻 steps , potentially in the order of hundreds or even thousands , significantly lowering communication overhead . c ) Device heterogeneity : While devices within an island need to be homogeneous , different islands can operate with different types of devices , enhancing flexibility . In the DiLoCo training process , a pretrained model 𝜃 ( 0 ) is replicated 𝑘 times . Each worker independently and in parallel trains a model replica on its own shard of data for 𝐻 steps . Subsequently , workers average their outer gradients , and an outer optimizer updates the global parameter copy 𝜃 ( 1 ) , which is then redistributed to the workers . This process repeats 𝑇 times . Notably , each replica can be trained in different global locations using various accelerators . On the widely-used C4 dataset , DiLoCo with 8 workers demonstrates performance on par with fully synchronous optimization while reducing communication by 500 times . Furthermore , DiLoCo exhibits remarkable robustness to variations in the data distribution of each worker and adapts seamlessly to changes in resource availability during training . In summary , DiLoCo presents a robust and effective solution for distributing the training of transformer language models when multiple machines are available but poorly connected . This innovative approach not only overcomes infrastructure challenges but also showcases superior performance and adaptability , marking a significant advancement in the field of language model optimization . The paper DiLoCo : Distributed Low-Communication Training of Language Models on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['language', 'model', 'demonstrate', 'exceptional', 'performance', 'various', 'realworld', 'application', 'however', 'contemporary', 'scale', 'conventional', 'training', 'method', 'employ', 'standard', 'backpropagation', 'introduce', 'formidable', 'engineering', 'infrastructure', 'challenge', 'difficulty', 'lie', 'colocate', 'synchronize', 'large', 'number', 'accelerator', 'response', 'challenge', 'new', 'paper', 'diloco', 'distribute', 'lowcommunication', 'training', 'language', 'model', 'research', 'team', 'present', 'distribute', 'lowcommunication', 'diloco', 'diloco', 'employ', 'distribute', 'optimization', 'facilitate', 'training', 'language', 'model', 'island', 'poorly', 'connect', 'device', 'surpass', 'performance', 'fully', 'synchronous', 'model', 'reduce', 'communication', 'time', 'draw', 'inspiration', 'federated', 'learn', 'literature', 'researcher', 'propose', 'variant', 'widelyused', 'federate', 'average', 'introduce', 'specific', 'instantiation', 'momentumbase', 'optimizer', 'akin', 'notably', 'diloco', 'replace', 'inner', 'optimizer', 'adamw', 'outer', 'optimizer', 'nesterov', 'momentum', 'optimal', 'performance', 'innovative', 'combination', 'effectively', 'address', 'challenge', 'pose', 'conventional', 'training', 'approach', 'approach', 'mitigate', 'aforementione', 'challenge', 'key', 'element', 'limited', 'colocation', 'requirement', 'worker', 'require', 'colocate', 'device', 'number', 'small', 'total', 'ease', 'logistical', 'burden', 'b', 'reduce', 'communication', 'frequency', 'worker', 'communicate', 'step', 'step', 'potentially', 'order', 'hundred', 'even', 'thousand', 'significantly', 'lower', 'communication', 'overhead', 'c', 'device', 'heterogeneity', 'device', 'island', 'need', 'homogeneous', 'different', 'island', 'operate', 'different', 'type', 'device', 'enhance', 'flexibility', 'diloco', 'training', 'process', 'pretraine', 'model', 'replicate', 'time', 'worker', 'independently', 'parallel', 'train', 'model', 'replica', 'shard', 'datum', 'step', 'subsequently', 'worker', 'average', 'outer', 'gradient', 'outer', 'optimizer', 'update', 'global', 'parameter', 'redistribute', 'worker', 'process', 'repeat', 'notably', 'replica', 'train', 'different', 'global', 'location', 'use', 'various', 'accelerator', 'widelyused', 'c4', 'dataset', 'diloco', 'worker', 'demonstrate', 'performance', 'par', 'fully', 'synchronous', 'optimization', 'reduce', 'communication', 'time', 'furthermore', 'diloco', 'exhibit', 'remarkable', 'robustness', 'variation', 'data', 'distribution', 'worker', 'adapt', 'seamlessly', 'change', 'resource', 'availability', 'training', 'summary', 'diloco', 'present', 'robust', 'effective', 'solution', 'distribute', 'training', 'transformer', 'language', 'model', 'multiple', 'machine', 'available', 'poorly', 'connect', 'innovative', 'approach', 'overcome', 'infrastructure', 'challenge', 'also', 'showcase', 'superior', 'performance', 'adaptability', 'mark', 'significant', 'advancement', 'field', 'language', 'model', 'optimization', 'paper', 'diloco', 'distribute', 'lowcommunication', 'training', 'language', 'model', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper DiLoCo: Distributed Low-Communication Training of Language Models, a Google DeepMind research team presents Distributed Low-Communication (DiLoCo). DiLoCo employs a distributed optimization algorithm that facilitates the training of language models on islands of poorly connected devices, surpassing the performance of fully synchronous models while reducing communication by 500 times.
"
Meet LEO: An Embodied Generalist Agent Excelling in 3D World Tasks,https://syncedreview.com/2023/11/26/meet-leo-an-embodied-generalist-agent-excelling-in-3d-world-tasks/,2023-11-26,"The quest to develop a single, versatile model capable of performing diverse tasks akin to human abilities has been a longstanding pursuit in the realms of artificial intelligence and neuroscience. Recent strides in the realm of large language models (LLMs) have presented a promising avenue for creating such generalist models. Leveraging expansive datasets and scalable Transformer architectures, these models have shown immense potential. However, a significant challenge persists: the limited capacity of these models to comprehend and engage with the three-dimensional environment that encompasses humans and other intelligent entities. This constraint acts as a bottleneck, impeding the successful execution of real-world tasks and the achievement of true general intelligence. In a new paper An Embodied Generalist Agent in 3D World, a research team from Beijing Institute for General Artificial Intelligence (BIGAI), Peking University, Carnegie Mellon University and Tsinghua University introduce LEO, which stands as an embodied multi-modal and multi-task generalist agent that excels in essential capabilities such as perception, grounding, reasoning, planning, and action within the intricate 3D world. The team summarizes their main contributions as follows: LEO undergoes training in two stages, utilizing shared LLM-based model architectures, objectives, and weights: (i) 3D vision-language alignment and (ii) 3D vision-language-action instruction tuning. LEO’s perceptual abilities stem from an egocentric 2D image encoder for embodied views and an object-centric 3D point cloud encoder for a global, third-person perspective. The output tokens from the 3D encoder, representing observed entities, are interleaved with text tokens to form a scene-grounded instructional task sequence. This sequence serves as input to a decoder-only LLM, framing all tasks as sequence prediction problems. Autoregressive training objectives allow LEO to be trained with task-agnostic inputs and outputs. The team conducts a comprehensive empirical study, quantitatively evaluating and ablating LEO on diverse 3D tasks. Tasks include object-level and scene-level captioning, 3D question answering, and robotic manipulation. Results indicate that LEO achieves state-of-the-art performance on most tasks. Task-agnostic instruction tuning, enabled by a unified model, surpasses previous task-specific models across various domains. Moreover, pretraining of 3D vision-language alignment significantly enhances the performance of VLA instruction-tuning. The study also highlights the positive impact of scaling up training data on the generalist agent’s performance. In conclusion, LEO stands as a pioneering embodiment of a generalist agent, showcasing remarkable capabilities in navigating and interacting within the 3D world. The insights and methodologies introduced by the research team open new avenues for the development of artificial intelligence with enhanced perceptual and action-oriented competencies. The paper An Embodied Generalist Agent in 3D World on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The quest to develop a single , versatile model capable of performing diverse tasks akin to human abilities has been a longstanding pursuit in the realms of artificial intelligence and neuroscience . Recent strides in the realm of large language models ( LLMs ) have presented a promising avenue for creating such generalist models . Leveraging expansive datasets and scalable Transformer architectures , these models have shown immense potential . However , a significant challenge persists : the limited capacity of these models to comprehend and engage with the three-dimensional environment that encompasses humans and other intelligent entities . This constraint acts as a bottleneck , impeding the successful execution of real-world tasks and the achievement of true general intelligence . In a new paper An Embodied Generalist Agent in 3D World , a research team from Beijing Institute for General Artificial Intelligence ( BIGAI ) , Peking University , Carnegie Mellon University and Tsinghua University introduce LEO , which stands as an embodied multi-modal and multi-task generalist agent that excels in essential capabilities such as perception , grounding , reasoning , planning , and action within the intricate 3D world . The team summarizes their main contributions as follows : LEO undergoes training in two stages , utilizing shared LLM-based model architectures , objectives , and weights : ( i ) 3D vision-language alignment and ( ii ) 3D vision-language-action instruction tuning . LEO ’ s perceptual abilities stem from an egocentric 2D image encoder for embodied views and an object-centric 3D point cloud encoder for a global , third-person perspective . The output tokens from the 3D encoder , representing observed entities , are interleaved with text tokens to form a scene-grounded instructional task sequence . This sequence serves as input to a decoder-only LLM , framing all tasks as sequence prediction problems . Autoregressive training objectives allow LEO to be trained with task-agnostic inputs and outputs . The team conducts a comprehensive empirical study , quantitatively evaluating and ablating LEO on diverse 3D tasks . Tasks include object-level and scene-level captioning , 3D question answering , and robotic manipulation . Results indicate that LEO achieves state-of-the-art performance on most tasks . Task-agnostic instruction tuning , enabled by a unified model , surpasses previous task-specific models across various domains . Moreover , pretraining of 3D vision-language alignment significantly enhances the performance of VLA instruction-tuning . The study also highlights the positive impact of scaling up training data on the generalist agent ’ s performance . In conclusion , LEO stands as a pioneering embodiment of a generalist agent , showcasing remarkable capabilities in navigating and interacting within the 3D world . The insights and methodologies introduced by the research team open new avenues for the development of artificial intelligence with enhanced perceptual and action-oriented competencies . The paper An Embodied Generalist Agent in 3D World on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['quest', 'develop', 'single', 'versatile', 'model', 'capable', 'perform', 'diverse', 'task', 'akin', 'human', 'ability', 'longstanding', 'pursuit', 'realm', 'artificial', 'intelligence', 'neuroscience', 'recent', 'stride', 'realm', 'large', 'language', 'model', 'llm', 'present', 'promising', 'avenue', 'create', 'generalist', 'model', 'leverage', 'expansive', 'dataset', 'scalable', 'transformer', 'architecture', 'model', 'show', 'immense', 'potential', 'however', 'significant', 'challenge', 'persist', 'limited', 'capacity', 'model', 'comprehend', 'engage', 'threedimensional', 'environment', 'encompass', 'human', 'intelligent', 'entity', 'constraint', 'act', 'bottleneck', 'impede', 'successful', 'execution', 'realworld', 'task', 'achievement', 'true', 'general', 'intelligence', 'new', 'paper', 'embody', 'generalist', 'agent', '3d', 'world', 'research', 'team', 'general', 'artificial', 'intelligence', 'bigai', 'peke', 'university', 'carnegie', 'introduce', 'stand', 'embody', 'multimodal', 'multitask', 'generalist', 'agent', 'excel', 'essential', 'capability', 'perception', 'ground', 'reasoning', 'planning', 'action', 'intricate', '3d', 'world', 'team', 'summarize', 'main', 'contribution', 'follow', 'training', 'stage', 'utilize', 'share', 'llmbased', 'model', 'architecture', 'objective', 'weight', '3d', 'visionlanguage', 'alignment', '3d', 'visionlanguageaction', 'instruction', 'tune', 'perceptual', 'ability', 'stem', 'egocentric', 'image', 'encoder', 'embody', 'view', 'objectcentric', '3d', 'point', 'cloud', 'encoder', 'global', 'perspective', 'output', 'token', '3d', 'encoder', 'represent', 'observed', 'entity', 'interleave', 'text', 'token', 'form', 'scenegrounde', 'instructional', 'task', 'sequence', 'sequence', 'serve', 'input', 'decoderonly', 'llm', 'frame', 'task', 'sequence', 'prediction', 'problem', 'autoregressive', 'training', 'objective', 'allow', 'train', 'taskagnostic', 'input', 'output', 'team', 'conduct', 'comprehensive', 'empirical', 'study', 'quantitatively', 'evaluate', 'ablate', 'diverse', '3d', 'task', 'task', 'include', 'objectlevel', 'scenelevel', 'captioning', '3d', 'question', 'answer', 'robotic', 'manipulation', 'result', 'indicate', 'achieve', 'stateoftheart', 'performance', 'task', 'taskagnostic', 'instruction', 'tuning', 'enable', 'unified', 'model', 'surpass', 'previous', 'taskspecific', 'model', 'various', 'domain', 'moreover', 'pretraining', '3d', 'visionlanguage', 'alignment', 'significantly', 'enhance', 'performance', 'vla', 'instructiontune', 'study', 'also', 'highlight', 'positive', 'impact', 'scale', 'training', 'datum', 'generalist', 'agent', 'performance', 'conclusion', 'stand', 'pioneer', 'embodiment', 'generalist', 'agent', 'showcase', 'remarkable', 'capability', 'navigate', 'interact', '3d', 'world', 'insight', 'methodology', 'introduce', 'research', 'team', 'open', 'new', 'avenue', 'development', 'artificial', 'intelligence', 'enhanced', 'perceptual', 'actionoriented', 'competency', 'paper', 'embody', 'generalist', 'agent', '3d', 'world', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper An Embodied Generalist Agent in 3D World, a research team introduces LEO, which stands as an embodied multi-modal and multi-task generalist agent that excels in essential capabilities such as perception, grounding, reasoning, planning, and action within the intricate 3D world.
"
ETH Zurich’s UltraFastBERT Realizes 78x Speedup for Language Models,https://syncedreview.com/2023/11/24/eth-zurichs-ultrafastbert-realizes-78x-speedup-for-language-models/,2023-11-24,"The ever-expanding scale of language models, now boasting tens of billions of parameters, has undeniably enhanced performance across diverse tasks. However, the consequential surge in computation costs poses a significant hurdle for real-world applications. In a bid to overcome this challenge, researchers are diligently working to enhance the efficiency of Large Language Models (LLMs). Recent studies have spotlighted a crucial observation: the majority of parameters in these expansive language models reside within their feedforward layers. Intriguingly, not every neuron in these layers needs to be active during inference, presenting an opportunity to optimize their computation efficiency. In a new paper Exponentially Faster Language Modelling, an ETH Zurich research team introduces UltraFastBERT, a variant of the BERT architecture. UltraFastBERT takes a revolutionary approach by replacing feedforward layers with fast feedforward networks, resulting in an impressive 78x speedup over the optimized baseline feedforward implementation. The main contributions of the research team can be summarized as follows: UltraFastBERT’s architecture draws inspiration from crammedBERT but distinguishes itself by replacing intermediate layer feedforward networks with fast feedforward networks. The simplifying changes applied to these networks include uniform activation functions, output weights for all nodes, removing output biases, fixing leaf size to 1, and allowing multiple FFF trees in parallel. During the training phase, the team follows the final training procedure of crammedBERT, while in the evaluation phase, they fine-tune UltraFastBERT models for various tasks in the GLUE benchmark. Remarkably, the results demonstrate that UltraFastBERT variants trained for just one day on a single A6000 GPU retain at least 96.0% of the GLUE downstream predictive performance of the original BERTbase model. UltraFastBERT-1×11-long even performs on par with the original BERT-base model while utilizing a mere 0.3% of its neurons. In addition to these achievements, the team provides high-level CPU code showcasing a 78x speedup over the optimized baseline feedforward implementation, along with a PyTorch implementation delivering a 40x speedup over the equivalent batched feedforward inference. In conclusion, this work not only verifies the impressive efficiency of UltraFastBERT but also aims to inspire the integration of primitives for conditional neural execution into device programming interfaces. The hope is that these efforts will pave the way for more streamlined and efficient language models in the future. The paper Exponentially Faster Language Modelling on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The ever-expanding scale of language models , now boasting tens of billions of parameters , has undeniably enhanced performance across diverse tasks . However , the consequential surge in computation costs poses a significant hurdle for real-world applications . In a bid to overcome this challenge , researchers are diligently working to enhance the efficiency of Large Language Models ( LLMs ) . Recent studies have spotlighted a crucial observation : the majority of parameters in these expansive language models reside within their feedforward layers . Intriguingly , not every neuron in these layers needs to be active during inference , presenting an opportunity to optimize their computation efficiency . In a new paper Exponentially Faster Language Modelling , an ETH Zurich research team introduces UltraFastBERT , a variant of the BERT architecture . UltraFastBERT takes a revolutionary approach by replacing feedforward layers with fast feedforward networks , resulting in an impressive 78x speedup over the optimized baseline feedforward implementation . The main contributions of the research team can be summarized as follows : UltraFastBERT ’ s architecture draws inspiration from crammedBERT but distinguishes itself by replacing intermediate layer feedforward networks with fast feedforward networks . The simplifying changes applied to these networks include uniform activation functions , output weights for all nodes , removing output biases , fixing leaf size to 1 , and allowing multiple FFF trees in parallel . During the training phase , the team follows the final training procedure of crammedBERT , while in the evaluation phase , they fine-tune UltraFastBERT models for various tasks in the GLUE benchmark . Remarkably , the results demonstrate that UltraFastBERT variants trained for just one day on a single A6000 GPU retain at least 96.0 % of the GLUE downstream predictive performance of the original BERTbase model . UltraFastBERT-1×11-long even performs on par with the original BERT-base model while utilizing a mere 0.3 % of its neurons . In addition to these achievements , the team provides high-level CPU code showcasing a 78x speedup over the optimized baseline feedforward implementation , along with a PyTorch implementation delivering a 40x speedup over the equivalent batched feedforward inference . In conclusion , this work not only verifies the impressive efficiency of UltraFastBERT but also aims to inspire the integration of primitives for conditional neural execution into device programming interfaces . The hope is that these efforts will pave the way for more streamlined and efficient language models in the future . The paper Exponentially Faster Language Modelling on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['everexpande', 'scale', 'language', 'model', 'boast', 'ten', 'billion', 'parameter', 'undeniably', 'enhance', 'performance', 'diverse', 'task', 'however', 'consequential', 'surge', 'computation', 'cost', 'pose', 'significant', 'hurdle', 'realworld', 'application', 'bid', 'overcome', 'challenge', 'researcher', 'diligently', 'work', 'enhance', 'efficiency', 'large', 'language', 'model', 'llm', 'recent', 'study', 'spotlight', 'crucial', 'observation', 'majority', 'parameter', 'expansive', 'language', 'model', 'reside', 'feedforward', 'layer', 'intriguingly', 'neuron', 'layer', 'need', 'active', 'inference', 'present', 'opportunity', 'optimize', 'computation', 'efficiency', 'new', 'paper', 'exponentially', 'fast', 'language', 'model', 'research', 'team', 'introduce', 'ultrafastbert', 'variant', 'architecture', 'ultrafastbert', 'take', 'revolutionary', 'approach', 'replace', 'feedforward', 'layer', 'fast', 'feedforward', 'network', 'result', 'impressive', '78x', 'speedup', 'optimize', 'baseline', 'feedforward', 'implementation', 'main', 'contribution', 'research', 'team', 'summarize', 'follow', 'architecture', 'draw', 'inspiration', 'crammedbert', 'distinguish', 'replace', 'intermediate', 'layer', 'feedforward', 'network', 'fast', 'feedforward', 'network', 'simplifying', 'change', 'apply', 'network', 'include', 'uniform', 'activation', 'function', 'output', 'weight', 'node', 'remove', 'output', 'bias', 'fix', 'leaf', 'size', 'allow', 'multiple', 'fff', 'tree', 'parallel', 'training', 'phase', 'team', 'follow', 'final', 'training', 'procedure', 'crammedbert', 'evaluation', 'phase', 'finetune', 'ultrafastbert', 'model', 'various', 'task', 'glue', 'benchmark', 'remarkably', 'result', 'demonstrate', 'ultrafastbert', 'variant', 'train', 'day', 'single', 'retain', 'least', 'glue', 'downstream', 'predictive', 'performance', 'original', 'bertbase', 'model', 'even', 'perform', 'par', 'original', 'bertbase', 'model', 'utilize', 'mere', 'neuron', 'addition', 'achievement', 'team', 'provide', 'highlevel', 'cpu', 'code', 'showcase', '78x', 'speedup', 'optimize', 'baseline', 'feedforward', 'implementation', 'pytorch', 'implementation', 'deliver', '40x', 'speedup', 'equivalent', 'batch', 'feedforward', 'inference', 'conclusion', 'work', 'verify', 'impressive', 'efficiency', 'also', 'aim', 'inspire', 'integration', 'primitive', 'conditional', 'neural', 'execution', 'device', 'programming', 'interface', 'hope', 'effort', 'pave', 'way', 'streamlined', 'efficient', 'language', 'model', 'future', 'paper', 'exponentially', 'fast', 'language', 'modelling', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper Exponentially Faster Language Modelling, an ETH Zurich research team introduces UltraFastBERT, a variant of the BERT architecture. UltraFastBERT takes a revolutionary approach by replacing feedforward layers with fast feedforward networks, resulting in an impressive 78x speedup over the optimized baseline feedforward implementation.
"
Microsoft Orca 2’s Triumph: Comparable or Superior Performance to Models 5-10x Its Size in Mastering Reasoning Tasks,https://syncedreview.com/2023/11/22/microsoft-orca-2s-triumph-comparable-or-superior-performance-to-models-5-10x-its-size-in-mastering-reasoning-tasks/,2023-11-22,"The continuous scaling of Large Language Models (LLMs) such as GPT-4 and PaLM-2, with an increasing number of parameters, has revealed unprecedented emergent abilities, most notably the remarkable capacity for zero-shot reasoning. As these models evolve and gain more power, researchers are now delving into the possibility of these models autonomously supervising their behavior or even guiding other AI models. Previous studies have demonstrated that by sampling output from an initial model, student models can be trained to emulate the style of their teachers. However, these student models often fall short in terms of reasoning and comprehension skills compared to their larger foundation models. In June 2023, a Microsoft research team addressed this challenge by introducing Orca, a 13-billion parameter model designed to mimic the reasoning process of large foundation models (LFMs). Orca outperformed conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. Continuing along this trajectory, Microsoft has recently unveiled Orca 2 in a new paper titled “Orca 2: Teaching Small Language Models How to Reason.” The focus of this release is to explore how enhanced training signals can augment the reasoning abilities of smaller language models. Notably, Orca 2 surpasses models of similar size, achieving performance levels comparable to or better than models 5-10 times larger. Orca 2’s objectives are twofold. Firstly, researchers aim to instruct smaller models in utilizing a suite of reasoning techniques, including step-by-step processing and recall-then-generate. Secondly, they aspire to assist these models in determining the most effective reasoning strategy for a given task, enabling optimal performance regardless of their size. Similar to its predecessor, Orca 1, the team leverages more capable LLMs to showcase various reasoning strategies across tasks. However, in Orca 2, these strategies are meticulously tailored to each task, considering the capabilities of the student model. A notable technique employed in Orca 2 is Prompt Erasure, making it a Cautious Reasoner. This technique enables the model not only to execute specific reasoning steps but also to strategize at a higher level in approaching a task. Instead of blindly imitating powerful LLMs, the team treats them as a repository of behaviors from which they judiciously select those best suited for the task at hand. In their empirical study, the researchers comprehensively evaluate Orca 2 on 15 benchmarks, covering approximately 100 tasks and over 36,000 unique prompts. The results show that Orca 2 significantly outperforms models of similar size, even matching or surpassing those 5 to 10 times larger, particularly on tasks requiring advanced reasoning. In conclusion, this work marks a significant step forward, emphasizing the importance of teaching smaller models to reason. The research team believes that advancing the capabilities of smaller models will pave the way for new applications with different deployment scenarios and trade-offs between efficiency and capability. The paper Orca 2: Teaching Small Language Models How to Reason on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The continuous scaling of Large Language Models ( LLMs ) such as GPT-4 and PaLM-2 , with an increasing number of parameters , has revealed unprecedented emergent abilities , most notably the remarkable capacity for zero-shot reasoning . As these models evolve and gain more power , researchers are now delving into the possibility of these models autonomously supervising their behavior or even guiding other AI models . Previous studies have demonstrated that by sampling output from an initial model , student models can be trained to emulate the style of their teachers . However , these student models often fall short in terms of reasoning and comprehension skills compared to their larger foundation models . In June 2023 , a Microsoft research team addressed this challenge by introducing Orca , a 13-billion parameter model designed to mimic the reasoning process of large foundation models ( LFMs ) . Orca outperformed conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval . Continuing along this trajectory , Microsoft has recently unveiled Orca 2 in a new paper titled “ Orca 2 : Teaching Small Language Models How to Reason. ” The focus of this release is to explore how enhanced training signals can augment the reasoning abilities of smaller language models . Notably , Orca 2 surpasses models of similar size , achieving performance levels comparable to or better than models 5-10 times larger . Orca 2 ’ s objectives are twofold . Firstly , researchers aim to instruct smaller models in utilizing a suite of reasoning techniques , including step-by-step processing and recall-then-generate . Secondly , they aspire to assist these models in determining the most effective reasoning strategy for a given task , enabling optimal performance regardless of their size . Similar to its predecessor , Orca 1 , the team leverages more capable LLMs to showcase various reasoning strategies across tasks . However , in Orca 2 , these strategies are meticulously tailored to each task , considering the capabilities of the student model . A notable technique employed in Orca 2 is Prompt Erasure , making it a Cautious Reasoner . This technique enables the model not only to execute specific reasoning steps but also to strategize at a higher level in approaching a task . Instead of blindly imitating powerful LLMs , the team treats them as a repository of behaviors from which they judiciously select those best suited for the task at hand . In their empirical study , the researchers comprehensively evaluate Orca 2 on 15 benchmarks , covering approximately 100 tasks and over 36,000 unique prompts . The results show that Orca 2 significantly outperforms models of similar size , even matching or surpassing those 5 to 10 times larger , particularly on tasks requiring advanced reasoning . In conclusion , this work marks a significant step forward , emphasizing the importance of teaching smaller models to reason . The research team believes that advancing the capabilities of smaller models will pave the way for new applications with different deployment scenarios and trade-offs between efficiency and capability . The paper Orca 2 : Teaching Small Language Models How to Reason on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['continuous', 'scaling', 'large', 'language', 'model', 'llm', 'gpt4', 'palm2', 'increase', 'number', 'parameter', 'reveal', 'unprecedented', 'emergent', 'ability', 'notably', 'remarkable', 'capacity', 'zeroshot', 'reasoning', 'model', 'evolve', 'gain', 'power', 'researcher', 'delve', 'possibility', 'model', 'autonomously', 'supervise', 'behavior', 'even', 'guide', 'model', 'previous', 'study', 'demonstrate', 'sample', 'output', 'initial', 'model', 'student', 'model', 'train', 'emulate', 'style', 'teacher', 'however', 'student', 'model', 'often', 'fall', 'short', 'term', 'reasoning', 'comprehension', 'skill', 'compare', 'large', 'foundation', 'model', 'research', 'team', 'address', 'challenge', 'introduce', 'orca', 'parameter', 'model', 'design', 'mimic', 'reasoning', 'process', 'large', 'foundation', 'model', 'orca', 'outperform', 'conventional', 'instructiontune', 'model', 'benchmark', 'bigbench', 'hard', 'agieval', 'continue', 'trajectory', 'recently', 'unveil', 'orca', 'new', 'paper', 'title', 'orca', 'teach', 'small', 'language', 'model', 'reason', 'focus', 'release', 'explore', 'enhanced', 'training', 'signal', 'augment', 'reasoning', 'ability', 'small', 'language', 'model', 'notably', 'orca', 'surpasse', 'model', 'similar', 'size', 'achieve', 'performance', 'level', 'comparable', 'well', 'model', 'time', 'large', 'orca', 'objective', 'twofold', 'firstly', 'researcher', 'aim', 'instruct', 'small', 'model', 'utilize', 'suite', 'reasoning', 'technique', 'include', 'stepbystep', 'processing', 'recallthengenerate', 'secondly', 'aspire', 'assist', 'model', 'determine', 'effective', 'reasoning', 'strategy', 'give', 'task', 'enable', 'optimal', 'performance', 'regardless', 'size', 'similar', 'predecessor', 'orca', 'team', 'leverage', 'capable', 'llm', 'showcase', 'various', 'reasoning', 'strategy', 'task', 'however', 'orca', 'strategy', 'meticulously', 'tailor', 'task', 'consider', 'capability', 'student', 'model', 'notable', 'technique', 'employ', 'orca', 'prompt', 'erasure', 'make', 'cautious', 'reasoner', 'technique', 'enable', 'model', 'execute', 'specific', 'reasoning', 'step', 'also', 'strategize', 'high', 'level', 'approach', 'task', 'instead', 'blindly', 'imitate', 'powerful', 'llm', 'team', 'treat', 'repository', 'behavior', 'judiciously', 'select', 'well', 'suit', 'task', 'hand', 'empirical', 'study', 'researcher', 'comprehensively', 'evaluate', 'orca', 'benchmark', 'cover', 'approximately', 'task', 'unique', 'prompt', 'result', 'show', 'orca', 'significantly', 'outperform', 'model', 'similar', 'size', 'even', 'match', 'surpass', 'time', 'large', 'particularly', 'task', 'require', 'advanced', 'reasoning', 'conclusion', 'work', 'mark', 'significant', 'step', 'forward', 'emphasize', 'importance', 'teach', 'small', 'model', 'reason', 'research', 'team', 'believe', 'advance', 'capability', 'small', 'model', 'pave', 'way', 'new', 'application', 'different', 'deployment', 'scenario', 'tradeoff', 'efficiency', 'capability', 'paper', 'orca', 'teach', 'small', 'language', 'model', 'reason', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
Microsoft has recently unveiled Orca 2 in a new paper titled “Orca 2: Teaching Small Language Models How to Reason.” to explore how enhanced training signals can augment the reasoning abilities of smaller language models. Notably, Orca 2 surpasses models of similar size, achieving performance levels comparable to or better than models 5-10 times larger.
"
Democratizing Data: How Apple and UW’s Data Filtering Networks Redefine Large-Scale Training Sets,https://syncedreview.com/2023/11/18/democratizing-data-how-apple-and-uws-data-filtering-networks-redefine-large-scale-training-sets/,2023-11-18,"Large training datasets have become integral to the field of machine learning, serving as the bedrock for recent breakthroughs in language modeling and multimodal learning. Despite their pivotal role, these datasets are seldom the focal point of active research, with many large-scale training sets remaining unreleased. This lack of accessibility hinders consistent dataset evaluation and the establishment of reproducible frameworks. In a new paper Data Filtering Networks, a research team from Apple and University of Washington introduces the concept of data filtering networks (DFNs). These neural networks, specifically designed for data filtration, demonstrate the capacity to generate extensive, high-quality pre-training datasets efficiently. Notably, DFNs can be trained from scratch and improved using the same techniques as standard machine learning models. The core focus of this work is on dataset filtering, assuming the existence of a large uncurated dataset. The research team highlights three key contributions: The ultimate goal of this research is to develop efficient functions capable of filtering potentially trillions of examples. The team refers to the dataset constructed by filtering a given pool with a DFN as the induced dataset, and the model trained exclusively on that dataset as the induced model. Employing a CLIP model as a DFN, the team defines its filtering performance based on the induced model’s evaluation on standard benchmarks. To enhance the DFN, the team initiates the process by training a CLIP model on a high-quality dataset. Subsequently, they fine-tune the filtering network on additional datasets, incorporating standard machine learning techniques such as augmentation, distinct initialization, and extended training with a larger batch size for further refinement. In their empirical study, the team demonstrates that the best performing dataset, DFN-5B, empowers the training of state-of-the-art CLIP models within specified compute budgets. Among various improvements, a ViT-H model trained on this dataset achieves an impressive 84.4% zero-shot transfer accuracy on ImageNet. In summary, this innovative research on data filtering networks opens new avenues for the efficient creation of high-quality datasets from public data, contributing significantly to the democratization of large-scale datasets and advancing the capabilities of machine learning models. The paper Data Filtering Networks on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Large training datasets have become integral to the field of machine learning , serving as the bedrock for recent breakthroughs in language modeling and multimodal learning . Despite their pivotal role , these datasets are seldom the focal point of active research , with many large-scale training sets remaining unreleased . This lack of accessibility hinders consistent dataset evaluation and the establishment of reproducible frameworks . In a new paper Data Filtering Networks , a research team from Apple and University of Washington introduces the concept of data filtering networks ( DFNs ) . These neural networks , specifically designed for data filtration , demonstrate the capacity to generate extensive , high-quality pre-training datasets efficiently . Notably , DFNs can be trained from scratch and improved using the same techniques as standard machine learning models . The core focus of this work is on dataset filtering , assuming the existence of a large uncurated dataset . The research team highlights three key contributions : The ultimate goal of this research is to develop efficient functions capable of filtering potentially trillions of examples . The team refers to the dataset constructed by filtering a given pool with a DFN as the induced dataset , and the model trained exclusively on that dataset as the induced model . Employing a CLIP model as a DFN , the team defines its filtering performance based on the induced model ’ s evaluation on standard benchmarks . To enhance the DFN , the team initiates the process by training a CLIP model on a high-quality dataset . Subsequently , they fine-tune the filtering network on additional datasets , incorporating standard machine learning techniques such as augmentation , distinct initialization , and extended training with a larger batch size for further refinement . In their empirical study , the team demonstrates that the best performing dataset , DFN-5B , empowers the training of state-of-the-art CLIP models within specified compute budgets . Among various improvements , a ViT-H model trained on this dataset achieves an impressive 84.4 % zero-shot transfer accuracy on ImageNet . In summary , this innovative research on data filtering networks opens new avenues for the efficient creation of high-quality datasets from public data , contributing significantly to the democratization of large-scale datasets and advancing the capabilities of machine learning models . The paper Data Filtering Networks on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['large', 'training', 'dataset', 'become', 'integral', 'field', 'machine', 'learn', 'serve', 'bedrock', 'recent', 'breakthrough', 'language', 'modeling', 'multimodal', 'learn', 'pivotal', 'role', 'dataset', 'seldom', 'focal', 'point', 'active', 'research', 'many', 'largescale', 'training', 'set', 'remain', 'unrelease', 'lack', 'accessibility', 'hinder', 'consistent', 'dataset', 'evaluation', 'establishment', 'reproducible', 'framework', 'new', 'paper', 'datum', 'filtering', 'network', 'research', 'team', 'apple', 'introduce', 'concept', 'datum', 'filtering', 'network', 'neural', 'network', 'specifically', 'design', 'datum', 'filtration', 'demonstrate', 'capacity', 'generate', 'extensive', 'highquality', 'pretraine', 'dataset', 'efficiently', 'notably', 'train', 'scratch', 'improve', 'use', 'technique', 'standard', 'machine', 'learning', 'model', 'core', 'focus', 'work', 'dataset', 'filtering', 'assume', 'existence', 'large', 'uncurated', 'dataset', 'research', 'team', 'highlight', 'key', 'contribution', 'ultimate', 'goal', 'research', 'develop', 'efficient', 'function', 'capable', 'filter', 'potentially', 'trillion', 'example', 'team', 'refer', 'dataset', 'construct', 'filter', 'give', 'pool', 'dfn', 'induce', 'dataset', 'model', 'train', 'exclusively', 'dataset', 'induce', 'model', 'employ', 'clip', 'model', 'dfn', 'team', 'define', 'filter', 'performance', 'base', 'induce', 'model', 'evaluation', 'standard', 'benchmark', 'enhance', 'dfn', 'team', 'initiate', 'process', 'train', 'clip', 'model', 'highquality', 'dataset', 'subsequently', 'finetune', 'filter', 'network', 'additional', 'dataset', 'incorporate', 'standard', 'machine', 'learn', 'technique', 'augmentation', 'distinct', 'initialization', 'extend', 'training', 'large', 'batch', 'size', 'refinement', 'empirical', 'study', 'team', 'demonstrate', 'well', 'perform', 'dataset', 'empower', 'training', 'stateoftheart', 'clip', 'model', 'specify', 'compute', 'budget', 'various', 'improvement', 'vith', 'model', 'train', 'dataset', 'achieve', 'impressive', 'zeroshot', 'transfer', 'accuracy', 'imagenet', 'summary', 'innovative', 'research', 'datum', 'filtering', 'network', 'open', 'new', 'avenue', 'efficient', 'creation', 'highquality', 'dataset', 'public', 'datum', 'contribute', 'significantly', 'democratization', 'largescale', 'dataset', 'advance', 'capability', 'machine', 'learning', 'model', 'paper', 'datum', 'filtering', 'network', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper Data Filtering Networks, a research team from Apple and University of Washington introduces the concept of data filtering networks (DFNs). These neural networks, specifically designed for data filtration, demonstrate the capacity to generate extensive, high-quality pre-training datasets efficiently. 
"
Adobe & ANU’s LRM Reconstructs Models For Single Image to 3D in 5s,https://syncedreview.com/2023/11/13/adobe-anus-lrm-reconstructs-models-for-single-image-to-3d-in-5s/,2023-11-13,"The concept of instantly generating a 3D representation from a single image of any object is undeniably captivating. This breakthrough promises to significantly advance applications in industrial design, animation, gaming, and the realms of Augmented Reality (AR) and Virtual Reality (VR). Besides, the remarkable achievements in natural language processing and image processing have inspired researchers to delve into the realms of learning a universal 3D foundation for reconstructing objects from single images. In a new paper LRM: Large Reconstruction Model for Single Image to 3D, a research team from Adobe Research and Australian National University introduces an innovative Large Reconstruction Model (LRM). This groundbreaking model has the remarkable ability to predict a 3D model of an object from a single input image in a mere 5 seconds. The LRM approach adopts a robust transformer-based encoder-decoder architecture for acquiring 3D object representations from a single image in a data-driven fashion. The model takes an image as input and regresses a Neural Radiance Field (NeRF) in the form of a triplane representation. To achieve this, LRM employs the pre-trained visual transformer DINO (Caron et al., 2021) as the image encoder to generate image features. Subsequently, it learns an image-to-triplane transformer decoder to project the 2D image features onto the 3D triplane through cross-attention, effectively modeling relationships among the spatially-structured triplane tokens via self-attention. The output tokens from the decoder are then reshaped and upsampled to create the final triplane feature maps. This enables LRM to render images from any viewpoint by decoding the triplane feature of each point. It does so with the aid of an additional shared multi-layer perceptron (MLP) to determine color and density, facilitating volume rendering. What sets LRM apart is its design, which boasts high scalability and efficiency. In addition to employing a fully transformer-based pipeline, the triplane NeRF it employs stands out as a concise and scalable 3D representation. Compared to other alternatives like volumes and point clouds, it is computationally efficient. Furthermore, it offers superior locality with respect to the input image. One of the remarkable aspects of LRM is its training process, which involves minimizing the difference between rendered images and ground truth images at novel perspectives. This is done without the need for excessive 3D-aware regularization or intricate hyper-parameter tuning, making the model exceedingly efficient during training and adaptable to a wide range of multi-view image datasets. Empirical results underscore the remarkable fidelity of LRM when handling various inputs, spanning real-world images, synthetic creations, and rendered images featuring diverse subjects with distinct textures. It stands out as a state-of-the-art solution for single-image-to-3D reconstruction when compared to One-2-3-45. In summary, this groundbreaking work demonstrates the potential of LRM to swiftly predict a 3D model of any object from a single, arbitrary image found in the wild. This development opens up a broad array of real-world applications that can benefit from this rapid and accurate 3D reconstruction capability. Video demos and interactable 3D meshes can be found on this website: https://yiconghong.me/LRM/. The paper LRM: Large Reconstruction Model for Single Image to 3D on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The concept of instantly generating a 3D representation from a single image of any object is undeniably captivating . This breakthrough promises to significantly advance applications in industrial design , animation , gaming , and the realms of Augmented Reality ( AR ) and Virtual Reality ( VR ) . Besides , the remarkable achievements in natural language processing and image processing have inspired researchers to delve into the realms of learning a universal 3D foundation for reconstructing objects from single images . In a new paper LRM : Large Reconstruction Model for Single Image to 3D , a research team from Adobe Research and Australian National University introduces an innovative Large Reconstruction Model ( LRM ) . This groundbreaking model has the remarkable ability to predict a 3D model of an object from a single input image in a mere 5 seconds . The LRM approach adopts a robust transformer-based encoder-decoder architecture for acquiring 3D object representations from a single image in a data-driven fashion . The model takes an image as input and regresses a Neural Radiance Field ( NeRF ) in the form of a triplane representation . To achieve this , LRM employs the pre-trained visual transformer DINO ( Caron et al. , 2021 ) as the image encoder to generate image features . Subsequently , it learns an image-to-triplane transformer decoder to project the 2D image features onto the 3D triplane through cross-attention , effectively modeling relationships among the spatially-structured triplane tokens via self-attention . The output tokens from the decoder are then reshaped and upsampled to create the final triplane feature maps . This enables LRM to render images from any viewpoint by decoding the triplane feature of each point . It does so with the aid of an additional shared multi-layer perceptron ( MLP ) to determine color and density , facilitating volume rendering . What sets LRM apart is its design , which boasts high scalability and efficiency . In addition to employing a fully transformer-based pipeline , the triplane NeRF it employs stands out as a concise and scalable 3D representation . Compared to other alternatives like volumes and point clouds , it is computationally efficient . Furthermore , it offers superior locality with respect to the input image . One of the remarkable aspects of LRM is its training process , which involves minimizing the difference between rendered images and ground truth images at novel perspectives . This is done without the need for excessive 3D-aware regularization or intricate hyper-parameter tuning , making the model exceedingly efficient during training and adaptable to a wide range of multi-view image datasets . Empirical results underscore the remarkable fidelity of LRM when handling various inputs , spanning real-world images , synthetic creations , and rendered images featuring diverse subjects with distinct textures . It stands out as a state-of-the-art solution for single-image-to-3D reconstruction when compared to One-2-3-45 . In summary , this groundbreaking work demonstrates the potential of LRM to swiftly predict a 3D model of any object from a single , arbitrary image found in the wild . This development opens up a broad array of real-world applications that can benefit from this rapid and accurate 3D reconstruction capability . Video demos and interactable 3D meshes can be found on this website : https : //yiconghong.me/LRM/ . The paper LRM : Large Reconstruction Model for Single Image to 3D on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['concept', 'instantly', 'generate', '3d', 'representation', 'single', 'image', 'object', 'undeniably', 'captivate', 'breakthrough', 'promise', 'significantly', 'advance', 'application', 'industrial', 'design', 'animation', 'gaming', 'realm', 'augmented', 'reality', 'ar', 'virtual', 'reality', 'vr', 'remarkable', 'achievement', 'natural', 'language', 'processing', 'image', 'processing', 'inspire', 'researcher', 'delve', 'realm', 'learn', 'universal', '3d', 'foundation', 'reconstruct', 'object', 'single', 'image', 'new', 'paper', 'lrm', 'large', 'reconstruction', 'model', 'single', 'image', '3d', 'research', 'team', 'adobe', 'research', 'australian', 'national', 'introduce', 'innovative', 'large', 'reconstruction', 'model', 'lrm', 'groundbreaking', 'model', 'remarkable', 'ability', 'predict', '3d', 'model', 'object', 'single', 'input', 'image', 'mere', 'second', 'approach', 'adopt', 'robust', 'transformerbase', 'encoderdecod', 'architecture', 'acquire', '3d', 'object', 'representation', 'single', 'image', 'datadriven', 'fashion', 'model', 'take', 'image', 'input', 'regress', 'neural', 'radiance', 'field', 'nerf', 'form', 'triplane', 'representation', 'achieve', 'lrm', 'employ', 'pretraine', 'visual', 'transformer', 'caron', 'image', 'encoder', 'generate', 'image', 'feature', 'subsequently', 'learn', 'imagetotriplane', 'transformer', 'decoder', 'project', 'image', 'feature', '3d', 'triplane', 'crossattention', 'effectively', 'model', 'relationship', 'spatiallystructured', 'triplane', 'token', 'selfattention', 'output', 'token', 'decoder', 'reshape', 'upsample', 'create', 'final', 'triplane', 'feature', 'map', 'enable', 'render', 'image', 'viewpoint', 'decode', 'triplane', 'feature', 'point', 'aid', 'additional', 'share', 'determine', 'color', 'density', 'facilitate', 'volume', 'render', 'set', 'apart', 'design', 'boast', 'high', 'scalability', 'efficiency', 'addition', 'employ', 'fully', 'transformerbase', 'pipeline', 'triplane', 'nerf', 'employ', 'stand', 'concise', 'scalable', '3d', 'representation', 'compare', 'alternative', 'volume', 'point', 'cloud', 'computationally', 'efficient', 'furthermore', 'offer', 'superior', 'locality', 'respect', 'input', 'image', 'remarkable', 'aspect', 'training', 'process', 'involve', 'minimize', 'difference', 'render', 'image', 'ground', 'truth', 'image', 'novel', 'perspective', 'need', 'excessive', 'regularization', 'intricate', 'hyperparameter', 'tune', 'make', 'model', 'exceedingly', 'efficient', 'training', 'adaptable', 'wide', 'range', 'multiview', 'image', 'dataset', 'empirical', 'result', 'underscore', 'remarkable', 'fidelity', 'handle', 'various', 'input', 'span', 'realworld', 'image', 'synthetic', 'creation', 'render', 'image', 'feature', 'diverse', 'subject', 'distinct', 'texture', 'stand', 'stateoftheart', 'solution', 'singleimageto3d', 'reconstruction', 'compare', 'one2345', 'summary', 'groundbreaking', 'work', 'demonstrate', 'potential', 'swiftly', 'predict', '3d', 'model', 'object', 'single', 'arbitrary', 'image', 'find', 'wild', 'development', 'open', 'broad', 'array', 'realworld', 'application', 'benefit', 'rapid', 'accurate', '3d', 'reconstruction', 'capability', 'video', 'demos', 'interactable', '3d', 'mesh', 'find', 'website', 'https', 'yiconghongmelrm', 'paper', 'lrm', 'large', 'reconstruction', 'model', 'single', 'image', '3d', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper LRM: Large Reconstruction Model for Single Image to 3D, a research team from Adobe Research and Australian National Univerisity introduces an innovative Large Reconstruction Model (LRM). This groundbreaking model has the remarkable ability to predict a 3D model of an object from a single input image in a mere 5 seconds.
"
Google’s E3 TTS Provides Effortless Approach to High-Quality Audio Synthesis Through Diffusion Models,https://syncedreview.com/2023/11/06/googles-e3-tts-provides-effortless-approach-to-high-quality-audio-synthesis-through-diffusion-models/,2023-11-06,"Diffusion models have garnered significant recognition for their outstanding performance in a wide range of image and audio generation tasks. Text-to-speech (TTS) systems employing diffusion models have proven their mettle by delivering high-fidelity speech that stands on par with state-of-the-art systems. Nonetheless, many existing TTS systems face a litany of issues, such as heavy reliance on intermediate features’ quality and complex deployment, training, and setup procedures. In a new paper E3 TTS: Easy End-to-End Diffusion-based Text to Speech, a Google research team proposes Easy End-to-End Diffusion-based Text to Speech. This streamlined and efficient text-to-speech model hinges solely on diffusion to preserve temporal structure, allowing it to accept plain text as input and generate audio waveforms directly. The E3 TTS model takes text as input and operates in a non-autoregressive manner, producing waveform outputs without delay. The architecture consists of two primary modules: In a tangible sense, the E3 TTS leverages recent advancements in large language models. It relies on text representations provided by a pretrained BERT model. Unlike some prior approaches, which require representations like phonemes or graphemes, the E3 TTS simplifies the process by depending solely on a pretrained text language model. This model can be trained on multiple languages using only text data, which streamlines the system’s versatility. The U-Net structure encompasses a sequence of downsampling and upsampling blocks linked by residuals. To enhance information extraction from the BERT output, the team incorporates crossattention in the top downsampling/upsampling blocks. In the lower blocks, an adaptive softmax Convolutional Neural Network (CNN) kernel is employed, with its kernel size determined by the timestep and speaker. In other layers, speaker and timestep embeddings are combined through Feature-wise Linear Modulation (FiLM), which includes a composite layer for channel-wise scaling and bias prediction. The downsampler plays a crucial role in refining the noisy information, converting it from 24kHz to a sequence of similar length to the encoded BERT output, which significantly improves the overall quality. On the flip side, the upsampler predicts noise with the same length as the input waveform. Empirical evidence demonstrates that E3 TTS can generate high-fidelity audio, approaching the performance of state-of-the-art neural TTS systems. Furthermore, it enables various zero-shot tasks, such as speech editing and prompt-based generation. In summary, this work underscores the remarkable capabilities of E3 TTS in generating high-quality audio directly from BERT features. It simplifies the design of end-to-end TTS systems and has proven to deliver impressive results in experiments. Audio samples are available at https://e3tts.github.io. The paper E3 TTS: Easy End-to-End Diffusion-based Text to Speech on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Diffusion models have garnered significant recognition for their outstanding performance in a wide range of image and audio generation tasks . Text-to-speech ( TTS ) systems employing diffusion models have proven their mettle by delivering high-fidelity speech that stands on par with state-of-the-art systems . Nonetheless , many existing TTS systems face a litany of issues , such as heavy reliance on intermediate features ’ quality and complex deployment , training , and setup procedures . In a new paper E3 TTS : Easy End-to-End Diffusion-based Text to Speech , a Google research team proposes Easy End-to-End Diffusion-based Text to Speech . This streamlined and efficient text-to-speech model hinges solely on diffusion to preserve temporal structure , allowing it to accept plain text as input and generate audio waveforms directly . The E3 TTS model takes text as input and operates in a non-autoregressive manner , producing waveform outputs without delay . The architecture consists of two primary modules : In a tangible sense , the E3 TTS leverages recent advancements in large language models . It relies on text representations provided by a pretrained BERT model . Unlike some prior approaches , which require representations like phonemes or graphemes , the E3 TTS simplifies the process by depending solely on a pretrained text language model . This model can be trained on multiple languages using only text data , which streamlines the system ’ s versatility . The U-Net structure encompasses a sequence of downsampling and upsampling blocks linked by residuals . To enhance information extraction from the BERT output , the team incorporates crossattention in the top downsampling/upsampling blocks . In the lower blocks , an adaptive softmax Convolutional Neural Network ( CNN ) kernel is employed , with its kernel size determined by the timestep and speaker . In other layers , speaker and timestep embeddings are combined through Feature-wise Linear Modulation ( FiLM ) , which includes a composite layer for channel-wise scaling and bias prediction . The downsampler plays a crucial role in refining the noisy information , converting it from 24kHz to a sequence of similar length to the encoded BERT output , which significantly improves the overall quality . On the flip side , the upsampler predicts noise with the same length as the input waveform . Empirical evidence demonstrates that E3 TTS can generate high-fidelity audio , approaching the performance of state-of-the-art neural TTS systems . Furthermore , it enables various zero-shot tasks , such as speech editing and prompt-based generation . In summary , this work underscores the remarkable capabilities of E3 TTS in generating high-quality audio directly from BERT features . It simplifies the design of end-to-end TTS systems and has proven to deliver impressive results in experiments . Audio samples are available at https : //e3tts.github.io . The paper E3 TTS : Easy End-to-End Diffusion-based Text to Speech on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['diffusion', 'model', 'garner', 'significant', 'recognition', 'outstanding', 'performance', 'wide', 'range', 'image', 'audio', 'generation', 'task', 'system', 'employ', 'diffusion', 'model', 'prove', 'mettle', 'deliver', 'highfidelity', 'speech', 'stand', 'par', 'stateoftheart', 'system', 'nonetheless', 'many', 'exist', 'tts', 'system', 'face', 'litany', 'issue', 'heavy', 'reliance', 'intermediate', 'feature', 'quality', 'complex', 'deployment', 'training', 'setup', 'procedure', 'new', 'paper', 'tts', 'easy', 'endtoend', 'diffusionbase', 'text', 'speech', 'research', 'team', 'propose', 'easy', 'endtoend', 'diffusionbase', 'text', 'speech', 'streamlined', 'efficient', 'texttospeech', 'model', 'hinge', 'solely', 'diffusion', 'preserve', 'temporal', 'structure', 'allow', 'accept', 'plain', 'text', 'input', 'generate', 'audio', 'waveform', 'directly', 'tts', 'model', 'take', 'text', 'input', 'operate', 'nonautoregressive', 'manner', 'produce', 'waveform', 'output', 'delay', 'architecture', 'consist', 'primary', 'module', 'tangible', 'sense', 'tts', 'leverage', 'recent', 'advancement', 'large', 'language', 'model', 'rely', 'text', 'representation', 'provide', 'pretraine', 'model', 'prior', 'approach', 'require', 'representation', 'phoneme', 'grapheme', 'tts', 'simplifie', 'process', 'depend', 'solely', 'pretraine', 'text', 'language', 'model', 'model', 'train', 'multiple', 'language', 'use', 'text', 'datum', 'streamline', 'system', 'versatility', 'unet', 'structure', 'encompass', 'sequence', 'downsampling', 'upsampling', 'block', 'link', 'residual', 'enhance', 'information', 'extraction', 'output', 'team', 'incorporate', 'crossattention', 'top', 'downsamplingupsample', 'block', 'low', 'block', 'adaptive', 'softmax', 'convolutional', 'neural', 'network', 'kernel', 'employ', 'kernel', 'size', 'determine', 'timestep', 'speaker', 'layer', 'speaker', 'timestep', 'embedding', 'combine', 'featurewise', 'linear', 'modulation', 'film', 'include', 'composite', 'layer', 'channelwise', 'scaling', 'bias', 'prediction', 'downsampler', 'play', 'crucial', 'role', 'refine', 'noisy', 'information', 'convert', '24khz', 'sequence', 'similar', 'length', 'encoded', 'bert', 'output', 'significantly', 'improve', 'overall', 'quality', 'flip', 'side', 'upsampler', 'predict', 'noise', 'length', 'input', 'waveform', 'empirical', 'evidence', 'demonstrate', 'tts', 'generate', 'highfidelity', 'audio', 'approach', 'performance', 'stateoftheart', 'neural', 'system', 'furthermore', 'enable', 'various', 'zeroshot', 'task', 'speech', 'editing', 'promptbase', 'generation', 'summary', 'work', 'underscore', 'remarkable', 'capability', 'tt', 'generate', 'highquality', 'audio', 'directly', 'feature', 'simplify', 'design', 'endtoend', 'system', 'prove', 'deliver', 'impressive', 'result', 'experiment', 'audio', 'sample', 'available', 'paper', 'tts', 'easy', 'endtoend', 'diffusionbase', 'text', 'speech', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper E3 TTS: Easy End-to-End Diffusion-based Text to Speech, a Google research team proposes Easy End-to-End Diffusion-based Text to Speech. This streamlined and efficient text-to-speech model hinges solely on diffusion to preserve temporal structure, allowing it to accept plain text as input and generate audio waveforms directly.
"
Apple Repurposes Large Language Models for Reinforcement Learning challenges in Embodied AI,https://syncedreview.com/2023/11/01/apple-repurposes-large-language-models-for-reinforcement-learning-challenges-in-embodied-ai/,2023-11-01,"Large Language Models (LLMs) have ushered in an era of unparalleled language understanding capabilities, raising the possibility of harnessing their prowess for complex embodied visual tasks. This new frontier explores whether these models can be the cornerstone of adaptable, generalizable policies for decision-making that seamlessly transfer to novel scenarios. In a new paper Large Language Models as Generalizable Policies for Embodied Tasks, an Apple research team presents Large LAnguage model Reinforcement Learning Policy (LLaRP). LLaRP effectively repurposes LLMs for Reinforcement Learning (RL) challenges within the realm of Embodied Artificial Intelligence (AI), achieving a remarkable 1.7 times higher success rate compared to other established baselines and zero-shot LLM applications. The LLaRP approach is a pioneering effort in adapting pre-trained LLMs to navigate multi-modal decision-making settings inherent to embodied tasks. The core of the problem is cast as a Partially-Observable Markov Decision Process (POMDP), wherein the policy’s inputs encompass task instructions and egocentric visual RGB frames from the current time step. These inputs are encoded using LLM embeddings or a vision encoder. These embeddings serve as the input to a pre-trained LLM, and the hidden outputs are subsequently projected to action and value predictions. Notably, the entire system learns through online RL, with the action output module and observation encoder MLP being the only trainable components while the others remain frozen. The research team demonstrates that using a pre-trained and frozen LLM as a Vision-Language Model (VLM) policy with learned input and output adapter layers results in a policy showcasing robust generalization capabilities. This policy is trained using online RL, and its generalization is assessed along two axes: Paraphrastic Robustness (PR) and Behavior Generalization (BG). LLaRP undergoes rigorous evaluation across over 1,000 unseen tasks, spanning the axes of PR and BG, and achieves an impressive 42% success rate. This surpasses the performance of alternative LSTM-based policies at 25% and zero-shot LLM applications at 22%. Importantly, LLaRP outperforms all baselines when given novel instructions and when assigned previously unseen tasks. Moreover, the researchers demonstrate that the LLaRP LLM-based policy provides a significant performance boost in a distinct domain, Atari, compared to a Transformer baseline. The research team further uncovers the benefits of infusing LLM-encoded world knowledge into RL. LLM-based models exhibit superior sample efficiency compared to other conventional architectures in both basic Proximal Policy Optimization (PPO) RL and continual learning settings. Furthermore, LLaRP proves to be more efficient in terms of required supervision when contrasted with commonly used imitation learning techniques. In a promising initiative to facilitate further exploration of generalization in Embodied AI, the researchers introduce the Language Rearrangement task. This task involves a staggering 150,000 distinct language instructions, each equipped with automatically generated rewards, providing a valuable framework for ongoing research in the field. In conclusion, this pioneering research paper exemplifies the transformative potential of integrating LLMs into embodied tasks. The LLaRP approach not only excels in achieving high success rates but also significantly enhances efficiency, opening up exciting possibilities for the future of Embodied AI research and development. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io. The paper Large Language Models as Generalizable Policies for Embodied Tasks on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Large Language Models ( LLMs ) have ushered in an era of unparalleled language understanding capabilities , raising the possibility of harnessing their prowess for complex embodied visual tasks . This new frontier explores whether these models can be the cornerstone of adaptable , generalizable policies for decision-making that seamlessly transfer to novel scenarios . In a new paper Large Language Models as Generalizable Policies for Embodied Tasks , an Apple research team presents Large LAnguage model Reinforcement Learning Policy ( LLaRP ) . LLaRP effectively repurposes LLMs for Reinforcement Learning ( RL ) challenges within the realm of Embodied Artificial Intelligence ( AI ) , achieving a remarkable 1.7 times higher success rate compared to other established baselines and zero-shot LLM applications . The LLaRP approach is a pioneering effort in adapting pre-trained LLMs to navigate multi-modal decision-making settings inherent to embodied tasks . The core of the problem is cast as a Partially-Observable Markov Decision Process ( POMDP ) , wherein the policy ’ s inputs encompass task instructions and egocentric visual RGB frames from the current time step . These inputs are encoded using LLM embeddings or a vision encoder . These embeddings serve as the input to a pre-trained LLM , and the hidden outputs are subsequently projected to action and value predictions . Notably , the entire system learns through online RL , with the action output module and observation encoder MLP being the only trainable components while the others remain frozen . The research team demonstrates that using a pre-trained and frozen LLM as a Vision-Language Model ( VLM ) policy with learned input and output adapter layers results in a policy showcasing robust generalization capabilities . This policy is trained using online RL , and its generalization is assessed along two axes : Paraphrastic Robustness ( PR ) and Behavior Generalization ( BG ) . LLaRP undergoes rigorous evaluation across over 1,000 unseen tasks , spanning the axes of PR and BG , and achieves an impressive 42 % success rate . This surpasses the performance of alternative LSTM-based policies at 25 % and zero-shot LLM applications at 22 % . Importantly , LLaRP outperforms all baselines when given novel instructions and when assigned previously unseen tasks . Moreover , the researchers demonstrate that the LLaRP LLM-based policy provides a significant performance boost in a distinct domain , Atari , compared to a Transformer baseline . The research team further uncovers the benefits of infusing LLM-encoded world knowledge into RL . LLM-based models exhibit superior sample efficiency compared to other conventional architectures in both basic Proximal Policy Optimization ( PPO ) RL and continual learning settings . Furthermore , LLaRP proves to be more efficient in terms of required supervision when contrasted with commonly used imitation learning techniques . In a promising initiative to facilitate further exploration of generalization in Embodied AI , the researchers introduce the Language Rearrangement task . This task involves a staggering 150,000 distinct language instructions , each equipped with automatically generated rewards , providing a valuable framework for ongoing research in the field . In conclusion , this pioneering research paper exemplifies the transformative potential of integrating LLMs into embodied tasks . The LLaRP approach not only excels in achieving high success rates but also significantly enhances efficiency , opening up exciting possibilities for the future of Embodied AI research and development . Video examples of LLaRP in unseen Language Rearrangement instructions are at https : //llm-rl.github.io . The paper Large Language Models as Generalizable Policies for Embodied Tasks on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['large', 'language', 'model', 'llm', 'usher', 'era', 'unparalleled', 'language', 'understand', 'capability', 'raise', 'possibility', 'harness', 'prowess', 'complex', 'embody', 'visual', 'task', 'new', 'frontier', 'explore', 'model', 'cornerstone', 'adaptable', 'generalizable', 'policy', 'decisionmake', 'seamlessly', 'transfer', 'novel', 'scenario', 'new', 'paper', 'large', 'language', 'model', 'generalizable', 'policy', 'embody', 'task', 'apple', 'research', 'team', 'present', 'large', 'language', 'model', 'reinforcement', 'learning', 'policy', 'llarp', 'llarp', 'effectively', 'repurpose', 'llm', 'reinforcement', 'learning', 'rl', 'challenge', 'realm', 'embody', 'artificial', 'intelligence', 'achieve', 'remarkable', 'time', 'high', 'success', 'rate', 'compare', 'establish', 'baseline', 'zeroshot', 'llm', 'application', 'llarp', 'approach', 'pioneering', 'effort', 'adapt', 'pretraine', 'llm', 'navigate', 'multimodal', 'decisionmake', 'setting', 'inherent', 'embody', 'task', 'core', 'problem', 'cast', 'partiallyobservable', 'markov', 'decision', 'process', 'pomdp', 'policy', 'input', 'encompass', 'task', 'instruction', 'egocentric', 'visual', 'frame', 'current', 'time', 'step', 'input', 'encode', 'use', 'llm', 'embedding', 'vision', 'encoder', 'embedding', 'serve', 'input', 'pretrained', 'llm', 'hidden', 'output', 'subsequently', 'project', 'action', 'value', 'prediction', 'notably', 'entire', 'system', 'learn', 'rl', 'action', 'output', 'module', 'observation', 'encoder', 'mlp', 'trainable', 'component', 'remain', 'frozen', 'research', 'team', 'demonstrate', 'use', 'pretraine', 'frozen', 'llm', 'visionlanguage', 'model', 'policy', 'learn', 'input', 'output', 'adapter', 'layer', 'result', 'policy', 'showcasing', 'robust', 'generalization', 'capability', 'policy', 'train', 'use', 'online', 'rl', 'generalization', 'assess', 'axis', 'paraphrastic', 'robustness', 'behavior', 'bg', 'rigorous', 'evaluation', 'unseen', 'task', 'span', 'axis', 'achieve', 'impressive', 'success', 'rate', 'surpass', 'performance', 'alternative', 'lstmbased', 'policy', 'zeroshot', 'llm', 'application', 'importantly', 'llarp', 'outperform', 'baseline', 'give', 'novel', 'instruction', 'assign', 'previously', 'unseen', 'task', 'moreover', 'researcher', 'demonstrate', 'llarp', 'llmbase', 'policy', 'provide', 'significant', 'performance', 'boost', 'distinct', 'domain', 'atari', 'compare', 'transformer', 'baseline', 'research', 'team', 'uncover', 'benefit', 'infuse', 'llmencoded', 'world', 'knowledge', 'llmbase', 'model', 'exhibit', 'superior', 'sample', 'efficiency', 'compare', 'conventional', 'architecture', 'basic', 'proximal', 'policy', 'optimization', 'ppo', 'continual', 'learning', 'setting', 'furthermore', 'llarp', 'prove', 'efficient', 'term', 'require', 'supervision', 'contrast', 'commonly', 'use', 'imitation', 'learn', 'technique', 'promising', 'initiative', 'facilitate', 'exploration', 'generalization', 'embody', 'ai', 'researcher', 'introduce', 'language', 'rearrangement', 'task', 'task', 'involve', 'staggering', 'distinct', 'language', 'instruction', 'equip', 'automatically', 'generate', 'reward', 'provide', 'valuable', 'framework', 'ongoing', 'research', 'field', 'conclusion', 'pioneer', 'research', 'paper', 'exemplify', 'transformative', 'potential', 'integrate', 'llm', 'embody', 'task', 'llarp', 'approach', 'excel', 'achieve', 'high', 'success', 'rate', 'also', 'significantly', 'enhance', 'efficiency', 'opening', 'exciting', 'possibility', 'future', 'embody', 'ai', 'research', 'development', 'video', 'example', 'llarp', 'unseen', 'language', 'rearrangement', 'instruction', 'https', 'llmrlgithubio', 'paper', 'large', 'language', 'model', 'generalizable', 'policy', 'embody', 'task', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
An Apple research team presents Large LAnguage model Reinforcement Learning Policy (LLaRP). LLaRP effectively repurposes LLMs for Reinforcement Learning (RL) challenges within the realm of Embodied Artificial Intelligence (AI), achieving a remarkable 1.7 times higher success rate compared to other established baselines and zero-shot LLM applications.
"
